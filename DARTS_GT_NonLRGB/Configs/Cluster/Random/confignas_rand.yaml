benchmark: false
bn:
  eps: 1.0e-05
  mom: 0.1
cfg_dest: config.yaml
custom_metrics: []
dataset:
  is_binary: 'multi_class'
  cache_load: false
  cache_save: false
  dir: ./datasets
  edge_dim: 128
  edge_encoder: true
  edge_encoder_bn: false
  edge_encoder_name: DummyEdge
  edge_encoder_num_types: 0
  edge_message_ratio: 0.8
  edge_negative_sampling_ratio: 1.0
  edge_train_mode: all
  encoder: true
  encoder_bn: true
  encoder_dim: 128
  encoder_name: db
  format: PyG-GNNBenchmarkDataset
  label_column: none
  label_table: none
  location: local
  name: CLUSTER
  node_encoder: true
  node_encoder_bn: false
  node_encoder_name: LapPE
  node_encoder_num_types: 0
  remove_feature: false
  resample_disjoint: false
  resample_negative: false
  shuffle_split: true
  slic_compactness: 10
  split:
  - 0.8
  - 0.1
  - 0.1
  split_dir: ./splits
  split_index: 0
  split_mode: standard
  task: graph
  task_type: classification
  to_undirected: false
  transductive: false
  transform: none
  tu_simple: true
device: auto
example_arg: example
example_group:
  example_arg: example
gnn:
  act: relu
  agg: mean
  att_final_linear: false
  att_final_linear_bn: false
  att_heads: 1
  batchnorm: true
  clear_feature: true
  dim_inner: 48
  dropout: 0.0
  head: inductive_node
  keep_edge: 0.5
  l2norm: true
  layer_type: generalconv
  layers_mp: 2
  layers_post_mp: 3
  layers_pre_mp: 0
  msg_direction: single
  normalize_adj: false
  residual: false
  self_msg: concat
  skip_every: 1
  stage_type: stack
gpu_mem: false
gt:
  routing_mode: 'none'  # Options: 'uniform', 'random', 'nas'
  perturbation_strength: 0.05  # Fixed: removed ##
  # Expert GNN types - SAME as MOE for compatibility
  head_gnn_types: ['GINE','CustomGatedGCN','GATV2']
  attn_gnn_uni: 'none'
  weight_fix: '1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1'
  weight_type: none
  edge_startup: 0.1
  gnns_type_used: 2
  teacher_student_training:
    enabled: false
    teacher_instruct_epochs: 20        # Epochs where the Teacher is frozen
    teacher_lr_multiplier_frozen: 0.0  # LR for kv_model during instruction (0 = frozen)
    teacher_lr_multiplier_finetune: 0.1  # LR for kv_model during co-learning (e.g., 10% of base_lr)
    student_q_lr_multiplier: 5.0       # Fast LR for q_projection during instruction

  
  # NAS configuration for DARTS search (only used in 'nas' mode)
  nas:
    enabled: false
    darts_epochs: 25            # Full DARTS search epochs (only for 'nas' mode)
    darts_split_ratio: 0.6     # 60% for DARTS training, 40% for DARTS validation
    arc_learning_rate: 4.0e-4  # Architecture parameter learning rate
    grad_clip: 5.0             # Gradient clipping for stability
    unrolled: false            # Use first-order approximation
    stabilization_epochs: 5    # Fixed: removed ###
    # DARTS learning rate scheduler parameters
    darts_lr_schedule:
      lr_reduce_factor: 0.5      # Changed from 'factor' to 'lr_reduce_factor'
      lr_schedule_patience: 10   # Changed from 'patience' to 'lr_schedule_patience'
      min_lr: 1.0e-6
      init_lr: 0.0025
      weight_decay: 3e-4
  
  # Uncertainty parameters - SAME as MOE (only last layer, test only)
  # NO variance computation (removed)
  uncertainty:
    enabled: false  # Whether to compute uncertainty metrics
    delta: 0.02    # Perturbation delta for uncertainty
    epsilon: 0.15  # Perturbation epsilon
    max_steps: 10  # Max perturbation steps
    samples: 5     # Number of uncertainty samples

  attn_dropout: 0.5
  batch_norm: true
  bigbird:
    add_cross_attention: false
    attention_type: block_sparse
    block_size: 3
    chunk_size_feed_forward: 0
    hidden_act: relu
    is_decoder: false
    layer_norm_eps: 1.0e-06
    max_position_embeddings: 128
    num_random_blocks: 3
    use_bias: false
  dim_hidden: 48
  dropout: 0.0
  full_graph: false
  gamma: 1.0e-05
  layer_norm: false
  layer_type: None+SparseTransformer
  layers: 16
  n_heads: 8
  pna_degrees: []
  residual: true
infer: {}
mem:
  inplace: false
metric_agg: argmax
metric_best: accuracy-SBM
model:
  edge_decoding: dot
  graph_pooling: add
  loss_fun: weighted_cross_entropy
  match_upper: true
  size_average: mean
  thresh: 0.5
  type: RANDOM_GTModelEDGE
name_tag: RANDOM_GTModelEDGErun
num_threads: 6
num_workers: 0
optim:
  base_lr: 0.001
  batch_accumulation: 1
  clip_grad_norm: true
  lr_decay: 0.1
  max_epoch: 100
  min_lr: 0.0
  momentum: 0.9
  num_warmup_epochs: 5
  optimizer: adamW
  reduce_factor: 0.1
  schedule_patience: 10
  scheduler: cosine_with_warmup
  steps:
  - 30
  - 60
  - 90
  weight_decay: 1.0e-05
out_dir: results/Cluster/Cluster-GINE-47
posenc_ElstaticSE:
  dim_pe: 16
  enable: false
  kernel:
    times: []
    times_func: range(10)
  layers: 3
  model: none
  n_heads: 4
  pass_as_var: false
  post_layers: 0
  raw_norm_type: none
posenc_EquivStableLapPE:
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: false
  raw_norm_type: none
posenc_HKdiagSE:
  dim_pe: 16
  enable: false
  kernel:
    times: []
    times_func: ''
  layers: 3
  model: none
  n_heads: 4
  pass_as_var: false
  post_layers: 0
  raw_norm_type: none
posenc_LapPE:
  dim_pe: 16
  eigen:
    eigvec_norm: L2
    laplacian_norm: none
    max_freqs: 10
  enable: true
  layers: 2
  model: DeepSet
  n_heads: 4
  pass_as_var: false
  post_layers: 0
  raw_norm_type: none
posenc_RWSE:
  dim_pe: 16
  enable: true
  kernel:
    times: [1, 2, 4, 8, 16]
    times_func: ''
  layers: 2
  model: Linear
  n_heads: 4
  pass_as_var: false
  post_layers: 0
  raw_norm_type: BatchNorm
posenc_SignNet:
  dim_pe: 16
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: false
  layers: 3
  model: none
  n_heads: 4
  pass_as_var: false
  phi_hidden_dim: 64
  phi_out_dim: 4
  post_layers: 0
  raw_norm_type: none
pretrained:
  dir: ''
  freeze_main: false
  reset_prediction_head: true
print: both
round: 5

run_multiple_splits: []
seed: 47
share:
  dim_in: 1
  dim_out: 1
  num_splits: 1
tensorboard_agg: true
tensorboard_each_run: false
train:
  auto_resume: false
  batch_size: 16
  ckpt_best: false
  ckpt_clean: true
  ckpt_period: 100
  enable_ckpt: true
  epoch_resume: -1
  eval_period: 1
  iter_per_epoch: 32
  mode: custom

  neighbor_sizes:
  - 20
  - 15
  - 10
  - 5
  node_per_graph: 32
  radius: extend
  sample_node: false
  sampler: full_batch
  skip_train_eval: false
  walk_length: 4
val:
  node_per_graph: 32
  radius: extend
  sample_node: false
  sampler: full_batch
view_emb: false
wandb:
  entity: gtransformers
  name: ''
  project: CLUSTER-fix
  use: false
