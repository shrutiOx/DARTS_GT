Lmod has detected the following error: The following module(s) are unknown:
"cuda/12.0"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore-cache load "cuda/12.0"

Also make sure that all modulefiles written in TCL start with the string
#%Module



              total        used        free      shared  buff/cache   available
Mem:          376Gi        19Gi       295Gi       2.9Gi        61Gi       351Gi
Swap:         1.9Gi       3.0Mi       1.9Gi
Fri Jul 11 06:58:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:1D:00.0 Off |                    0 |
| N/A   39C    P0             68W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Running experiment with seed: 41
Starting training for seed 41...
Changed working directory to: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GATEDGCN
Loading config from: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GATEDGCN/confignas.yaml
Using device: cuda
2025-07-11 06:58:38,046 - INFO - GPU Mem: 34.1GB
2025-07-11 06:58:38,047 - INFO - Run directory: results/Cluster/Cluster-GATEDGCN-41
2025-07-11 06:58:38,047 - INFO - Seed: 41
2025-07-11 06:58:38,047 - INFO - === OPTIMIZED MOE CONFIGURATION ===
2025-07-11 06:58:38,047 - INFO - Routing mode: none
2025-07-11 06:58:38,047 - INFO - Expert types: ['GINE', 'CustomGatedGCN', 'GATV2']
2025-07-11 06:58:38,047 - INFO - Number of layers: 16
2025-07-11 06:58:38,047 - INFO - Uncertainty enabled: False
2025-07-11 06:58:38,047 - INFO - Training mode: custom
2025-07-11 06:58:38,047 - INFO - Optimization: Uncertainty only for last layer + test/val
2025-07-11 06:58:38,047 - INFO - Additional features: Router weights logging + JSON export
2025-07-11 06:58:51,413 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 06:58:51,414 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 06:58:51,415 - INFO -   undirected: True
2025-07-11 06:58:51,415 - INFO -   num graphs: 12000
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 06:58:51,416 - INFO -   avg num_nodes/graph: 117
2025-07-11 06:58:51,416 - INFO -   num node features: 7
2025-07-11 06:58:51,416 - INFO -   num edge features: 0
2025-07-11 06:58:51,417 - INFO -   num classes: 6
2025-07-11 06:58:51,418 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 06:58:51,418 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 06:58:51,425 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s] 17%|█▋        | 2073/12000 [00:10<00:47, 207.17it/s] 35%|███▍      | 4149/12000 [00:20<00:37, 207.40it/s] 52%|█████▏    | 6269/12000 [00:30<00:27, 209.46it/s] 70%|██████▉   | 8360/12000 [00:40<00:17, 209.31it/s] 87%|████████▋ | 10384/12000 [00:50<00:07, 206.78it/s]100%|██████████| 12000/12000 [00:57<00:00, 207.66it/s]
2025-07-11 06:59:49,932 - INFO - Done! Took 00:00:58.51
2025-07-11 06:59:49,953 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 06:59:50,173 - INFO - Created model type: <class 'torch_geometric.graphgym.model_builder.GraphGymModule'>
2025-07-11 06:59:50,173 - INFO - Inner model type: <class 'graphgps.network.uniform_gt_model_edge.UNIFORM_GTModelEdge'>
2025-07-11 06:59:50,173 - INFO - Inner model has get_darts_model: False
2025-07-11 06:59:50,177 - INFO - GraphGymModule(
  (model): UNIFORM_GTModelEdge(
    (encoder): FeatureEncoder(
      (node_encoder): LapPENodeEncoder(
        (linear_x): Linear(in_features=7, out_features=32, bias=True)
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (edge_encoder): DummyEdgeEncoder(
        (encoder): Embedding(1, 48)
      )
    )
    (layers): Sequential(
      (0): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (1): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (2): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (3): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (4): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (5): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (6): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (7): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (8): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (9): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (10): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (11): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (12): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (13): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (14): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (15): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (post_mp): GNNInductiveNodeHead(
      (layer_post_mp): MLP(
        (model): Sequential(
          (0): GeneralMultiLayer(
            (Layer_0): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
            (Layer_1): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
          )
          (1): Linear(
            (model): Linear(48, 6, bias=True)
          )
        )
      )
    )
  )
)
2025-07-11 06:59:50,182 - INFO - Number of parameters: 541,238
2025-07-11 06:59:50,182 - INFO - Starting optimized training: 2025-07-11 06:59:50.182670
2025-07-11 06:59:55,560 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
2025-07-11 06:59:55,560 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 06:59:55,562 - INFO -   undirected: True
2025-07-11 06:59:55,562 - INFO -   num graphs: 12000
2025-07-11 06:59:55,562 - INFO -   avg num_nodes/graph: 117
2025-07-11 06:59:55,562 - INFO -   num node features: 7
2025-07-11 06:59:55,562 - INFO -   num edge features: 0
2025-07-11 06:59:55,564 - INFO -   num classes: 6
2025-07-11 06:59:55,564 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 06:59:55,564 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 06:59:55,572 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s] 17%|█▋        | 2052/12000 [00:10<00:48, 205.18it/s] 34%|███▍      | 4119/12000 [00:20<00:38, 206.03it/s] 51%|█████     | 6099/12000 [00:30<00:29, 202.36it/s] 68%|██████▊   | 8125/12000 [00:40<00:19, 202.43it/s] 85%|████████▍ | 10196/12000 [00:50<00:08, 204.10it/s]100%|██████████| 12000/12000 [00:58<00:00, 203.76it/s]
2025-07-11 07:00:55,146 - INFO - Done! Took 00:00:59.58
2025-07-11 07:00:55,168 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 07:00:55,180 - INFO - === STARTING OPTIMIZED TRAINING ===
2025-07-11 07:00:55,180 - INFO - Start from epoch 0
2025-07-11 07:02:18,121 - INFO - train: {'epoch': 0, 'time_epoch': 82.27555, 'eta': 8145.27958, 'eta_hours': 2.26258, 'loss': 1.79546515, 'lr': 0.0, 'params': 541238, 'time_iter': 0.13164, 'accuracy': 0.16605, 'f1': 0.11433, 'accuracy-SBM': 0.16617, 'auc': 0.49848}
2025-07-11 07:02:18,127 - INFO - ...computing epoch stats took: 0.65s
2025-07-11 07:02:22,212 - INFO - val: {'epoch': 0, 'time_epoch': 4.04177, 'loss': 1.79499343, 'lr': 0, 'params': 541238, 'time_iter': 0.06416, 'accuracy': 0.16672, 'f1': 0.11884, 'accuracy-SBM': 0.1672, 'auc': 0.50007}
2025-07-11 07:02:22,214 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 07:02:26,272 - INFO - test: {'epoch': 0, 'time_epoch': 4.02152, 'loss': 1.79551656, 'lr': 0, 'params': 541238, 'time_iter': 0.06383, 'accuracy': 0.16372, 'f1': 0.11872, 'accuracy-SBM': 0.16495, 'auc': 0.4994}
2025-07-11 07:02:26,274 - INFO - ...computing epoch stats took: 0.03s
2025-07-11 07:02:26,275 - INFO - > Epoch 0: took 91.1s (avg 91.1s) | Best so far: epoch 0	train_loss: 1.7955 train_accuracy-SBM: 0.1662	val_loss: 1.7950 val_accuracy-SBM: 0.1672	test_loss: 1.7955 test_accuracy-SBM: 0.1650
2025-07-11 07:03:47,898 - INFO - train: {'epoch': 1, 'time_epoch': 81.27773, 'eta': 8014.11057, 'eta_hours': 2.22614, 'loss': 1.66214204, 'lr': 0.0002, 'params': 541238, 'time_iter': 0.13004, 'accuracy': 0.33921, 'f1': 0.30083, 'accuracy-SBM': 0.33929, 'auc': 0.69463}
2025-07-11 07:03:47,904 - INFO - ...computing epoch stats took: 0.33s
2025-07-11 07:03:51,968 - INFO - val: {'epoch': 1, 'time_epoch': 4.0124, 'loss': 1.70713732, 'lr': 0, 'params': 541238, 'time_iter': 0.06369, 'accuracy': 0.28466, 'f1': 0.22346, 'accuracy-SBM': 0.28623, 'auc': 0.68804}
2025-07-11 07:03:51,970 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 07:03:56,024 - INFO - test: {'epoch': 1, 'time_epoch': 4.00932, 'loss': 1.70382858, 'lr': 0, 'params': 541238, 'time_iter': 0.06364, 'accuracy': 0.28812, 'f1': 0.22665, 'accuracy-SBM': 0.28981, 'auc': 0.68875}
2025-07-11 07:03:56,025 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 07:03:56,026 - INFO - > Epoch 1: took 89.8s (avg 90.4s) | Best so far: epoch 1	train_loss: 1.6621 train_accuracy-SBM: 0.3393	val_loss: 1.7071 val_accuracy-SBM: 0.2862	test_loss: 1.7038 test_accuracy-SBM: 0.2898
2025-07-11 07:05:18,717 - INFO - train: {'epoch': 2, 'time_epoch': 82.33759, 'eta': 7950.47149, 'eta_hours': 2.20846, 'loss': 1.45491826, 'lr': 0.0004, 'params': 541238, 'time_iter': 0.13174, 'accuracy': 0.43722, 'f1': 0.4149, 'accuracy-SBM': 0.43727, 'auc': 0.78103}
2025-07-11 07:05:18,723 - INFO - ...computing epoch stats took: 0.34s
2025-07-11 07:05:22,838 - INFO - val: {'epoch': 2, 'time_epoch': 4.06747, 'loss': 1.65016697, 'lr': 0, 'params': 541238, 'time_iter': 0.06456, 'accuracy': 0.2994, 'f1': 0.25932, 'accuracy-SBM': 0.29969, 'auc': 0.72526}
2025-07-11 07:05:22,840 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 07:05:26,895 - INFO - test: {'epoch': 2, 'time_epoch': 4.01587, 'loss': 1.64800851, 'lr': 0, 'params': 541238, 'time_iter': 0.06374, 'accuracy': 0.30597, 'f1': 0.2647, 'accuracy-SBM': 0.30764, 'auc': 0.72659}
2025-07-11 07:05:26,896 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 07:05:26,897 - INFO - > Epoch 2: took 90.9s (avg 90.6s) | Best so far: epoch 2	train_loss: 1.4549 train_accuracy-SBM: 0.4373	val_loss: 1.6502 val_accuracy-SBM: 0.2997	test_loss: 1.6480 test_accuracy-SBM: 0.3076
2025-07-11 07:06:51,147 - INFO - train: {'epoch': 3, 'time_epoch': 84.00921, 'eta': 7917.60185, 'eta_hours': 2.19933, 'loss': 1.30993939, 'lr': 0.0006, 'params': 541238, 'time_iter': 0.13441, 'accuracy': 0.47849, 'f1': 0.46764, 'accuracy-SBM': 0.47854, 'auc': 0.82155}
2025-07-11 07:06:55,452 - INFO - val: {'epoch': 3, 'time_epoch': 4.25463, 'loss': 1.36808675, 'lr': 0, 'params': 541238, 'time_iter': 0.06753, 'accuracy': 0.45002, 'f1': 0.4485, 'accuracy-SBM': 0.45033, 'auc': 0.82288}
2025-07-11 07:06:59,647 - INFO - test: {'epoch': 3, 'time_epoch': 4.15695, 'loss': 1.36223462, 'lr': 0, 'params': 541238, 'time_iter': 0.06598, 'accuracy': 0.45037, 'f1': 0.44886, 'accuracy-SBM': 0.4514, 'auc': 0.82429}
2025-07-11 07:06:59,650 - INFO - > Epoch 3: took 92.8s (avg 91.1s) | Best so far: epoch 3	train_loss: 1.3099 train_accuracy-SBM: 0.4785	val_loss: 1.3681 val_accuracy-SBM: 0.4503	test_loss: 1.3622 test_accuracy-SBM: 0.4514
2025-07-11 07:08:23,614 - INFO - train: {'epoch': 4, 'time_epoch': 83.71791, 'eta': 7858.74174, 'eta_hours': 2.18298, 'loss': 1.1326897, 'lr': 0.0008, 'params': 541238, 'time_iter': 0.13395, 'accuracy': 0.55335, 'f1': 0.5431, 'accuracy-SBM': 0.55334, 'auc': 0.86628}
2025-07-11 07:08:27,762 - INFO - val: {'epoch': 4, 'time_epoch': 4.09752, 'loss': 1.09454456, 'lr': 0, 'params': 541238, 'time_iter': 0.06504, 'accuracy': 0.56266, 'f1': 0.54037, 'accuracy-SBM': 0.56319, 'auc': 0.87668}
2025-07-11 07:08:31,880 - INFO - test: {'epoch': 4, 'time_epoch': 4.07963, 'loss': 1.09386797, 'lr': 0, 'params': 541238, 'time_iter': 0.06476, 'accuracy': 0.56369, 'f1': 0.54158, 'accuracy-SBM': 0.56538, 'auc': 0.87602}
2025-07-11 07:08:31,882 - INFO - > Epoch 4: took 92.2s (avg 91.3s) | Best so far: epoch 4	train_loss: 1.1327 train_accuracy-SBM: 0.5533	val_loss: 1.0945 val_accuracy-SBM: 0.5632	test_loss: 1.0939 test_accuracy-SBM: 0.5654
2025-07-11 07:09:54,517 - INFO - train: {'epoch': 5, 'time_epoch': 82.30456, 'eta': 7769.45316, 'eta_hours': 2.15818, 'loss': 1.04220476, 'lr': 0.001, 'params': 541238, 'time_iter': 0.13169, 'accuracy': 0.57959, 'f1': 0.56863, 'accuracy-SBM': 0.57957, 'auc': 0.88418}
2025-07-11 07:09:58,560 - INFO - val: {'epoch': 5, 'time_epoch': 3.9984, 'loss': 1.01756227, 'lr': 0, 'params': 541238, 'time_iter': 0.06347, 'accuracy': 0.58519, 'f1': 0.57517, 'accuracy-SBM': 0.58656, 'auc': 0.89047}
2025-07-11 07:10:02,581 - INFO - test: {'epoch': 5, 'time_epoch': 3.9874, 'loss': 1.02371978, 'lr': 0, 'params': 541238, 'time_iter': 0.06329, 'accuracy': 0.58366, 'f1': 0.57285, 'accuracy-SBM': 0.5854, 'auc': 0.88823}
2025-07-11 07:10:02,583 - INFO - > Epoch 5: took 90.7s (avg 91.2s) | Best so far: epoch 5	train_loss: 1.0422 train_accuracy-SBM: 0.5796	val_loss: 1.0176 val_accuracy-SBM: 0.5866	test_loss: 1.0237 test_accuracy-SBM: 0.5854
2025-07-11 07:11:25,121 - INFO - train: {'epoch': 6, 'time_epoch': 82.29948, 'eta': 7682.09257, 'eta_hours': 2.13391, 'loss': 0.999985, 'lr': 0.00099973, 'params': 541238, 'time_iter': 0.13168, 'accuracy': 0.59312, 'f1': 0.58206, 'accuracy-SBM': 0.5931, 'auc': 0.89202}
2025-07-11 07:11:29,183 - INFO - val: {'epoch': 6, 'time_epoch': 4.01713, 'loss': 0.97592949, 'lr': 0, 'params': 541238, 'time_iter': 0.06376, 'accuracy': 0.60201, 'f1': 0.58555, 'accuracy-SBM': 0.60262, 'auc': 0.89974}
2025-07-11 07:11:33,264 - INFO - test: {'epoch': 6, 'time_epoch': 4.04521, 'loss': 0.97764259, 'lr': 0, 'params': 541238, 'time_iter': 0.06421, 'accuracy': 0.59941, 'f1': 0.58222, 'accuracy-SBM': 0.6003, 'auc': 0.89821}
2025-07-11 07:11:33,266 - INFO - > Epoch 6: took 90.7s (avg 91.2s) | Best so far: epoch 6	train_loss: 1.0000 train_accuracy-SBM: 0.5931	val_loss: 0.9759 val_accuracy-SBM: 0.6026	test_loss: 0.9776 test_accuracy-SBM: 0.6003
2025-07-11 07:12:56,799 - INFO - train: {'epoch': 7, 'time_epoch': 83.27694, 'eta': 7607.23807, 'eta_hours': 2.11312, 'loss': 0.97057202, 'lr': 0.00099891, 'params': 541238, 'time_iter': 0.13324, 'accuracy': 0.60133, 'f1': 0.59041, 'accuracy-SBM': 0.60131, 'auc': 0.89722}
2025-07-11 07:13:00,962 - INFO - val: {'epoch': 7, 'time_epoch': 4.11584, 'loss': 0.93655505, 'lr': 0, 'params': 541238, 'time_iter': 0.06533, 'accuracy': 0.61148, 'f1': 0.6008, 'accuracy-SBM': 0.61218, 'auc': 0.90417}
2025-07-11 07:13:04,992 - INFO - test: {'epoch': 7, 'time_epoch': 3.99623, 'loss': 0.92822107, 'lr': 0, 'params': 541238, 'time_iter': 0.06343, 'accuracy': 0.613, 'f1': 0.60087, 'accuracy-SBM': 0.61431, 'auc': 0.90513}
2025-07-11 07:13:04,994 - INFO - > Epoch 7: took 91.7s (avg 91.2s) | Best so far: epoch 7	train_loss: 0.9706 train_accuracy-SBM: 0.6013	val_loss: 0.9366 val_accuracy-SBM: 0.6122	test_loss: 0.9282 test_accuracy-SBM: 0.6143
2025-07-11 07:14:27,215 - INFO - train: {'epoch': 8, 'time_epoch': 81.98145, 'eta': 7517.41308, 'eta_hours': 2.08817, 'loss': 0.9593418, 'lr': 0.00099754, 'params': 541238, 'time_iter': 0.13117, 'accuracy': 0.60369, 'f1': 0.59237, 'accuracy-SBM': 0.60367, 'auc': 0.89911}
2025-07-11 07:14:31,263 - INFO - val: {'epoch': 8, 'time_epoch': 4.00264, 'loss': 0.92432389, 'lr': 0, 'params': 541238, 'time_iter': 0.06353, 'accuracy': 0.61513, 'f1': 0.59586, 'accuracy-SBM': 0.61585, 'auc': 0.90631}
2025-07-11 07:14:35,271 - INFO - test: {'epoch': 8, 'time_epoch': 3.97597, 'loss': 0.92142088, 'lr': 0, 'params': 541238, 'time_iter': 0.06311, 'accuracy': 0.61766, 'f1': 0.59895, 'accuracy-SBM': 0.61877, 'auc': 0.90622}
2025-07-11 07:14:35,273 - INFO - > Epoch 8: took 90.3s (avg 91.1s) | Best so far: epoch 8	train_loss: 0.9593 train_accuracy-SBM: 0.6037	val_loss: 0.9243 val_accuracy-SBM: 0.6159	test_loss: 0.9214 test_accuracy-SBM: 0.6188
2025-07-11 07:16:01,119 - INFO - train: {'epoch': 9, 'time_epoch': 85.60389, 'eta': 7461.75872, 'eta_hours': 2.07271, 'loss': 0.94216304, 'lr': 0.00099563, 'params': 541238, 'time_iter': 0.13697, 'accuracy': 0.61011, 'f1': 0.59902, 'accuracy-SBM': 0.61009, 'auc': 0.90215}
2025-07-11 07:16:05,209 - INFO - val: {'epoch': 9, 'time_epoch': 4.03678, 'loss': 0.94396234, 'lr': 0, 'params': 541238, 'time_iter': 0.06408, 'accuracy': 0.60884, 'f1': 0.60378, 'accuracy-SBM': 0.60916, 'auc': 0.90361}
2025-07-11 07:16:09,296 - INFO - test: {'epoch': 9, 'time_epoch': 4.05507, 'loss': 0.94449678, 'lr': 0, 'params': 541238, 'time_iter': 0.06437, 'accuracy': 0.60901, 'f1': 0.60345, 'accuracy-SBM': 0.61008, 'auc': 0.90242}
2025-07-11 07:16:09,298 - INFO - > Epoch 9: took 94.0s (avg 91.4s) | Best so far: epoch 8	train_loss: 0.9593 train_accuracy-SBM: 0.6037	val_loss: 0.9243 val_accuracy-SBM: 0.6159	test_loss: 0.9214 test_accuracy-SBM: 0.6188
2025-07-11 07:17:33,377 - INFO - train: {'epoch': 10, 'time_epoch': 83.83806, 'eta': 7386.37187, 'eta_hours': 2.05177, 'loss': 0.93537145, 'lr': 0.00099318, 'params': 541238, 'time_iter': 0.13414, 'accuracy': 0.61232, 'f1': 0.60078, 'accuracy-SBM': 0.6123, 'auc': 0.90331}
2025-07-11 07:17:37,419 - INFO - val: {'epoch': 10, 'time_epoch': 3.99647, 'loss': 0.93454901, 'lr': 0, 'params': 541238, 'time_iter': 0.06344, 'accuracy': 0.61187, 'f1': 0.59993, 'accuracy-SBM': 0.61252, 'auc': 0.90444}
2025-07-11 07:17:41,448 - INFO - test: {'epoch': 10, 'time_epoch': 3.98973, 'loss': 0.92506232, 'lr': 0, 'params': 541238, 'time_iter': 0.06333, 'accuracy': 0.61354, 'f1': 0.6002, 'accuracy-SBM': 0.61518, 'auc': 0.90582}
2025-07-11 07:17:41,450 - INFO - > Epoch 10: took 92.2s (avg 91.5s) | Best so far: epoch 8	train_loss: 0.9593 train_accuracy-SBM: 0.6037	val_loss: 0.9243 val_accuracy-SBM: 0.6159	test_loss: 0.9214 test_accuracy-SBM: 0.6188
2025-07-11 07:19:04,177 - INFO - train: {'epoch': 11, 'time_epoch': 82.4841, 'eta': 7299.64739, 'eta_hours': 2.02768, 'loss': 0.92748177, 'lr': 0.00099019, 'params': 541238, 'time_iter': 0.13197, 'accuracy': 0.61424, 'f1': 0.60381, 'accuracy-SBM': 0.61422, 'auc': 0.90481}
2025-07-11 07:19:08,262 - INFO - val: {'epoch': 11, 'time_epoch': 4.03933, 'loss': 0.8989692, 'lr': 0, 'params': 541238, 'time_iter': 0.06412, 'accuracy': 0.62326, 'f1': 0.60628, 'accuracy-SBM': 0.62385, 'auc': 0.91059}
2025-07-11 07:19:12,346 - INFO - test: {'epoch': 11, 'time_epoch': 4.0417, 'loss': 0.89672419, 'lr': 0, 'params': 541238, 'time_iter': 0.06415, 'accuracy': 0.62194, 'f1': 0.60451, 'accuracy-SBM': 0.62307, 'auc': 0.91049}
2025-07-11 07:19:12,348 - INFO - > Epoch 11: took 90.9s (avg 91.4s) | Best so far: epoch 11	train_loss: 0.9275 train_accuracy-SBM: 0.6142	val_loss: 0.8990 val_accuracy-SBM: 0.6239	test_loss: 0.8967 test_accuracy-SBM: 0.6231
2025-07-11 07:20:36,008 - INFO - train: {'epoch': 12, 'time_epoch': 83.419, 'eta': 7219.83195, 'eta_hours': 2.00551, 'loss': 0.91704184, 'lr': 0.00098666, 'params': 541238, 'time_iter': 0.13347, 'accuracy': 0.61792, 'f1': 0.60641, 'accuracy-SBM': 0.6179, 'auc': 0.90651}
2025-07-11 07:20:40,036 - INFO - val: {'epoch': 12, 'time_epoch': 3.98208, 'loss': 0.91624659, 'lr': 0, 'params': 541238, 'time_iter': 0.06321, 'accuracy': 0.61843, 'f1': 0.58831, 'accuracy-SBM': 0.61916, 'auc': 0.90762}
2025-07-11 07:20:43,964 - INFO - test: {'epoch': 12, 'time_epoch': 3.89076, 'loss': 0.908274, 'lr': 0, 'params': 541238, 'time_iter': 0.06176, 'accuracy': 0.61815, 'f1': 0.58876, 'accuracy-SBM': 0.6201, 'auc': 0.90835}
2025-07-11 07:20:43,965 - INFO - > Epoch 12: took 91.6s (avg 91.4s) | Best so far: epoch 11	train_loss: 0.9275 train_accuracy-SBM: 0.6142	val_loss: 0.8990 val_accuracy-SBM: 0.6239	test_loss: 0.8967 test_accuracy-SBM: 0.6231
2025-07-11 07:22:07,876 - INFO - train: {'epoch': 13, 'time_epoch': 83.54701, 'eta': 7140.28807, 'eta_hours': 1.98341, 'loss': 0.91236566, 'lr': 0.0009826, 'params': 541238, 'time_iter': 0.13368, 'accuracy': 0.61849, 'f1': 0.6043, 'accuracy-SBM': 0.61847, 'auc': 0.90738}
2025-07-11 07:22:11,727 - INFO - val: {'epoch': 13, 'time_epoch': 3.80685, 'loss': 0.89352714, 'lr': 0, 'params': 541238, 'time_iter': 0.06043, 'accuracy': 0.62589, 'f1': 0.58106, 'accuracy-SBM': 0.62612, 'auc': 0.91131}
2025-07-11 07:22:15,528 - INFO - test: {'epoch': 13, 'time_epoch': 3.76797, 'loss': 0.89185283, 'lr': 0, 'params': 541238, 'time_iter': 0.05981, 'accuracy': 0.6251, 'f1': 0.58101, 'accuracy-SBM': 0.62575, 'auc': 0.91108}
2025-07-11 07:22:15,530 - INFO - > Epoch 13: took 91.6s (avg 91.5s) | Best so far: epoch 13	train_loss: 0.9124 train_accuracy-SBM: 0.6185	val_loss: 0.8935 val_accuracy-SBM: 0.6261	test_loss: 0.8919 test_accuracy-SBM: 0.6258
2025-07-11 07:23:41,907 - INFO - train: {'epoch': 14, 'time_epoch': 86.01958, 'eta': 7074.22168, 'eta_hours': 1.96506, 'loss': 0.90476684, 'lr': 0.00097802, 'params': 541238, 'time_iter': 0.13763, 'accuracy': 0.62084, 'f1': 0.61037, 'accuracy-SBM': 0.62081, 'auc': 0.90869}
2025-07-11 07:23:46,022 - INFO - val: {'epoch': 14, 'time_epoch': 4.068, 'loss': 0.88820321, 'lr': 0, 'params': 541238, 'time_iter': 0.06457, 'accuracy': 0.6263, 'f1': 0.6138, 'accuracy-SBM': 0.62681, 'auc': 0.91246}
2025-07-11 07:23:50,133 - INFO - test: {'epoch': 14, 'time_epoch': 4.07287, 'loss': 0.89474162, 'lr': 0, 'params': 541238, 'time_iter': 0.06465, 'accuracy': 0.62096, 'f1': 0.60831, 'accuracy-SBM': 0.6221, 'auc': 0.91094}
2025-07-11 07:23:50,135 - INFO - > Epoch 14: took 94.6s (avg 91.7s) | Best so far: epoch 14	train_loss: 0.9048 train_accuracy-SBM: 0.6208	val_loss: 0.8882 val_accuracy-SBM: 0.6268	test_loss: 0.8947 test_accuracy-SBM: 0.6221
2025-07-11 07:25:14,643 - INFO - train: {'epoch': 15, 'time_epoch': 84.27253, 'eta': 6996.48909, 'eta_hours': 1.94347, 'loss': 0.90009878, 'lr': 0.00097291, 'params': 541238, 'time_iter': 0.13484, 'accuracy': 0.62174, 'f1': 0.61172, 'accuracy-SBM': 0.62172, 'auc': 0.90948}
2025-07-11 07:25:18,688 - INFO - val: {'epoch': 15, 'time_epoch': 3.9995, 'loss': 0.87929928, 'lr': 0, 'params': 541238, 'time_iter': 0.06348, 'accuracy': 0.62476, 'f1': 0.61266, 'accuracy-SBM': 0.62526, 'auc': 0.91392}
2025-07-11 07:25:22,701 - INFO - test: {'epoch': 15, 'time_epoch': 3.97966, 'loss': 0.87826905, 'lr': 0, 'params': 541238, 'time_iter': 0.06317, 'accuracy': 0.6249, 'f1': 0.61192, 'accuracy-SBM': 0.6265, 'auc': 0.91356}
2025-07-11 07:25:22,703 - INFO - > Epoch 15: took 92.6s (avg 91.7s) | Best so far: epoch 14	train_loss: 0.9048 train_accuracy-SBM: 0.6208	val_loss: 0.8882 val_accuracy-SBM: 0.6268	test_loss: 0.8947 test_accuracy-SBM: 0.6221
2025-07-11 07:26:45,739 - INFO - train: {'epoch': 16, 'time_epoch': 82.69504, 'eta': 6910.28527, 'eta_hours': 1.91952, 'loss': 0.89466344, 'lr': 0.00096728, 'params': 541238, 'time_iter': 0.13231, 'accuracy': 0.62408, 'f1': 0.6138, 'accuracy-SBM': 0.62406, 'auc': 0.91046}
2025-07-11 07:26:49,626 - INFO - val: {'epoch': 16, 'time_epoch': 3.84216, 'loss': 0.86824083, 'lr': 0, 'params': 541238, 'time_iter': 0.06099, 'accuracy': 0.63256, 'f1': 0.61897, 'accuracy-SBM': 0.63329, 'auc': 0.91556}
2025-07-11 07:26:53,544 - INFO - test: {'epoch': 16, 'time_epoch': 3.87717, 'loss': 0.86303411, 'lr': 0, 'params': 541238, 'time_iter': 0.06154, 'accuracy': 0.63237, 'f1': 0.61786, 'accuracy-SBM': 0.63368, 'auc': 0.91595}
2025-07-11 07:26:53,547 - INFO - > Epoch 16: took 90.8s (avg 91.7s) | Best so far: epoch 16	train_loss: 0.8947 train_accuracy-SBM: 0.6241	val_loss: 0.8682 val_accuracy-SBM: 0.6333	test_loss: 0.8630 test_accuracy-SBM: 0.6337
2025-07-11 07:28:22,073 - INFO - train: {'epoch': 17, 'time_epoch': 88.16983, 'eta': 6849.412, 'eta_hours': 1.90261, 'loss': 0.88698006, 'lr': 0.00096114, 'params': 541238, 'time_iter': 0.14107, 'accuracy': 0.62658, 'f1': 0.61692, 'accuracy-SBM': 0.62656, 'auc': 0.91182}
2025-07-11 07:28:26,240 - INFO - val: {'epoch': 17, 'time_epoch': 4.10566, 'loss': 0.86110599, 'lr': 0, 'params': 541238, 'time_iter': 0.06517, 'accuracy': 0.63131, 'f1': 0.59022, 'accuracy-SBM': 0.63229, 'auc': 0.91657}
2025-07-11 07:28:30,302 - INFO - test: {'epoch': 17, 'time_epoch': 4.02601, 'loss': 0.86321663, 'lr': 0, 'params': 541238, 'time_iter': 0.0639, 'accuracy': 0.63192, 'f1': 0.59198, 'accuracy-SBM': 0.63404, 'auc': 0.9158}
2025-07-11 07:28:30,304 - INFO - > Epoch 17: took 96.8s (avg 92.0s) | Best so far: epoch 16	train_loss: 0.8947 train_accuracy-SBM: 0.6241	val_loss: 0.8682 val_accuracy-SBM: 0.6333	test_loss: 0.8630 test_accuracy-SBM: 0.6337
2025-07-11 07:29:55,335 - INFO - train: {'epoch': 18, 'time_epoch': 84.80037, 'eta': 6771.30085, 'eta_hours': 1.88092, 'loss': 0.88571567, 'lr': 0.0009545, 'params': 541238, 'time_iter': 0.13568, 'accuracy': 0.62627, 'f1': 0.6161, 'accuracy-SBM': 0.62624, 'auc': 0.912}
2025-07-11 07:29:59,393 - INFO - val: {'epoch': 18, 'time_epoch': 4.01183, 'loss': 0.88160094, 'lr': 0, 'params': 541238, 'time_iter': 0.06368, 'accuracy': 0.628, 'f1': 0.60009, 'accuracy-SBM': 0.62861, 'auc': 0.91384}
2025-07-11 07:30:03,508 - INFO - test: {'epoch': 18, 'time_epoch': 4.08151, 'loss': 0.87700281, 'lr': 0, 'params': 541238, 'time_iter': 0.06479, 'accuracy': 0.63, 'f1': 0.60158, 'accuracy-SBM': 0.63071, 'auc': 0.91415}
2025-07-11 07:30:03,511 - INFO - > Epoch 18: took 93.2s (avg 92.0s) | Best so far: epoch 16	train_loss: 0.8947 train_accuracy-SBM: 0.6241	val_loss: 0.8682 val_accuracy-SBM: 0.6333	test_loss: 0.8630 test_accuracy-SBM: 0.6337
2025-07-11 07:31:27,488 - INFO - train: {'epoch': 19, 'time_epoch': 83.73708, 'eta': 6688.26764, 'eta_hours': 1.85785, 'loss': 0.88115856, 'lr': 0.00094736, 'params': 541238, 'time_iter': 0.13398, 'accuracy': 0.62783, 'f1': 0.61798, 'accuracy-SBM': 0.62781, 'auc': 0.91288}
2025-07-11 07:31:31,606 - INFO - val: {'epoch': 19, 'time_epoch': 4.06817, 'loss': 0.85979721, 'lr': 0, 'params': 541238, 'time_iter': 0.06457, 'accuracy': 0.63395, 'f1': 0.58926, 'accuracy-SBM': 0.6349, 'auc': 0.91695}
2025-07-11 07:31:35,750 - INFO - test: {'epoch': 19, 'time_epoch': 4.10418, 'loss': 0.85055544, 'lr': 0, 'params': 541238, 'time_iter': 0.06515, 'accuracy': 0.63579, 'f1': 0.59174, 'accuracy-SBM': 0.63793, 'auc': 0.91789}
2025-07-11 07:31:35,751 - INFO - > Epoch 19: took 92.2s (avg 92.0s) | Best so far: epoch 19	train_loss: 0.8812 train_accuracy-SBM: 0.6278	val_loss: 0.8598 val_accuracy-SBM: 0.6349	test_loss: 0.8506 test_accuracy-SBM: 0.6379
2025-07-11 07:33:00,155 - INFO - train: {'epoch': 20, 'time_epoch': 84.15829, 'eta': 6606.75196, 'eta_hours': 1.83521, 'loss': 0.87055507, 'lr': 0.00093974, 'params': 541238, 'time_iter': 0.13465, 'accuracy': 0.63012, 'f1': 0.61983, 'accuracy-SBM': 0.6301, 'auc': 0.91456}
2025-07-11 07:33:04,110 - INFO - val: {'epoch': 20, 'time_epoch': 3.91092, 'loss': 0.86480638, 'lr': 0, 'params': 541238, 'time_iter': 0.06208, 'accuracy': 0.6319, 'f1': 0.62593, 'accuracy-SBM': 0.63255, 'auc': 0.9161}
2025-07-11 07:33:07,963 - INFO - test: {'epoch': 20, 'time_epoch': 3.81973, 'loss': 0.86462421, 'lr': 0, 'params': 541238, 'time_iter': 0.06063, 'accuracy': 0.63425, 'f1': 0.62847, 'accuracy-SBM': 0.63572, 'auc': 0.91568}
2025-07-11 07:33:07,965 - INFO - > Epoch 20: took 92.2s (avg 92.0s) | Best so far: epoch 19	train_loss: 0.8812 train_accuracy-SBM: 0.6278	val_loss: 0.8598 val_accuracy-SBM: 0.6349	test_loss: 0.8506 test_accuracy-SBM: 0.6379
2025-07-11 07:34:30,831 - INFO - train: {'epoch': 21, 'time_epoch': 82.62526, 'eta': 6519.56074, 'eta_hours': 1.81099, 'loss': 0.86789928, 'lr': 0.00093163, 'params': 541238, 'time_iter': 0.1322, 'accuracy': 0.63183, 'f1': 0.62255, 'accuracy-SBM': 0.6318, 'auc': 0.91509}
2025-07-11 07:34:34,927 - INFO - val: {'epoch': 21, 'time_epoch': 4.04235, 'loss': 0.86827173, 'lr': 0, 'params': 541238, 'time_iter': 0.06416, 'accuracy': 0.63071, 'f1': 0.62365, 'accuracy-SBM': 0.63158, 'auc': 0.91584}
2025-07-11 07:34:39,009 - INFO - test: {'epoch': 21, 'time_epoch': 4.04842, 'loss': 0.85704364, 'lr': 0, 'params': 541238, 'time_iter': 0.06426, 'accuracy': 0.6345, 'f1': 0.62734, 'accuracy-SBM': 0.6359, 'auc': 0.91742}
2025-07-11 07:34:39,117 - INFO - > Epoch 21: took 91.2s (avg 92.0s) | Best so far: epoch 19	train_loss: 0.8812 train_accuracy-SBM: 0.6278	val_loss: 0.8598 val_accuracy-SBM: 0.6349	test_loss: 0.8506 test_accuracy-SBM: 0.6379
2025-07-11 07:36:02,313 - INFO - train: {'epoch': 22, 'time_epoch': 82.86067, 'eta': 6433.55467, 'eta_hours': 1.7871, 'loss': 0.86650055, 'lr': 0.00092305, 'params': 541238, 'time_iter': 0.13258, 'accuracy': 0.63205, 'f1': 0.62125, 'accuracy-SBM': 0.63202, 'auc': 0.91538}
2025-07-11 07:36:06,389 - INFO - val: {'epoch': 22, 'time_epoch': 4.02658, 'loss': 0.86720468, 'lr': 0, 'params': 541238, 'time_iter': 0.06391, 'accuracy': 0.63473, 'f1': 0.61552, 'accuracy-SBM': 0.63548, 'auc': 0.91653}
2025-07-11 07:36:10,469 - INFO - test: {'epoch': 22, 'time_epoch': 4.04022, 'loss': 0.86644946, 'lr': 0, 'params': 541238, 'time_iter': 0.06413, 'accuracy': 0.63531, 'f1': 0.61642, 'accuracy-SBM': 0.63713, 'auc': 0.91615}
2025-07-11 07:36:10,487 - INFO - > Epoch 22: took 91.4s (avg 92.0s) | Best so far: epoch 22	train_loss: 0.8665 train_accuracy-SBM: 0.6320	val_loss: 0.8672 val_accuracy-SBM: 0.6355	test_loss: 0.8664 test_accuracy-SBM: 0.6371
2025-07-11 07:37:32,482 - INFO - train: {'epoch': 23, 'time_epoch': 81.75755, 'eta': 6344.31749, 'eta_hours': 1.76231, 'loss': 0.85741927, 'lr': 0.000914, 'params': 541238, 'time_iter': 0.13081, 'accuracy': 0.63594, 'f1': 0.62685, 'accuracy-SBM': 0.63591, 'auc': 0.91682}
2025-07-11 07:37:36,560 - INFO - val: {'epoch': 23, 'time_epoch': 4.03318, 'loss': 0.84138918, 'lr': 0, 'params': 541238, 'time_iter': 0.06402, 'accuracy': 0.64094, 'f1': 0.62946, 'accuracy-SBM': 0.64141, 'auc': 0.91986}
2025-07-11 07:37:40,616 - INFO - test: {'epoch': 23, 'time_epoch': 4.02342, 'loss': 0.83538749, 'lr': 0, 'params': 541238, 'time_iter': 0.06386, 'accuracy': 0.6459, 'f1': 0.63471, 'accuracy-SBM': 0.64699, 'auc': 0.9204}
2025-07-11 07:37:40,618 - INFO - > Epoch 23: took 90.1s (avg 91.9s) | Best so far: epoch 23	train_loss: 0.8574 train_accuracy-SBM: 0.6359	val_loss: 0.8414 val_accuracy-SBM: 0.6414	test_loss: 0.8354 test_accuracy-SBM: 0.6470
2025-07-11 07:39:03,790 - INFO - train: {'epoch': 24, 'time_epoch': 82.93249, 'eta': 6259.20351, 'eta_hours': 1.73867, 'loss': 0.859028, 'lr': 0.00090451, 'params': 541238, 'time_iter': 0.13269, 'accuracy': 0.63505, 'f1': 0.62608, 'accuracy-SBM': 0.63502, 'auc': 0.91661}
2025-07-11 07:39:07,854 - INFO - val: {'epoch': 24, 'time_epoch': 4.01946, 'loss': 0.84628835, 'lr': 0, 'params': 541238, 'time_iter': 0.0638, 'accuracy': 0.6398, 'f1': 0.62369, 'accuracy-SBM': 0.64041, 'auc': 0.91923}
2025-07-11 07:39:11,849 - INFO - test: {'epoch': 24, 'time_epoch': 3.96289, 'loss': 0.84135058, 'lr': 0, 'params': 541238, 'time_iter': 0.0629, 'accuracy': 0.63976, 'f1': 0.62291, 'accuracy-SBM': 0.64095, 'auc': 0.91973}
2025-07-11 07:39:11,851 - INFO - > Epoch 24: took 91.2s (avg 91.9s) | Best so far: epoch 23	train_loss: 0.8574 train_accuracy-SBM: 0.6359	val_loss: 0.8414 val_accuracy-SBM: 0.6414	test_loss: 0.8354 test_accuracy-SBM: 0.6470
2025-07-11 07:40:38,907 - INFO - train: {'epoch': 25, 'time_epoch': 86.78424, 'eta': 6185.22, 'eta_hours': 1.71812, 'loss': 0.85134761, 'lr': 0.00089457, 'params': 541238, 'time_iter': 0.13885, 'accuracy': 0.63771, 'f1': 0.6277, 'accuracy-SBM': 0.63769, 'auc': 0.91783}
2025-07-11 07:40:43,020 - INFO - val: {'epoch': 25, 'time_epoch': 4.05404, 'loss': 0.83519457, 'lr': 0, 'params': 541238, 'time_iter': 0.06435, 'accuracy': 0.64215, 'f1': 0.60444, 'accuracy-SBM': 0.64248, 'auc': 0.92087}
2025-07-11 07:40:47,085 - INFO - test: {'epoch': 25, 'time_epoch': 4.0322, 'loss': 0.82943713, 'lr': 0, 'params': 541238, 'time_iter': 0.064, 'accuracy': 0.6436, 'f1': 0.60703, 'accuracy-SBM': 0.64431, 'auc': 0.92129}
2025-07-11 07:40:47,088 - INFO - > Epoch 25: took 95.2s (avg 92.0s) | Best so far: epoch 25	train_loss: 0.8513 train_accuracy-SBM: 0.6377	val_loss: 0.8352 val_accuracy-SBM: 0.6425	test_loss: 0.8294 test_accuracy-SBM: 0.6443
2025-07-11 07:42:12,698 - INFO - train: {'epoch': 26, 'time_epoch': 85.35126, 'eta': 6106.41394, 'eta_hours': 1.69623, 'loss': 0.84750925, 'lr': 0.0008842, 'params': 541238, 'time_iter': 0.13656, 'accuracy': 0.63912, 'f1': 0.6299, 'accuracy-SBM': 0.6391, 'auc': 0.91845}
2025-07-11 07:42:16,788 - INFO - val: {'epoch': 26, 'time_epoch': 4.04408, 'loss': 0.83792732, 'lr': 0, 'params': 541238, 'time_iter': 0.06419, 'accuracy': 0.64287, 'f1': 0.63293, 'accuracy-SBM': 0.64351, 'auc': 0.92086}
2025-07-11 07:42:20,850 - INFO - test: {'epoch': 26, 'time_epoch': 4.02783, 'loss': 0.82782509, 'lr': 0, 'params': 541238, 'time_iter': 0.06393, 'accuracy': 0.64575, 'f1': 0.63504, 'accuracy-SBM': 0.64725, 'auc': 0.9221}
2025-07-11 07:42:20,852 - INFO - > Epoch 26: took 93.8s (avg 92.1s) | Best so far: epoch 26	train_loss: 0.8475 train_accuracy-SBM: 0.6391	val_loss: 0.8379 val_accuracy-SBM: 0.6435	test_loss: 0.8278 test_accuracy-SBM: 0.6472
2025-07-11 07:43:47,007 - INFO - train: {'epoch': 27, 'time_epoch': 85.80669, 'eta': 6028.31149, 'eta_hours': 1.67453, 'loss': 0.84497238, 'lr': 0.00087341, 'params': 541238, 'time_iter': 0.13729, 'accuracy': 0.64027, 'f1': 0.63159, 'accuracy-SBM': 0.64025, 'auc': 0.9188}
2025-07-11 07:43:51,061 - INFO - val: {'epoch': 27, 'time_epoch': 4.00871, 'loss': 0.83182311, 'lr': 0, 'params': 541238, 'time_iter': 0.06363, 'accuracy': 0.64528, 'f1': 0.63337, 'accuracy-SBM': 0.64585, 'auc': 0.92163}
2025-07-11 07:43:55,103 - INFO - test: {'epoch': 27, 'time_epoch': 4.00765, 'loss': 0.82735039, 'lr': 0, 'params': 541238, 'time_iter': 0.06361, 'accuracy': 0.64407, 'f1': 0.63187, 'accuracy-SBM': 0.64522, 'auc': 0.922}
2025-07-11 07:43:55,106 - INFO - > Epoch 27: took 94.3s (avg 92.1s) | Best so far: epoch 27	train_loss: 0.8450 train_accuracy-SBM: 0.6402	val_loss: 0.8318 val_accuracy-SBM: 0.6459	test_loss: 0.8274 test_accuracy-SBM: 0.6452
2025-07-11 07:45:21,541 - INFO - train: {'epoch': 28, 'time_epoch': 86.19231, 'eta': 5950.62179, 'eta_hours': 1.65295, 'loss': 0.83987263, 'lr': 0.00086221, 'params': 541238, 'time_iter': 0.13791, 'accuracy': 0.64109, 'f1': 0.63167, 'accuracy-SBM': 0.64107, 'auc': 0.91966}
2025-07-11 07:45:25,574 - INFO - val: {'epoch': 28, 'time_epoch': 3.98318, 'loss': 0.84117319, 'lr': 0, 'params': 541238, 'time_iter': 0.06323, 'accuracy': 0.6402, 'f1': 0.63444, 'accuracy-SBM': 0.64095, 'auc': 0.92077}
2025-07-11 07:45:29,571 - INFO - test: {'epoch': 28, 'time_epoch': 3.96214, 'loss': 0.82731144, 'lr': 0, 'params': 541238, 'time_iter': 0.06289, 'accuracy': 0.64611, 'f1': 0.64051, 'accuracy-SBM': 0.64754, 'auc': 0.92232}
2025-07-11 07:45:29,573 - INFO - > Epoch 28: took 94.5s (avg 92.2s) | Best so far: epoch 27	train_loss: 0.8450 train_accuracy-SBM: 0.6402	val_loss: 0.8318 val_accuracy-SBM: 0.6459	test_loss: 0.8274 test_accuracy-SBM: 0.6452
2025-07-11 07:46:55,707 - INFO - train: {'epoch': 29, 'time_epoch': 85.89498, 'eta': 5871.6715, 'eta_hours': 1.63102, 'loss': 0.83791057, 'lr': 0.00085062, 'params': 541238, 'time_iter': 0.13743, 'accuracy': 0.64117, 'f1': 0.63189, 'accuracy-SBM': 0.64115, 'auc': 0.91996}
2025-07-11 07:46:59,817 - INFO - val: {'epoch': 29, 'time_epoch': 4.06262, 'loss': 0.82400625, 'lr': 0, 'params': 541238, 'time_iter': 0.06449, 'accuracy': 0.64272, 'f1': 0.62422, 'accuracy-SBM': 0.64336, 'auc': 0.92274}
2025-07-11 07:47:04,033 - INFO - test: {'epoch': 29, 'time_epoch': 4.16145, 'loss': 0.82104432, 'lr': 0, 'params': 541238, 'time_iter': 0.06605, 'accuracy': 0.64501, 'f1': 0.62771, 'accuracy-SBM': 0.64682, 'auc': 0.92276}
2025-07-11 07:47:04,036 - INFO - > Epoch 29: took 94.5s (avg 92.3s) | Best so far: epoch 27	train_loss: 0.8450 train_accuracy-SBM: 0.6402	val_loss: 0.8318 val_accuracy-SBM: 0.6459	test_loss: 0.8274 test_accuracy-SBM: 0.6452
2025-07-11 07:48:26,064 - INFO - train: {'epoch': 30, 'time_epoch': 81.76445, 'eta': 5783.0794, 'eta_hours': 1.60641, 'loss': 0.8350372, 'lr': 0.00083864, 'params': 541238, 'time_iter': 0.13082, 'accuracy': 0.64132, 'f1': 0.6326, 'accuracy-SBM': 0.6413, 'auc': 0.92044}
2025-07-11 07:48:30,098 - INFO - val: {'epoch': 30, 'time_epoch': 3.98563, 'loss': 0.83508924, 'lr': 0, 'params': 541238, 'time_iter': 0.06326, 'accuracy': 0.64062, 'f1': 0.6183, 'accuracy-SBM': 0.64126, 'auc': 0.92133}
2025-07-11 07:48:34,095 - INFO - test: {'epoch': 30, 'time_epoch': 3.96476, 'loss': 0.82632077, 'lr': 0, 'params': 541238, 'time_iter': 0.06293, 'accuracy': 0.64475, 'f1': 0.62327, 'accuracy-SBM': 0.6457, 'auc': 0.92225}
2025-07-11 07:48:34,097 - INFO - > Epoch 30: took 90.1s (avg 92.2s) | Best so far: epoch 27	train_loss: 0.8450 train_accuracy-SBM: 0.6402	val_loss: 0.8318 val_accuracy-SBM: 0.6459	test_loss: 0.8274 test_accuracy-SBM: 0.6452
2025-07-11 07:49:56,436 - INFO - train: {'epoch': 31, 'time_epoch': 82.08129, 'eta': 5695.58731, 'eta_hours': 1.58211, 'loss': 0.83156994, 'lr': 0.00082629, 'params': 541238, 'time_iter': 0.13133, 'accuracy': 0.64377, 'f1': 0.63545, 'accuracy-SBM': 0.64374, 'auc': 0.92099}
2025-07-11 07:50:00,391 - INFO - val: {'epoch': 31, 'time_epoch': 3.90692, 'loss': 0.83334843, 'lr': 0, 'params': 541238, 'time_iter': 0.06201, 'accuracy': 0.64464, 'f1': 0.60008, 'accuracy-SBM': 0.64512, 'auc': 0.92186}
2025-07-11 07:50:04,302 - INFO - test: {'epoch': 31, 'time_epoch': 3.87301, 'loss': 0.8295543, 'lr': 0, 'params': 541238, 'time_iter': 0.06148, 'accuracy': 0.64653, 'f1': 0.60198, 'accuracy-SBM': 0.64726, 'auc': 0.92181}
2025-07-11 07:50:04,305 - INFO - > Epoch 31: took 90.2s (avg 92.2s) | Best so far: epoch 27	train_loss: 0.8450 train_accuracy-SBM: 0.6402	val_loss: 0.8318 val_accuracy-SBM: 0.6459	test_loss: 0.8274 test_accuracy-SBM: 0.6452
2025-07-11 07:51:25,228 - INFO - train: {'epoch': 32, 'time_epoch': 80.4937, 'eta': 5605.19986, 'eta_hours': 1.557, 'loss': 0.83009078, 'lr': 0.00081359, 'params': 541238, 'time_iter': 0.12879, 'accuracy': 0.64341, 'f1': 0.63421, 'accuracy-SBM': 0.64339, 'auc': 0.92121}
2025-07-11 07:51:29,044 - INFO - val: {'epoch': 32, 'time_epoch': 3.7707, 'loss': 0.82871853, 'lr': 0, 'params': 541238, 'time_iter': 0.05985, 'accuracy': 0.64204, 'f1': 0.62878, 'accuracy-SBM': 0.64284, 'auc': 0.92166}
2025-07-11 07:51:32,834 - INFO - test: {'epoch': 32, 'time_epoch': 3.75869, 'loss': 0.81485945, 'lr': 0, 'params': 541238, 'time_iter': 0.05966, 'accuracy': 0.64887, 'f1': 0.63646, 'accuracy-SBM': 0.65058, 'auc': 0.92356}
2025-07-11 07:51:32,836 - INFO - > Epoch 32: took 88.5s (avg 92.0s) | Best so far: epoch 27	train_loss: 0.8450 train_accuracy-SBM: 0.6402	val_loss: 0.8318 val_accuracy-SBM: 0.6459	test_loss: 0.8274 test_accuracy-SBM: 0.6452
2025-07-11 07:52:55,165 - INFO - train: {'epoch': 33, 'time_epoch': 81.99389, 'eta': 5518.30653, 'eta_hours': 1.53286, 'loss': 0.82786915, 'lr': 0.00080054, 'params': 541238, 'time_iter': 0.13119, 'accuracy': 0.64426, 'f1': 0.63539, 'accuracy-SBM': 0.64424, 'auc': 0.92153}
2025-07-11 07:52:59,173 - INFO - val: {'epoch': 33, 'time_epoch': 3.96285, 'loss': 0.82124066, 'lr': 0, 'params': 541238, 'time_iter': 0.0629, 'accuracy': 0.64644, 'f1': 0.63362, 'accuracy-SBM': 0.64709, 'auc': 0.92336}
2025-07-11 07:53:03,160 - INFO - test: {'epoch': 33, 'time_epoch': 3.95274, 'loss': 0.81476553, 'lr': 0, 'params': 541238, 'time_iter': 0.06274, 'accuracy': 0.64709, 'f1': 0.63323, 'accuracy-SBM': 0.64825, 'auc': 0.9239}
2025-07-11 07:53:03,162 - INFO - > Epoch 33: took 90.3s (avg 92.0s) | Best so far: epoch 33	train_loss: 0.8279 train_accuracy-SBM: 0.6442	val_loss: 0.8212 val_accuracy-SBM: 0.6471	test_loss: 0.8148 test_accuracy-SBM: 0.6482
2025-07-11 07:54:27,221 - INFO - train: {'epoch': 34, 'time_epoch': 83.69197, 'eta': 5434.84674, 'eta_hours': 1.50968, 'loss': 0.82413109, 'lr': 0.00078716, 'params': 541238, 'time_iter': 0.13391, 'accuracy': 0.6462, 'f1': 0.6378, 'accuracy-SBM': 0.64618, 'auc': 0.92211}
2025-07-11 07:54:31,217 - INFO - val: {'epoch': 34, 'time_epoch': 3.94868, 'loss': 0.83502467, 'lr': 0, 'params': 541238, 'time_iter': 0.06268, 'accuracy': 0.64092, 'f1': 0.62639, 'accuracy-SBM': 0.64138, 'auc': 0.92123}
2025-07-11 07:54:35,254 - INFO - test: {'epoch': 34, 'time_epoch': 4.00171, 'loss': 0.82749128, 'lr': 0, 'params': 541238, 'time_iter': 0.06352, 'accuracy': 0.64769, 'f1': 0.63276, 'accuracy-SBM': 0.64879, 'auc': 0.92208}
2025-07-11 07:54:35,257 - INFO - > Epoch 34: took 92.1s (avg 92.0s) | Best so far: epoch 33	train_loss: 0.8279 train_accuracy-SBM: 0.6442	val_loss: 0.8212 val_accuracy-SBM: 0.6471	test_loss: 0.8148 test_accuracy-SBM: 0.6482
2025-07-11 07:55:58,032 - INFO - train: {'epoch': 35, 'time_epoch': 82.50645, 'eta': 5349.26648, 'eta_hours': 1.48591, 'loss': 0.82172517, 'lr': 0.00077347, 'params': 541238, 'time_iter': 0.13201, 'accuracy': 0.64709, 'f1': 0.63856, 'accuracy-SBM': 0.64707, 'auc': 0.92252}
2025-07-11 07:56:01,941 - INFO - val: {'epoch': 35, 'time_epoch': 3.85602, 'loss': 0.82400924, 'lr': 0, 'params': 541238, 'time_iter': 0.06121, 'accuracy': 0.65049, 'f1': 0.6247, 'accuracy-SBM': 0.65091, 'auc': 0.92265}
2025-07-11 07:56:05,805 - INFO - test: {'epoch': 35, 'time_epoch': 3.82698, 'loss': 0.81365758, 'lr': 0, 'params': 541238, 'time_iter': 0.06075, 'accuracy': 0.64999, 'f1': 0.62499, 'accuracy-SBM': 0.65081, 'auc': 0.92376}
2025-07-11 07:56:05,807 - INFO - > Epoch 35: took 90.5s (avg 92.0s) | Best so far: epoch 35	train_loss: 0.8217 train_accuracy-SBM: 0.6471	val_loss: 0.8240 val_accuracy-SBM: 0.6509	test_loss: 0.8137 test_accuracy-SBM: 0.6508
2025-07-11 07:57:27,466 - INFO - train: {'epoch': 36, 'time_epoch': 81.42989, 'eta': 5262.0193, 'eta_hours': 1.46167, 'loss': 0.81873034, 'lr': 0.00075948, 'params': 541238, 'time_iter': 0.13029, 'accuracy': 0.64814, 'f1': 0.63941, 'accuracy-SBM': 0.64812, 'auc': 0.92297}
2025-07-11 07:57:31,332 - INFO - val: {'epoch': 36, 'time_epoch': 3.82246, 'loss': 0.81554748, 'lr': 0, 'params': 541238, 'time_iter': 0.06067, 'accuracy': 0.64674, 'f1': 0.61956, 'accuracy-SBM': 0.64699, 'auc': 0.92414}
2025-07-11 07:57:35,183 - INFO - test: {'epoch': 36, 'time_epoch': 3.81683, 'loss': 0.81726789, 'lr': 0, 'params': 541238, 'time_iter': 0.06058, 'accuracy': 0.64893, 'f1': 0.62205, 'accuracy-SBM': 0.64979, 'auc': 0.92348}
2025-07-11 07:57:35,185 - INFO - > Epoch 36: took 89.4s (avg 91.9s) | Best so far: epoch 35	train_loss: 0.8217 train_accuracy-SBM: 0.6471	val_loss: 0.8240 val_accuracy-SBM: 0.6509	test_loss: 0.8137 test_accuracy-SBM: 0.6508
2025-07-11 07:58:57,903 - INFO - train: {'epoch': 37, 'time_epoch': 82.46407, 'eta': 5176.76563, 'eta_hours': 1.43799, 'loss': 0.81643652, 'lr': 0.00074521, 'params': 541238, 'time_iter': 0.13194, 'accuracy': 0.64894, 'f1': 0.64038, 'accuracy-SBM': 0.64892, 'auc': 0.92333}
2025-07-11 07:59:01,934 - INFO - val: {'epoch': 37, 'time_epoch': 3.98644, 'loss': 0.80980486, 'lr': 0, 'params': 541238, 'time_iter': 0.06328, 'accuracy': 0.6523, 'f1': 0.64738, 'accuracy-SBM': 0.65288, 'auc': 0.92507}
2025-07-11 07:59:05,927 - INFO - test: {'epoch': 37, 'time_epoch': 3.95243, 'loss': 0.80962197, 'lr': 0, 'params': 541238, 'time_iter': 0.06274, 'accuracy': 0.65193, 'f1': 0.64634, 'accuracy-SBM': 0.65322, 'auc': 0.92465}
2025-07-11 07:59:05,930 - INFO - > Epoch 37: took 90.7s (avg 91.9s) | Best so far: epoch 37	train_loss: 0.8164 train_accuracy-SBM: 0.6489	val_loss: 0.8098 val_accuracy-SBM: 0.6529	test_loss: 0.8096 test_accuracy-SBM: 0.6532
2025-07-11 08:00:27,646 - INFO - train: {'epoch': 38, 'time_epoch': 81.46463, 'eta': 5090.0918, 'eta_hours': 1.41391, 'loss': 0.81538409, 'lr': 0.00073067, 'params': 541238, 'time_iter': 0.13034, 'accuracy': 0.64874, 'f1': 0.63995, 'accuracy-SBM': 0.64872, 'auc': 0.92345}
2025-07-11 08:00:31,666 - INFO - val: {'epoch': 38, 'time_epoch': 3.97485, 'loss': 0.81330349, 'lr': 0, 'params': 541238, 'time_iter': 0.06309, 'accuracy': 0.65066, 'f1': 0.63409, 'accuracy-SBM': 0.65111, 'auc': 0.92419}
2025-07-11 08:00:35,653 - INFO - test: {'epoch': 38, 'time_epoch': 3.95381, 'loss': 0.81763752, 'lr': 0, 'params': 541238, 'time_iter': 0.06276, 'accuracy': 0.64884, 'f1': 0.63288, 'accuracy-SBM': 0.64984, 'auc': 0.92334}
2025-07-11 08:00:35,655 - INFO - > Epoch 38: took 89.7s (avg 91.8s) | Best so far: epoch 37	train_loss: 0.8164 train_accuracy-SBM: 0.6489	val_loss: 0.8098 val_accuracy-SBM: 0.6529	test_loss: 0.8096 test_accuracy-SBM: 0.6532
2025-07-11 08:01:57,061 - INFO - train: {'epoch': 39, 'time_epoch': 81.17823, 'eta': 5003.24882, 'eta_hours': 1.38979, 'loss': 0.81321956, 'lr': 0.00071588, 'params': 541238, 'time_iter': 0.12989, 'accuracy': 0.64969, 'f1': 0.64107, 'accuracy-SBM': 0.64966, 'auc': 0.9238}
2025-07-11 08:02:00,836 - INFO - val: {'epoch': 39, 'time_epoch': 3.72707, 'loss': 0.81491018, 'lr': 0, 'params': 541238, 'time_iter': 0.05916, 'accuracy': 0.64909, 'f1': 0.63319, 'accuracy-SBM': 0.64994, 'auc': 0.92424}
2025-07-11 08:02:04,531 - INFO - test: {'epoch': 39, 'time_epoch': 3.6622, 'loss': 0.80604272, 'lr': 0, 'params': 541238, 'time_iter': 0.05813, 'accuracy': 0.65114, 'f1': 0.63508, 'accuracy-SBM': 0.65295, 'auc': 0.9251}
2025-07-11 08:02:04,533 - INFO - > Epoch 39: took 88.9s (avg 91.7s) | Best so far: epoch 37	train_loss: 0.8164 train_accuracy-SBM: 0.6489	val_loss: 0.8098 val_accuracy-SBM: 0.6529	test_loss: 0.8096 test_accuracy-SBM: 0.6532
2025-07-11 08:03:21,723 - INFO - train: {'epoch': 40, 'time_epoch': 76.96532, 'eta': 4910.61969, 'eta_hours': 1.36406, 'loss': 0.80949088, 'lr': 0.00070085, 'params': 541238, 'time_iter': 0.12314, 'accuracy': 0.65033, 'f1': 0.64185, 'accuracy-SBM': 0.65031, 'auc': 0.92431}
2025-07-11 08:03:25,549 - INFO - val: {'epoch': 40, 'time_epoch': 3.78287, 'loss': 0.80816752, 'lr': 0, 'params': 541238, 'time_iter': 0.06005, 'accuracy': 0.64872, 'f1': 0.63042, 'accuracy-SBM': 0.6495, 'auc': 0.92515}
2025-07-11 08:03:29,300 - INFO - test: {'epoch': 40, 'time_epoch': 3.71729, 'loss': 0.80308758, 'lr': 0, 'params': 541238, 'time_iter': 0.059, 'accuracy': 0.64948, 'f1': 0.63253, 'accuracy-SBM': 0.6513, 'auc': 0.92527}
2025-07-11 08:03:29,302 - INFO - > Epoch 40: took 84.8s (avg 91.6s) | Best so far: epoch 37	train_loss: 0.8164 train_accuracy-SBM: 0.6489	val_loss: 0.8098 val_accuracy-SBM: 0.6529	test_loss: 0.8096 test_accuracy-SBM: 0.6532
2025-07-11 08:04:47,305 - INFO - train: {'epoch': 41, 'time_epoch': 77.77983, 'eta': 4819.86125, 'eta_hours': 1.33885, 'loss': 0.80656275, 'lr': 0.0006856, 'params': 541238, 'time_iter': 0.12445, 'accuracy': 0.65171, 'f1': 0.64331, 'accuracy-SBM': 0.65169, 'auc': 0.92477}
2025-07-11 08:04:51,277 - INFO - val: {'epoch': 41, 'time_epoch': 3.92802, 'loss': 0.82471898, 'lr': 0, 'params': 541238, 'time_iter': 0.06235, 'accuracy': 0.64604, 'f1': 0.63195, 'accuracy-SBM': 0.64677, 'auc': 0.92364}
2025-07-11 08:04:55,224 - INFO - test: {'epoch': 41, 'time_epoch': 3.91317, 'loss': 0.81396893, 'lr': 0, 'params': 541238, 'time_iter': 0.06211, 'accuracy': 0.65253, 'f1': 0.63812, 'accuracy-SBM': 0.6538, 'auc': 0.92465}
2025-07-11 08:04:55,226 - INFO - > Epoch 41: took 85.9s (avg 91.4s) | Best so far: epoch 37	train_loss: 0.8164 train_accuracy-SBM: 0.6489	val_loss: 0.8098 val_accuracy-SBM: 0.6529	test_loss: 0.8096 test_accuracy-SBM: 0.6532
2025-07-11 08:06:12,926 - INFO - train: {'epoch': 42, 'time_epoch': 77.47982, 'eta': 4729.30879, 'eta_hours': 1.3137, 'loss': 0.80400992, 'lr': 0.00067015, 'params': 541238, 'time_iter': 0.12397, 'accuracy': 0.65215, 'f1': 0.64368, 'accuracy-SBM': 0.65213, 'auc': 0.9252}
2025-07-11 08:06:16,673 - INFO - val: {'epoch': 42, 'time_epoch': 3.70326, 'loss': 0.81868861, 'lr': 0, 'params': 541238, 'time_iter': 0.05878, 'accuracy': 0.647, 'f1': 0.64021, 'accuracy-SBM': 0.64787, 'auc': 0.92378}
2025-07-11 08:06:20,355 - INFO - test: {'epoch': 42, 'time_epoch': 3.65065, 'loss': 0.81194706, 'lr': 0, 'params': 541238, 'time_iter': 0.05795, 'accuracy': 0.64627, 'f1': 0.63937, 'accuracy-SBM': 0.64783, 'auc': 0.92437}
2025-07-11 08:06:20,357 - INFO - > Epoch 42: took 85.1s (avg 91.3s) | Best so far: epoch 37	train_loss: 0.8164 train_accuracy-SBM: 0.6489	val_loss: 0.8098 val_accuracy-SBM: 0.6529	test_loss: 0.8096 test_accuracy-SBM: 0.6532
2025-07-11 08:07:39,004 - INFO - train: {'epoch': 43, 'time_epoch': 78.41762, 'eta': 4640.5441, 'eta_hours': 1.28904, 'loss': 0.80270977, 'lr': 0.00065451, 'params': 541238, 'time_iter': 0.12547, 'accuracy': 0.65327, 'f1': 0.64507, 'accuracy-SBM': 0.65324, 'auc': 0.92534}
2025-07-11 08:07:42,809 - INFO - val: {'epoch': 43, 'time_epoch': 3.76289, 'loss': 0.81406737, 'lr': 0, 'params': 541238, 'time_iter': 0.05973, 'accuracy': 0.64795, 'f1': 0.63755, 'accuracy-SBM': 0.64848, 'auc': 0.92468}
2025-07-11 08:07:46,519 - INFO - test: {'epoch': 43, 'time_epoch': 3.67794, 'loss': 0.80933671, 'lr': 0, 'params': 541238, 'time_iter': 0.05838, 'accuracy': 0.65278, 'f1': 0.64245, 'accuracy-SBM': 0.6538, 'auc': 0.92475}
2025-07-11 08:07:46,521 - INFO - > Epoch 43: took 86.2s (avg 91.2s) | Best so far: epoch 37	train_loss: 0.8164 train_accuracy-SBM: 0.6489	val_loss: 0.8098 val_accuracy-SBM: 0.6529	test_loss: 0.8096 test_accuracy-SBM: 0.6532
2025-07-11 08:09:03,717 - INFO - train: {'epoch': 44, 'time_epoch': 76.97232, 'eta': 4550.47281, 'eta_hours': 1.26402, 'loss': 0.7994453, 'lr': 0.0006387, 'params': 541238, 'time_iter': 0.12316, 'accuracy': 0.65363, 'f1': 0.6443, 'accuracy-SBM': 0.65361, 'auc': 0.9258}
2025-07-11 08:09:07,519 - INFO - val: {'epoch': 44, 'time_epoch': 3.7586, 'loss': 0.80749028, 'lr': 0, 'params': 541238, 'time_iter': 0.05966, 'accuracy': 0.65223, 'f1': 0.63631, 'accuracy-SBM': 0.65302, 'auc': 0.9252}
2025-07-11 08:09:11,284 - INFO - test: {'epoch': 44, 'time_epoch': 3.71739, 'loss': 0.80216897, 'lr': 0, 'params': 541238, 'time_iter': 0.05901, 'accuracy': 0.65169, 'f1': 0.63562, 'accuracy-SBM': 0.65349, 'auc': 0.92559}
2025-07-11 08:09:11,287 - INFO - > Epoch 44: took 84.8s (avg 91.0s) | Best so far: epoch 44	train_loss: 0.7994 train_accuracy-SBM: 0.6536	val_loss: 0.8075 val_accuracy-SBM: 0.6530	test_loss: 0.8022 test_accuracy-SBM: 0.6535
2025-07-11 08:10:27,722 - INFO - train: {'epoch': 45, 'time_epoch': 76.21629, 'eta': 4460.08352, 'eta_hours': 1.23891, 'loss': 0.79664878, 'lr': 0.00062274, 'params': 541238, 'time_iter': 0.12195, 'accuracy': 0.65512, 'f1': 0.64713, 'accuracy-SBM': 0.6551, 'auc': 0.92627}
2025-07-11 08:10:31,453 - INFO - val: {'epoch': 45, 'time_epoch': 3.6883, 'loss': 0.81426669, 'lr': 0, 'params': 541238, 'time_iter': 0.05854, 'accuracy': 0.64834, 'f1': 0.63194, 'accuracy-SBM': 0.64916, 'auc': 0.92472}
2025-07-11 08:10:35,134 - INFO - test: {'epoch': 45, 'time_epoch': 3.64364, 'loss': 0.80606126, 'lr': 0, 'params': 541238, 'time_iter': 0.05784, 'accuracy': 0.65337, 'f1': 0.63706, 'accuracy-SBM': 0.65514, 'auc': 0.92532}
2025-07-11 08:10:35,167 - INFO - > Epoch 45: took 83.9s (avg 90.9s) | Best so far: epoch 44	train_loss: 0.7994 train_accuracy-SBM: 0.6536	val_loss: 0.8075 val_accuracy-SBM: 0.6530	test_loss: 0.8022 test_accuracy-SBM: 0.6535
2025-07-11 08:11:51,730 - INFO - train: {'epoch': 46, 'time_epoch': 76.24901, 'eta': 4370.33423, 'eta_hours': 1.21398, 'loss': 0.79348083, 'lr': 0.00060665, 'params': 541238, 'time_iter': 0.122, 'accuracy': 0.65587, 'f1': 0.64768, 'accuracy-SBM': 0.65585, 'auc': 0.92672}
2025-07-11 08:11:55,447 - INFO - val: {'epoch': 46, 'time_epoch': 3.67386, 'loss': 0.80756347, 'lr': 0, 'params': 541238, 'time_iter': 0.05832, 'accuracy': 0.64771, 'f1': 0.63624, 'accuracy-SBM': 0.64855, 'auc': 0.92528}
2025-07-11 08:11:59,163 - INFO - test: {'epoch': 46, 'time_epoch': 3.68474, 'loss': 0.80318327, 'lr': 0, 'params': 541238, 'time_iter': 0.05849, 'accuracy': 0.65158, 'f1': 0.64023, 'accuracy-SBM': 0.65327, 'auc': 0.92553}
2025-07-11 08:11:59,165 - INFO - > Epoch 46: took 84.0s (avg 90.7s) | Best so far: epoch 44	train_loss: 0.7994 train_accuracy-SBM: 0.6536	val_loss: 0.8075 val_accuracy-SBM: 0.6530	test_loss: 0.8022 test_accuracy-SBM: 0.6535
2025-07-11 08:13:16,597 - INFO - train: {'epoch': 47, 'time_epoch': 77.20445, 'eta': 4282.18252, 'eta_hours': 1.1895, 'loss': 0.7946886, 'lr': 0.00059044, 'params': 541238, 'time_iter': 0.12353, 'accuracy': 0.65586, 'f1': 0.64766, 'accuracy-SBM': 0.65584, 'auc': 0.92652}
2025-07-11 08:13:20,384 - INFO - val: {'epoch': 47, 'time_epoch': 3.74504, 'loss': 0.8098608, 'lr': 0, 'params': 541238, 'time_iter': 0.05945, 'accuracy': 0.64925, 'f1': 0.63615, 'accuracy-SBM': 0.64975, 'auc': 0.92507}
2025-07-11 08:13:24,067 - INFO - test: {'epoch': 47, 'time_epoch': 3.65124, 'loss': 0.79749719, 'lr': 0, 'params': 541238, 'time_iter': 0.05796, 'accuracy': 0.65314, 'f1': 0.64024, 'accuracy-SBM': 0.65422, 'auc': 0.92624}
2025-07-11 08:13:24,069 - INFO - > Epoch 47: took 84.9s (avg 90.6s) | Best so far: epoch 44	train_loss: 0.7994 train_accuracy-SBM: 0.6536	val_loss: 0.8075 val_accuracy-SBM: 0.6530	test_loss: 0.8022 test_accuracy-SBM: 0.6535
2025-07-11 08:14:40,416 - INFO - train: {'epoch': 48, 'time_epoch': 76.03202, 'eta': 4193.25735, 'eta_hours': 1.16479, 'loss': 0.78963493, 'lr': 0.00057413, 'params': 541238, 'time_iter': 0.12165, 'accuracy': 0.65755, 'f1': 0.64691, 'accuracy-SBM': 0.65753, 'auc': 0.9272}
2025-07-11 08:14:44,105 - INFO - val: {'epoch': 48, 'time_epoch': 3.64665, 'loss': 0.81039981, 'lr': 0, 'params': 541238, 'time_iter': 0.05788, 'accuracy': 0.64967, 'f1': 0.63939, 'accuracy-SBM': 0.65021, 'auc': 0.92511}
2025-07-11 08:14:47,772 - INFO - test: {'epoch': 48, 'time_epoch': 3.63435, 'loss': 0.79543935, 'lr': 0, 'params': 541238, 'time_iter': 0.05769, 'accuracy': 0.65782, 'f1': 0.64732, 'accuracy-SBM': 0.65905, 'auc': 0.92657}
2025-07-11 08:14:47,774 - INFO - > Epoch 48: took 83.7s (avg 90.5s) | Best so far: epoch 44	train_loss: 0.7994 train_accuracy-SBM: 0.6536	val_loss: 0.8075 val_accuracy-SBM: 0.6530	test_loss: 0.8022 test_accuracy-SBM: 0.6535
2025-07-11 08:16:04,543 - INFO - train: {'epoch': 49, 'time_epoch': 76.45303, 'eta': 4105.26891, 'eta_hours': 1.14035, 'loss': 0.78998778, 'lr': 0.00055774, 'params': 541238, 'time_iter': 0.12232, 'accuracy': 0.65699, 'f1': 0.64901, 'accuracy-SBM': 0.65697, 'auc': 0.92725}
2025-07-11 08:16:08,256 - INFO - val: {'epoch': 49, 'time_epoch': 3.66991, 'loss': 0.80815145, 'lr': 0, 'params': 541238, 'time_iter': 0.05825, 'accuracy': 0.65288, 'f1': 0.61974, 'accuracy-SBM': 0.65328, 'auc': 0.92546}
2025-07-11 08:16:11,900 - INFO - test: {'epoch': 49, 'time_epoch': 3.61274, 'loss': 0.79930141, 'lr': 0, 'params': 541238, 'time_iter': 0.05735, 'accuracy': 0.65446, 'f1': 0.62034, 'accuracy-SBM': 0.65516, 'auc': 0.92628}
2025-07-11 08:16:11,902 - INFO - > Epoch 49: took 84.1s (avg 90.3s) | Best so far: epoch 49	train_loss: 0.7900 train_accuracy-SBM: 0.6570	val_loss: 0.8082 val_accuracy-SBM: 0.6533	test_loss: 0.7993 test_accuracy-SBM: 0.6552
2025-07-11 08:17:27,794 - INFO - train: {'epoch': 50, 'time_epoch': 75.6681, 'eta': 4016.9787, 'eta_hours': 1.11583, 'loss': 0.78563913, 'lr': 0.00054129, 'params': 541238, 'time_iter': 0.12107, 'accuracy': 0.65868, 'f1': 0.64986, 'accuracy-SBM': 0.65866, 'auc': 0.92779}
2025-07-11 08:17:31,525 - INFO - val: {'epoch': 50, 'time_epoch': 3.68873, 'loss': 0.80369499, 'lr': 0, 'params': 541238, 'time_iter': 0.05855, 'accuracy': 0.653, 'f1': 0.62754, 'accuracy-SBM': 0.6535, 'auc': 0.92585}
2025-07-11 08:17:35,186 - INFO - test: {'epoch': 50, 'time_epoch': 3.63039, 'loss': 0.80047354, 'lr': 0, 'params': 541238, 'time_iter': 0.05763, 'accuracy': 0.65625, 'f1': 0.63113, 'accuracy-SBM': 0.65713, 'auc': 0.92597}
2025-07-11 08:17:35,189 - INFO - > Epoch 50: took 83.3s (avg 90.2s) | Best so far: epoch 50	train_loss: 0.7856 train_accuracy-SBM: 0.6587	val_loss: 0.8037 val_accuracy-SBM: 0.6535	test_loss: 0.8005 test_accuracy-SBM: 0.6571
2025-07-11 08:18:50,528 - INFO - train: {'epoch': 51, 'time_epoch': 75.11906, 'eta': 3928.66714, 'eta_hours': 1.0913, 'loss': 0.78391989, 'lr': 0.00052479, 'params': 541238, 'time_iter': 0.12019, 'accuracy': 0.66005, 'f1': 0.65179, 'accuracy-SBM': 0.66003, 'auc': 0.92811}
2025-07-11 08:18:54,155 - INFO - val: {'epoch': 51, 'time_epoch': 3.58374, 'loss': 0.79150717, 'lr': 0, 'params': 541238, 'time_iter': 0.05688, 'accuracy': 0.65668, 'f1': 0.65102, 'accuracy-SBM': 0.65732, 'auc': 0.92759}
2025-07-11 08:18:57,750 - INFO - test: {'epoch': 51, 'time_epoch': 3.56413, 'loss': 0.78967999, 'lr': 0, 'params': 541238, 'time_iter': 0.05657, 'accuracy': 0.65661, 'f1': 0.6507, 'accuracy-SBM': 0.65803, 'auc': 0.92729}
2025-07-11 08:18:57,752 - INFO - > Epoch 51: took 82.6s (avg 90.0s) | Best so far: epoch 51	train_loss: 0.7839 train_accuracy-SBM: 0.6600	val_loss: 0.7915 val_accuracy-SBM: 0.6573	test_loss: 0.7897 test_accuracy-SBM: 0.6580
2025-07-11 08:20:12,853 - INFO - train: {'epoch': 52, 'time_epoch': 74.88269, 'eta': 3840.64381, 'eta_hours': 1.06685, 'loss': 0.78138661, 'lr': 0.00050827, 'params': 541238, 'time_iter': 0.11981, 'accuracy': 0.66033, 'f1': 0.65231, 'accuracy-SBM': 0.66031, 'auc': 0.92842}
2025-07-11 08:20:16,553 - INFO - val: {'epoch': 52, 'time_epoch': 3.64567, 'loss': 0.81599989, 'lr': 0, 'params': 541238, 'time_iter': 0.05787, 'accuracy': 0.65207, 'f1': 0.60963, 'accuracy-SBM': 0.65243, 'auc': 0.92527}
2025-07-11 08:20:20,343 - INFO - test: {'epoch': 52, 'time_epoch': 3.75832, 'loss': 0.8021958, 'lr': 0, 'params': 541238, 'time_iter': 0.05966, 'accuracy': 0.65453, 'f1': 0.61165, 'accuracy-SBM': 0.65518, 'auc': 0.92632}
2025-07-11 08:20:20,345 - INFO - > Epoch 52: took 82.6s (avg 89.9s) | Best so far: epoch 51	train_loss: 0.7839 train_accuracy-SBM: 0.6600	val_loss: 0.7915 val_accuracy-SBM: 0.6573	test_loss: 0.7897 test_accuracy-SBM: 0.6580
2025-07-11 08:21:37,821 - INFO - train: {'epoch': 53, 'time_epoch': 77.2558, 'eta': 3755.1287, 'eta_hours': 1.04309, 'loss': 0.77966995, 'lr': 0.00049173, 'params': 541238, 'time_iter': 0.12361, 'accuracy': 0.66061, 'f1': 0.65222, 'accuracy-SBM': 0.66058, 'auc': 0.92866}
2025-07-11 08:21:41,523 - INFO - val: {'epoch': 53, 'time_epoch': 3.65933, 'loss': 0.80521591, 'lr': 0, 'params': 541238, 'time_iter': 0.05808, 'accuracy': 0.65606, 'f1': 0.64952, 'accuracy-SBM': 0.65669, 'auc': 0.92597}
2025-07-11 08:21:45,196 - INFO - test: {'epoch': 53, 'time_epoch': 3.64027, 'loss': 0.80256363, 'lr': 0, 'params': 541238, 'time_iter': 0.05778, 'accuracy': 0.65541, 'f1': 0.64855, 'accuracy-SBM': 0.65675, 'auc': 0.92571}
2025-07-11 08:21:45,198 - INFO - > Epoch 53: took 84.9s (avg 89.8s) | Best so far: epoch 51	train_loss: 0.7839 train_accuracy-SBM: 0.6600	val_loss: 0.7915 val_accuracy-SBM: 0.6573	test_loss: 0.7897 test_accuracy-SBM: 0.6580
2025-07-11 08:23:01,848 - INFO - train: {'epoch': 54, 'time_epoch': 76.42899, 'eta': 3669.23746, 'eta_hours': 1.01923, 'loss': 0.77638179, 'lr': 0.00047521, 'params': 541238, 'time_iter': 0.12229, 'accuracy': 0.66151, 'f1': 0.6535, 'accuracy-SBM': 0.66149, 'auc': 0.92913}
2025-07-11 08:23:05,583 - INFO - val: {'epoch': 54, 'time_epoch': 3.69125, 'loss': 0.80020832, 'lr': 0, 'params': 541238, 'time_iter': 0.05859, 'accuracy': 0.65524, 'f1': 0.65045, 'accuracy-SBM': 0.65581, 'auc': 0.92621}
2025-07-11 08:23:09,286 - INFO - test: {'epoch': 54, 'time_epoch': 3.67161, 'loss': 0.79373493, 'lr': 0, 'params': 541238, 'time_iter': 0.05828, 'accuracy': 0.65461, 'f1': 0.64927, 'accuracy-SBM': 0.65602, 'auc': 0.92696}
2025-07-11 08:23:09,288 - INFO - > Epoch 54: took 84.1s (avg 89.7s) | Best so far: epoch 51	train_loss: 0.7839 train_accuracy-SBM: 0.6600	val_loss: 0.7915 val_accuracy-SBM: 0.6573	test_loss: 0.7897 test_accuracy-SBM: 0.6580
2025-07-11 08:24:26,822 - INFO - train: {'epoch': 55, 'time_epoch': 77.19699, 'eta': 3584.28757, 'eta_hours': 0.99564, 'loss': 0.77398641, 'lr': 0.00045871, 'params': 541238, 'time_iter': 0.12352, 'accuracy': 0.66224, 'f1': 0.6545, 'accuracy-SBM': 0.66222, 'auc': 0.92945}
2025-07-11 08:24:30,601 - INFO - val: {'epoch': 55, 'time_epoch': 3.73523, 'loss': 0.79898156, 'lr': 0, 'params': 541238, 'time_iter': 0.05929, 'accuracy': 0.65466, 'f1': 0.64841, 'accuracy-SBM': 0.65543, 'auc': 0.92667}
2025-07-11 08:24:34,292 - INFO - test: {'epoch': 55, 'time_epoch': 3.6598, 'loss': 0.79273499, 'lr': 0, 'params': 541238, 'time_iter': 0.05809, 'accuracy': 0.65541, 'f1': 0.64833, 'accuracy-SBM': 0.65683, 'auc': 0.92695}
2025-07-11 08:24:34,294 - INFO - > Epoch 55: took 85.0s (avg 89.6s) | Best so far: epoch 51	train_loss: 0.7839 train_accuracy-SBM: 0.6600	val_loss: 0.7915 val_accuracy-SBM: 0.6573	test_loss: 0.7897 test_accuracy-SBM: 0.6580
2025-07-11 08:25:50,557 - INFO - train: {'epoch': 56, 'time_epoch': 76.04511, 'eta': 3498.74076, 'eta_hours': 0.97187, 'loss': 0.77080773, 'lr': 0.00044226, 'params': 541238, 'time_iter': 0.12167, 'accuracy': 0.66349, 'f1': 0.65569, 'accuracy-SBM': 0.66347, 'auc': 0.92991}
2025-07-11 08:25:54,266 - INFO - val: {'epoch': 56, 'time_epoch': 3.665, 'loss': 0.80483453, 'lr': 0, 'params': 541238, 'time_iter': 0.05817, 'accuracy': 0.65446, 'f1': 0.64542, 'accuracy-SBM': 0.65501, 'auc': 0.92611}
2025-07-11 08:25:57,980 - INFO - test: {'epoch': 56, 'time_epoch': 3.68252, 'loss': 0.79787375, 'lr': 0, 'params': 541238, 'time_iter': 0.05845, 'accuracy': 0.65471, 'f1': 0.64565, 'accuracy-SBM': 0.65606, 'auc': 0.92646}
2025-07-11 08:25:57,983 - INFO - > Epoch 56: took 83.7s (avg 89.5s) | Best so far: epoch 51	train_loss: 0.7839 train_accuracy-SBM: 0.6600	val_loss: 0.7915 val_accuracy-SBM: 0.6573	test_loss: 0.7897 test_accuracy-SBM: 0.6580
2025-07-11 08:27:14,352 - INFO - train: {'epoch': 57, 'time_epoch': 76.04149, 'eta': 3413.51897, 'eta_hours': 0.9482, 'loss': 0.76830882, 'lr': 0.00042587, 'params': 541238, 'time_iter': 0.12167, 'accuracy': 0.66428, 'f1': 0.65649, 'accuracy-SBM': 0.66425, 'auc': 0.9303}
2025-07-11 08:27:18,169 - INFO - val: {'epoch': 57, 'time_epoch': 3.77398, 'loss': 0.80591924, 'lr': 0, 'params': 541238, 'time_iter': 0.0599, 'accuracy': 0.65447, 'f1': 0.64089, 'accuracy-SBM': 0.65522, 'auc': 0.92628}
2025-07-11 08:27:21,965 - INFO - test: {'epoch': 57, 'time_epoch': 3.76379, 'loss': 0.79694898, 'lr': 0, 'params': 541238, 'time_iter': 0.05974, 'accuracy': 0.6539, 'f1': 0.63886, 'accuracy-SBM': 0.65563, 'auc': 0.92673}
2025-07-11 08:27:21,967 - INFO - > Epoch 57: took 84.0s (avg 89.4s) | Best so far: epoch 51	train_loss: 0.7839 train_accuracy-SBM: 0.6600	val_loss: 0.7915 val_accuracy-SBM: 0.6573	test_loss: 0.7897 test_accuracy-SBM: 0.6580
2025-07-11 08:28:38,909 - INFO - train: {'epoch': 58, 'time_epoch': 76.72027, 'eta': 3329.08007, 'eta_hours': 0.92474, 'loss': 0.7662201, 'lr': 0.00040956, 'params': 541238, 'time_iter': 0.12275, 'accuracy': 0.6645, 'f1': 0.65669, 'accuracy-SBM': 0.66448, 'auc': 0.93054}
2025-07-11 08:28:42,671 - INFO - val: {'epoch': 58, 'time_epoch': 3.71868, 'loss': 0.80166999, 'lr': 0, 'params': 541238, 'time_iter': 0.05903, 'accuracy': 0.65268, 'f1': 0.63098, 'accuracy-SBM': 0.65309, 'auc': 0.92661}
2025-07-11 08:28:46,389 - INFO - test: {'epoch': 58, 'time_epoch': 3.68702, 'loss': 0.79716553, 'lr': 0, 'params': 541238, 'time_iter': 0.05852, 'accuracy': 0.65552, 'f1': 0.63366, 'accuracy-SBM': 0.65637, 'auc': 0.92664}
2025-07-11 08:28:46,391 - INFO - > Epoch 58: took 84.4s (avg 89.3s) | Best so far: epoch 51	train_loss: 0.7839 train_accuracy-SBM: 0.6600	val_loss: 0.7915 val_accuracy-SBM: 0.6573	test_loss: 0.7897 test_accuracy-SBM: 0.6580
2025-07-11 08:30:02,874 - INFO - train: {'epoch': 59, 'time_epoch': 76.24611, 'eta': 3244.58235, 'eta_hours': 0.90127, 'loss': 0.76501494, 'lr': 0.00039335, 'params': 541238, 'time_iter': 0.12199, 'accuracy': 0.66455, 'f1': 0.65624, 'accuracy-SBM': 0.66452, 'auc': 0.93072}
2025-07-11 08:30:06,878 - INFO - val: {'epoch': 59, 'time_epoch': 3.95193, 'loss': 0.79914906, 'lr': 0, 'params': 541238, 'time_iter': 0.06273, 'accuracy': 0.65477, 'f1': 0.64765, 'accuracy-SBM': 0.65555, 'auc': 0.92694}
2025-07-11 08:30:10,825 - INFO - test: {'epoch': 59, 'time_epoch': 3.91556, 'loss': 0.79401138, 'lr': 0, 'params': 541238, 'time_iter': 0.06215, 'accuracy': 0.65538, 'f1': 0.6475, 'accuracy-SBM': 0.65688, 'auc': 0.92708}
2025-07-11 08:30:10,828 - INFO - > Epoch 59: took 84.4s (avg 89.3s) | Best so far: epoch 51	train_loss: 0.7839 train_accuracy-SBM: 0.6600	val_loss: 0.7915 val_accuracy-SBM: 0.6573	test_loss: 0.7897 test_accuracy-SBM: 0.6580
2025-07-11 08:31:26,400 - INFO - train: {'epoch': 60, 'time_epoch': 75.35295, 'eta': 3159.78414, 'eta_hours': 0.87772, 'loss': 0.76458336, 'lr': 0.00037726, 'params': 541238, 'time_iter': 0.12056, 'accuracy': 0.66536, 'f1': 0.65767, 'accuracy-SBM': 0.66534, 'auc': 0.9308}
2025-07-11 08:31:30,106 - INFO - val: {'epoch': 60, 'time_epoch': 3.66297, 'loss': 0.79761695, 'lr': 0, 'params': 541238, 'time_iter': 0.05814, 'accuracy': 0.65593, 'f1': 0.64866, 'accuracy-SBM': 0.65669, 'auc': 0.92704}
2025-07-11 08:31:33,843 - INFO - test: {'epoch': 60, 'time_epoch': 3.70605, 'loss': 0.79095829, 'lr': 0, 'params': 541238, 'time_iter': 0.05883, 'accuracy': 0.65682, 'f1': 0.64884, 'accuracy-SBM': 0.65837, 'auc': 0.92745}
2025-07-11 08:31:33,845 - INFO - > Epoch 60: took 83.0s (avg 89.2s) | Best so far: epoch 51	train_loss: 0.7839 train_accuracy-SBM: 0.6600	val_loss: 0.7915 val_accuracy-SBM: 0.6573	test_loss: 0.7897 test_accuracy-SBM: 0.6580
2025-07-11 08:32:50,264 - INFO - train: {'epoch': 61, 'time_epoch': 76.10794, 'eta': 3075.75335, 'eta_hours': 0.85438, 'loss': 0.76041777, 'lr': 0.0003613, 'params': 541238, 'time_iter': 0.12177, 'accuracy': 0.66681, 'f1': 0.65907, 'accuracy-SBM': 0.66679, 'auc': 0.9313}
2025-07-11 08:32:53,947 - INFO - val: {'epoch': 61, 'time_epoch': 3.64096, 'loss': 0.8005028, 'lr': 0, 'params': 541238, 'time_iter': 0.05779, 'accuracy': 0.65589, 'f1': 0.64386, 'accuracy-SBM': 0.65646, 'auc': 0.92708}
2025-07-11 08:32:57,632 - INFO - test: {'epoch': 61, 'time_epoch': 3.65209, 'loss': 0.79153193, 'lr': 0, 'params': 541238, 'time_iter': 0.05797, 'accuracy': 0.65844, 'f1': 0.64641, 'accuracy-SBM': 0.65956, 'auc': 0.92754}
2025-07-11 08:32:57,634 - INFO - > Epoch 61: took 83.8s (avg 89.1s) | Best so far: epoch 51	train_loss: 0.7839 train_accuracy-SBM: 0.6600	val_loss: 0.7915 val_accuracy-SBM: 0.6573	test_loss: 0.7897 test_accuracy-SBM: 0.6580
2025-07-11 08:34:13,485 - INFO - train: {'epoch': 62, 'time_epoch': 75.538, 'eta': 2991.63935, 'eta_hours': 0.83101, 'loss': 0.75909447, 'lr': 0.00034549, 'params': 541238, 'time_iter': 0.12086, 'accuracy': 0.66794, 'f1': 0.66009, 'accuracy-SBM': 0.66792, 'auc': 0.93158}
2025-07-11 08:34:17,235 - INFO - val: {'epoch': 62, 'time_epoch': 3.70047, 'loss': 0.80903754, 'lr': 0, 'params': 541238, 'time_iter': 0.05874, 'accuracy': 0.65698, 'f1': 0.64386, 'accuracy-SBM': 0.65778, 'auc': 0.92624}
2025-07-11 08:34:21,002 - INFO - test: {'epoch': 62, 'time_epoch': 3.73487, 'loss': 0.79054102, 'lr': 0, 'params': 541238, 'time_iter': 0.05928, 'accuracy': 0.65652, 'f1': 0.642, 'accuracy-SBM': 0.6583, 'auc': 0.9279}
2025-07-11 08:34:21,004 - INFO - > Epoch 62: took 83.4s (avg 89.0s) | Best so far: epoch 62	train_loss: 0.7591 train_accuracy-SBM: 0.6679	val_loss: 0.8090 val_accuracy-SBM: 0.6578	test_loss: 0.7905 test_accuracy-SBM: 0.6583
2025-07-11 08:35:37,748 - INFO - train: {'epoch': 63, 'time_epoch': 76.42851, 'eta': 2908.29427, 'eta_hours': 0.80786, 'loss': 0.75603325, 'lr': 0.00032985, 'params': 541238, 'time_iter': 0.12229, 'accuracy': 0.66873, 'f1': 0.66116, 'accuracy-SBM': 0.66871, 'auc': 0.93196}
2025-07-11 08:35:41,539 - INFO - val: {'epoch': 63, 'time_epoch': 3.74799, 'loss': 0.8044613, 'lr': 0, 'params': 541238, 'time_iter': 0.05949, 'accuracy': 0.65514, 'f1': 0.64604, 'accuracy-SBM': 0.65576, 'auc': 0.92633}
2025-07-11 08:35:45,271 - INFO - test: {'epoch': 63, 'time_epoch': 3.69989, 'loss': 0.78892188, 'lr': 0, 'params': 541238, 'time_iter': 0.05873, 'accuracy': 0.65696, 'f1': 0.64611, 'accuracy-SBM': 0.65862, 'auc': 0.92766}
2025-07-11 08:35:45,273 - INFO - > Epoch 63: took 84.3s (avg 88.9s) | Best so far: epoch 62	train_loss: 0.7591 train_accuracy-SBM: 0.6679	val_loss: 0.8090 val_accuracy-SBM: 0.6578	test_loss: 0.7905 test_accuracy-SBM: 0.6583
2025-07-11 08:37:03,481 - INFO - train: {'epoch': 64, 'time_epoch': 77.98604, 'eta': 2826.00067, 'eta_hours': 0.785, 'loss': 0.75504424, 'lr': 0.0003144, 'params': 541238, 'time_iter': 0.12478, 'accuracy': 0.66877, 'f1': 0.66112, 'accuracy-SBM': 0.66874, 'auc': 0.9321}
2025-07-11 08:37:07,215 - INFO - val: {'epoch': 64, 'time_epoch': 3.69167, 'loss': 0.8037448, 'lr': 0, 'params': 541238, 'time_iter': 0.0586, 'accuracy': 0.65502, 'f1': 0.64432, 'accuracy-SBM': 0.65583, 'auc': 0.9262}
2025-07-11 08:37:10,928 - INFO - test: {'epoch': 64, 'time_epoch': 3.66073, 'loss': 0.7844979, 'lr': 0, 'params': 541238, 'time_iter': 0.05811, 'accuracy': 0.65826, 'f1': 0.64654, 'accuracy-SBM': 0.65991, 'auc': 0.92832}
2025-07-11 08:37:10,930 - INFO - > Epoch 64: took 85.7s (avg 88.9s) | Best so far: epoch 62	train_loss: 0.7591 train_accuracy-SBM: 0.6679	val_loss: 0.8090 val_accuracy-SBM: 0.6578	test_loss: 0.7905 test_accuracy-SBM: 0.6583
2025-07-11 08:38:27,277 - INFO - train: {'epoch': 65, 'time_epoch': 76.10814, 'eta': 2742.87021, 'eta_hours': 0.76191, 'loss': 0.75201969, 'lr': 0.00029915, 'params': 541238, 'time_iter': 0.12177, 'accuracy': 0.67033, 'f1': 0.66289, 'accuracy-SBM': 0.67031, 'auc': 0.93255}
2025-07-11 08:38:31,031 - INFO - val: {'epoch': 65, 'time_epoch': 3.71161, 'loss': 0.80132678, 'lr': 0, 'params': 541238, 'time_iter': 0.05891, 'accuracy': 0.65652, 'f1': 0.64971, 'accuracy-SBM': 0.65716, 'auc': 0.92674}
2025-07-11 08:38:34,701 - INFO - test: {'epoch': 65, 'time_epoch': 3.63844, 'loss': 0.78397784, 'lr': 0, 'params': 541238, 'time_iter': 0.05775, 'accuracy': 0.65992, 'f1': 0.65197, 'accuracy-SBM': 0.66145, 'auc': 0.92841}
2025-07-11 08:38:34,704 - INFO - > Epoch 65: took 83.8s (avg 88.8s) | Best so far: epoch 62	train_loss: 0.7591 train_accuracy-SBM: 0.6679	val_loss: 0.8090 val_accuracy-SBM: 0.6578	test_loss: 0.7905 test_accuracy-SBM: 0.6583
2025-07-11 08:39:51,380 - INFO - train: {'epoch': 66, 'time_epoch': 76.45143, 'eta': 2660.11844, 'eta_hours': 0.73892, 'loss': 0.74944572, 'lr': 0.00028412, 'params': 541238, 'time_iter': 0.12232, 'accuracy': 0.67021, 'f1': 0.66282, 'accuracy-SBM': 0.67018, 'auc': 0.93287}
2025-07-11 08:39:55,134 - INFO - val: {'epoch': 66, 'time_epoch': 3.71043, 'loss': 0.80354533, 'lr': 0, 'params': 541238, 'time_iter': 0.0589, 'accuracy': 0.65562, 'f1': 0.64864, 'accuracy-SBM': 0.65621, 'auc': 0.9268}
2025-07-11 08:39:58,840 - INFO - test: {'epoch': 66, 'time_epoch': 3.67467, 'loss': 0.78820519, 'lr': 0, 'params': 541238, 'time_iter': 0.05833, 'accuracy': 0.65773, 'f1': 0.65016, 'accuracy-SBM': 0.65906, 'auc': 0.92796}
2025-07-11 08:39:58,843 - INFO - > Epoch 66: took 84.1s (avg 88.7s) | Best so far: epoch 62	train_loss: 0.7591 train_accuracy-SBM: 0.6679	val_loss: 0.8090 val_accuracy-SBM: 0.6578	test_loss: 0.7905 test_accuracy-SBM: 0.6583
2025-07-11 08:41:15,002 - INFO - train: {'epoch': 67, 'time_epoch': 75.92507, 'eta': 2577.30429, 'eta_hours': 0.71592, 'loss': 0.74685239, 'lr': 0.00026933, 'params': 541238, 'time_iter': 0.12148, 'accuracy': 0.67189, 'f1': 0.66449, 'accuracy-SBM': 0.67187, 'auc': 0.93322}
2025-07-11 08:41:18,723 - INFO - val: {'epoch': 67, 'time_epoch': 3.67804, 'loss': 0.79660607, 'lr': 0, 'params': 541238, 'time_iter': 0.05838, 'accuracy': 0.65633, 'f1': 0.6401, 'accuracy-SBM': 0.65681, 'auc': 0.92753}
2025-07-11 08:41:22,386 - INFO - test: {'epoch': 67, 'time_epoch': 3.6304, 'loss': 0.79024449, 'lr': 0, 'params': 541238, 'time_iter': 0.05763, 'accuracy': 0.65858, 'f1': 0.6417, 'accuracy-SBM': 0.65966, 'auc': 0.9278}
2025-07-11 08:41:22,397 - INFO - > Epoch 67: took 83.6s (avg 88.6s) | Best so far: epoch 62	train_loss: 0.7591 train_accuracy-SBM: 0.6679	val_loss: 0.8090 val_accuracy-SBM: 0.6578	test_loss: 0.7905 test_accuracy-SBM: 0.6583
2025-07-11 08:42:39,154 - INFO - train: {'epoch': 68, 'time_epoch': 76.53199, 'eta': 2494.96249, 'eta_hours': 0.69305, 'loss': 0.7466888, 'lr': 0.00025479, 'params': 541238, 'time_iter': 0.12245, 'accuracy': 0.67236, 'f1': 0.66474, 'accuracy-SBM': 0.67234, 'auc': 0.93325}
2025-07-11 08:42:42,959 - INFO - val: {'epoch': 68, 'time_epoch': 3.76099, 'loss': 0.79264254, 'lr': 0, 'params': 541238, 'time_iter': 0.0597, 'accuracy': 0.65634, 'f1': 0.64237, 'accuracy-SBM': 0.65678, 'auc': 0.92753}
2025-07-11 08:42:46,736 - INFO - test: {'epoch': 68, 'time_epoch': 3.74494, 'loss': 0.7817313, 'lr': 0, 'params': 541238, 'time_iter': 0.05944, 'accuracy': 0.66071, 'f1': 0.64624, 'accuracy-SBM': 0.66175, 'auc': 0.92857}
2025-07-11 08:42:46,738 - INFO - > Epoch 68: took 84.3s (avg 88.6s) | Best so far: epoch 62	train_loss: 0.7591 train_accuracy-SBM: 0.6679	val_loss: 0.8090 val_accuracy-SBM: 0.6578	test_loss: 0.7905 test_accuracy-SBM: 0.6583
2025-07-11 08:44:03,735 - INFO - train: {'epoch': 69, 'time_epoch': 76.76006, 'eta': 2412.88443, 'eta_hours': 0.67025, 'loss': 0.74431403, 'lr': 0.00024052, 'params': 541238, 'time_iter': 0.12282, 'accuracy': 0.67321, 'f1': 0.66571, 'accuracy-SBM': 0.67319, 'auc': 0.93361}
2025-07-11 08:44:07,431 - INFO - val: {'epoch': 69, 'time_epoch': 3.65291, 'loss': 0.80507618, 'lr': 0, 'params': 541238, 'time_iter': 0.05798, 'accuracy': 0.65665, 'f1': 0.64703, 'accuracy-SBM': 0.65748, 'auc': 0.92667}
2025-07-11 08:44:11,113 - INFO - test: {'epoch': 69, 'time_epoch': 3.65077, 'loss': 0.79139652, 'lr': 0, 'params': 541238, 'time_iter': 0.05795, 'accuracy': 0.65672, 'f1': 0.6457, 'accuracy-SBM': 0.65842, 'auc': 0.92767}
2025-07-11 08:44:11,115 - INFO - > Epoch 69: took 84.4s (avg 88.5s) | Best so far: epoch 62	train_loss: 0.7591 train_accuracy-SBM: 0.6679	val_loss: 0.8090 val_accuracy-SBM: 0.6578	test_loss: 0.7905 test_accuracy-SBM: 0.6583
2025-07-11 08:45:28,039 - INFO - train: {'epoch': 70, 'time_epoch': 76.70023, 'eta': 2330.93173, 'eta_hours': 0.64748, 'loss': 0.74255876, 'lr': 0.00022653, 'params': 541238, 'time_iter': 0.12272, 'accuracy': 0.67307, 'f1': 0.66565, 'accuracy-SBM': 0.67304, 'auc': 0.93379}
2025-07-11 08:45:31,780 - INFO - val: {'epoch': 70, 'time_epoch': 3.69057, 'loss': 0.80043932, 'lr': 0, 'params': 541238, 'time_iter': 0.05858, 'accuracy': 0.66, 'f1': 0.6538, 'accuracy-SBM': 0.66058, 'auc': 0.9273}
2025-07-11 08:45:35,400 - INFO - test: {'epoch': 70, 'time_epoch': 3.58159, 'loss': 0.78983671, 'lr': 0, 'params': 541238, 'time_iter': 0.05685, 'accuracy': 0.65847, 'f1': 0.65158, 'accuracy-SBM': 0.65989, 'auc': 0.92802}
2025-07-11 08:45:35,402 - INFO - > Epoch 70: took 84.3s (avg 88.5s) | Best so far: epoch 70	train_loss: 0.7426 train_accuracy-SBM: 0.6730	val_loss: 0.8004 val_accuracy-SBM: 0.6606	test_loss: 0.7898 test_accuracy-SBM: 0.6599
2025-07-11 08:46:52,137 - INFO - train: {'epoch': 71, 'time_epoch': 76.51367, 'eta': 2249.05239, 'eta_hours': 0.62474, 'loss': 0.74057378, 'lr': 0.00021284, 'params': 541238, 'time_iter': 0.12242, 'accuracy': 0.67453, 'f1': 0.66697, 'accuracy-SBM': 0.6745, 'auc': 0.93406}
2025-07-11 08:46:55,819 - INFO - val: {'epoch': 71, 'time_epoch': 3.63804, 'loss': 0.80357083, 'lr': 0, 'params': 541238, 'time_iter': 0.05775, 'accuracy': 0.65511, 'f1': 0.63799, 'accuracy-SBM': 0.65553, 'auc': 0.92675}
2025-07-11 08:46:59,453 - INFO - test: {'epoch': 71, 'time_epoch': 3.60312, 'loss': 0.78779248, 'lr': 0, 'params': 541238, 'time_iter': 0.05719, 'accuracy': 0.65895, 'f1': 0.64138, 'accuracy-SBM': 0.65987, 'auc': 0.92799}
2025-07-11 08:46:59,455 - INFO - > Epoch 71: took 84.1s (avg 88.4s) | Best so far: epoch 70	train_loss: 0.7426 train_accuracy-SBM: 0.6730	val_loss: 0.8004 val_accuracy-SBM: 0.6606	test_loss: 0.7898 test_accuracy-SBM: 0.6599
2025-07-11 08:48:15,125 - INFO - train: {'epoch': 72, 'time_epoch': 75.44902, 'eta': 2166.92627, 'eta_hours': 0.60192, 'loss': 0.73887426, 'lr': 0.00019946, 'params': 541238, 'time_iter': 0.12072, 'accuracy': 0.6746, 'f1': 0.66711, 'accuracy-SBM': 0.67458, 'auc': 0.93424}
2025-07-11 08:48:18,902 - INFO - val: {'epoch': 72, 'time_epoch': 3.7347, 'loss': 0.79869462, 'lr': 0, 'params': 541238, 'time_iter': 0.05928, 'accuracy': 0.65913, 'f1': 0.65316, 'accuracy-SBM': 0.65976, 'auc': 0.9272}
2025-07-11 08:48:22,607 - INFO - test: {'epoch': 72, 'time_epoch': 3.67273, 'loss': 0.78399521, 'lr': 0, 'params': 541238, 'time_iter': 0.0583, 'accuracy': 0.66008, 'f1': 0.65317, 'accuracy-SBM': 0.6615, 'auc': 0.92841}
2025-07-11 08:48:22,609 - INFO - > Epoch 72: took 83.2s (avg 88.3s) | Best so far: epoch 70	train_loss: 0.7426 train_accuracy-SBM: 0.6730	val_loss: 0.8004 val_accuracy-SBM: 0.6606	test_loss: 0.7898 test_accuracy-SBM: 0.6599
2025-07-11 08:49:39,442 - INFO - train: {'epoch': 73, 'time_epoch': 76.60708, 'eta': 2085.3875, 'eta_hours': 0.57927, 'loss': 0.73795604, 'lr': 0.00018641, 'params': 541238, 'time_iter': 0.12257, 'accuracy': 0.67562, 'f1': 0.6684, 'accuracy-SBM': 0.6756, 'auc': 0.93445}
2025-07-11 08:49:43,188 - INFO - val: {'epoch': 73, 'time_epoch': 3.70232, 'loss': 0.80320538, 'lr': 0, 'params': 541238, 'time_iter': 0.05877, 'accuracy': 0.66016, 'f1': 0.65404, 'accuracy-SBM': 0.66084, 'auc': 0.92713}
2025-07-11 08:49:46,862 - INFO - test: {'epoch': 73, 'time_epoch': 3.64138, 'loss': 0.78752392, 'lr': 0, 'params': 541238, 'time_iter': 0.0578, 'accuracy': 0.65887, 'f1': 0.65153, 'accuracy-SBM': 0.66034, 'auc': 0.92822}
2025-07-11 08:49:46,864 - INFO - > Epoch 73: took 84.3s (avg 88.3s) | Best so far: epoch 73	train_loss: 0.7380 train_accuracy-SBM: 0.6756	val_loss: 0.8032 val_accuracy-SBM: 0.6608	test_loss: 0.7875 test_accuracy-SBM: 0.6603
2025-07-11 08:51:03,782 - INFO - train: {'epoch': 74, 'time_epoch': 76.58856, 'eta': 2003.97407, 'eta_hours': 0.55666, 'loss': 0.73672493, 'lr': 0.00017371, 'params': 541238, 'time_iter': 0.12254, 'accuracy': 0.67618, 'f1': 0.66882, 'accuracy-SBM': 0.67616, 'auc': 0.93462}
2025-07-11 08:51:07,558 - INFO - val: {'epoch': 74, 'time_epoch': 3.73168, 'loss': 0.79895024, 'lr': 0, 'params': 541238, 'time_iter': 0.05923, 'accuracy': 0.65881, 'f1': 0.65211, 'accuracy-SBM': 0.65942, 'auc': 0.92734}
2025-07-11 08:51:11,296 - INFO - test: {'epoch': 74, 'time_epoch': 3.70709, 'loss': 0.78555195, 'lr': 0, 'params': 541238, 'time_iter': 0.05884, 'accuracy': 0.65923, 'f1': 0.65172, 'accuracy-SBM': 0.66049, 'auc': 0.92824}
2025-07-11 08:51:11,298 - INFO - > Epoch 74: took 84.4s (avg 88.2s) | Best so far: epoch 73	train_loss: 0.7380 train_accuracy-SBM: 0.6756	val_loss: 0.8032 val_accuracy-SBM: 0.6608	test_loss: 0.7875 test_accuracy-SBM: 0.6603
2025-07-11 08:52:30,028 - INFO - train: {'epoch': 75, 'time_epoch': 78.40633, 'eta': 1923.26165, 'eta_hours': 0.53424, 'loss': 0.73610231, 'lr': 0.00016136, 'params': 541238, 'time_iter': 0.12545, 'accuracy': 0.67605, 'f1': 0.66875, 'accuracy-SBM': 0.67603, 'auc': 0.93467}
2025-07-11 08:52:33,754 - INFO - val: {'epoch': 75, 'time_epoch': 3.68294, 'loss': 0.79500906, 'lr': 0, 'params': 541238, 'time_iter': 0.05846, 'accuracy': 0.66036, 'f1': 0.65141, 'accuracy-SBM': 0.66087, 'auc': 0.92775}
2025-07-11 08:52:37,441 - INFO - test: {'epoch': 75, 'time_epoch': 3.65566, 'loss': 0.7832626, 'lr': 0, 'params': 541238, 'time_iter': 0.05803, 'accuracy': 0.66041, 'f1': 0.6507, 'accuracy-SBM': 0.66153, 'auc': 0.92853}
2025-07-11 08:52:37,444 - INFO - > Epoch 75: took 86.1s (avg 88.2s) | Best so far: epoch 75	train_loss: 0.7361 train_accuracy-SBM: 0.6760	val_loss: 0.7950 val_accuracy-SBM: 0.6609	test_loss: 0.7833 test_accuracy-SBM: 0.6615
2025-07-11 08:53:55,199 - INFO - train: {'epoch': 76, 'time_epoch': 77.52226, 'eta': 1842.34505, 'eta_hours': 0.51176, 'loss': 0.73228223, 'lr': 0.00014938, 'params': 541238, 'time_iter': 0.12404, 'accuracy': 0.67688, 'f1': 0.66955, 'accuracy-SBM': 0.67685, 'auc': 0.93513}
2025-07-11 08:53:59,127 - INFO - val: {'epoch': 76, 'time_epoch': 3.88445, 'loss': 0.7953314, 'lr': 0, 'params': 541238, 'time_iter': 0.06166, 'accuracy': 0.65967, 'f1': 0.65447, 'accuracy-SBM': 0.66032, 'auc': 0.92764}
2025-07-11 08:54:03,027 - INFO - test: {'epoch': 76, 'time_epoch': 3.86755, 'loss': 0.78150344, 'lr': 0, 'params': 541238, 'time_iter': 0.06139, 'accuracy': 0.65949, 'f1': 0.65383, 'accuracy-SBM': 0.6609, 'auc': 0.92869}
2025-07-11 08:54:03,029 - INFO - > Epoch 76: took 85.6s (avg 88.2s) | Best so far: epoch 75	train_loss: 0.7361 train_accuracy-SBM: 0.6760	val_loss: 0.7950 val_accuracy-SBM: 0.6609	test_loss: 0.7833 test_accuracy-SBM: 0.6615
2025-07-11 08:55:20,652 - INFO - train: {'epoch': 77, 'time_epoch': 77.3024, 'eta': 1761.45347, 'eta_hours': 0.48929, 'loss': 0.7315201, 'lr': 0.00013779, 'params': 541238, 'time_iter': 0.12368, 'accuracy': 0.67791, 'f1': 0.67061, 'accuracy-SBM': 0.67789, 'auc': 0.93528}
2025-07-11 08:55:24,428 - INFO - val: {'epoch': 77, 'time_epoch': 3.73282, 'loss': 0.80087345, 'lr': 0, 'params': 541238, 'time_iter': 0.05925, 'accuracy': 0.65823, 'f1': 0.65181, 'accuracy-SBM': 0.65896, 'auc': 0.92734}
2025-07-11 08:55:28,178 - INFO - test: {'epoch': 77, 'time_epoch': 3.71756, 'loss': 0.78844235, 'lr': 0, 'params': 541238, 'time_iter': 0.05901, 'accuracy': 0.65917, 'f1': 0.65195, 'accuracy-SBM': 0.66066, 'auc': 0.92808}
2025-07-11 08:55:28,180 - INFO - > Epoch 77: took 85.2s (avg 88.1s) | Best so far: epoch 75	train_loss: 0.7361 train_accuracy-SBM: 0.6760	val_loss: 0.7950 val_accuracy-SBM: 0.6609	test_loss: 0.7833 test_accuracy-SBM: 0.6615
2025-07-11 08:56:45,139 - INFO - train: {'epoch': 78, 'time_epoch': 76.74129, 'eta': 1680.5036, 'eta_hours': 0.46681, 'loss': 0.72980541, 'lr': 0.00012659, 'params': 541238, 'time_iter': 0.12279, 'accuracy': 0.67783, 'f1': 0.6705, 'accuracy-SBM': 0.67781, 'auc': 0.93545}
2025-07-11 08:56:48,891 - INFO - val: {'epoch': 78, 'time_epoch': 3.70916, 'loss': 0.80387099, 'lr': 0, 'params': 541238, 'time_iter': 0.05888, 'accuracy': 0.65819, 'f1': 0.64925, 'accuracy-SBM': 0.65879, 'auc': 0.92676}
2025-07-11 08:56:52,715 - INFO - test: {'epoch': 78, 'time_epoch': 3.79179, 'loss': 0.78734859, 'lr': 0, 'params': 541238, 'time_iter': 0.06019, 'accuracy': 0.65959, 'f1': 0.64964, 'accuracy-SBM': 0.66085, 'auc': 0.92819}
2025-07-11 08:56:52,717 - INFO - > Epoch 78: took 84.5s (avg 88.1s) | Best so far: epoch 75	train_loss: 0.7361 train_accuracy-SBM: 0.6760	val_loss: 0.7950 val_accuracy-SBM: 0.6609	test_loss: 0.7833 test_accuracy-SBM: 0.6615
2025-07-11 08:58:09,622 - INFO - train: {'epoch': 79, 'time_epoch': 76.67562, 'eta': 1599.64253, 'eta_hours': 0.44435, 'loss': 0.72945094, 'lr': 0.0001158, 'params': 541238, 'time_iter': 0.12268, 'accuracy': 0.67841, 'f1': 0.67126, 'accuracy-SBM': 0.67839, 'auc': 0.93554}
2025-07-11 08:58:13,295 - INFO - val: {'epoch': 79, 'time_epoch': 3.63064, 'loss': 0.80396899, 'lr': 0, 'params': 541238, 'time_iter': 0.05763, 'accuracy': 0.65874, 'f1': 0.65294, 'accuracy-SBM': 0.65932, 'auc': 0.92715}
2025-07-11 08:58:16,974 - INFO - test: {'epoch': 79, 'time_epoch': 3.63954, 'loss': 0.78734278, 'lr': 0, 'params': 541238, 'time_iter': 0.05777, 'accuracy': 0.66028, 'f1': 0.65387, 'accuracy-SBM': 0.66162, 'auc': 0.92845}
2025-07-11 08:58:16,978 - INFO - > Epoch 79: took 84.3s (avg 88.0s) | Best so far: epoch 75	train_loss: 0.7361 train_accuracy-SBM: 0.6760	val_loss: 0.7950 val_accuracy-SBM: 0.6609	test_loss: 0.7833 test_accuracy-SBM: 0.6615
2025-07-11 08:59:33,756 - INFO - train: {'epoch': 80, 'time_epoch': 76.56098, 'eta': 1518.85791, 'eta_hours': 0.4219, 'loss': 0.72754591, 'lr': 0.00010543, 'params': 541238, 'time_iter': 0.1225, 'accuracy': 0.67945, 'f1': 0.67228, 'accuracy-SBM': 0.67943, 'auc': 0.93582}
2025-07-11 08:59:37,546 - INFO - val: {'epoch': 80, 'time_epoch': 3.74739, 'loss': 0.79973211, 'lr': 0, 'params': 541238, 'time_iter': 0.05948, 'accuracy': 0.65894, 'f1': 0.65203, 'accuracy-SBM': 0.65963, 'auc': 0.92744}
2025-07-11 08:59:41,257 - INFO - test: {'epoch': 80, 'time_epoch': 3.67846, 'loss': 0.78291248, 'lr': 0, 'params': 541238, 'time_iter': 0.05839, 'accuracy': 0.65986, 'f1': 0.65239, 'accuracy-SBM': 0.66137, 'auc': 0.92876}
2025-07-11 08:59:41,258 - INFO - > Epoch 80: took 84.3s (avg 88.0s) | Best so far: epoch 75	train_loss: 0.7361 train_accuracy-SBM: 0.6760	val_loss: 0.7950 val_accuracy-SBM: 0.6609	test_loss: 0.7833 test_accuracy-SBM: 0.6615
2025-07-11 09:00:58,242 - INFO - train: {'epoch': 81, 'time_epoch': 76.76216, 'eta': 1438.22047, 'eta_hours': 0.39951, 'loss': 0.72651149, 'lr': 9.549e-05, 'params': 541238, 'time_iter': 0.12282, 'accuracy': 0.67983, 'f1': 0.67269, 'accuracy-SBM': 0.67981, 'auc': 0.93597}
2025-07-11 09:01:02,030 - INFO - val: {'epoch': 81, 'time_epoch': 3.74475, 'loss': 0.80183708, 'lr': 0, 'params': 541238, 'time_iter': 0.05944, 'accuracy': 0.65996, 'f1': 0.65014, 'accuracy-SBM': 0.66047, 'auc': 0.92712}
2025-07-11 09:01:05,744 - INFO - test: {'epoch': 81, 'time_epoch': 3.68065, 'loss': 0.78286027, 'lr': 0, 'params': 541238, 'time_iter': 0.05842, 'accuracy': 0.66021, 'f1': 0.6493, 'accuracy-SBM': 0.66138, 'auc': 0.92872}
2025-07-11 09:01:05,746 - INFO - > Epoch 81: took 84.5s (avg 87.9s) | Best so far: epoch 75	train_loss: 0.7361 train_accuracy-SBM: 0.6760	val_loss: 0.7950 val_accuracy-SBM: 0.6609	test_loss: 0.7833 test_accuracy-SBM: 0.6615
2025-07-11 09:02:22,328 - INFO - train: {'epoch': 82, 'time_epoch': 76.36278, 'eta': 1357.59461, 'eta_hours': 0.37711, 'loss': 0.72482045, 'lr': 8.6e-05, 'params': 541238, 'time_iter': 0.12218, 'accuracy': 0.6802, 'f1': 0.67263, 'accuracy-SBM': 0.68018, 'auc': 0.93617}
2025-07-11 09:02:26,095 - INFO - val: {'epoch': 82, 'time_epoch': 3.71496, 'loss': 0.80079465, 'lr': 0, 'params': 541238, 'time_iter': 0.05897, 'accuracy': 0.65862, 'f1': 0.65326, 'accuracy-SBM': 0.65926, 'auc': 0.92731}
2025-07-11 09:02:29,818 - INFO - test: {'epoch': 82, 'time_epoch': 3.68444, 'loss': 0.78730308, 'lr': 0, 'params': 541238, 'time_iter': 0.05848, 'accuracy': 0.65998, 'f1': 0.65432, 'accuracy-SBM': 0.66139, 'auc': 0.92834}
2025-07-11 09:02:29,821 - INFO - > Epoch 82: took 84.1s (avg 87.9s) | Best so far: epoch 75	train_loss: 0.7361 train_accuracy-SBM: 0.6760	val_loss: 0.7950 val_accuracy-SBM: 0.6609	test_loss: 0.7833 test_accuracy-SBM: 0.6615
2025-07-11 09:03:47,044 - INFO - train: {'epoch': 83, 'time_epoch': 76.79997, 'eta': 1277.15353, 'eta_hours': 0.35476, 'loss': 0.72435773, 'lr': 7.695e-05, 'params': 541238, 'time_iter': 0.12288, 'accuracy': 0.68063, 'f1': 0.6736, 'accuracy-SBM': 0.68061, 'auc': 0.93627}
2025-07-11 09:03:50,773 - INFO - val: {'epoch': 83, 'time_epoch': 3.68624, 'loss': 0.80243985, 'lr': 0, 'params': 541238, 'time_iter': 0.05851, 'accuracy': 0.65914, 'f1': 0.65296, 'accuracy-SBM': 0.65974, 'auc': 0.92709}
2025-07-11 09:03:54,484 - INFO - test: {'epoch': 83, 'time_epoch': 3.67886, 'loss': 0.78668538, 'lr': 0, 'params': 541238, 'time_iter': 0.05839, 'accuracy': 0.66056, 'f1': 0.65389, 'accuracy-SBM': 0.66188, 'auc': 0.92834}
2025-07-11 09:03:54,486 - INFO - > Epoch 83: took 84.7s (avg 87.8s) | Best so far: epoch 75	train_loss: 0.7361 train_accuracy-SBM: 0.6760	val_loss: 0.7950 val_accuracy-SBM: 0.6609	test_loss: 0.7833 test_accuracy-SBM: 0.6615
2025-07-11 09:05:10,786 - INFO - train: {'epoch': 84, 'time_epoch': 76.07388, 'eta': 1196.66998, 'eta_hours': 0.33241, 'loss': 0.72232304, 'lr': 6.837e-05, 'params': 541238, 'time_iter': 0.12172, 'accuracy': 0.68099, 'f1': 0.67368, 'accuracy-SBM': 0.68097, 'auc': 0.93641}
2025-07-11 09:05:14,471 - INFO - val: {'epoch': 84, 'time_epoch': 3.64195, 'loss': 0.80545514, 'lr': 0, 'params': 541238, 'time_iter': 0.05781, 'accuracy': 0.65892, 'f1': 0.65276, 'accuracy-SBM': 0.65957, 'auc': 0.927}
2025-07-11 09:05:18,155 - INFO - test: {'epoch': 84, 'time_epoch': 3.65304, 'loss': 0.78600393, 'lr': 0, 'params': 541238, 'time_iter': 0.05798, 'accuracy': 0.66023, 'f1': 0.65363, 'accuracy-SBM': 0.66166, 'auc': 0.9287}
2025-07-11 09:05:18,157 - INFO - > Epoch 84: took 83.7s (avg 87.8s) | Best so far: epoch 75	train_loss: 0.7361 train_accuracy-SBM: 0.6760	val_loss: 0.7950 val_accuracy-SBM: 0.6609	test_loss: 0.7833 test_accuracy-SBM: 0.6615
2025-07-11 09:06:35,631 - INFO - train: {'epoch': 85, 'time_epoch': 77.18415, 'eta': 1116.46973, 'eta_hours': 0.31013, 'loss': 0.72303508, 'lr': 6.026e-05, 'params': 541238, 'time_iter': 0.12349, 'accuracy': 0.68083, 'f1': 0.67369, 'accuracy-SBM': 0.6808, 'auc': 0.93638}
2025-07-11 09:06:39,810 - INFO - val: {'epoch': 85, 'time_epoch': 4.13155, 'loss': 0.8042576, 'lr': 0, 'params': 541238, 'time_iter': 0.06558, 'accuracy': 0.65976, 'f1': 0.65375, 'accuracy-SBM': 0.66037, 'auc': 0.92691}
2025-07-11 09:06:43,662 - INFO - test: {'epoch': 85, 'time_epoch': 3.81618, 'loss': 0.78591075, 'lr': 0, 'params': 541238, 'time_iter': 0.06057, 'accuracy': 0.66066, 'f1': 0.65421, 'accuracy-SBM': 0.662, 'auc': 0.92848}
2025-07-11 09:06:43,664 - INFO - > Epoch 85: took 85.5s (avg 87.8s) | Best so far: epoch 75	train_loss: 0.7361 train_accuracy-SBM: 0.6760	val_loss: 0.7950 val_accuracy-SBM: 0.6609	test_loss: 0.7833 test_accuracy-SBM: 0.6615
2025-07-11 09:08:02,196 - INFO - train: {'epoch': 86, 'time_epoch': 78.30453, 'eta': 1036.50622, 'eta_hours': 0.28792, 'loss': 0.72181864, 'lr': 5.264e-05, 'params': 541238, 'time_iter': 0.12529, 'accuracy': 0.68108, 'f1': 0.67402, 'accuracy-SBM': 0.68106, 'auc': 0.93655}
2025-07-11 09:08:06,030 - INFO - val: {'epoch': 86, 'time_epoch': 3.79005, 'loss': 0.80342886, 'lr': 0, 'params': 541238, 'time_iter': 0.06016, 'accuracy': 0.65959, 'f1': 0.65366, 'accuracy-SBM': 0.66017, 'auc': 0.9272}
2025-07-11 09:08:09,805 - INFO - test: {'epoch': 86, 'time_epoch': 3.74075, 'loss': 0.78515785, 'lr': 0, 'params': 541238, 'time_iter': 0.05938, 'accuracy': 0.6606, 'f1': 0.65423, 'accuracy-SBM': 0.66193, 'auc': 0.92865}
2025-07-11 09:08:09,807 - INFO - > Epoch 86: took 86.1s (avg 87.8s) | Best so far: epoch 75	train_loss: 0.7361 train_accuracy-SBM: 0.6760	val_loss: 0.7950 val_accuracy-SBM: 0.6609	test_loss: 0.7833 test_accuracy-SBM: 0.6615
2025-07-11 09:09:27,484 - INFO - train: {'epoch': 87, 'time_epoch': 77.34769, 'eta': 956.44994, 'eta_hours': 0.26568, 'loss': 0.72057266, 'lr': 4.55e-05, 'params': 541238, 'time_iter': 0.12376, 'accuracy': 0.68217, 'f1': 0.67516, 'accuracy-SBM': 0.68215, 'auc': 0.93674}
2025-07-11 09:09:31,304 - INFO - val: {'epoch': 87, 'time_epoch': 3.77552, 'loss': 0.80144925, 'lr': 0, 'params': 541238, 'time_iter': 0.05993, 'accuracy': 0.66034, 'f1': 0.65406, 'accuracy-SBM': 0.66095, 'auc': 0.9272}
2025-07-11 09:09:35,079 - INFO - test: {'epoch': 87, 'time_epoch': 3.74227, 'loss': 0.78675064, 'lr': 0, 'params': 541238, 'time_iter': 0.0594, 'accuracy': 0.66062, 'f1': 0.65384, 'accuracy-SBM': 0.66191, 'auc': 0.92826}
2025-07-11 09:09:35,081 - INFO - > Epoch 87: took 85.3s (avg 87.7s) | Best so far: epoch 87	train_loss: 0.7206 train_accuracy-SBM: 0.6822	val_loss: 0.8014 val_accuracy-SBM: 0.6610	test_loss: 0.7868 test_accuracy-SBM: 0.6619
2025-07-11 09:10:52,128 - INFO - train: {'epoch': 88, 'time_epoch': 76.82891, 'eta': 876.39041, 'eta_hours': 0.24344, 'loss': 0.71991285, 'lr': 3.886e-05, 'params': 541238, 'time_iter': 0.12293, 'accuracy': 0.68224, 'f1': 0.67504, 'accuracy-SBM': 0.68221, 'auc': 0.9368}
2025-07-11 09:10:55,776 - INFO - val: {'epoch': 88, 'time_epoch': 3.5865, 'loss': 0.80273513, 'lr': 0, 'params': 541238, 'time_iter': 0.05693, 'accuracy': 0.66162, 'f1': 0.65551, 'accuracy-SBM': 0.66227, 'auc': 0.9273}
2025-07-11 09:10:59,377 - INFO - test: {'epoch': 88, 'time_epoch': 3.56924, 'loss': 0.78571722, 'lr': 0, 'params': 541238, 'time_iter': 0.05665, 'accuracy': 0.66102, 'f1': 0.65413, 'accuracy-SBM': 0.66236, 'auc': 0.92861}
2025-07-11 09:10:59,379 - INFO - > Epoch 88: took 84.3s (avg 87.7s) | Best so far: epoch 88	train_loss: 0.7199 train_accuracy-SBM: 0.6822	val_loss: 0.8027 val_accuracy-SBM: 0.6623	test_loss: 0.7857 test_accuracy-SBM: 0.6624
2025-07-11 09:12:14,243 - INFO - train: {'epoch': 89, 'time_epoch': 74.6466, 'eta': 796.1602, 'eta_hours': 0.22116, 'loss': 0.71932878, 'lr': 3.272e-05, 'params': 541238, 'time_iter': 0.11943, 'accuracy': 0.68236, 'f1': 0.67514, 'accuracy-SBM': 0.68234, 'auc': 0.9369}
2025-07-11 09:12:17,956 - INFO - val: {'epoch': 89, 'time_epoch': 3.66974, 'loss': 0.80182836, 'lr': 0, 'params': 541238, 'time_iter': 0.05825, 'accuracy': 0.66116, 'f1': 0.6556, 'accuracy-SBM': 0.66175, 'auc': 0.92736}
2025-07-11 09:12:21,598 - INFO - test: {'epoch': 89, 'time_epoch': 3.60302, 'loss': 0.7866673, 'lr': 0, 'params': 541238, 'time_iter': 0.05719, 'accuracy': 0.65939, 'f1': 0.65294, 'accuracy-SBM': 0.66071, 'auc': 0.92839}
2025-07-11 09:12:21,654 - INFO - > Epoch 89: took 82.3s (avg 87.6s) | Best so far: epoch 88	train_loss: 0.7199 train_accuracy-SBM: 0.6822	val_loss: 0.8027 val_accuracy-SBM: 0.6623	test_loss: 0.7857 test_accuracy-SBM: 0.6624
2025-07-11 09:13:37,708 - INFO - train: {'epoch': 90, 'time_epoch': 75.83624, 'eta': 716.17035, 'eta_hours': 0.19894, 'loss': 0.71858589, 'lr': 2.709e-05, 'params': 541238, 'time_iter': 0.12134, 'accuracy': 0.68249, 'f1': 0.67523, 'accuracy-SBM': 0.68246, 'auc': 0.93695}
2025-07-11 09:13:41,408 - INFO - val: {'epoch': 90, 'time_epoch': 3.65746, 'loss': 0.80002996, 'lr': 0, 'params': 541238, 'time_iter': 0.05805, 'accuracy': 0.65993, 'f1': 0.65349, 'accuracy-SBM': 0.66056, 'auc': 0.92737}
2025-07-11 09:13:45,072 - INFO - test: {'epoch': 90, 'time_epoch': 3.6333, 'loss': 0.78510824, 'lr': 0, 'params': 541238, 'time_iter': 0.05767, 'accuracy': 0.66013, 'f1': 0.65294, 'accuracy-SBM': 0.6615, 'auc': 0.92844}
2025-07-11 09:13:45,074 - INFO - > Epoch 90: took 83.4s (avg 87.6s) | Best so far: epoch 88	train_loss: 0.7199 train_accuracy-SBM: 0.6822	val_loss: 0.8027 val_accuracy-SBM: 0.6623	test_loss: 0.7857 test_accuracy-SBM: 0.6624
2025-07-11 09:15:01,628 - INFO - train: {'epoch': 91, 'time_epoch': 76.32659, 'eta': 636.31344, 'eta_hours': 0.17675, 'loss': 0.72119463, 'lr': 2.198e-05, 'params': 541238, 'time_iter': 0.12212, 'accuracy': 0.68252, 'f1': 0.67544, 'accuracy-SBM': 0.6825, 'auc': 0.93673}
2025-07-11 09:15:05,714 - INFO - val: {'epoch': 91, 'time_epoch': 4.03207, 'loss': 0.80078056, 'lr': 0, 'params': 541238, 'time_iter': 0.064, 'accuracy': 0.65998, 'f1': 0.65424, 'accuracy-SBM': 0.66059, 'auc': 0.92737}
2025-07-11 09:15:09,653 - INFO - test: {'epoch': 91, 'time_epoch': 3.90714, 'loss': 0.78449847, 'lr': 0, 'params': 541238, 'time_iter': 0.06202, 'accuracy': 0.66062, 'f1': 0.65436, 'accuracy-SBM': 0.66195, 'auc': 0.92864}
2025-07-11 09:15:09,655 - INFO - > Epoch 91: took 84.6s (avg 87.5s) | Best so far: epoch 88	train_loss: 0.7199 train_accuracy-SBM: 0.6822	val_loss: 0.8027 val_accuracy-SBM: 0.6623	test_loss: 0.7857 test_accuracy-SBM: 0.6624
2025-07-11 09:16:25,555 - INFO - train: {'epoch': 92, 'time_epoch': 75.68007, 'eta': 556.48379, 'eta_hours': 0.15458, 'loss': 0.71874634, 'lr': 1.74e-05, 'params': 541238, 'time_iter': 0.12109, 'accuracy': 0.68242, 'f1': 0.67542, 'accuracy-SBM': 0.6824, 'auc': 0.93697}
2025-07-11 09:16:29,261 - INFO - val: {'epoch': 92, 'time_epoch': 3.66341, 'loss': 0.80190238, 'lr': 0, 'params': 541238, 'time_iter': 0.05815, 'accuracy': 0.65987, 'f1': 0.65403, 'accuracy-SBM': 0.66051, 'auc': 0.92725}
2025-07-11 09:16:32,962 - INFO - test: {'epoch': 92, 'time_epoch': 3.66917, 'loss': 0.78382192, 'lr': 0, 'params': 541238, 'time_iter': 0.05824, 'accuracy': 0.66096, 'f1': 0.65463, 'accuracy-SBM': 0.6623, 'auc': 0.9287}
2025-07-11 09:16:32,964 - INFO - > Epoch 92: took 83.3s (avg 87.5s) | Best so far: epoch 88	train_loss: 0.7199 train_accuracy-SBM: 0.6822	val_loss: 0.8027 val_accuracy-SBM: 0.6623	test_loss: 0.7857 test_accuracy-SBM: 0.6624
2025-07-11 09:17:49,847 - INFO - train: {'epoch': 93, 'time_epoch': 76.65731, 'eta': 476.80481, 'eta_hours': 0.13245, 'loss': 0.71802322, 'lr': 1.334e-05, 'params': 541238, 'time_iter': 0.12265, 'accuracy': 0.68268, 'f1': 0.67556, 'accuracy-SBM': 0.68265, 'auc': 0.93709}
2025-07-11 09:17:53,628 - INFO - val: {'epoch': 93, 'time_epoch': 3.73797, 'loss': 0.8035923, 'lr': 0, 'params': 541238, 'time_iter': 0.05933, 'accuracy': 0.66006, 'f1': 0.65395, 'accuracy-SBM': 0.66065, 'auc': 0.92704}
2025-07-11 09:17:57,381 - INFO - test: {'epoch': 93, 'time_epoch': 3.72092, 'loss': 0.78439033, 'lr': 0, 'params': 541238, 'time_iter': 0.05906, 'accuracy': 0.66111, 'f1': 0.65434, 'accuracy-SBM': 0.6624, 'auc': 0.92866}
2025-07-11 09:17:57,383 - INFO - > Epoch 93: took 84.4s (avg 87.5s) | Best so far: epoch 88	train_loss: 0.7199 train_accuracy-SBM: 0.6822	val_loss: 0.8027 val_accuracy-SBM: 0.6623	test_loss: 0.7857 test_accuracy-SBM: 0.6624
2025-07-11 09:19:13,463 - INFO - train: {'epoch': 94, 'time_epoch': 75.85206, 'eta': 397.14705, 'eta_hours': 0.11032, 'loss': 0.71719576, 'lr': 9.81e-06, 'params': 541238, 'time_iter': 0.12136, 'accuracy': 0.68296, 'f1': 0.67582, 'accuracy-SBM': 0.68294, 'auc': 0.93715}
2025-07-11 09:19:17,211 - INFO - val: {'epoch': 94, 'time_epoch': 3.7052, 'loss': 0.80079649, 'lr': 0, 'params': 541238, 'time_iter': 0.05881, 'accuracy': 0.66019, 'f1': 0.65393, 'accuracy-SBM': 0.66082, 'auc': 0.92728}
2025-07-11 09:19:20,893 - INFO - test: {'epoch': 94, 'time_epoch': 3.64331, 'loss': 0.78455891, 'lr': 0, 'params': 541238, 'time_iter': 0.05783, 'accuracy': 0.66156, 'f1': 0.65463, 'accuracy-SBM': 0.66288, 'auc': 0.92858}
2025-07-11 09:19:20,895 - INFO - > Epoch 94: took 83.5s (avg 87.4s) | Best so far: epoch 88	train_loss: 0.7199 train_accuracy-SBM: 0.6822	val_loss: 0.8027 val_accuracy-SBM: 0.6623	test_loss: 0.7857 test_accuracy-SBM: 0.6624
2025-07-11 09:20:37,143 - INFO - train: {'epoch': 95, 'time_epoch': 76.02834, 'eta': 317.57593, 'eta_hours': 0.08822, 'loss': 0.71592876, 'lr': 6.82e-06, 'params': 541238, 'time_iter': 0.12165, 'accuracy': 0.68389, 'f1': 0.67664, 'accuracy-SBM': 0.68386, 'auc': 0.93734}
2025-07-11 09:20:40,862 - INFO - val: {'epoch': 95, 'time_epoch': 3.6748, 'loss': 0.80206675, 'lr': 0, 'params': 541238, 'time_iter': 0.05833, 'accuracy': 0.65985, 'f1': 0.6537, 'accuracy-SBM': 0.66046, 'auc': 0.92727}
2025-07-11 09:20:44,568 - INFO - test: {'epoch': 95, 'time_epoch': 3.67185, 'loss': 0.78514238, 'lr': 0, 'params': 541238, 'time_iter': 0.05828, 'accuracy': 0.66134, 'f1': 0.65466, 'accuracy-SBM': 0.66265, 'auc': 0.92865}
2025-07-11 09:20:44,570 - INFO - > Epoch 95: took 83.7s (avg 87.4s) | Best so far: epoch 88	train_loss: 0.7199 train_accuracy-SBM: 0.6822	val_loss: 0.8027 val_accuracy-SBM: 0.6623	test_loss: 0.7857 test_accuracy-SBM: 0.6624
2025-07-11 09:22:01,968 - INFO - train: {'epoch': 96, 'time_epoch': 77.16934, 'eta': 238.11315, 'eta_hours': 0.06614, 'loss': 0.71682593, 'lr': 4.37e-06, 'params': 541238, 'time_iter': 0.12347, 'accuracy': 0.68334, 'f1': 0.67618, 'accuracy-SBM': 0.68331, 'auc': 0.93725}
2025-07-11 09:22:05,743 - INFO - val: {'epoch': 96, 'time_epoch': 3.73232, 'loss': 0.80513715, 'lr': 0, 'params': 541238, 'time_iter': 0.05924, 'accuracy': 0.65982, 'f1': 0.65321, 'accuracy-SBM': 0.66042, 'auc': 0.92695}
2025-07-11 09:22:09,434 - INFO - test: {'epoch': 96, 'time_epoch': 3.65877, 'loss': 0.78651107, 'lr': 0, 'params': 541238, 'time_iter': 0.05808, 'accuracy': 0.66075, 'f1': 0.65354, 'accuracy-SBM': 0.66207, 'auc': 0.92847}
2025-07-11 09:22:09,436 - INFO - > Epoch 96: took 84.9s (avg 87.4s) | Best so far: epoch 88	train_loss: 0.7199 train_accuracy-SBM: 0.6822	val_loss: 0.8027 val_accuracy-SBM: 0.6623	test_loss: 0.7857 test_accuracy-SBM: 0.6624
2025-07-11 09:23:25,884 - INFO - train: {'epoch': 97, 'time_epoch': 76.22932, 'eta': 158.67798, 'eta_hours': 0.04408, 'loss': 0.71722029, 'lr': 2.46e-06, 'params': 541238, 'time_iter': 0.12197, 'accuracy': 0.68297, 'f1': 0.67565, 'accuracy-SBM': 0.68295, 'auc': 0.93714}
2025-07-11 09:23:29,678 - INFO - val: {'epoch': 97, 'time_epoch': 3.74996, 'loss': 0.80406759, 'lr': 0, 'params': 541238, 'time_iter': 0.05952, 'accuracy': 0.66038, 'f1': 0.65437, 'accuracy-SBM': 0.66098, 'auc': 0.92707}
2025-07-11 09:23:33,350 - INFO - test: {'epoch': 97, 'time_epoch': 3.63994, 'loss': 0.78625414, 'lr': 0, 'params': 541238, 'time_iter': 0.05778, 'accuracy': 0.66078, 'f1': 0.65424, 'accuracy-SBM': 0.66209, 'auc': 0.92846}
2025-07-11 09:23:33,352 - INFO - > Epoch 97: took 83.9s (avg 87.3s) | Best so far: epoch 88	train_loss: 0.7199 train_accuracy-SBM: 0.6822	val_loss: 0.8027 val_accuracy-SBM: 0.6623	test_loss: 0.7857 test_accuracy-SBM: 0.6624
2025-07-11 09:24:49,640 - INFO - train: {'epoch': 98, 'time_epoch': 76.06811, 'eta': 79.30595, 'eta_hours': 0.02203, 'loss': 0.7161983, 'lr': 1.09e-06, 'params': 541238, 'time_iter': 0.12171, 'accuracy': 0.6833, 'f1': 0.67619, 'accuracy-SBM': 0.68327, 'auc': 0.93733}
2025-07-11 09:24:53,359 - INFO - val: {'epoch': 98, 'time_epoch': 3.66864, 'loss': 0.8045161, 'lr': 0, 'params': 541238, 'time_iter': 0.05823, 'accuracy': 0.65969, 'f1': 0.65334, 'accuracy-SBM': 0.6603, 'auc': 0.92706}
2025-07-11 09:24:57,061 - INFO - test: {'epoch': 98, 'time_epoch': 3.67029, 'loss': 0.78529319, 'lr': 0, 'params': 541238, 'time_iter': 0.05826, 'accuracy': 0.66128, 'f1': 0.65434, 'accuracy-SBM': 0.66261, 'auc': 0.92865}
2025-07-11 09:24:57,063 - INFO - > Epoch 98: took 83.7s (avg 87.3s) | Best so far: epoch 88	train_loss: 0.7199 train_accuracy-SBM: 0.6822	val_loss: 0.8027 val_accuracy-SBM: 0.6623	test_loss: 0.7857 test_accuracy-SBM: 0.6624
2025-07-11 09:26:13,695 - INFO - train: {'epoch': 99, 'time_epoch': 76.40843, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.71823233, 'lr': 2.7e-07, 'params': 541238, 'time_iter': 0.12225, 'accuracy': 0.68331, 'f1': 0.67613, 'accuracy-SBM': 0.68328, 'auc': 0.93706}
2025-07-11 09:26:17,420 - INFO - val: {'epoch': 99, 'time_epoch': 3.68107, 'loss': 0.80496711, 'lr': 0, 'params': 541238, 'time_iter': 0.05843, 'accuracy': 0.66017, 'f1': 0.65421, 'accuracy-SBM': 0.66078, 'auc': 0.92699}
2025-07-11 09:26:21,051 - INFO - test: {'epoch': 99, 'time_epoch': 3.60027, 'loss': 0.78592208, 'lr': 0, 'params': 541238, 'time_iter': 0.05715, 'accuracy': 0.66147, 'f1': 0.65494, 'accuracy-SBM': 0.66279, 'auc': 0.9286}
2025-07-11 09:26:21,272 - INFO - > Epoch 99: took 84.0s (avg 87.3s) | Best so far: epoch 88	train_loss: 0.7199 train_accuracy-SBM: 0.6822	val_loss: 0.8027 val_accuracy-SBM: 0.6623	test_loss: 0.7857 test_accuracy-SBM: 0.6624
2025-07-11 09:26:21,273 - INFO - Avg time per epoch: 87.26s
2025-07-11 09:26:21,273 - INFO - Total train loop time: 2.42h
2025-07-11 09:26:21,274 - INFO - Task done, results saved in results/Cluster/Cluster-GATEDGCN-41
2025-07-11 09:26:21,274 - INFO - Total time: 8791.09s (2.44h)
2025-07-11 09:26:21,292 - INFO - Results aggregated across runs saved in results/Cluster/Cluster-GATEDGCN-41/agg
2025-07-11 09:26:21,292 - INFO - === OPTIMIZED TRAINING COMPLETED SUCCESSFULLY! ===
2025-07-11 09:26:21,292 - INFO - Results saved in: results/Cluster/Cluster-GATEDGCN-41
2025-07-11 09:26:21,292 - INFO - Test results JSON files saved in: results/Cluster/Cluster-GATEDGCN-41/test_results/
Completed seed 41. Results saved in results/Cluster/Cluster-GATEDGCN-41
----------------------------------------
Submitting next job for seed 45
Submitted batch job 5348529
