Lmod has detected the following error: The following module(s) are unknown:
"cuda/12.0"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore-cache load "cuda/12.0"

Also make sure that all modulefiles written in TCL start with the string
#%Module



              total        used        free      shared  buff/cache   available
Mem:          376Gi        18Gi       298Gi       2.0Gi        60Gi       353Gi
Swap:         1.9Gi       3.0Mi       1.9Gi
Fri Jul 11 12:16:32 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:1D:00.0 Off |                    0 |
| N/A   30C    P0             42W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Running experiment with seed: 47
Starting training for seed 47...
Changed working directory to: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GATEDGCN
Loading config from: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GATEDGCN/confignas.yaml
Using device: cuda
2025-07-11 12:17:59,889 - INFO - GPU Mem: 34.1GB
2025-07-11 12:17:59,889 - INFO - Run directory: results/Cluster/Cluster-GATEDGCN-47
2025-07-11 12:17:59,889 - INFO - Seed: 47
2025-07-11 12:17:59,889 - INFO - === OPTIMIZED MOE CONFIGURATION ===
2025-07-11 12:17:59,889 - INFO - Routing mode: none
2025-07-11 12:17:59,889 - INFO - Expert types: ['GINE', 'CustomGatedGCN', 'GATV2']
2025-07-11 12:17:59,889 - INFO - Number of layers: 16
2025-07-11 12:17:59,890 - INFO - Uncertainty enabled: False
2025-07-11 12:17:59,890 - INFO - Training mode: custom
2025-07-11 12:17:59,890 - INFO - Optimization: Uncertainty only for last layer + test/val
2025-07-11 12:17:59,890 - INFO - Additional features: Router weights logging + JSON export
2025-07-11 12:18:13,322 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 12:18:13,325 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 12:18:13,383 - INFO -   undirected: True
2025-07-11 12:18:13,383 - INFO -   num graphs: 12000
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 12:18:13,383 - INFO -   avg num_nodes/graph: 117
2025-07-11 12:18:13,383 - INFO -   num node features: 7
2025-07-11 12:18:13,384 - INFO -   num edge features: 0
2025-07-11 12:18:13,385 - INFO -   num classes: 6
2025-07-11 12:18:13,385 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 12:18:13,385 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 12:18:13,393 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s] 17%|█▋        | 2056/12000 [00:10<00:48, 205.57it/s] 34%|███▍      | 4078/12000 [00:20<00:38, 203.58it/s] 51%|█████     | 6126/12000 [00:30<00:28, 204.13it/s] 68%|██████▊   | 8176/12000 [00:40<00:18, 204.44it/s] 85%|████████▍ | 10193/12000 [00:50<00:08, 203.43it/s]100%|██████████| 12000/12000 [00:58<00:00, 204.10it/s]
2025-07-11 12:19:12,911 - INFO - Done! Took 00:00:59.53
2025-07-11 12:19:12,932 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 12:19:13,250 - INFO - Created model type: <class 'torch_geometric.graphgym.model_builder.GraphGymModule'>
2025-07-11 12:19:13,250 - INFO - Inner model type: <class 'graphgps.network.uniform_gt_model_edge.UNIFORM_GTModelEdge'>
2025-07-11 12:19:13,251 - INFO - Inner model has get_darts_model: False
2025-07-11 12:19:13,255 - INFO - GraphGymModule(
  (model): UNIFORM_GTModelEdge(
    (encoder): FeatureEncoder(
      (node_encoder): LapPENodeEncoder(
        (linear_x): Linear(in_features=7, out_features=32, bias=True)
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (edge_encoder): DummyEdgeEncoder(
        (encoder): Embedding(1, 48)
      )
    )
    (layers): Sequential(
      (0): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (1): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (2): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (3): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (4): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (5): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (6): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (7): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (8): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (9): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (10): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (11): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (12): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (13): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (14): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (15): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (post_mp): GNNInductiveNodeHead(
      (layer_post_mp): MLP(
        (model): Sequential(
          (0): GeneralMultiLayer(
            (Layer_0): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
            (Layer_1): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
          )
          (1): Linear(
            (model): Linear(48, 6, bias=True)
          )
        )
      )
    )
  )
)
2025-07-11 12:19:13,260 - INFO - Number of parameters: 541,238
2025-07-11 12:19:13,260 - INFO - Starting optimized training: 2025-07-11 12:19:13.260338
2025-07-11 12:19:18,624 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
2025-07-11 12:19:18,624 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 12:19:18,625 - INFO -   undirected: True
2025-07-11 12:19:18,626 - INFO -   num graphs: 12000
2025-07-11 12:19:18,626 - INFO -   avg num_nodes/graph: 117
2025-07-11 12:19:18,626 - INFO -   num node features: 7
2025-07-11 12:19:18,626 - INFO -   num edge features: 0
2025-07-11 12:19:18,628 - INFO -   num classes: 6
2025-07-11 12:19:18,628 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 12:19:18,628 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 12:19:18,636 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s] 17%|█▋        | 2095/12000 [00:10<00:47, 209.43it/s] 35%|███▍      | 4174/12000 [00:20<00:37, 208.49it/s] 52%|█████▏    | 6256/12000 [00:30<00:27, 208.31it/s] 69%|██████▉   | 8292/12000 [00:40<00:17, 206.44it/s] 86%|████████▋ | 10353/12000 [00:50<00:07, 206.30it/s]100%|██████████| 12000/12000 [00:57<00:00, 207.13it/s]
2025-07-11 12:20:17,281 - INFO - Done! Took 00:00:58.65
2025-07-11 12:20:17,305 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 12:20:17,363 - INFO - === STARTING OPTIMIZED TRAINING ===
2025-07-11 12:20:17,363 - INFO - Start from epoch 0
2025-07-11 12:21:46,289 - INFO - train: {'epoch': 0, 'time_epoch': 87.94242, 'eta': 8706.2993, 'eta_hours': 2.41842, 'loss': 1.79688477, 'lr': 0.0, 'params': 541238, 'time_iter': 0.14071, 'accuracy': 0.16756, 'f1': 0.07683, 'accuracy-SBM': 0.16754, 'auc': 0.49567}
2025-07-11 12:21:46,303 - INFO - ...computing epoch stats took: 0.97s
2025-07-11 12:21:50,697 - INFO - val: {'epoch': 0, 'time_epoch': 4.32974, 'loss': 1.79675553, 'lr': 0, 'params': 541238, 'time_iter': 0.06873, 'accuracy': 0.16849, 'f1': 0.07813, 'accuracy-SBM': 0.16649, 'auc': 0.49306}
2025-07-11 12:21:50,700 - INFO - ...computing epoch stats took: 0.06s
2025-07-11 12:21:54,934 - INFO - test: {'epoch': 0, 'time_epoch': 4.19794, 'loss': 1.79656256, 'lr': 0, 'params': 541238, 'time_iter': 0.06663, 'accuracy': 0.16869, 'f1': 0.07806, 'accuracy-SBM': 0.16645, 'auc': 0.49479}
2025-07-11 12:21:54,937 - INFO - ...computing epoch stats took: 0.03s
2025-07-11 12:21:54,937 - INFO - > Epoch 0: took 97.6s (avg 97.6s) | Best so far: epoch 0	train_loss: 1.7969 train_accuracy-SBM: 0.1675	val_loss: 1.7968 val_accuracy-SBM: 0.1665	test_loss: 1.7966 test_accuracy-SBM: 0.1664
2025-07-11 12:23:18,770 - INFO - train: {'epoch': 1, 'time_epoch': 83.576, 'eta': 8404.40264, 'eta_hours': 2.33456, 'loss': 1.66967885, 'lr': 0.0002, 'params': 541238, 'time_iter': 0.13372, 'accuracy': 0.35798, 'f1': 0.34235, 'accuracy-SBM': 0.35798, 'auc': 0.69542}
2025-07-11 12:23:18,776 - INFO - ...computing epoch stats took: 0.24s
2025-07-11 12:23:22,998 - INFO - val: {'epoch': 1, 'time_epoch': 4.16976, 'loss': 1.67615845, 'lr': 0, 'params': 541238, 'time_iter': 0.06619, 'accuracy': 0.33589, 'f1': 0.29015, 'accuracy-SBM': 0.33426, 'auc': 0.72596}
2025-07-11 12:23:22,999 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 12:23:27,150 - INFO - test: {'epoch': 1, 'time_epoch': 4.10213, 'loss': 1.67230362, 'lr': 0, 'params': 541238, 'time_iter': 0.06511, 'accuracy': 0.33701, 'f1': 0.2924, 'accuracy-SBM': 0.33724, 'auc': 0.73096}
2025-07-11 12:23:27,152 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 12:23:27,152 - INFO - > Epoch 1: took 92.2s (avg 94.9s) | Best so far: epoch 1	train_loss: 1.6697 train_accuracy-SBM: 0.3580	val_loss: 1.6762 val_accuracy-SBM: 0.3343	test_loss: 1.6723 test_accuracy-SBM: 0.3372
2025-07-11 12:24:50,514 - INFO - train: {'epoch': 2, 'time_epoch': 83.11033, 'eta': 8232.99626, 'eta_hours': 2.28694, 'loss': 1.40877203, 'lr': 0.0004, 'params': 541238, 'time_iter': 0.13298, 'accuracy': 0.50486, 'f1': 0.50018, 'accuracy-SBM': 0.50488, 'auc': 0.80581}
2025-07-11 12:24:50,520 - INFO - ...computing epoch stats took: 0.24s
2025-07-11 12:24:54,564 - INFO - val: {'epoch': 2, 'time_epoch': 3.99467, 'loss': 1.67291838, 'lr': 0, 'params': 541238, 'time_iter': 0.06341, 'accuracy': 0.33217, 'f1': 0.27581, 'accuracy-SBM': 0.33461, 'auc': 0.73208}
2025-07-11 12:24:54,566 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 12:24:58,586 - INFO - test: {'epoch': 2, 'time_epoch': 3.98012, 'loss': 1.66793511, 'lr': 0, 'params': 541238, 'time_iter': 0.06318, 'accuracy': 0.33805, 'f1': 0.28453, 'accuracy-SBM': 0.34009, 'auc': 0.73273}
2025-07-11 12:24:58,587 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 12:24:58,588 - INFO - > Epoch 2: took 91.4s (avg 93.7s) | Best so far: epoch 2	train_loss: 1.4088 train_accuracy-SBM: 0.5049	val_loss: 1.6729 val_accuracy-SBM: 0.3346	test_loss: 1.6679 test_accuracy-SBM: 0.3401
2025-07-11 12:26:21,444 - INFO - train: {'epoch': 3, 'time_epoch': 82.61814, 'eta': 8093.92534, 'eta_hours': 2.24831, 'loss': 1.20008259, 'lr': 0.0006, 'params': 541238, 'time_iter': 0.13219, 'accuracy': 0.55898, 'f1': 0.55336, 'accuracy-SBM': 0.55902, 'auc': 0.8548}
2025-07-11 12:26:25,515 - INFO - val: {'epoch': 3, 'time_epoch': 4.02579, 'loss': 1.33085141, 'lr': 0, 'params': 541238, 'time_iter': 0.0639, 'accuracy': 0.49376, 'f1': 0.48524, 'accuracy-SBM': 0.49245, 'auc': 0.83851}
2025-07-11 12:26:29,581 - INFO - test: {'epoch': 3, 'time_epoch': 4.03149, 'loss': 1.3183928, 'lr': 0, 'params': 541238, 'time_iter': 0.06399, 'accuracy': 0.49763, 'f1': 0.49101, 'accuracy-SBM': 0.49639, 'auc': 0.84141}
2025-07-11 12:26:29,583 - INFO - > Epoch 3: took 91.0s (avg 93.1s) | Best so far: epoch 3	train_loss: 1.2001 train_accuracy-SBM: 0.5590	val_loss: 1.3309 val_accuracy-SBM: 0.4924	test_loss: 1.3184 test_accuracy-SBM: 0.4964
2025-07-11 12:27:56,025 - INFO - train: {'epoch': 4, 'time_epoch': 86.19284, 'eta': 8045.35493, 'eta_hours': 2.23482, 'loss': 1.06491521, 'lr': 0.0008, 'params': 541238, 'time_iter': 0.13791, 'accuracy': 0.60396, 'f1': 0.5994, 'accuracy-SBM': 0.60401, 'auc': 0.88343}
2025-07-11 12:28:00,346 - INFO - val: {'epoch': 4, 'time_epoch': 4.27226, 'loss': 1.07283406, 'lr': 0, 'params': 541238, 'time_iter': 0.06781, 'accuracy': 0.60998, 'f1': 0.61524, 'accuracy-SBM': 0.61045, 'auc': 0.89707}
2025-07-11 12:28:04,560 - INFO - test: {'epoch': 4, 'time_epoch': 4.18085, 'loss': 1.06236985, 'lr': 0, 'params': 541238, 'time_iter': 0.06636, 'accuracy': 0.61652, 'f1': 0.62047, 'accuracy-SBM': 0.61647, 'auc': 0.89929}
2025-07-11 12:28:04,561 - INFO - > Epoch 4: took 95.0s (avg 93.4s) | Best so far: epoch 4	train_loss: 1.0649 train_accuracy-SBM: 0.6040	val_loss: 1.0728 val_accuracy-SBM: 0.6105	test_loss: 1.0624 test_accuracy-SBM: 0.6165
2025-07-11 12:29:27,884 - INFO - train: {'epoch': 5, 'time_epoch': 83.08088, 'eta': 7935.48966, 'eta_hours': 2.2043, 'loss': 0.90435568, 'lr': 0.001, 'params': 541238, 'time_iter': 0.13293, 'accuracy': 0.67736, 'f1': 0.67729, 'accuracy-SBM': 0.67737, 'auc': 0.9191}
2025-07-11 12:29:31,912 - INFO - val: {'epoch': 5, 'time_epoch': 3.98395, 'loss': 0.86281658, 'lr': 0, 'params': 541238, 'time_iter': 0.06324, 'accuracy': 0.68814, 'f1': 0.688, 'accuracy-SBM': 0.68851, 'auc': 0.92875}
2025-07-11 12:29:35,917 - INFO - test: {'epoch': 5, 'time_epoch': 3.96949, 'loss': 0.85257875, 'lr': 0, 'params': 541238, 'time_iter': 0.06301, 'accuracy': 0.69421, 'f1': 0.6937, 'accuracy-SBM': 0.69416, 'auc': 0.93052}
2025-07-11 12:29:35,919 - INFO - > Epoch 5: took 91.4s (avg 93.1s) | Best so far: epoch 5	train_loss: 0.9044 train_accuracy-SBM: 0.6774	val_loss: 0.8628 val_accuracy-SBM: 0.6885	test_loss: 0.8526 test_accuracy-SBM: 0.6942
2025-07-11 12:31:00,605 - INFO - train: {'epoch': 6, 'time_epoch': 84.43706, 'eta': 7851.29478, 'eta_hours': 2.18092, 'loss': 0.82441146, 'lr': 0.00099973, 'params': 541238, 'time_iter': 0.1351, 'accuracy': 0.70479, 'f1': 0.70479, 'accuracy-SBM': 0.70479, 'auc': 0.93277}
2025-07-11 12:31:04,880 - INFO - val: {'epoch': 6, 'time_epoch': 4.21846, 'loss': 0.76447367, 'lr': 0, 'params': 541238, 'time_iter': 0.06696, 'accuracy': 0.72864, 'f1': 0.72818, 'accuracy-SBM': 0.72832, 'auc': 0.94364}
2025-07-11 12:31:09,191 - INFO - test: {'epoch': 6, 'time_epoch': 4.26701, 'loss': 0.75247891, 'lr': 0, 'params': 541238, 'time_iter': 0.06773, 'accuracy': 0.72988, 'f1': 0.72973, 'accuracy-SBM': 0.72981, 'auc': 0.94582}
2025-07-11 12:31:09,193 - INFO - > Epoch 6: took 93.3s (avg 93.1s) | Best so far: epoch 6	train_loss: 0.8244 train_accuracy-SBM: 0.7048	val_loss: 0.7645 val_accuracy-SBM: 0.7283	test_loss: 0.7525 test_accuracy-SBM: 0.7298
2025-07-11 12:32:34,053 - INFO - train: {'epoch': 7, 'time_epoch': 84.61778, 'eta': 7769.11769, 'eta_hours': 2.15809, 'loss': 0.77896537, 'lr': 0.00099891, 'params': 541238, 'time_iter': 0.13539, 'accuracy': 0.72062, 'f1': 0.72062, 'accuracy-SBM': 0.72062, 'auc': 0.93971}
2025-07-11 12:32:38,137 - INFO - val: {'epoch': 7, 'time_epoch': 4.03609, 'loss': 0.75107641, 'lr': 0, 'params': 541238, 'time_iter': 0.06406, 'accuracy': 0.7321, 'f1': 0.73192, 'accuracy-SBM': 0.73176, 'auc': 0.94476}
2025-07-11 12:32:42,189 - INFO - test: {'epoch': 7, 'time_epoch': 4.01934, 'loss': 0.75300173, 'lr': 0, 'params': 541238, 'time_iter': 0.0638, 'accuracy': 0.73037, 'f1': 0.73021, 'accuracy-SBM': 0.73027, 'auc': 0.94464}
2025-07-11 12:32:42,191 - INFO - > Epoch 7: took 93.0s (avg 93.1s) | Best so far: epoch 7	train_loss: 0.7790 train_accuracy-SBM: 0.7206	val_loss: 0.7511 val_accuracy-SBM: 0.7318	test_loss: 0.7530 test_accuracy-SBM: 0.7303
2025-07-11 12:34:06,588 - INFO - train: {'epoch': 8, 'time_epoch': 84.158, 'eta': 7681.74933, 'eta_hours': 2.13382, 'loss': 0.75763284, 'lr': 0.00099754, 'params': 541238, 'time_iter': 0.13465, 'accuracy': 0.72778, 'f1': 0.72778, 'accuracy-SBM': 0.72778, 'auc': 0.94289}
2025-07-11 12:34:10,716 - INFO - val: {'epoch': 8, 'time_epoch': 4.08097, 'loss': 0.71091632, 'lr': 0, 'params': 541238, 'time_iter': 0.06478, 'accuracy': 0.74551, 'f1': 0.74539, 'accuracy-SBM': 0.74527, 'auc': 0.95031}
2025-07-11 12:34:14,932 - INFO - test: {'epoch': 8, 'time_epoch': 4.18038, 'loss': 0.70934954, 'lr': 0, 'params': 541238, 'time_iter': 0.06636, 'accuracy': 0.74437, 'f1': 0.74438, 'accuracy-SBM': 0.74426, 'auc': 0.95069}
2025-07-11 12:34:14,933 - INFO - > Epoch 8: took 92.7s (avg 93.1s) | Best so far: epoch 8	train_loss: 0.7576 train_accuracy-SBM: 0.7278	val_loss: 0.7109 val_accuracy-SBM: 0.7453	test_loss: 0.7093 test_accuracy-SBM: 0.7443
2025-07-11 12:35:40,175 - INFO - train: {'epoch': 9, 'time_epoch': 84.89732, 'eta': 7601.67693, 'eta_hours': 2.11158, 'loss': 0.74417883, 'lr': 0.00099563, 'params': 541238, 'time_iter': 0.13584, 'accuracy': 0.73248, 'f1': 0.73248, 'accuracy-SBM': 0.73248, 'auc': 0.94476}
2025-07-11 12:35:44,288 - INFO - val: {'epoch': 9, 'time_epoch': 4.0575, 'loss': 0.7195616, 'lr': 0, 'params': 541238, 'time_iter': 0.0644, 'accuracy': 0.74567, 'f1': 0.74536, 'accuracy-SBM': 0.74528, 'auc': 0.94886}
2025-07-11 12:35:48,351 - INFO - test: {'epoch': 9, 'time_epoch': 4.02821, 'loss': 0.71645481, 'lr': 0, 'params': 541238, 'time_iter': 0.06394, 'accuracy': 0.74203, 'f1': 0.74179, 'accuracy-SBM': 0.74187, 'auc': 0.94967}
2025-07-11 12:35:48,353 - INFO - > Epoch 9: took 93.4s (avg 93.1s) | Best so far: epoch 9	train_loss: 0.7442 train_accuracy-SBM: 0.7325	val_loss: 0.7196 val_accuracy-SBM: 0.7453	test_loss: 0.7165 test_accuracy-SBM: 0.7419
2025-07-11 12:37:11,305 - INFO - train: {'epoch': 10, 'time_epoch': 82.71433, 'eta': 7503.06492, 'eta_hours': 2.08418, 'loss': 0.73364019, 'lr': 0.00099318, 'params': 541238, 'time_iter': 0.13234, 'accuracy': 0.73609, 'f1': 0.73609, 'accuracy-SBM': 0.73609, 'auc': 0.94626}
2025-07-11 12:37:15,343 - INFO - val: {'epoch': 10, 'time_epoch': 3.99241, 'loss': 0.67730077, 'lr': 0, 'params': 541238, 'time_iter': 0.06337, 'accuracy': 0.75706, 'f1': 0.75702, 'accuracy-SBM': 0.757, 'auc': 0.95466}
2025-07-11 12:37:19,383 - INFO - test: {'epoch': 10, 'time_epoch': 4.00623, 'loss': 0.68239549, 'lr': 0, 'params': 541238, 'time_iter': 0.06359, 'accuracy': 0.75398, 'f1': 0.75404, 'accuracy-SBM': 0.75405, 'auc': 0.95414}
2025-07-11 12:37:19,385 - INFO - > Epoch 10: took 91.0s (avg 92.9s) | Best so far: epoch 10	train_loss: 0.7336 train_accuracy-SBM: 0.7361	val_loss: 0.6773 val_accuracy-SBM: 0.7570	test_loss: 0.6824 test_accuracy-SBM: 0.7540
2025-07-11 12:38:43,692 - INFO - train: {'epoch': 11, 'time_epoch': 83.95541, 'eta': 7416.20374, 'eta_hours': 2.06006, 'loss': 0.72165753, 'lr': 0.00099019, 'params': 541238, 'time_iter': 0.13433, 'accuracy': 0.74071, 'f1': 0.74071, 'accuracy-SBM': 0.74071, 'auc': 0.94797}
2025-07-11 12:38:47,851 - INFO - val: {'epoch': 11, 'time_epoch': 4.10402, 'loss': 0.68580189, 'lr': 0, 'params': 541238, 'time_iter': 0.06514, 'accuracy': 0.75388, 'f1': 0.75369, 'accuracy-SBM': 0.75375, 'auc': 0.95357}
2025-07-11 12:38:51,928 - INFO - test: {'epoch': 11, 'time_epoch': 4.04455, 'loss': 0.68234842, 'lr': 0, 'params': 541238, 'time_iter': 0.0642, 'accuracy': 0.75385, 'f1': 0.75383, 'accuracy-SBM': 0.75378, 'auc': 0.95416}
2025-07-11 12:38:51,930 - INFO - > Epoch 11: took 92.5s (avg 92.9s) | Best so far: epoch 10	train_loss: 0.7336 train_accuracy-SBM: 0.7361	val_loss: 0.6773 val_accuracy-SBM: 0.7570	test_loss: 0.6824 test_accuracy-SBM: 0.7540
2025-07-11 12:40:14,997 - INFO - train: {'epoch': 12, 'time_epoch': 82.63601, 'eta': 7320.9598, 'eta_hours': 2.0336, 'loss': 0.71308876, 'lr': 0.00098666, 'params': 541238, 'time_iter': 0.13222, 'accuracy': 0.74328, 'f1': 0.74328, 'accuracy-SBM': 0.74328, 'auc': 0.94918}
2025-07-11 12:40:19,019 - INFO - val: {'epoch': 12, 'time_epoch': 3.97623, 'loss': 0.68941157, 'lr': 0, 'params': 541238, 'time_iter': 0.06311, 'accuracy': 0.75243, 'f1': 0.75218, 'accuracy-SBM': 0.75225, 'auc': 0.95281}
2025-07-11 12:40:22,994 - INFO - test: {'epoch': 12, 'time_epoch': 3.93264, 'loss': 0.68481153, 'lr': 0, 'params': 541238, 'time_iter': 0.06242, 'accuracy': 0.75326, 'f1': 0.75326, 'accuracy-SBM': 0.75322, 'auc': 0.95359}
2025-07-11 12:40:22,996 - INFO - > Epoch 12: took 91.1s (avg 92.7s) | Best so far: epoch 10	train_loss: 0.7336 train_accuracy-SBM: 0.7361	val_loss: 0.6773 val_accuracy-SBM: 0.7570	test_loss: 0.6824 test_accuracy-SBM: 0.7540
2025-07-11 12:41:47,561 - INFO - train: {'epoch': 13, 'time_epoch': 84.31644, 'eta': 7237.83961, 'eta_hours': 2.01051, 'loss': 0.71201127, 'lr': 0.0009826, 'params': 541238, 'time_iter': 0.13491, 'accuracy': 0.74402, 'f1': 0.74403, 'accuracy-SBM': 0.74403, 'auc': 0.94926}
2025-07-11 12:41:51,729 - INFO - val: {'epoch': 13, 'time_epoch': 4.11524, 'loss': 0.67334048, 'lr': 0, 'params': 541238, 'time_iter': 0.06532, 'accuracy': 0.75816, 'f1': 0.7581, 'accuracy-SBM': 0.75802, 'auc': 0.95474}
2025-07-11 12:41:55,981 - INFO - test: {'epoch': 13, 'time_epoch': 4.21857, 'loss': 0.66332943, 'lr': 0, 'params': 541238, 'time_iter': 0.06696, 'accuracy': 0.76128, 'f1': 0.76133, 'accuracy-SBM': 0.76128, 'auc': 0.95648}
2025-07-11 12:41:55,983 - INFO - > Epoch 13: took 93.0s (avg 92.8s) | Best so far: epoch 13	train_loss: 0.7120 train_accuracy-SBM: 0.7440	val_loss: 0.6733 val_accuracy-SBM: 0.7580	test_loss: 0.6633 test_accuracy-SBM: 0.7613
2025-07-11 12:43:19,003 - INFO - train: {'epoch': 14, 'time_epoch': 82.77778, 'eta': 7145.84083, 'eta_hours': 1.98496, 'loss': 0.69807802, 'lr': 0.00097802, 'params': 541238, 'time_iter': 0.13244, 'accuracy': 0.7485, 'f1': 0.74851, 'accuracy-SBM': 0.74851, 'auc': 0.95128}
2025-07-11 12:43:23,060 - INFO - val: {'epoch': 14, 'time_epoch': 4.00977, 'loss': 0.67134966, 'lr': 0, 'params': 541238, 'time_iter': 0.06365, 'accuracy': 0.75945, 'f1': 0.75937, 'accuracy-SBM': 0.75946, 'auc': 0.95519}
2025-07-11 12:43:27,069 - INFO - test: {'epoch': 14, 'time_epoch': 3.97523, 'loss': 0.66585568, 'lr': 0, 'params': 541238, 'time_iter': 0.0631, 'accuracy': 0.76118, 'f1': 0.76123, 'accuracy-SBM': 0.76123, 'auc': 0.95605}
2025-07-11 12:43:27,071 - INFO - > Epoch 14: took 91.1s (avg 92.6s) | Best so far: epoch 14	train_loss: 0.6981 train_accuracy-SBM: 0.7485	val_loss: 0.6713 val_accuracy-SBM: 0.7595	test_loss: 0.6659 test_accuracy-SBM: 0.7612
2025-07-11 12:44:50,297 - INFO - train: {'epoch': 15, 'time_epoch': 82.98072, 'eta': 7056.06014, 'eta_hours': 1.96002, 'loss': 0.6954266, 'lr': 0.00097291, 'params': 541238, 'time_iter': 0.13277, 'accuracy': 0.74867, 'f1': 0.74867, 'accuracy-SBM': 0.74867, 'auc': 0.95163}
2025-07-11 12:44:54,426 - INFO - val: {'epoch': 15, 'time_epoch': 4.08295, 'loss': 0.65893285, 'lr': 0, 'params': 541238, 'time_iter': 0.06481, 'accuracy': 0.76137, 'f1': 0.76132, 'accuracy-SBM': 0.76145, 'auc': 0.95709}
2025-07-11 12:44:58,586 - INFO - test: {'epoch': 15, 'time_epoch': 4.12395, 'loss': 0.65889994, 'lr': 0, 'params': 541238, 'time_iter': 0.06546, 'accuracy': 0.76304, 'f1': 0.76299, 'accuracy-SBM': 0.76291, 'auc': 0.95703}
2025-07-11 12:44:58,588 - INFO - > Epoch 15: took 91.5s (avg 92.6s) | Best so far: epoch 15	train_loss: 0.6954 train_accuracy-SBM: 0.7487	val_loss: 0.6589 val_accuracy-SBM: 0.7614	test_loss: 0.6589 test_accuracy-SBM: 0.7629
2025-07-11 12:46:22,348 - INFO - train: {'epoch': 16, 'time_epoch': 83.41977, 'eta': 6969.22304, 'eta_hours': 1.9359, 'loss': 0.69252353, 'lr': 0.00096728, 'params': 541238, 'time_iter': 0.13347, 'accuracy': 0.75021, 'f1': 0.75021, 'accuracy-SBM': 0.75021, 'auc': 0.95202}
2025-07-11 12:46:26,362 - INFO - val: {'epoch': 16, 'time_epoch': 3.96874, 'loss': 0.66002614, 'lr': 0, 'params': 541238, 'time_iter': 0.063, 'accuracy': 0.76503, 'f1': 0.76485, 'accuracy-SBM': 0.7648, 'auc': 0.95673}
2025-07-11 12:46:30,346 - INFO - test: {'epoch': 16, 'time_epoch': 3.94992, 'loss': 0.66415679, 'lr': 0, 'params': 541238, 'time_iter': 0.0627, 'accuracy': 0.76008, 'f1': 0.76004, 'accuracy-SBM': 0.76, 'auc': 0.95629}
2025-07-11 12:46:30,348 - INFO - > Epoch 16: took 91.8s (avg 92.5s) | Best so far: epoch 16	train_loss: 0.6925 train_accuracy-SBM: 0.7502	val_loss: 0.6600 val_accuracy-SBM: 0.7648	test_loss: 0.6642 test_accuracy-SBM: 0.7600
2025-07-11 12:47:52,532 - INFO - train: {'epoch': 17, 'time_epoch': 81.94689, 'eta': 6876.05586, 'eta_hours': 1.91002, 'loss': 0.68881234, 'lr': 0.00096114, 'params': 541238, 'time_iter': 0.13112, 'accuracy': 0.75084, 'f1': 0.75084, 'accuracy-SBM': 0.75084, 'auc': 0.95254}
2025-07-11 12:47:56,559 - INFO - val: {'epoch': 17, 'time_epoch': 3.98086, 'loss': 0.65931871, 'lr': 0, 'params': 541238, 'time_iter': 0.06319, 'accuracy': 0.76606, 'f1': 0.7661, 'accuracy-SBM': 0.76608, 'auc': 0.95677}
2025-07-11 12:48:00,553 - INFO - test: {'epoch': 17, 'time_epoch': 3.96052, 'loss': 0.65396446, 'lr': 0, 'params': 541238, 'time_iter': 0.06287, 'accuracy': 0.76365, 'f1': 0.76367, 'accuracy-SBM': 0.76377, 'auc': 0.95768}
2025-07-11 12:48:00,555 - INFO - > Epoch 17: took 90.2s (avg 92.4s) | Best so far: epoch 17	train_loss: 0.6888 train_accuracy-SBM: 0.7508	val_loss: 0.6593 val_accuracy-SBM: 0.7661	test_loss: 0.6540 test_accuracy-SBM: 0.7638
2025-07-11 12:49:27,028 - INFO - train: {'epoch': 18, 'time_epoch': 86.11066, 'eta': 6801.82058, 'eta_hours': 1.88939, 'loss': 0.68266646, 'lr': 0.0009545, 'params': 541238, 'time_iter': 0.13778, 'accuracy': 0.75279, 'f1': 0.75278, 'accuracy-SBM': 0.75279, 'auc': 0.95339}
2025-07-11 12:49:31,161 - INFO - val: {'epoch': 18, 'time_epoch': 4.08704, 'loss': 0.66763984, 'lr': 0, 'params': 541238, 'time_iter': 0.06487, 'accuracy': 0.76127, 'f1': 0.76104, 'accuracy-SBM': 0.76104, 'auc': 0.95574}
2025-07-11 12:49:35,224 - INFO - test: {'epoch': 18, 'time_epoch': 4.02912, 'loss': 0.65616728, 'lr': 0, 'params': 541238, 'time_iter': 0.06395, 'accuracy': 0.76486, 'f1': 0.76469, 'accuracy-SBM': 0.76458, 'auc': 0.95733}
2025-07-11 12:49:35,225 - INFO - > Epoch 18: took 94.7s (avg 92.5s) | Best so far: epoch 17	train_loss: 0.6888 train_accuracy-SBM: 0.7508	val_loss: 0.6593 val_accuracy-SBM: 0.7661	test_loss: 0.6540 test_accuracy-SBM: 0.7638
2025-07-11 12:50:58,484 - INFO - train: {'epoch': 19, 'time_epoch': 82.91443, 'eta': 6713.61285, 'eta_hours': 1.86489, 'loss': 0.67580048, 'lr': 0.00094736, 'params': 541238, 'time_iter': 0.13266, 'accuracy': 0.75577, 'f1': 0.75577, 'accuracy-SBM': 0.75577, 'auc': 0.95431}
2025-07-11 12:51:02,597 - INFO - val: {'epoch': 19, 'time_epoch': 4.06682, 'loss': 0.66593633, 'lr': 0, 'params': 541238, 'time_iter': 0.06455, 'accuracy': 0.76344, 'f1': 0.76348, 'accuracy-SBM': 0.76342, 'auc': 0.95602}
2025-07-11 12:51:06,688 - INFO - test: {'epoch': 19, 'time_epoch': 4.04932, 'loss': 0.66087329, 'lr': 0, 'params': 541238, 'time_iter': 0.06427, 'accuracy': 0.76161, 'f1': 0.76172, 'accuracy-SBM': 0.76176, 'auc': 0.95679}
2025-07-11 12:51:06,690 - INFO - > Epoch 19: took 91.5s (avg 92.5s) | Best so far: epoch 17	train_loss: 0.6888 train_accuracy-SBM: 0.7508	val_loss: 0.6593 val_accuracy-SBM: 0.7661	test_loss: 0.6540 test_accuracy-SBM: 0.7638
2025-07-11 12:52:31,358 - INFO - train: {'epoch': 20, 'time_epoch': 84.42136, 'eta': 6631.57815, 'eta_hours': 1.84211, 'loss': 0.67371067, 'lr': 0.00093974, 'params': 541238, 'time_iter': 0.13507, 'accuracy': 0.7572, 'f1': 0.7572, 'accuracy-SBM': 0.7572, 'auc': 0.95456}
2025-07-11 12:52:35,429 - INFO - val: {'epoch': 20, 'time_epoch': 4.0261, 'loss': 0.66686238, 'lr': 0, 'params': 541238, 'time_iter': 0.06391, 'accuracy': 0.76128, 'f1': 0.76138, 'accuracy-SBM': 0.76132, 'auc': 0.95579}
2025-07-11 12:52:39,500 - INFO - test: {'epoch': 20, 'time_epoch': 4.03685, 'loss': 0.65969939, 'lr': 0, 'params': 541238, 'time_iter': 0.06408, 'accuracy': 0.76162, 'f1': 0.7616, 'accuracy-SBM': 0.76165, 'auc': 0.95696}
2025-07-11 12:52:39,502 - INFO - > Epoch 20: took 92.8s (avg 92.5s) | Best so far: epoch 17	train_loss: 0.6888 train_accuracy-SBM: 0.7508	val_loss: 0.6593 val_accuracy-SBM: 0.7661	test_loss: 0.6540 test_accuracy-SBM: 0.7638
2025-07-11 12:54:03,165 - INFO - train: {'epoch': 21, 'time_epoch': 83.31913, 'eta': 6545.41859, 'eta_hours': 1.81817, 'loss': 0.67052992, 'lr': 0.00093163, 'params': 541238, 'time_iter': 0.13331, 'accuracy': 0.75732, 'f1': 0.75731, 'accuracy-SBM': 0.75732, 'auc': 0.95502}
2025-07-11 12:54:07,241 - INFO - val: {'epoch': 21, 'time_epoch': 4.0308, 'loss': 0.64923008, 'lr': 0, 'params': 541238, 'time_iter': 0.06398, 'accuracy': 0.76737, 'f1': 0.76717, 'accuracy-SBM': 0.76714, 'auc': 0.95805}
2025-07-11 12:54:11,301 - INFO - test: {'epoch': 21, 'time_epoch': 4.01727, 'loss': 0.65057381, 'lr': 0, 'params': 541238, 'time_iter': 0.06377, 'accuracy': 0.76405, 'f1': 0.76397, 'accuracy-SBM': 0.76393, 'auc': 0.95792}
2025-07-11 12:54:11,303 - INFO - > Epoch 21: took 91.8s (avg 92.5s) | Best so far: epoch 21	train_loss: 0.6705 train_accuracy-SBM: 0.7573	val_loss: 0.6492 val_accuracy-SBM: 0.7671	test_loss: 0.6506 test_accuracy-SBM: 0.7639
2025-07-11 12:55:35,054 - INFO - train: {'epoch': 22, 'time_epoch': 83.30282, 'eta': 6459.4514, 'eta_hours': 1.79429, 'loss': 0.66733281, 'lr': 0.00092305, 'params': 541238, 'time_iter': 0.13328, 'accuracy': 0.75952, 'f1': 0.75952, 'accuracy-SBM': 0.75952, 'auc': 0.95542}
2025-07-11 12:55:39,142 - INFO - val: {'epoch': 22, 'time_epoch': 4.04093, 'loss': 0.65000835, 'lr': 0, 'params': 541238, 'time_iter': 0.06414, 'accuracy': 0.76905, 'f1': 0.76901, 'accuracy-SBM': 0.76904, 'auc': 0.95801}
2025-07-11 12:55:43,261 - INFO - test: {'epoch': 22, 'time_epoch': 4.08414, 'loss': 0.65209683, 'lr': 0, 'params': 541238, 'time_iter': 0.06483, 'accuracy': 0.76694, 'f1': 0.76704, 'accuracy-SBM': 0.76705, 'auc': 0.95787}
2025-07-11 12:55:43,263 - INFO - > Epoch 22: took 92.0s (avg 92.4s) | Best so far: epoch 22	train_loss: 0.6673 train_accuracy-SBM: 0.7595	val_loss: 0.6500 val_accuracy-SBM: 0.7690	test_loss: 0.6521 test_accuracy-SBM: 0.7671
2025-07-11 12:57:07,077 - INFO - train: {'epoch': 23, 'time_epoch': 83.4709, 'eta': 6374.2385, 'eta_hours': 1.77062, 'loss': 0.66332867, 'lr': 0.000914, 'params': 541238, 'time_iter': 0.13355, 'accuracy': 0.76107, 'f1': 0.76107, 'accuracy-SBM': 0.76107, 'auc': 0.95593}
2025-07-11 12:57:11,148 - INFO - val: {'epoch': 23, 'time_epoch': 4.02573, 'loss': 0.64670761, 'lr': 0, 'params': 541238, 'time_iter': 0.0639, 'accuracy': 0.76791, 'f1': 0.76787, 'accuracy-SBM': 0.76792, 'auc': 0.95831}
2025-07-11 12:57:15,198 - INFO - test: {'epoch': 23, 'time_epoch': 4.01392, 'loss': 0.64732532, 'lr': 0, 'params': 541238, 'time_iter': 0.06371, 'accuracy': 0.76652, 'f1': 0.76649, 'accuracy-SBM': 0.76646, 'auc': 0.95829}
2025-07-11 12:57:15,199 - INFO - > Epoch 23: took 91.9s (avg 92.4s) | Best so far: epoch 22	train_loss: 0.6673 train_accuracy-SBM: 0.7595	val_loss: 0.6500 val_accuracy-SBM: 0.7690	test_loss: 0.6521 test_accuracy-SBM: 0.7671
2025-07-11 12:58:38,317 - INFO - train: {'epoch': 24, 'time_epoch': 82.87624, 'eta': 6287.38099, 'eta_hours': 1.74649, 'loss': 0.65992602, 'lr': 0.00090451, 'params': 541238, 'time_iter': 0.1326, 'accuracy': 0.76238, 'f1': 0.76238, 'accuracy-SBM': 0.76238, 'auc': 0.9564}
2025-07-11 12:58:42,430 - INFO - val: {'epoch': 24, 'time_epoch': 4.06666, 'loss': 0.66025788, 'lr': 0, 'params': 541238, 'time_iter': 0.06455, 'accuracy': 0.76329, 'f1': 0.76288, 'accuracy-SBM': 0.76312, 'auc': 0.95699}
2025-07-11 12:58:46,529 - INFO - test: {'epoch': 24, 'time_epoch': 4.0659, 'loss': 0.6572589, 'lr': 0, 'params': 541238, 'time_iter': 0.06454, 'accuracy': 0.76245, 'f1': 0.76229, 'accuracy-SBM': 0.76226, 'auc': 0.95725}
2025-07-11 12:58:46,531 - INFO - > Epoch 24: took 91.3s (avg 92.4s) | Best so far: epoch 22	train_loss: 0.6673 train_accuracy-SBM: 0.7595	val_loss: 0.6500 val_accuracy-SBM: 0.7690	test_loss: 0.6521 test_accuracy-SBM: 0.7671
2025-07-11 13:00:11,831 - INFO - train: {'epoch': 25, 'time_epoch': 85.0554, 'eta': 6207.03195, 'eta_hours': 1.72418, 'loss': 0.65486201, 'lr': 0.00089457, 'params': 541238, 'time_iter': 0.13609, 'accuracy': 0.76305, 'f1': 0.76305, 'accuracy-SBM': 0.76305, 'auc': 0.95707}
2025-07-11 13:00:15,931 - INFO - val: {'epoch': 25, 'time_epoch': 4.04639, 'loss': 0.65273189, 'lr': 0, 'params': 541238, 'time_iter': 0.06423, 'accuracy': 0.7675, 'f1': 0.76747, 'accuracy-SBM': 0.76743, 'auc': 0.95736}
2025-07-11 13:00:20,033 - INFO - test: {'epoch': 25, 'time_epoch': 4.06832, 'loss': 0.63969189, 'lr': 0, 'params': 541238, 'time_iter': 0.06458, 'accuracy': 0.7687, 'f1': 0.76872, 'accuracy-SBM': 0.76868, 'auc': 0.95921}
2025-07-11 13:00:20,034 - INFO - > Epoch 25: took 93.5s (avg 92.4s) | Best so far: epoch 22	train_loss: 0.6673 train_accuracy-SBM: 0.7595	val_loss: 0.6500 val_accuracy-SBM: 0.7690	test_loss: 0.6521 test_accuracy-SBM: 0.7671
2025-07-11 13:01:43,498 - INFO - train: {'epoch': 26, 'time_epoch': 83.2162, 'eta': 6121.36163, 'eta_hours': 1.70038, 'loss': 0.65172773, 'lr': 0.0008842, 'params': 541238, 'time_iter': 0.13315, 'accuracy': 0.76387, 'f1': 0.76387, 'accuracy-SBM': 0.76387, 'auc': 0.9575}
2025-07-11 13:01:47,596 - INFO - val: {'epoch': 26, 'time_epoch': 4.05192, 'loss': 0.64346223, 'lr': 0, 'params': 541238, 'time_iter': 0.06432, 'accuracy': 0.77026, 'f1': 0.7703, 'accuracy-SBM': 0.77037, 'auc': 0.95871}
2025-07-11 13:01:51,668 - INFO - test: {'epoch': 26, 'time_epoch': 4.03855, 'loss': 0.63645853, 'lr': 0, 'params': 541238, 'time_iter': 0.0641, 'accuracy': 0.76861, 'f1': 0.76861, 'accuracy-SBM': 0.76861, 'auc': 0.95982}
2025-07-11 13:01:51,670 - INFO - > Epoch 26: took 91.6s (avg 92.4s) | Best so far: epoch 26	train_loss: 0.6517 train_accuracy-SBM: 0.7639	val_loss: 0.6435 val_accuracy-SBM: 0.7704	test_loss: 0.6365 test_accuracy-SBM: 0.7686
2025-07-11 13:03:15,167 - INFO - train: {'epoch': 27, 'time_epoch': 83.25643, 'eta': 6035.97006, 'eta_hours': 1.67666, 'loss': 0.64981757, 'lr': 0.00087341, 'params': 541238, 'time_iter': 0.13321, 'accuracy': 0.76571, 'f1': 0.7657, 'accuracy-SBM': 0.76571, 'auc': 0.9577}
2025-07-11 13:03:19,266 - INFO - val: {'epoch': 27, 'time_epoch': 4.05297, 'loss': 0.64223659, 'lr': 0, 'params': 541238, 'time_iter': 0.06433, 'accuracy': 0.77025, 'f1': 0.77012, 'accuracy-SBM': 0.77004, 'auc': 0.95879}
2025-07-11 13:03:23,353 - INFO - test: {'epoch': 27, 'time_epoch': 4.05393, 'loss': 0.63822326, 'lr': 0, 'params': 541238, 'time_iter': 0.06435, 'accuracy': 0.76999, 'f1': 0.77006, 'accuracy-SBM': 0.76999, 'auc': 0.95952}
2025-07-11 13:03:23,355 - INFO - > Epoch 27: took 91.7s (avg 92.4s) | Best so far: epoch 26	train_loss: 0.6517 train_accuracy-SBM: 0.7639	val_loss: 0.6435 val_accuracy-SBM: 0.7704	test_loss: 0.6365 test_accuracy-SBM: 0.7686
2025-07-11 13:04:46,142 - INFO - train: {'epoch': 28, 'time_epoch': 82.5488, 'eta': 5948.99326, 'eta_hours': 1.6525, 'loss': 0.64220166, 'lr': 0.00086221, 'params': 541238, 'time_iter': 0.13208, 'accuracy': 0.76772, 'f1': 0.76772, 'accuracy-SBM': 0.76772, 'auc': 0.95872}
2025-07-11 13:04:50,152 - INFO - val: {'epoch': 28, 'time_epoch': 3.96499, 'loss': 0.63612557, 'lr': 0, 'params': 541238, 'time_iter': 0.06294, 'accuracy': 0.77465, 'f1': 0.7746, 'accuracy-SBM': 0.77464, 'auc': 0.96002}
2025-07-11 13:04:54,168 - INFO - test: {'epoch': 28, 'time_epoch': 3.98284, 'loss': 0.63155519, 'lr': 0, 'params': 541238, 'time_iter': 0.06322, 'accuracy': 0.77462, 'f1': 0.77462, 'accuracy-SBM': 0.77462, 'auc': 0.96056}
2025-07-11 13:04:54,170 - INFO - > Epoch 28: took 90.8s (avg 92.3s) | Best so far: epoch 28	train_loss: 0.6422 train_accuracy-SBM: 0.7677	val_loss: 0.6361 val_accuracy-SBM: 0.7746	test_loss: 0.6316 test_accuracy-SBM: 0.7746
2025-07-11 13:06:16,872 - INFO - train: {'epoch': 29, 'time_epoch': 82.35048, 'eta': 5861.84893, 'eta_hours': 1.62829, 'loss': 0.64433383, 'lr': 0.00085062, 'params': 541238, 'time_iter': 0.13176, 'accuracy': 0.76705, 'f1': 0.76705, 'accuracy-SBM': 0.76705, 'auc': 0.95845}
2025-07-11 13:06:21,043 - INFO - val: {'epoch': 29, 'time_epoch': 4.12295, 'loss': 0.63645174, 'lr': 0, 'params': 541238, 'time_iter': 0.06544, 'accuracy': 0.77433, 'f1': 0.77425, 'accuracy-SBM': 0.77415, 'auc': 0.9596}
2025-07-11 13:06:25,229 - INFO - test: {'epoch': 29, 'time_epoch': 4.1441, 'loss': 0.63468604, 'lr': 0, 'params': 541238, 'time_iter': 0.06578, 'accuracy': 0.77156, 'f1': 0.77147, 'accuracy-SBM': 0.77149, 'auc': 0.95992}
2025-07-11 13:06:25,231 - INFO - > Epoch 29: took 91.1s (avg 92.3s) | Best so far: epoch 28	train_loss: 0.6422 train_accuracy-SBM: 0.7677	val_loss: 0.6361 val_accuracy-SBM: 0.7746	test_loss: 0.6316 test_accuracy-SBM: 0.7746
2025-07-11 13:07:48,634 - INFO - train: {'epoch': 30, 'time_epoch': 83.0634, 'eta': 5776.6007, 'eta_hours': 1.60461, 'loss': 0.64007943, 'lr': 0.00083864, 'params': 541238, 'time_iter': 0.1329, 'accuracy': 0.76857, 'f1': 0.76857, 'accuracy-SBM': 0.76857, 'auc': 0.95899}
2025-07-11 13:07:52,665 - INFO - val: {'epoch': 30, 'time_epoch': 3.98473, 'loss': 0.64411451, 'lr': 0, 'params': 541238, 'time_iter': 0.06325, 'accuracy': 0.77197, 'f1': 0.77191, 'accuracy-SBM': 0.77181, 'auc': 0.95905}
2025-07-11 13:07:56,660 - INFO - test: {'epoch': 30, 'time_epoch': 3.96011, 'loss': 0.63854124, 'lr': 0, 'params': 541238, 'time_iter': 0.06286, 'accuracy': 0.77165, 'f1': 0.77158, 'accuracy-SBM': 0.77149, 'auc': 0.95975}
2025-07-11 13:07:56,662 - INFO - > Epoch 30: took 91.4s (avg 92.2s) | Best so far: epoch 28	train_loss: 0.6422 train_accuracy-SBM: 0.7677	val_loss: 0.6361 val_accuracy-SBM: 0.7746	test_loss: 0.6316 test_accuracy-SBM: 0.7746
2025-07-11 13:09:18,583 - INFO - train: {'epoch': 31, 'time_epoch': 81.57687, 'eta': 5688.33015, 'eta_hours': 1.58009, 'loss': 0.63627097, 'lr': 0.00082629, 'params': 541238, 'time_iter': 0.13052, 'accuracy': 0.77, 'f1': 0.77, 'accuracy-SBM': 0.77, 'auc': 0.95948}
2025-07-11 13:09:22,584 - INFO - val: {'epoch': 31, 'time_epoch': 3.95605, 'loss': 0.63543398, 'lr': 0, 'params': 541238, 'time_iter': 0.06279, 'accuracy': 0.77216, 'f1': 0.77204, 'accuracy-SBM': 0.77189, 'auc': 0.9597}
2025-07-11 13:09:26,586 - INFO - test: {'epoch': 31, 'time_epoch': 3.95993, 'loss': 0.63847709, 'lr': 0, 'params': 541238, 'time_iter': 0.06286, 'accuracy': 0.7717, 'f1': 0.77171, 'accuracy-SBM': 0.7716, 'auc': 0.95929}
2025-07-11 13:09:26,588 - INFO - > Epoch 31: took 89.9s (avg 92.2s) | Best so far: epoch 28	train_loss: 0.6422 train_accuracy-SBM: 0.7677	val_loss: 0.6361 val_accuracy-SBM: 0.7746	test_loss: 0.6316 test_accuracy-SBM: 0.7746
2025-07-11 13:10:47,785 - INFO - train: {'epoch': 32, 'time_epoch': 80.85606, 'eta': 5599.0018, 'eta_hours': 1.55528, 'loss': 0.63368696, 'lr': 0.00081359, 'params': 541238, 'time_iter': 0.12937, 'accuracy': 0.77055, 'f1': 0.77055, 'accuracy-SBM': 0.77055, 'auc': 0.95981}
2025-07-11 13:10:51,802 - INFO - val: {'epoch': 32, 'time_epoch': 3.9722, 'loss': 0.64671481, 'lr': 0, 'params': 541238, 'time_iter': 0.06305, 'accuracy': 0.77044, 'f1': 0.77043, 'accuracy-SBM': 0.77037, 'auc': 0.95811}
2025-07-11 13:10:55,803 - INFO - test: {'epoch': 32, 'time_epoch': 3.96785, 'loss': 0.62800723, 'lr': 0, 'params': 541238, 'time_iter': 0.06298, 'accuracy': 0.77426, 'f1': 0.77426, 'accuracy-SBM': 0.77431, 'auc': 0.96063}
2025-07-11 13:10:55,805 - INFO - > Epoch 32: took 89.2s (avg 92.1s) | Best so far: epoch 28	train_loss: 0.6422 train_accuracy-SBM: 0.7677	val_loss: 0.6361 val_accuracy-SBM: 0.7746	test_loss: 0.6316 test_accuracy-SBM: 0.7746
2025-07-11 13:12:16,430 - INFO - train: {'epoch': 33, 'time_epoch': 80.3781, 'eta': 5509.24403, 'eta_hours': 1.53035, 'loss': 0.63047379, 'lr': 0.00080054, 'params': 541238, 'time_iter': 0.1286, 'accuracy': 0.77213, 'f1': 0.77214, 'accuracy-SBM': 0.77214, 'auc': 0.96019}
2025-07-11 13:12:20,413 - INFO - val: {'epoch': 33, 'time_epoch': 3.9376, 'loss': 0.63828761, 'lr': 0, 'params': 541238, 'time_iter': 0.0625, 'accuracy': 0.7748, 'f1': 0.77479, 'accuracy-SBM': 0.77479, 'auc': 0.95924}
2025-07-11 13:12:24,385 - INFO - test: {'epoch': 33, 'time_epoch': 3.93779, 'loss': 0.62744616, 'lr': 0, 'params': 541238, 'time_iter': 0.0625, 'accuracy': 0.77525, 'f1': 0.77527, 'accuracy-SBM': 0.7753, 'auc': 0.96071}
2025-07-11 13:12:24,387 - INFO - > Epoch 33: took 88.6s (avg 92.0s) | Best so far: epoch 33	train_loss: 0.6305 train_accuracy-SBM: 0.7721	val_loss: 0.6383 val_accuracy-SBM: 0.7748	test_loss: 0.6274 test_accuracy-SBM: 0.7753
2025-07-11 13:13:45,622 - INFO - train: {'epoch': 34, 'time_epoch': 80.89744, 'eta': 5420.98673, 'eta_hours': 1.50583, 'loss': 0.62826808, 'lr': 0.00078716, 'params': 541238, 'time_iter': 0.12944, 'accuracy': 0.77256, 'f1': 0.77256, 'accuracy-SBM': 0.77256, 'auc': 0.96048}
2025-07-11 13:13:49,621 - INFO - val: {'epoch': 34, 'time_epoch': 3.95531, 'loss': 0.63413067, 'lr': 0, 'params': 541238, 'time_iter': 0.06278, 'accuracy': 0.77192, 'f1': 0.77184, 'accuracy-SBM': 0.77183, 'auc': 0.95976}
2025-07-11 13:13:53,578 - INFO - test: {'epoch': 34, 'time_epoch': 3.924, 'loss': 0.63286192, 'lr': 0, 'params': 541238, 'time_iter': 0.06229, 'accuracy': 0.77241, 'f1': 0.77241, 'accuracy-SBM': 0.77243, 'auc': 0.95988}
2025-07-11 13:13:53,580 - INFO - > Epoch 34: took 89.2s (avg 91.9s) | Best so far: epoch 33	train_loss: 0.6305 train_accuracy-SBM: 0.7721	val_loss: 0.6383 val_accuracy-SBM: 0.7748	test_loss: 0.6274 test_accuracy-SBM: 0.7753
2025-07-11 13:15:14,896 - INFO - train: {'epoch': 35, 'time_epoch': 80.98241, 'eta': 5333.28937, 'eta_hours': 1.48147, 'loss': 0.62764657, 'lr': 0.00077347, 'params': 541238, 'time_iter': 0.12957, 'accuracy': 0.77299, 'f1': 0.77299, 'accuracy-SBM': 0.77299, 'auc': 0.96057}
2025-07-11 13:15:18,864 - INFO - val: {'epoch': 35, 'time_epoch': 3.91295, 'loss': 0.64265595, 'lr': 0, 'params': 541238, 'time_iter': 0.06211, 'accuracy': 0.77298, 'f1': 0.7729, 'accuracy-SBM': 0.77291, 'auc': 0.95887}
2025-07-11 13:15:22,791 - INFO - test: {'epoch': 35, 'time_epoch': 3.89481, 'loss': 0.62946887, 'lr': 0, 'params': 541238, 'time_iter': 0.06182, 'accuracy': 0.7739, 'f1': 0.7738, 'accuracy-SBM': 0.7739, 'auc': 0.96062}
2025-07-11 13:15:22,793 - INFO - > Epoch 35: took 89.2s (avg 91.8s) | Best so far: epoch 33	train_loss: 0.6305 train_accuracy-SBM: 0.7721	val_loss: 0.6383 val_accuracy-SBM: 0.7748	test_loss: 0.6274 test_accuracy-SBM: 0.7753
2025-07-11 13:16:43,914 - INFO - train: {'epoch': 36, 'time_epoch': 80.88137, 'eta': 5245.78292, 'eta_hours': 1.45716, 'loss': 0.62140284, 'lr': 0.00075948, 'params': 541238, 'time_iter': 0.12941, 'accuracy': 0.77428, 'f1': 0.77428, 'accuracy-SBM': 0.77428, 'auc': 0.96135}
2025-07-11 13:16:47,966 - INFO - val: {'epoch': 36, 'time_epoch': 4.00527, 'loss': 0.6301708, 'lr': 0, 'params': 541238, 'time_iter': 0.06358, 'accuracy': 0.77629, 'f1': 0.77623, 'accuracy-SBM': 0.77621, 'auc': 0.96034}
2025-07-11 13:16:51,957 - INFO - test: {'epoch': 36, 'time_epoch': 3.95746, 'loss': 0.62772379, 'lr': 0, 'params': 541238, 'time_iter': 0.06282, 'accuracy': 0.77533, 'f1': 0.7754, 'accuracy-SBM': 0.77535, 'auc': 0.96076}
2025-07-11 13:16:51,959 - INFO - > Epoch 36: took 89.2s (avg 91.7s) | Best so far: epoch 36	train_loss: 0.6214 train_accuracy-SBM: 0.7743	val_loss: 0.6302 val_accuracy-SBM: 0.7762	test_loss: 0.6277 test_accuracy-SBM: 0.7753
2025-07-11 13:18:12,935 - INFO - train: {'epoch': 37, 'time_epoch': 80.73976, 'eta': 5158.39413, 'eta_hours': 1.43289, 'loss': 0.61878116, 'lr': 0.00074521, 'params': 541238, 'time_iter': 0.12918, 'accuracy': 0.7762, 'f1': 0.7762, 'accuracy-SBM': 0.7762, 'auc': 0.96168}
2025-07-11 13:18:16,918 - INFO - val: {'epoch': 37, 'time_epoch': 3.92305, 'loss': 0.63435178, 'lr': 0, 'params': 541238, 'time_iter': 0.06227, 'accuracy': 0.77599, 'f1': 0.77587, 'accuracy-SBM': 0.77597, 'auc': 0.95996}
2025-07-11 13:18:20,866 - INFO - test: {'epoch': 37, 'time_epoch': 3.91481, 'loss': 0.62861238, 'lr': 0, 'params': 541238, 'time_iter': 0.06214, 'accuracy': 0.77614, 'f1': 0.77606, 'accuracy-SBM': 0.77607, 'auc': 0.96059}
2025-07-11 13:18:20,868 - INFO - > Epoch 37: took 88.9s (avg 91.7s) | Best so far: epoch 36	train_loss: 0.6214 train_accuracy-SBM: 0.7743	val_loss: 0.6302 val_accuracy-SBM: 0.7762	test_loss: 0.6277 test_accuracy-SBM: 0.7753
2025-07-11 13:19:41,749 - INFO - train: {'epoch': 38, 'time_epoch': 80.6407, 'eta': 5071.19135, 'eta_hours': 1.40866, 'loss': 0.61489335, 'lr': 0.00073067, 'params': 541238, 'time_iter': 0.12903, 'accuracy': 0.77769, 'f1': 0.77769, 'accuracy-SBM': 0.77769, 'auc': 0.96216}
2025-07-11 13:19:45,719 - INFO - val: {'epoch': 38, 'time_epoch': 3.92487, 'loss': 0.63205154, 'lr': 0, 'params': 541238, 'time_iter': 0.0623, 'accuracy': 0.77692, 'f1': 0.77674, 'accuracy-SBM': 0.77669, 'auc': 0.96047}
2025-07-11 13:19:49,657 - INFO - test: {'epoch': 38, 'time_epoch': 3.90513, 'loss': 0.62056119, 'lr': 0, 'params': 541238, 'time_iter': 0.06199, 'accuracy': 0.7772, 'f1': 0.77709, 'accuracy-SBM': 0.77703, 'auc': 0.96181}
2025-07-11 13:19:49,659 - INFO - > Epoch 38: took 88.8s (avg 91.6s) | Best so far: epoch 38	train_loss: 0.6149 train_accuracy-SBM: 0.7777	val_loss: 0.6321 val_accuracy-SBM: 0.7767	test_loss: 0.6206 test_accuracy-SBM: 0.7770
2025-07-11 13:21:10,563 - INFO - train: {'epoch': 39, 'time_epoch': 80.6614, 'eta': 4984.34774, 'eta_hours': 1.38454, 'loss': 0.61262859, 'lr': 0.00071588, 'params': 541238, 'time_iter': 0.12906, 'accuracy': 0.77839, 'f1': 0.77839, 'accuracy-SBM': 0.77839, 'auc': 0.96244}
2025-07-11 13:21:14,561 - INFO - val: {'epoch': 39, 'time_epoch': 3.95349, 'loss': 0.62361416, 'lr': 0, 'params': 541238, 'time_iter': 0.06275, 'accuracy': 0.78039, 'f1': 0.78029, 'accuracy-SBM': 0.7802, 'auc': 0.96127}
2025-07-11 13:21:18,559 - INFO - test: {'epoch': 39, 'time_epoch': 3.96491, 'loss': 0.61774668, 'lr': 0, 'params': 541238, 'time_iter': 0.06294, 'accuracy': 0.7791, 'f1': 0.7791, 'accuracy-SBM': 0.77917, 'auc': 0.96212}
2025-07-11 13:21:18,561 - INFO - > Epoch 39: took 88.9s (avg 91.5s) | Best so far: epoch 39	train_loss: 0.6126 train_accuracy-SBM: 0.7784	val_loss: 0.6236 val_accuracy-SBM: 0.7802	test_loss: 0.6177 test_accuracy-SBM: 0.7792
2025-07-11 13:22:39,809 - INFO - train: {'epoch': 40, 'time_epoch': 81.00808, 'eta': 4898.30458, 'eta_hours': 1.36064, 'loss': 0.6103352, 'lr': 0.00070085, 'params': 541238, 'time_iter': 0.12961, 'accuracy': 0.77918, 'f1': 0.77918, 'accuracy-SBM': 0.77918, 'auc': 0.96271}
2025-07-11 13:22:43,773 - INFO - val: {'epoch': 40, 'time_epoch': 3.91969, 'loss': 0.62445916, 'lr': 0, 'params': 541238, 'time_iter': 0.06222, 'accuracy': 0.77783, 'f1': 0.77789, 'accuracy-SBM': 0.77786, 'auc': 0.96102}
2025-07-11 13:22:47,694 - INFO - test: {'epoch': 40, 'time_epoch': 3.88834, 'loss': 0.62893269, 'lr': 0, 'params': 541238, 'time_iter': 0.06172, 'accuracy': 0.77568, 'f1': 0.77569, 'accuracy-SBM': 0.77576, 'auc': 0.96053}
2025-07-11 13:22:47,696 - INFO - > Epoch 40: took 89.1s (avg 91.5s) | Best so far: epoch 39	train_loss: 0.6126 train_accuracy-SBM: 0.7784	val_loss: 0.6236 val_accuracy-SBM: 0.7802	test_loss: 0.6177 test_accuracy-SBM: 0.7792
2025-07-11 13:24:08,760 - INFO - train: {'epoch': 41, 'time_epoch': 80.62688, 'eta': 4811.97477, 'eta_hours': 1.33666, 'loss': 0.61045389, 'lr': 0.0006856, 'params': 541238, 'time_iter': 0.129, 'accuracy': 0.77937, 'f1': 0.77937, 'accuracy-SBM': 0.77937, 'auc': 0.96269}
2025-07-11 13:24:12,784 - INFO - val: {'epoch': 41, 'time_epoch': 3.97057, 'loss': 0.62049484, 'lr': 0, 'params': 541238, 'time_iter': 0.06302, 'accuracy': 0.78013, 'f1': 0.78008, 'accuracy-SBM': 0.78007, 'auc': 0.9616}
2025-07-11 13:24:16,792 - INFO - test: {'epoch': 41, 'time_epoch': 3.97505, 'loss': 0.61641978, 'lr': 0, 'params': 541238, 'time_iter': 0.0631, 'accuracy': 0.77776, 'f1': 0.77779, 'accuracy-SBM': 0.77778, 'auc': 0.96214}
2025-07-11 13:24:16,794 - INFO - > Epoch 41: took 89.1s (avg 91.4s) | Best so far: epoch 39	train_loss: 0.6126 train_accuracy-SBM: 0.7784	val_loss: 0.6236 val_accuracy-SBM: 0.7802	test_loss: 0.6177 test_accuracy-SBM: 0.7792
2025-07-11 13:25:37,004 - INFO - train: {'epoch': 42, 'time_epoch': 79.97006, 'eta': 4725.03953, 'eta_hours': 1.31251, 'loss': 0.60378557, 'lr': 0.00067015, 'params': 541238, 'time_iter': 0.12795, 'accuracy': 0.78156, 'f1': 0.78156, 'accuracy-SBM': 0.78156, 'auc': 0.96349}
2025-07-11 13:25:40,954 - INFO - val: {'epoch': 42, 'time_epoch': 3.90597, 'loss': 0.6236145, 'lr': 0, 'params': 541238, 'time_iter': 0.062, 'accuracy': 0.78021, 'f1': 0.78013, 'accuracy-SBM': 0.78008, 'auc': 0.96112}
2025-07-11 13:25:44,873 - INFO - test: {'epoch': 42, 'time_epoch': 3.88635, 'loss': 0.61233421, 'lr': 0, 'params': 541238, 'time_iter': 0.06169, 'accuracy': 0.77975, 'f1': 0.77969, 'accuracy-SBM': 0.77969, 'auc': 0.96261}
2025-07-11 13:25:44,875 - INFO - > Epoch 42: took 88.1s (avg 91.3s) | Best so far: epoch 39	train_loss: 0.6126 train_accuracy-SBM: 0.7784	val_loss: 0.6236 val_accuracy-SBM: 0.7802	test_loss: 0.6177 test_accuracy-SBM: 0.7792
2025-07-11 13:27:05,180 - INFO - train: {'epoch': 43, 'time_epoch': 80.06738, 'eta': 4638.54477, 'eta_hours': 1.28848, 'loss': 0.60306477, 'lr': 0.00065451, 'params': 541238, 'time_iter': 0.12811, 'accuracy': 0.78112, 'f1': 0.78112, 'accuracy-SBM': 0.78112, 'auc': 0.96361}
2025-07-11 13:27:09,135 - INFO - val: {'epoch': 43, 'time_epoch': 3.911, 'loss': 0.6277897, 'lr': 0, 'params': 541238, 'time_iter': 0.06208, 'accuracy': 0.7782, 'f1': 0.77808, 'accuracy-SBM': 0.77808, 'auc': 0.96051}
2025-07-11 13:27:13,059 - INFO - test: {'epoch': 43, 'time_epoch': 3.89115, 'loss': 0.61589435, 'lr': 0, 'params': 541238, 'time_iter': 0.06176, 'accuracy': 0.77797, 'f1': 0.77796, 'accuracy-SBM': 0.77798, 'auc': 0.96206}
2025-07-11 13:27:13,061 - INFO - > Epoch 43: took 88.2s (avg 91.3s) | Best so far: epoch 39	train_loss: 0.6126 train_accuracy-SBM: 0.7784	val_loss: 0.6236 val_accuracy-SBM: 0.7802	test_loss: 0.6177 test_accuracy-SBM: 0.7792
2025-07-11 13:28:33,869 - INFO - train: {'epoch': 44, 'time_epoch': 80.56819, 'eta': 4552.94776, 'eta_hours': 1.26471, 'loss': 0.59895582, 'lr': 0.0006387, 'params': 541238, 'time_iter': 0.12891, 'accuracy': 0.78311, 'f1': 0.78311, 'accuracy-SBM': 0.78311, 'auc': 0.96409}
2025-07-11 13:28:37,902 - INFO - val: {'epoch': 44, 'time_epoch': 3.98661, 'loss': 0.61878442, 'lr': 0, 'params': 541238, 'time_iter': 0.06328, 'accuracy': 0.78346, 'f1': 0.78341, 'accuracy-SBM': 0.7833, 'auc': 0.96157}
2025-07-11 13:28:41,926 - INFO - test: {'epoch': 44, 'time_epoch': 3.99072, 'loss': 0.62040745, 'lr': 0, 'params': 541238, 'time_iter': 0.06334, 'accuracy': 0.77792, 'f1': 0.77787, 'accuracy-SBM': 0.77793, 'auc': 0.96161}
2025-07-11 13:28:41,928 - INFO - > Epoch 44: took 88.9s (avg 91.2s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:30:02,562 - INFO - train: {'epoch': 45, 'time_epoch': 80.28174, 'eta': 4467.23313, 'eta_hours': 1.2409, 'loss': 0.59644436, 'lr': 0.00062274, 'params': 541238, 'time_iter': 0.12845, 'accuracy': 0.7839, 'f1': 0.7839, 'accuracy-SBM': 0.7839, 'auc': 0.9644}
2025-07-11 13:30:06,630 - INFO - val: {'epoch': 45, 'time_epoch': 4.0235, 'loss': 0.62238804, 'lr': 0, 'params': 541238, 'time_iter': 0.06387, 'accuracy': 0.77898, 'f1': 0.77884, 'accuracy-SBM': 0.77879, 'auc': 0.96124}
2025-07-11 13:30:10,695 - INFO - test: {'epoch': 45, 'time_epoch': 4.01947, 'loss': 0.61395632, 'lr': 0, 'params': 541238, 'time_iter': 0.0638, 'accuracy': 0.78021, 'f1': 0.78016, 'accuracy-SBM': 0.78008, 'auc': 0.96235}
2025-07-11 13:30:10,697 - INFO - > Epoch 45: took 88.8s (avg 91.2s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:31:32,240 - INFO - train: {'epoch': 46, 'time_epoch': 81.29026, 'eta': 4382.88696, 'eta_hours': 1.21747, 'loss': 0.59533232, 'lr': 0.00060665, 'params': 541238, 'time_iter': 0.13006, 'accuracy': 0.78464, 'f1': 0.78464, 'accuracy-SBM': 0.78464, 'auc': 0.96452}
2025-07-11 13:31:36,555 - INFO - val: {'epoch': 46, 'time_epoch': 4.25975, 'loss': 0.62404269, 'lr': 0, 'params': 541238, 'time_iter': 0.06762, 'accuracy': 0.7787, 'f1': 0.77839, 'accuracy-SBM': 0.77836, 'auc': 0.96115}
2025-07-11 13:31:40,830 - INFO - test: {'epoch': 46, 'time_epoch': 4.24005, 'loss': 0.6219144, 'lr': 0, 'params': 541238, 'time_iter': 0.0673, 'accuracy': 0.77628, 'f1': 0.77622, 'accuracy-SBM': 0.77615, 'auc': 0.96153}
2025-07-11 13:31:40,831 - INFO - > Epoch 46: took 90.1s (avg 91.1s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:33:04,961 - INFO - train: {'epoch': 47, 'time_epoch': 83.87312, 'eta': 4301.46621, 'eta_hours': 1.19485, 'loss': 0.59198733, 'lr': 0.00059044, 'params': 541238, 'time_iter': 0.1342, 'accuracy': 0.78562, 'f1': 0.78562, 'accuracy-SBM': 0.78562, 'auc': 0.96493}
2025-07-11 13:33:08,989 - INFO - val: {'epoch': 47, 'time_epoch': 3.98223, 'loss': 0.63152044, 'lr': 0, 'params': 541238, 'time_iter': 0.06321, 'accuracy': 0.77789, 'f1': 0.77788, 'accuracy-SBM': 0.77795, 'auc': 0.96026}
2025-07-11 13:33:12,986 - INFO - test: {'epoch': 47, 'time_epoch': 3.96381, 'loss': 0.6244922, 'lr': 0, 'params': 541238, 'time_iter': 0.06292, 'accuracy': 0.77903, 'f1': 0.77898, 'accuracy-SBM': 0.77903, 'auc': 0.96115}
2025-07-11 13:33:12,988 - INFO - > Epoch 47: took 92.2s (avg 91.2s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:34:32,891 - INFO - train: {'epoch': 48, 'time_epoch': 79.64893, 'eta': 4215.54876, 'eta_hours': 1.17099, 'loss': 0.5885818, 'lr': 0.00057413, 'params': 541238, 'time_iter': 0.12744, 'accuracy': 0.78671, 'f1': 0.78671, 'accuracy-SBM': 0.78671, 'auc': 0.96534}
2025-07-11 13:34:36,942 - INFO - val: {'epoch': 48, 'time_epoch': 4.00602, 'loss': 0.62017139, 'lr': 0, 'params': 541238, 'time_iter': 0.06359, 'accuracy': 0.78205, 'f1': 0.78193, 'accuracy-SBM': 0.78194, 'auc': 0.9619}
2025-07-11 13:34:40,960 - INFO - test: {'epoch': 48, 'time_epoch': 3.98547, 'loss': 0.62256018, 'lr': 0, 'params': 541238, 'time_iter': 0.06326, 'accuracy': 0.77968, 'f1': 0.77964, 'accuracy-SBM': 0.77964, 'auc': 0.96165}
2025-07-11 13:34:40,962 - INFO - > Epoch 48: took 88.0s (avg 91.1s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:36:02,754 - INFO - train: {'epoch': 49, 'time_epoch': 81.55316, 'eta': 4131.78628, 'eta_hours': 1.14772, 'loss': 0.58637536, 'lr': 0.00055774, 'params': 541238, 'time_iter': 0.13049, 'accuracy': 0.78679, 'f1': 0.78679, 'accuracy-SBM': 0.78679, 'auc': 0.96558}
2025-07-11 13:36:06,785 - INFO - val: {'epoch': 49, 'time_epoch': 3.98426, 'loss': 0.61793279, 'lr': 0, 'params': 541238, 'time_iter': 0.06324, 'accuracy': 0.78096, 'f1': 0.78091, 'accuracy-SBM': 0.78076, 'auc': 0.96214}
2025-07-11 13:36:10,783 - INFO - test: {'epoch': 49, 'time_epoch': 3.96401, 'loss': 0.62314901, 'lr': 0, 'params': 541238, 'time_iter': 0.06292, 'accuracy': 0.77876, 'f1': 0.77874, 'accuracy-SBM': 0.77876, 'auc': 0.96158}
2025-07-11 13:36:10,785 - INFO - > Epoch 49: took 89.8s (avg 91.1s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:37:32,700 - INFO - train: {'epoch': 50, 'time_epoch': 81.67362, 'eta': 4048.22618, 'eta_hours': 1.12451, 'loss': 0.58361916, 'lr': 0.00054129, 'params': 541238, 'time_iter': 0.13068, 'accuracy': 0.7884, 'f1': 0.7884, 'accuracy-SBM': 0.7884, 'auc': 0.9659}
2025-07-11 13:37:36,706 - INFO - val: {'epoch': 50, 'time_epoch': 3.96078, 'loss': 0.62169156, 'lr': 0, 'params': 541238, 'time_iter': 0.06287, 'accuracy': 0.78137, 'f1': 0.78123, 'accuracy-SBM': 0.78118, 'auc': 0.96139}
2025-07-11 13:37:40,689 - INFO - test: {'epoch': 50, 'time_epoch': 3.94252, 'loss': 0.6105867, 'lr': 0, 'params': 541238, 'time_iter': 0.06258, 'accuracy': 0.78219, 'f1': 0.78209, 'accuracy-SBM': 0.78212, 'auc': 0.96272}
2025-07-11 13:37:40,691 - INFO - > Epoch 50: took 89.9s (avg 91.0s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:39:01,821 - INFO - train: {'epoch': 51, 'time_epoch': 80.89749, 'eta': 3964.02221, 'eta_hours': 1.10112, 'loss': 0.58083455, 'lr': 0.00052479, 'params': 541238, 'time_iter': 0.12944, 'accuracy': 0.78973, 'f1': 0.78973, 'accuracy-SBM': 0.78973, 'auc': 0.96623}
2025-07-11 13:39:05,828 - INFO - val: {'epoch': 51, 'time_epoch': 3.96201, 'loss': 0.62338996, 'lr': 0, 'params': 541238, 'time_iter': 0.06289, 'accuracy': 0.78022, 'f1': 0.78016, 'accuracy-SBM': 0.78012, 'auc': 0.9614}
2025-07-11 13:39:09,807 - INFO - test: {'epoch': 51, 'time_epoch': 3.94549, 'loss': 0.61504696, 'lr': 0, 'params': 541238, 'time_iter': 0.06263, 'accuracy': 0.77971, 'f1': 0.77973, 'accuracy-SBM': 0.7797, 'auc': 0.96239}
2025-07-11 13:39:09,809 - INFO - > Epoch 51: took 89.1s (avg 91.0s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:40:31,401 - INFO - train: {'epoch': 52, 'time_epoch': 81.35464, 'eta': 3880.34841, 'eta_hours': 1.07787, 'loss': 0.57926483, 'lr': 0.00050827, 'params': 541238, 'time_iter': 0.13017, 'accuracy': 0.78978, 'f1': 0.78978, 'accuracy-SBM': 0.78978, 'auc': 0.96643}
2025-07-11 13:40:35,307 - INFO - val: {'epoch': 52, 'time_epoch': 3.86124, 'loss': 0.62586697, 'lr': 0, 'params': 541238, 'time_iter': 0.06129, 'accuracy': 0.78123, 'f1': 0.78121, 'accuracy-SBM': 0.78119, 'auc': 0.96129}
2025-07-11 13:40:39,291 - INFO - test: {'epoch': 52, 'time_epoch': 3.9507, 'loss': 0.61740138, 'lr': 0, 'params': 541238, 'time_iter': 0.06271, 'accuracy': 0.7799, 'f1': 0.77986, 'accuracy-SBM': 0.77989, 'auc': 0.96223}
2025-07-11 13:40:39,293 - INFO - > Epoch 52: took 89.5s (avg 91.0s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:42:01,250 - INFO - train: {'epoch': 53, 'time_epoch': 81.61081, 'eta': 3796.97872, 'eta_hours': 1.05472, 'loss': 0.57262868, 'lr': 0.00049173, 'params': 541238, 'time_iter': 0.13058, 'accuracy': 0.79251, 'f1': 0.79251, 'accuracy-SBM': 0.79251, 'auc': 0.96718}
2025-07-11 13:42:05,281 - INFO - val: {'epoch': 53, 'time_epoch': 3.98444, 'loss': 0.62752187, 'lr': 0, 'params': 541238, 'time_iter': 0.06325, 'accuracy': 0.7795, 'f1': 0.77945, 'accuracy-SBM': 0.7794, 'auc': 0.96091}
2025-07-11 13:42:09,286 - INFO - test: {'epoch': 53, 'time_epoch': 3.97, 'loss': 0.61848962, 'lr': 0, 'params': 541238, 'time_iter': 0.06302, 'accuracy': 0.78031, 'f1': 0.78035, 'accuracy-SBM': 0.78028, 'auc': 0.96194}
2025-07-11 13:42:09,289 - INFO - > Epoch 53: took 90.0s (avg 91.0s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:43:30,595 - INFO - train: {'epoch': 54, 'time_epoch': 81.06596, 'eta': 3713.2272, 'eta_hours': 1.03145, 'loss': 0.57214102, 'lr': 0.00047521, 'params': 541238, 'time_iter': 0.12971, 'accuracy': 0.79219, 'f1': 0.79219, 'accuracy-SBM': 0.79219, 'auc': 0.96725}
2025-07-11 13:43:34,566 - INFO - val: {'epoch': 54, 'time_epoch': 3.92693, 'loss': 0.63008076, 'lr': 0, 'params': 541238, 'time_iter': 0.06233, 'accuracy': 0.78039, 'f1': 0.78028, 'accuracy-SBM': 0.78023, 'auc': 0.96072}
2025-07-11 13:43:38,549 - INFO - test: {'epoch': 54, 'time_epoch': 3.94997, 'loss': 0.61219257, 'lr': 0, 'params': 541238, 'time_iter': 0.0627, 'accuracy': 0.78034, 'f1': 0.78024, 'accuracy-SBM': 0.78025, 'auc': 0.96282}
2025-07-11 13:43:38,551 - INFO - > Epoch 54: took 89.3s (avg 90.9s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:45:00,316 - INFO - train: {'epoch': 55, 'time_epoch': 81.3327, 'eta': 3629.78118, 'eta_hours': 1.00827, 'loss': 0.56894035, 'lr': 0.00045871, 'params': 541238, 'time_iter': 0.13013, 'accuracy': 0.79376, 'f1': 0.79376, 'accuracy-SBM': 0.79376, 'auc': 0.96761}
2025-07-11 13:45:04,274 - INFO - val: {'epoch': 55, 'time_epoch': 3.91348, 'loss': 0.6153028, 'lr': 0, 'params': 541238, 'time_iter': 0.06212, 'accuracy': 0.78252, 'f1': 0.78245, 'accuracy-SBM': 0.78243, 'auc': 0.96224}
2025-07-11 13:45:08,259 - INFO - test: {'epoch': 55, 'time_epoch': 3.94466, 'loss': 0.62211879, 'lr': 0, 'params': 541238, 'time_iter': 0.06261, 'accuracy': 0.78097, 'f1': 0.78093, 'accuracy-SBM': 0.78093, 'auc': 0.96145}
2025-07-11 13:45:08,261 - INFO - > Epoch 55: took 89.7s (avg 90.9s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:46:29,557 - INFO - train: {'epoch': 56, 'time_epoch': 81.06492, 'eta': 3546.2073, 'eta_hours': 0.98506, 'loss': 0.56870483, 'lr': 0.00044226, 'params': 541238, 'time_iter': 0.1297, 'accuracy': 0.79352, 'f1': 0.79352, 'accuracy-SBM': 0.79352, 'auc': 0.96764}
2025-07-11 13:46:33,576 - INFO - val: {'epoch': 56, 'time_epoch': 3.96609, 'loss': 0.61778131, 'lr': 0, 'params': 541238, 'time_iter': 0.06295, 'accuracy': 0.78097, 'f1': 0.78086, 'accuracy-SBM': 0.78073, 'auc': 0.96183}
2025-07-11 13:46:37,559 - INFO - test: {'epoch': 56, 'time_epoch': 3.949, 'loss': 0.61083319, 'lr': 0, 'params': 541238, 'time_iter': 0.06268, 'accuracy': 0.78189, 'f1': 0.78185, 'accuracy-SBM': 0.78184, 'auc': 0.96269}
2025-07-11 13:46:37,561 - INFO - > Epoch 56: took 89.3s (avg 90.9s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:47:59,072 - INFO - train: {'epoch': 57, 'time_epoch': 81.27331, 'eta': 3462.87083, 'eta_hours': 0.96191, 'loss': 0.56527372, 'lr': 0.00042587, 'params': 541238, 'time_iter': 0.13004, 'accuracy': 0.79454, 'f1': 0.79454, 'accuracy-SBM': 0.79454, 'auc': 0.96803}
2025-07-11 13:48:02,899 - INFO - val: {'epoch': 57, 'time_epoch': 3.78332, 'loss': 0.62062768, 'lr': 0, 'params': 541238, 'time_iter': 0.06005, 'accuracy': 0.782, 'f1': 0.782, 'accuracy-SBM': 0.78201, 'auc': 0.96169}
2025-07-11 13:48:06,590 - INFO - test: {'epoch': 57, 'time_epoch': 3.65874, 'loss': 0.61764242, 'lr': 0, 'params': 541238, 'time_iter': 0.05808, 'accuracy': 0.78131, 'f1': 0.78133, 'accuracy-SBM': 0.78135, 'auc': 0.96205}
2025-07-11 13:48:06,592 - INFO - > Epoch 57: took 89.0s (avg 90.8s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:49:27,943 - INFO - train: {'epoch': 58, 'time_epoch': 81.01022, 'eta': 3379.42148, 'eta_hours': 0.93873, 'loss': 0.56161146, 'lr': 0.00040956, 'params': 541238, 'time_iter': 0.12962, 'accuracy': 0.79653, 'f1': 0.79653, 'accuracy-SBM': 0.79653, 'auc': 0.96843}
2025-07-11 13:49:31,974 - INFO - val: {'epoch': 58, 'time_epoch': 3.98619, 'loss': 0.6170123, 'lr': 0, 'params': 541238, 'time_iter': 0.06327, 'accuracy': 0.78313, 'f1': 0.78299, 'accuracy-SBM': 0.78293, 'auc': 0.96205}
2025-07-11 13:49:35,972 - INFO - test: {'epoch': 58, 'time_epoch': 3.95387, 'loss': 0.61290029, 'lr': 0, 'params': 541238, 'time_iter': 0.06276, 'accuracy': 0.78246, 'f1': 0.78246, 'accuracy-SBM': 0.78244, 'auc': 0.96257}
2025-07-11 13:49:35,974 - INFO - > Epoch 58: took 89.4s (avg 90.8s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:50:57,913 - INFO - train: {'epoch': 59, 'time_epoch': 81.59111, 'eta': 3296.4407, 'eta_hours': 0.91568, 'loss': 0.56056857, 'lr': 0.00039335, 'params': 541238, 'time_iter': 0.13055, 'accuracy': 0.79634, 'f1': 0.79634, 'accuracy-SBM': 0.79634, 'auc': 0.96854}
2025-07-11 13:51:02,014 - INFO - val: {'epoch': 59, 'time_epoch': 4.05639, 'loss': 0.6202259, 'lr': 0, 'params': 541238, 'time_iter': 0.06439, 'accuracy': 0.78094, 'f1': 0.78091, 'accuracy-SBM': 0.78093, 'auc': 0.96178}
2025-07-11 13:51:06,060 - INFO - test: {'epoch': 59, 'time_epoch': 4.01241, 'loss': 0.61749029, 'lr': 0, 'params': 541238, 'time_iter': 0.06369, 'accuracy': 0.78029, 'f1': 0.7803, 'accuracy-SBM': 0.78032, 'auc': 0.96216}
2025-07-11 13:51:06,062 - INFO - > Epoch 59: took 90.1s (avg 90.8s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:52:27,754 - INFO - train: {'epoch': 60, 'time_epoch': 81.45753, 'eta': 3213.42008, 'eta_hours': 0.89262, 'loss': 0.55771669, 'lr': 0.00037726, 'params': 541238, 'time_iter': 0.13033, 'accuracy': 0.79809, 'f1': 0.7981, 'accuracy-SBM': 0.79809, 'auc': 0.96887}
2025-07-11 13:52:31,781 - INFO - val: {'epoch': 60, 'time_epoch': 3.98172, 'loss': 0.61801257, 'lr': 0, 'params': 541238, 'time_iter': 0.0632, 'accuracy': 0.78268, 'f1': 0.78258, 'accuracy-SBM': 0.78254, 'auc': 0.96177}
2025-07-11 13:52:35,756 - INFO - test: {'epoch': 60, 'time_epoch': 3.94204, 'loss': 0.61961104, 'lr': 0, 'params': 541238, 'time_iter': 0.06257, 'accuracy': 0.78187, 'f1': 0.78185, 'accuracy-SBM': 0.78186, 'auc': 0.96161}
2025-07-11 13:52:35,758 - INFO - > Epoch 60: took 89.7s (avg 90.8s) | Best so far: epoch 44	train_loss: 0.5990 train_accuracy-SBM: 0.7831	val_loss: 0.6188 val_accuracy-SBM: 0.7833	test_loss: 0.6204 test_accuracy-SBM: 0.7779
2025-07-11 13:53:57,200 - INFO - train: {'epoch': 61, 'time_epoch': 81.10407, 'eta': 3130.23324, 'eta_hours': 0.86951, 'loss': 0.55429924, 'lr': 0.0003613, 'params': 541238, 'time_iter': 0.12977, 'accuracy': 0.79838, 'f1': 0.79838, 'accuracy-SBM': 0.79839, 'auc': 0.96926}
2025-07-11 13:54:01,220 - INFO - val: {'epoch': 61, 'time_epoch': 3.97575, 'loss': 0.62364161, 'lr': 0, 'params': 541238, 'time_iter': 0.06311, 'accuracy': 0.78346, 'f1': 0.78336, 'accuracy-SBM': 0.78336, 'auc': 0.96134}
2025-07-11 13:54:05,193 - INFO - test: {'epoch': 61, 'time_epoch': 3.93987, 'loss': 0.6152244, 'lr': 0, 'params': 541238, 'time_iter': 0.06254, 'accuracy': 0.78263, 'f1': 0.78261, 'accuracy-SBM': 0.78264, 'auc': 0.96237}
2025-07-11 13:54:05,195 - INFO - > Epoch 61: took 89.4s (avg 90.8s) | Best so far: epoch 61	train_loss: 0.5543 train_accuracy-SBM: 0.7984	val_loss: 0.6236 val_accuracy-SBM: 0.7834	test_loss: 0.6152 test_accuracy-SBM: 0.7826
2025-07-11 13:55:25,895 - INFO - train: {'epoch': 62, 'time_epoch': 80.36668, 'eta': 3046.67945, 'eta_hours': 0.8463, 'loss': 0.55204282, 'lr': 0.00034549, 'params': 541238, 'time_iter': 0.12859, 'accuracy': 0.79922, 'f1': 0.79922, 'accuracy-SBM': 0.79922, 'auc': 0.9695}
2025-07-11 13:55:29,870 - INFO - val: {'epoch': 62, 'time_epoch': 3.93068, 'loss': 0.62384564, 'lr': 0, 'params': 541238, 'time_iter': 0.06239, 'accuracy': 0.78275, 'f1': 0.78262, 'accuracy-SBM': 0.78263, 'auc': 0.96126}
2025-07-11 13:55:33,840 - INFO - test: {'epoch': 62, 'time_epoch': 3.93768, 'loss': 0.61860347, 'lr': 0, 'params': 541238, 'time_iter': 0.0625, 'accuracy': 0.78092, 'f1': 0.78082, 'accuracy-SBM': 0.78083, 'auc': 0.96196}
2025-07-11 13:55:33,842 - INFO - > Epoch 62: took 88.6s (avg 90.7s) | Best so far: epoch 61	train_loss: 0.5543 train_accuracy-SBM: 0.7984	val_loss: 0.6236 val_accuracy-SBM: 0.7834	test_loss: 0.6152 test_accuracy-SBM: 0.7826
2025-07-11 13:56:55,497 - INFO - train: {'epoch': 63, 'time_epoch': 81.4156, 'eta': 2963.81528, 'eta_hours': 0.82328, 'loss': 0.54743966, 'lr': 0.00032985, 'params': 541238, 'time_iter': 0.13026, 'accuracy': 0.80133, 'f1': 0.80133, 'accuracy-SBM': 0.80133, 'auc': 0.97001}
2025-07-11 13:56:59,504 - INFO - val: {'epoch': 63, 'time_epoch': 3.96311, 'loss': 0.62638094, 'lr': 0, 'params': 541238, 'time_iter': 0.06291, 'accuracy': 0.78402, 'f1': 0.7839, 'accuracy-SBM': 0.78396, 'auc': 0.96133}
2025-07-11 13:57:03,477 - INFO - test: {'epoch': 63, 'time_epoch': 3.93841, 'loss': 0.62309193, 'lr': 0, 'params': 541238, 'time_iter': 0.06251, 'accuracy': 0.77925, 'f1': 0.77925, 'accuracy-SBM': 0.77924, 'auc': 0.96177}
2025-07-11 13:57:03,479 - INFO - > Epoch 63: took 89.6s (avg 90.7s) | Best so far: epoch 63	train_loss: 0.5474 train_accuracy-SBM: 0.8013	val_loss: 0.6264 val_accuracy-SBM: 0.7840	test_loss: 0.6231 test_accuracy-SBM: 0.7792
2025-07-11 13:58:24,744 - INFO - train: {'epoch': 64, 'time_epoch': 81.02683, 'eta': 2880.78634, 'eta_hours': 0.80022, 'loss': 0.54662749, 'lr': 0.0003144, 'params': 541238, 'time_iter': 0.12964, 'accuracy': 0.80116, 'f1': 0.80116, 'accuracy-SBM': 0.80116, 'auc': 0.97011}
2025-07-11 13:58:28,756 - INFO - val: {'epoch': 64, 'time_epoch': 3.95876, 'loss': 0.62747766, 'lr': 0, 'params': 541238, 'time_iter': 0.06284, 'accuracy': 0.78397, 'f1': 0.78389, 'accuracy-SBM': 0.78382, 'auc': 0.96136}
2025-07-11 13:58:32,735 - INFO - test: {'epoch': 64, 'time_epoch': 3.94684, 'loss': 0.61949969, 'lr': 0, 'params': 541238, 'time_iter': 0.06265, 'accuracy': 0.78235, 'f1': 0.78233, 'accuracy-SBM': 0.78236, 'auc': 0.96226}
2025-07-11 13:58:32,738 - INFO - > Epoch 64: took 89.3s (avg 90.7s) | Best so far: epoch 63	train_loss: 0.5474 train_accuracy-SBM: 0.8013	val_loss: 0.6264 val_accuracy-SBM: 0.7840	test_loss: 0.6231 test_accuracy-SBM: 0.7792
2025-07-11 13:59:53,707 - INFO - train: {'epoch': 65, 'time_epoch': 80.73554, 'eta': 2797.66801, 'eta_hours': 0.77713, 'loss': 0.54556849, 'lr': 0.00029915, 'params': 541238, 'time_iter': 0.12918, 'accuracy': 0.80203, 'f1': 0.80202, 'accuracy-SBM': 0.80202, 'auc': 0.97021}
2025-07-11 13:59:57,730 - INFO - val: {'epoch': 65, 'time_epoch': 3.97878, 'loss': 0.62844878, 'lr': 0, 'params': 541238, 'time_iter': 0.06316, 'accuracy': 0.78178, 'f1': 0.78169, 'accuracy-SBM': 0.78169, 'auc': 0.96091}
2025-07-11 14:00:01,698 - INFO - test: {'epoch': 65, 'time_epoch': 3.93117, 'loss': 0.62268501, 'lr': 0, 'params': 541238, 'time_iter': 0.0624, 'accuracy': 0.78022, 'f1': 0.78017, 'accuracy-SBM': 0.78024, 'auc': 0.96165}
2025-07-11 14:00:01,700 - INFO - > Epoch 65: took 89.0s (avg 90.7s) | Best so far: epoch 63	train_loss: 0.5474 train_accuracy-SBM: 0.8013	val_loss: 0.6264 val_accuracy-SBM: 0.7840	test_loss: 0.6231 test_accuracy-SBM: 0.7792
2025-07-11 14:01:21,247 - INFO - train: {'epoch': 66, 'time_epoch': 79.30514, 'eta': 2713.91628, 'eta_hours': 0.75387, 'loss': 0.54234666, 'lr': 0.00028412, 'params': 541238, 'time_iter': 0.12689, 'accuracy': 0.80283, 'f1': 0.80283, 'accuracy-SBM': 0.80283, 'auc': 0.97057}
2025-07-11 14:01:25,091 - INFO - val: {'epoch': 66, 'time_epoch': 3.80001, 'loss': 0.62386882, 'lr': 0, 'params': 541238, 'time_iter': 0.06032, 'accuracy': 0.78339, 'f1': 0.78331, 'accuracy-SBM': 0.78328, 'auc': 0.96126}
2025-07-11 14:01:28,933 - INFO - test: {'epoch': 66, 'time_epoch': 3.80727, 'loss': 0.62145568, 'lr': 0, 'params': 541238, 'time_iter': 0.06043, 'accuracy': 0.78125, 'f1': 0.78122, 'accuracy-SBM': 0.78124, 'auc': 0.96162}
2025-07-11 14:01:28,935 - INFO - > Epoch 66: took 87.2s (avg 90.6s) | Best so far: epoch 63	train_loss: 0.5474 train_accuracy-SBM: 0.8013	val_loss: 0.6264 val_accuracy-SBM: 0.7840	test_loss: 0.6231 test_accuracy-SBM: 0.7792
2025-07-11 14:02:48,320 - INFO - train: {'epoch': 67, 'time_epoch': 79.1395, 'eta': 2630.21739, 'eta_hours': 0.73062, 'loss': 0.54232401, 'lr': 0.00026933, 'params': 541238, 'time_iter': 0.12662, 'accuracy': 0.80308, 'f1': 0.80308, 'accuracy-SBM': 0.80308, 'auc': 0.97057}
2025-07-11 14:02:52,383 - INFO - val: {'epoch': 67, 'time_epoch': 4.01836, 'loss': 0.62290577, 'lr': 0, 'params': 541238, 'time_iter': 0.06378, 'accuracy': 0.7837, 'f1': 0.78365, 'accuracy-SBM': 0.78361, 'auc': 0.96132}
2025-07-11 14:02:56,401 - INFO - test: {'epoch': 67, 'time_epoch': 3.98388, 'loss': 0.62002956, 'lr': 0, 'params': 541238, 'time_iter': 0.06324, 'accuracy': 0.78028, 'f1': 0.78025, 'accuracy-SBM': 0.78031, 'auc': 0.96175}
2025-07-11 14:02:56,402 - INFO - > Epoch 67: took 87.5s (avg 90.6s) | Best so far: epoch 63	train_loss: 0.5474 train_accuracy-SBM: 0.8013	val_loss: 0.6264 val_accuracy-SBM: 0.7840	test_loss: 0.6231 test_accuracy-SBM: 0.7792
2025-07-11 14:04:17,965 - INFO - train: {'epoch': 68, 'time_epoch': 81.31737, 'eta': 2547.62912, 'eta_hours': 0.70767, 'loss': 0.53857452, 'lr': 0.00025479, 'params': 541238, 'time_iter': 0.13011, 'accuracy': 0.80448, 'f1': 0.80448, 'accuracy-SBM': 0.80448, 'auc': 0.97098}
2025-07-11 14:04:22,031 - INFO - val: {'epoch': 68, 'time_epoch': 4.01887, 'loss': 0.62150348, 'lr': 0, 'params': 541238, 'time_iter': 0.06379, 'accuracy': 0.78566, 'f1': 0.78555, 'accuracy-SBM': 0.78554, 'auc': 0.96175}
2025-07-11 14:04:26,080 - INFO - test: {'epoch': 68, 'time_epoch': 4.00824, 'loss': 0.61956032, 'lr': 0, 'params': 541238, 'time_iter': 0.06362, 'accuracy': 0.78257, 'f1': 0.78257, 'accuracy-SBM': 0.78255, 'auc': 0.96205}
2025-07-11 14:04:26,082 - INFO - > Epoch 68: took 89.7s (avg 90.6s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:05:47,156 - INFO - train: {'epoch': 69, 'time_epoch': 80.72384, 'eta': 2464.82278, 'eta_hours': 0.68467, 'loss': 0.53501474, 'lr': 0.00024052, 'params': 541238, 'time_iter': 0.12916, 'accuracy': 0.80579, 'f1': 0.80579, 'accuracy-SBM': 0.80579, 'auc': 0.97136}
2025-07-11 14:05:50,943 - INFO - val: {'epoch': 69, 'time_epoch': 3.73658, 'loss': 0.63147829, 'lr': 0, 'params': 541238, 'time_iter': 0.05931, 'accuracy': 0.78414, 'f1': 0.78405, 'accuracy-SBM': 0.78407, 'auc': 0.96091}
2025-07-11 14:05:54,678 - INFO - test: {'epoch': 69, 'time_epoch': 3.70235, 'loss': 0.62124121, 'lr': 0, 'params': 541238, 'time_iter': 0.05877, 'accuracy': 0.78353, 'f1': 0.78351, 'accuracy-SBM': 0.78352, 'auc': 0.96219}
2025-07-11 14:05:54,680 - INFO - > Epoch 69: took 88.6s (avg 90.5s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:07:14,398 - INFO - train: {'epoch': 70, 'time_epoch': 79.46775, 'eta': 2381.56206, 'eta_hours': 0.66155, 'loss': 0.53336442, 'lr': 0.00022653, 'params': 541238, 'time_iter': 0.12715, 'accuracy': 0.80642, 'f1': 0.80642, 'accuracy-SBM': 0.80642, 'auc': 0.97153}
2025-07-11 14:07:18,385 - INFO - val: {'epoch': 70, 'time_epoch': 3.94335, 'loss': 0.62240014, 'lr': 0, 'params': 541238, 'time_iter': 0.06259, 'accuracy': 0.78531, 'f1': 0.78515, 'accuracy-SBM': 0.78516, 'auc': 0.9617}
2025-07-11 14:07:22,291 - INFO - test: {'epoch': 70, 'time_epoch': 3.86717, 'loss': 0.62333637, 'lr': 0, 'params': 541238, 'time_iter': 0.06138, 'accuracy': 0.7827, 'f1': 0.78268, 'accuracy-SBM': 0.78267, 'auc': 0.96174}
2025-07-11 14:07:22,294 - INFO - > Epoch 70: took 87.6s (avg 90.5s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:08:41,348 - INFO - train: {'epoch': 71, 'time_epoch': 78.81881, 'eta': 2298.15434, 'eta_hours': 0.63838, 'loss': 0.53184378, 'lr': 0.00021284, 'params': 541238, 'time_iter': 0.12611, 'accuracy': 0.80673, 'f1': 0.80673, 'accuracy-SBM': 0.80673, 'auc': 0.9717}
2025-07-11 14:08:45,132 - INFO - val: {'epoch': 71, 'time_epoch': 3.73208, 'loss': 0.62921395, 'lr': 0, 'params': 541238, 'time_iter': 0.05924, 'accuracy': 0.78306, 'f1': 0.78295, 'accuracy-SBM': 0.7829, 'auc': 0.96117}
2025-07-11 14:08:48,884 - INFO - test: {'epoch': 71, 'time_epoch': 3.71964, 'loss': 0.62528979, 'lr': 0, 'params': 541238, 'time_iter': 0.05904, 'accuracy': 0.78194, 'f1': 0.78187, 'accuracy-SBM': 0.7819, 'auc': 0.96157}
2025-07-11 14:08:48,886 - INFO - > Epoch 71: took 86.6s (avg 90.4s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:10:06,994 - INFO - train: {'epoch': 72, 'time_epoch': 77.86545, 'eta': 2214.51972, 'eta_hours': 0.61514, 'loss': 0.52997001, 'lr': 0.00019946, 'params': 541238, 'time_iter': 0.12458, 'accuracy': 0.80763, 'f1': 0.80763, 'accuracy-SBM': 0.80763, 'auc': 0.97189}
2025-07-11 14:10:10,766 - INFO - val: {'epoch': 72, 'time_epoch': 3.72066, 'loss': 0.62416264, 'lr': 0, 'params': 541238, 'time_iter': 0.05906, 'accuracy': 0.78528, 'f1': 0.78516, 'accuracy-SBM': 0.78511, 'auc': 0.96157}
2025-07-11 14:10:14,507 - INFO - test: {'epoch': 72, 'time_epoch': 3.70837, 'loss': 0.62758064, 'lr': 0, 'params': 541238, 'time_iter': 0.05886, 'accuracy': 0.78168, 'f1': 0.78168, 'accuracy-SBM': 0.78168, 'auc': 0.96119}
2025-07-11 14:10:14,510 - INFO - > Epoch 72: took 85.6s (avg 90.4s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:11:34,319 - INFO - train: {'epoch': 73, 'time_epoch': 79.57216, 'eta': 2131.64068, 'eta_hours': 0.59212, 'loss': 0.52881536, 'lr': 0.00018641, 'params': 541238, 'time_iter': 0.12732, 'accuracy': 0.80786, 'f1': 0.80786, 'accuracy-SBM': 0.80786, 'auc': 0.97202}
2025-07-11 14:11:38,297 - INFO - val: {'epoch': 73, 'time_epoch': 3.92529, 'loss': 0.62776647, 'lr': 0, 'params': 541238, 'time_iter': 0.06231, 'accuracy': 0.78477, 'f1': 0.78466, 'accuracy-SBM': 0.78464, 'auc': 0.96109}
2025-07-11 14:11:42,260 - INFO - test: {'epoch': 73, 'time_epoch': 3.93025, 'loss': 0.62624455, 'lr': 0, 'params': 541238, 'time_iter': 0.06238, 'accuracy': 0.77966, 'f1': 0.77967, 'accuracy-SBM': 0.77967, 'auc': 0.96138}
2025-07-11 14:11:42,262 - INFO - > Epoch 73: took 87.8s (avg 90.3s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:13:02,906 - INFO - train: {'epoch': 74, 'time_epoch': 80.40744, 'eta': 2049.12826, 'eta_hours': 0.5692, 'loss': 0.5243715, 'lr': 0.00017371, 'params': 541238, 'time_iter': 0.12865, 'accuracy': 0.80974, 'f1': 0.80973, 'accuracy-SBM': 0.80973, 'auc': 0.97249}
2025-07-11 14:13:06,929 - INFO - val: {'epoch': 74, 'time_epoch': 3.97777, 'loss': 0.62643474, 'lr': 0, 'params': 541238, 'time_iter': 0.06314, 'accuracy': 0.78489, 'f1': 0.78481, 'accuracy-SBM': 0.78481, 'auc': 0.96133}
2025-07-11 14:13:10,918 - INFO - test: {'epoch': 74, 'time_epoch': 3.9489, 'loss': 0.62202062, 'lr': 0, 'params': 541238, 'time_iter': 0.06268, 'accuracy': 0.78355, 'f1': 0.78352, 'accuracy-SBM': 0.78355, 'auc': 0.96185}
2025-07-11 14:13:10,920 - INFO - > Epoch 74: took 88.7s (avg 90.3s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:14:32,686 - INFO - train: {'epoch': 75, 'time_epoch': 81.51963, 'eta': 1967.02244, 'eta_hours': 0.5464, 'loss': 0.5250511, 'lr': 0.00016136, 'params': 541238, 'time_iter': 0.13043, 'accuracy': 0.80973, 'f1': 0.80973, 'accuracy-SBM': 0.80973, 'auc': 0.97241}
2025-07-11 14:14:36,724 - INFO - val: {'epoch': 75, 'time_epoch': 3.99191, 'loss': 0.62360752, 'lr': 0, 'params': 541238, 'time_iter': 0.06336, 'accuracy': 0.78435, 'f1': 0.78424, 'accuracy-SBM': 0.78418, 'auc': 0.96149}
2025-07-11 14:14:40,729 - INFO - test: {'epoch': 75, 'time_epoch': 3.97183, 'loss': 0.61826499, 'lr': 0, 'params': 541238, 'time_iter': 0.06304, 'accuracy': 0.78245, 'f1': 0.78245, 'accuracy-SBM': 0.78246, 'auc': 0.96212}
2025-07-11 14:14:40,732 - INFO - > Epoch 75: took 89.8s (avg 90.3s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:16:01,204 - INFO - train: {'epoch': 76, 'time_epoch': 80.2375, 'eta': 1884.54888, 'eta_hours': 0.52349, 'loss': 0.52324403, 'lr': 0.00014938, 'params': 541238, 'time_iter': 0.12838, 'accuracy': 0.81002, 'f1': 0.81002, 'accuracy-SBM': 0.81002, 'auc': 0.9726}
2025-07-11 14:16:04,994 - INFO - val: {'epoch': 76, 'time_epoch': 3.74618, 'loss': 0.62644799, 'lr': 0, 'params': 541238, 'time_iter': 0.05946, 'accuracy': 0.78367, 'f1': 0.78359, 'accuracy-SBM': 0.78353, 'auc': 0.96129}
2025-07-11 14:16:08,717 - INFO - test: {'epoch': 76, 'time_epoch': 3.6916, 'loss': 0.6248034, 'lr': 0, 'params': 541238, 'time_iter': 0.0586, 'accuracy': 0.78125, 'f1': 0.78123, 'accuracy-SBM': 0.78126, 'auc': 0.96152}
2025-07-11 14:16:08,719 - INFO - > Epoch 76: took 88.0s (avg 90.3s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:17:26,737 - INFO - train: {'epoch': 77, 'time_epoch': 77.78616, 'eta': 1801.44125, 'eta_hours': 0.5004, 'loss': 0.51867183, 'lr': 0.00013779, 'params': 541238, 'time_iter': 0.12446, 'accuracy': 0.81145, 'f1': 0.81145, 'accuracy-SBM': 0.81145, 'auc': 0.97308}
2025-07-11 14:17:30,568 - INFO - val: {'epoch': 77, 'time_epoch': 3.78629, 'loss': 0.62503367, 'lr': 0, 'params': 541238, 'time_iter': 0.0601, 'accuracy': 0.78509, 'f1': 0.78503, 'accuracy-SBM': 0.785, 'auc': 0.96141}
2025-07-11 14:17:34,324 - INFO - test: {'epoch': 77, 'time_epoch': 3.72395, 'loss': 0.62343717, 'lr': 0, 'params': 541238, 'time_iter': 0.05911, 'accuracy': 0.78283, 'f1': 0.78281, 'accuracy-SBM': 0.78285, 'auc': 0.96172}
2025-07-11 14:17:34,327 - INFO - > Epoch 77: took 85.6s (avg 90.2s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:18:55,505 - INFO - train: {'epoch': 78, 'time_epoch': 80.93416, 'eta': 1719.30515, 'eta_hours': 0.47758, 'loss': 0.51971397, 'lr': 0.00012659, 'params': 541238, 'time_iter': 0.12949, 'accuracy': 0.81107, 'f1': 0.81107, 'accuracy-SBM': 0.81107, 'auc': 0.97297}
2025-07-11 14:18:59,496 - INFO - val: {'epoch': 78, 'time_epoch': 3.94774, 'loss': 0.62969437, 'lr': 0, 'params': 541238, 'time_iter': 0.06266, 'accuracy': 0.78513, 'f1': 0.78501, 'accuracy-SBM': 0.78506, 'auc': 0.96106}
2025-07-11 14:19:03,472 - INFO - test: {'epoch': 78, 'time_epoch': 3.94316, 'loss': 0.62806696, 'lr': 0, 'params': 541238, 'time_iter': 0.06259, 'accuracy': 0.78019, 'f1': 0.78023, 'accuracy-SBM': 0.78019, 'auc': 0.96134}
2025-07-11 14:19:03,474 - INFO - > Epoch 78: took 89.1s (avg 90.2s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:20:24,499 - INFO - train: {'epoch': 79, 'time_epoch': 80.77467, 'eta': 1637.15922, 'eta_hours': 0.45477, 'loss': 0.51819834, 'lr': 0.0001158, 'params': 541238, 'time_iter': 0.12924, 'accuracy': 0.81196, 'f1': 0.81196, 'accuracy-SBM': 0.81196, 'auc': 0.97312}
2025-07-11 14:20:28,491 - INFO - val: {'epoch': 79, 'time_epoch': 3.9488, 'loss': 0.62838317, 'lr': 0, 'params': 541238, 'time_iter': 0.06268, 'accuracy': 0.78406, 'f1': 0.78395, 'accuracy-SBM': 0.78395, 'auc': 0.9611}
2025-07-11 14:20:32,449 - INFO - test: {'epoch': 79, 'time_epoch': 3.92577, 'loss': 0.62278157, 'lr': 0, 'params': 541238, 'time_iter': 0.06231, 'accuracy': 0.7834, 'f1': 0.78339, 'accuracy-SBM': 0.7834, 'auc': 0.9618}
2025-07-11 14:20:32,451 - INFO - > Epoch 79: took 89.0s (avg 90.2s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:21:52,436 - INFO - train: {'epoch': 80, 'time_epoch': 79.74437, 'eta': 1554.80548, 'eta_hours': 0.43189, 'loss': 0.51541041, 'lr': 0.00010543, 'params': 541238, 'time_iter': 0.12759, 'accuracy': 0.81292, 'f1': 0.81292, 'accuracy-SBM': 0.81292, 'auc': 0.97342}
2025-07-11 14:21:56,459 - INFO - val: {'epoch': 80, 'time_epoch': 3.97636, 'loss': 0.63102839, 'lr': 0, 'params': 541238, 'time_iter': 0.06312, 'accuracy': 0.78527, 'f1': 0.78519, 'accuracy-SBM': 0.78516, 'auc': 0.96097}
2025-07-11 14:22:00,427 - INFO - test: {'epoch': 80, 'time_epoch': 3.92817, 'loss': 0.62597557, 'lr': 0, 'params': 541238, 'time_iter': 0.06235, 'accuracy': 0.7817, 'f1': 0.78168, 'accuracy-SBM': 0.7817, 'auc': 0.96158}
2025-07-11 14:22:00,429 - INFO - > Epoch 80: took 88.0s (avg 90.2s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:23:19,816 - INFO - train: {'epoch': 81, 'time_epoch': 79.1558, 'eta': 1472.38618, 'eta_hours': 0.409, 'loss': 0.51549902, 'lr': 9.549e-05, 'params': 541238, 'time_iter': 0.12665, 'accuracy': 0.81293, 'f1': 0.81293, 'accuracy-SBM': 0.81293, 'auc': 0.97341}
2025-07-11 14:23:23,591 - INFO - val: {'epoch': 81, 'time_epoch': 3.73141, 'loss': 0.62412859, 'lr': 0, 'params': 541238, 'time_iter': 0.05923, 'accuracy': 0.78546, 'f1': 0.78534, 'accuracy-SBM': 0.78535, 'auc': 0.96162}
2025-07-11 14:23:27,332 - INFO - test: {'epoch': 81, 'time_epoch': 3.70926, 'loss': 0.62093436, 'lr': 0, 'params': 541238, 'time_iter': 0.05888, 'accuracy': 0.78327, 'f1': 0.78328, 'accuracy-SBM': 0.78327, 'auc': 0.96204}
2025-07-11 14:23:27,334 - INFO - > Epoch 81: took 86.9s (avg 90.1s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:24:45,184 - INFO - train: {'epoch': 82, 'time_epoch': 77.60393, 'eta': 1389.72767, 'eta_hours': 0.38604, 'loss': 0.51444279, 'lr': 8.6e-05, 'params': 541238, 'time_iter': 0.12417, 'accuracy': 0.81308, 'f1': 0.81308, 'accuracy-SBM': 0.81308, 'auc': 0.97352}
2025-07-11 14:24:48,930 - INFO - val: {'epoch': 82, 'time_epoch': 3.70315, 'loss': 0.62946322, 'lr': 0, 'params': 541238, 'time_iter': 0.05878, 'accuracy': 0.78497, 'f1': 0.7849, 'accuracy-SBM': 0.78484, 'auc': 0.9612}
2025-07-11 14:24:52,839 - INFO - test: {'epoch': 82, 'time_epoch': 3.87719, 'loss': 0.62731164, 'lr': 0, 'params': 541238, 'time_iter': 0.06154, 'accuracy': 0.78257, 'f1': 0.78254, 'accuracy-SBM': 0.78257, 'auc': 0.9615}
2025-07-11 14:24:52,841 - INFO - > Epoch 82: took 85.5s (avg 90.1s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:26:14,119 - INFO - train: {'epoch': 83, 'time_epoch': 81.04308, 'eta': 1307.84458, 'eta_hours': 0.36329, 'loss': 0.51374184, 'lr': 7.695e-05, 'params': 541238, 'time_iter': 0.12967, 'accuracy': 0.81332, 'f1': 0.81332, 'accuracy-SBM': 0.81332, 'auc': 0.97359}
2025-07-11 14:26:18,115 - INFO - val: {'epoch': 83, 'time_epoch': 3.95112, 'loss': 0.63068812, 'lr': 0, 'params': 541238, 'time_iter': 0.06272, 'accuracy': 0.78539, 'f1': 0.78531, 'accuracy-SBM': 0.7853, 'auc': 0.96099}
2025-07-11 14:26:22,068 - INFO - test: {'epoch': 83, 'time_epoch': 3.92039, 'loss': 0.6280637, 'lr': 0, 'params': 541238, 'time_iter': 0.06223, 'accuracy': 0.78311, 'f1': 0.78308, 'accuracy-SBM': 0.7831, 'auc': 0.96134}
2025-07-11 14:26:22,070 - INFO - > Epoch 83: took 89.2s (avg 90.1s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:27:42,914 - INFO - train: {'epoch': 84, 'time_epoch': 80.60364, 'eta': 1225.90371, 'eta_hours': 0.34053, 'loss': 0.51283378, 'lr': 6.837e-05, 'params': 541238, 'time_iter': 0.12897, 'accuracy': 0.81387, 'f1': 0.81386, 'accuracy-SBM': 0.81386, 'auc': 0.97368}
2025-07-11 14:27:46,914 - INFO - val: {'epoch': 84, 'time_epoch': 3.95494, 'loss': 0.63147326, 'lr': 0, 'params': 541238, 'time_iter': 0.06278, 'accuracy': 0.78529, 'f1': 0.78523, 'accuracy-SBM': 0.7852, 'auc': 0.96089}
2025-07-11 14:27:50,888 - INFO - test: {'epoch': 84, 'time_epoch': 3.93263, 'loss': 0.62687234, 'lr': 0, 'params': 541238, 'time_iter': 0.06242, 'accuracy': 0.78312, 'f1': 0.78311, 'accuracy-SBM': 0.78314, 'auc': 0.96146}
2025-07-11 14:27:50,889 - INFO - > Epoch 84: took 88.8s (avg 90.0s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:29:12,771 - INFO - train: {'epoch': 85, 'time_epoch': 81.62189, 'eta': 1144.1597, 'eta_hours': 0.31782, 'loss': 0.5116954, 'lr': 6.026e-05, 'params': 541238, 'time_iter': 0.1306, 'accuracy': 0.81395, 'f1': 0.81395, 'accuracy-SBM': 0.81395, 'auc': 0.97381}
2025-07-11 14:29:16,770 - INFO - val: {'epoch': 85, 'time_epoch': 3.95397, 'loss': 0.63125827, 'lr': 0, 'params': 541238, 'time_iter': 0.06276, 'accuracy': 0.78464, 'f1': 0.78455, 'accuracy-SBM': 0.78453, 'auc': 0.96084}
2025-07-11 14:29:20,692 - INFO - test: {'epoch': 85, 'time_epoch': 3.88182, 'loss': 0.62541238, 'lr': 0, 'params': 541238, 'time_iter': 0.06162, 'accuracy': 0.78231, 'f1': 0.78224, 'accuracy-SBM': 0.78226, 'auc': 0.96153}
2025-07-11 14:29:20,694 - INFO - > Epoch 85: took 89.8s (avg 90.0s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:30:41,497 - INFO - train: {'epoch': 86, 'time_epoch': 80.55221, 'eta': 1062.25866, 'eta_hours': 0.29507, 'loss': 0.50862613, 'lr': 5.264e-05, 'params': 541238, 'time_iter': 0.12888, 'accuracy': 0.81524, 'f1': 0.81524, 'accuracy-SBM': 0.81524, 'auc': 0.97412}
2025-07-11 14:30:45,491 - INFO - val: {'epoch': 86, 'time_epoch': 3.94947, 'loss': 0.6277667, 'lr': 0, 'params': 541238, 'time_iter': 0.06269, 'accuracy': 0.78507, 'f1': 0.785, 'accuracy-SBM': 0.78501, 'auc': 0.96139}
2025-07-11 14:30:49,454 - INFO - test: {'epoch': 86, 'time_epoch': 3.93006, 'loss': 0.62882063, 'lr': 0, 'params': 541238, 'time_iter': 0.06238, 'accuracy': 0.78253, 'f1': 0.78252, 'accuracy-SBM': 0.78253, 'auc': 0.96135}
2025-07-11 14:30:49,456 - INFO - > Epoch 86: took 88.8s (avg 90.0s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:32:10,832 - INFO - train: {'epoch': 87, 'time_epoch': 81.03912, 'eta': 980.45467, 'eta_hours': 0.27235, 'loss': 0.5084959, 'lr': 4.55e-05, 'params': 541238, 'time_iter': 0.12966, 'accuracy': 0.81526, 'f1': 0.81526, 'accuracy-SBM': 0.81526, 'auc': 0.97412}
2025-07-11 14:32:14,838 - INFO - val: {'epoch': 87, 'time_epoch': 3.96143, 'loss': 0.6299055, 'lr': 0, 'params': 541238, 'time_iter': 0.06288, 'accuracy': 0.78536, 'f1': 0.78524, 'accuracy-SBM': 0.78526, 'auc': 0.96108}
2025-07-11 14:32:18,868 - INFO - test: {'epoch': 87, 'time_epoch': 3.9959, 'loss': 0.62809773, 'lr': 0, 'params': 541238, 'time_iter': 0.06343, 'accuracy': 0.78337, 'f1': 0.78337, 'accuracy-SBM': 0.78336, 'auc': 0.96134}
2025-07-11 14:32:18,870 - INFO - > Epoch 87: took 89.4s (avg 90.0s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:33:39,819 - INFO - train: {'epoch': 88, 'time_epoch': 80.70928, 'eta': 898.62711, 'eta_hours': 0.24962, 'loss': 0.5076032, 'lr': 3.886e-05, 'params': 541238, 'time_iter': 0.12913, 'accuracy': 0.81588, 'f1': 0.81588, 'accuracy-SBM': 0.81588, 'auc': 0.97422}
2025-07-11 14:33:43,806 - INFO - val: {'epoch': 88, 'time_epoch': 3.93993, 'loss': 0.63005094, 'lr': 0, 'params': 541238, 'time_iter': 0.06254, 'accuracy': 0.78511, 'f1': 0.78502, 'accuracy-SBM': 0.78499, 'auc': 0.96116}
2025-07-11 14:33:47,805 - INFO - test: {'epoch': 88, 'time_epoch': 3.96637, 'loss': 0.63086143, 'lr': 0, 'params': 541238, 'time_iter': 0.06296, 'accuracy': 0.78247, 'f1': 0.78245, 'accuracy-SBM': 0.78247, 'auc': 0.96111}
2025-07-11 14:33:47,807 - INFO - > Epoch 88: took 88.9s (avg 90.0s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:35:09,576 - INFO - train: {'epoch': 89, 'time_epoch': 81.40958, 'eta': 816.9022, 'eta_hours': 0.22692, 'loss': 0.50750834, 'lr': 3.272e-05, 'params': 541238, 'time_iter': 0.13026, 'accuracy': 0.81569, 'f1': 0.81569, 'accuracy-SBM': 0.81569, 'auc': 0.97423}
2025-07-11 14:35:13,578 - INFO - val: {'epoch': 89, 'time_epoch': 3.95663, 'loss': 0.63423414, 'lr': 0, 'params': 541238, 'time_iter': 0.0628, 'accuracy': 0.78489, 'f1': 0.7848, 'accuracy-SBM': 0.78481, 'auc': 0.96078}
2025-07-11 14:35:17,570 - INFO - test: {'epoch': 89, 'time_epoch': 3.94648, 'loss': 0.63256791, 'lr': 0, 'params': 541238, 'time_iter': 0.06264, 'accuracy': 0.78269, 'f1': 0.78268, 'accuracy-SBM': 0.7827, 'auc': 0.96104}
2025-07-11 14:35:17,572 - INFO - > Epoch 89: took 89.8s (avg 90.0s) | Best so far: epoch 68	train_loss: 0.5386 train_accuracy-SBM: 0.8045	val_loss: 0.6215 val_accuracy-SBM: 0.7855	test_loss: 0.6196 test_accuracy-SBM: 0.7825
2025-07-11 14:36:39,877 - INFO - train: {'epoch': 90, 'time_epoch': 82.04057, 'eta': 735.24663, 'eta_hours': 0.20424, 'loss': 0.50595207, 'lr': 2.709e-05, 'params': 541238, 'time_iter': 0.13126, 'accuracy': 0.81628, 'f1': 0.81628, 'accuracy-SBM': 0.81628, 'auc': 0.97439}
2025-07-11 14:36:43,965 - INFO - val: {'epoch': 90, 'time_epoch': 4.04314, 'loss': 0.63222827, 'lr': 0, 'params': 541238, 'time_iter': 0.06418, 'accuracy': 0.78602, 'f1': 0.78592, 'accuracy-SBM': 0.78591, 'auc': 0.96104}
2025-07-11 14:36:47,935 - INFO - test: {'epoch': 90, 'time_epoch': 3.93745, 'loss': 0.6322716, 'lr': 0, 'params': 541238, 'time_iter': 0.0625, 'accuracy': 0.78298, 'f1': 0.78294, 'accuracy-SBM': 0.78297, 'auc': 0.9611}
2025-07-11 14:36:47,937 - INFO - > Epoch 90: took 90.4s (avg 90.0s) | Best so far: epoch 90	train_loss: 0.5060 train_accuracy-SBM: 0.8163	val_loss: 0.6322 val_accuracy-SBM: 0.7859	test_loss: 0.6323 test_accuracy-SBM: 0.7830
2025-07-11 14:38:09,075 - INFO - train: {'epoch': 91, 'time_epoch': 80.89503, 'eta': 653.48308, 'eta_hours': 0.18152, 'loss': 0.50542103, 'lr': 2.198e-05, 'params': 541238, 'time_iter': 0.12943, 'accuracy': 0.8165, 'f1': 0.8165, 'accuracy-SBM': 0.8165, 'auc': 0.97444}
2025-07-11 14:38:13,086 - INFO - val: {'epoch': 91, 'time_epoch': 3.96425, 'loss': 0.63204096, 'lr': 0, 'params': 541238, 'time_iter': 0.06292, 'accuracy': 0.78542, 'f1': 0.7853, 'accuracy-SBM': 0.7853, 'auc': 0.96087}
2025-07-11 14:38:17,056 - INFO - test: {'epoch': 91, 'time_epoch': 3.93752, 'loss': 0.62788798, 'lr': 0, 'params': 541238, 'time_iter': 0.0625, 'accuracy': 0.78369, 'f1': 0.78368, 'accuracy-SBM': 0.78368, 'auc': 0.96139}
2025-07-11 14:38:17,058 - INFO - > Epoch 91: took 89.1s (avg 90.0s) | Best so far: epoch 90	train_loss: 0.5060 train_accuracy-SBM: 0.8163	val_loss: 0.6322 val_accuracy-SBM: 0.7859	test_loss: 0.6323 test_accuracy-SBM: 0.7830
2025-07-11 14:39:38,663 - INFO - train: {'epoch': 92, 'time_epoch': 81.26823, 'eta': 571.76629, 'eta_hours': 0.15882, 'loss': 0.50744915, 'lr': 1.74e-05, 'params': 541238, 'time_iter': 0.13003, 'accuracy': 0.81593, 'f1': 0.81593, 'accuracy-SBM': 0.81593, 'auc': 0.97424}
2025-07-11 14:39:42,683 - INFO - val: {'epoch': 92, 'time_epoch': 3.97508, 'loss': 0.63105503, 'lr': 0, 'params': 541238, 'time_iter': 0.0631, 'accuracy': 0.78532, 'f1': 0.78527, 'accuracy-SBM': 0.78526, 'auc': 0.96101}
2025-07-11 14:39:46,692 - INFO - test: {'epoch': 92, 'time_epoch': 3.97604, 'loss': 0.6281714, 'lr': 0, 'params': 541238, 'time_iter': 0.06311, 'accuracy': 0.78247, 'f1': 0.78246, 'accuracy-SBM': 0.78249, 'auc': 0.96142}
2025-07-11 14:39:46,694 - INFO - > Epoch 92: took 89.6s (avg 90.0s) | Best so far: epoch 90	train_loss: 0.5060 train_accuracy-SBM: 0.8163	val_loss: 0.6322 val_accuracy-SBM: 0.7859	test_loss: 0.6323 test_accuracy-SBM: 0.7830
2025-07-11 14:41:08,115 - INFO - train: {'epoch': 93, 'time_epoch': 81.19004, 'eta': 490.05406, 'eta_hours': 0.13613, 'loss': 0.50501036, 'lr': 1.334e-05, 'params': 541238, 'time_iter': 0.1299, 'accuracy': 0.81688, 'f1': 0.81688, 'accuracy-SBM': 0.81688, 'auc': 0.97448}
2025-07-11 14:41:12,106 - INFO - val: {'epoch': 93, 'time_epoch': 3.94627, 'loss': 0.63200926, 'lr': 0, 'params': 541238, 'time_iter': 0.06264, 'accuracy': 0.78494, 'f1': 0.78483, 'accuracy-SBM': 0.78482, 'auc': 0.96099}
2025-07-11 14:41:16,066 - INFO - test: {'epoch': 93, 'time_epoch': 3.92751, 'loss': 0.62989127, 'lr': 0, 'params': 541238, 'time_iter': 0.06234, 'accuracy': 0.78309, 'f1': 0.78307, 'accuracy-SBM': 0.78308, 'auc': 0.96128}
2025-07-11 14:41:16,068 - INFO - > Epoch 93: took 89.4s (avg 90.0s) | Best so far: epoch 90	train_loss: 0.5060 train_accuracy-SBM: 0.8163	val_loss: 0.6322 val_accuracy-SBM: 0.7859	test_loss: 0.6323 test_accuracy-SBM: 0.7830
2025-07-11 14:42:37,155 - INFO - train: {'epoch': 94, 'time_epoch': 80.82104, 'eta': 408.33341, 'eta_hours': 0.11343, 'loss': 0.50424776, 'lr': 9.81e-06, 'params': 541238, 'time_iter': 0.12931, 'accuracy': 0.81682, 'f1': 0.81682, 'accuracy-SBM': 0.81682, 'auc': 0.97456}
2025-07-11 14:42:41,159 - INFO - val: {'epoch': 94, 'time_epoch': 3.95837, 'loss': 0.62958218, 'lr': 0, 'params': 541238, 'time_iter': 0.06283, 'accuracy': 0.78611, 'f1': 0.78602, 'accuracy-SBM': 0.78603, 'auc': 0.9613}
2025-07-11 14:42:45,129 - INFO - test: {'epoch': 94, 'time_epoch': 3.93762, 'loss': 0.63177707, 'lr': 0, 'params': 541238, 'time_iter': 0.0625, 'accuracy': 0.78229, 'f1': 0.78227, 'accuracy-SBM': 0.78228, 'auc': 0.96114}
2025-07-11 14:42:45,131 - INFO - > Epoch 94: took 89.1s (avg 90.0s) | Best so far: epoch 94	train_loss: 0.5042 train_accuracy-SBM: 0.8168	val_loss: 0.6296 val_accuracy-SBM: 0.7860	test_loss: 0.6318 test_accuracy-SBM: 0.7823
2025-07-11 14:44:06,419 - INFO - train: {'epoch': 95, 'time_epoch': 81.04574, 'eta': 326.64085, 'eta_hours': 0.09073, 'loss': 0.50389064, 'lr': 6.82e-06, 'params': 541238, 'time_iter': 0.12967, 'accuracy': 0.81712, 'f1': 0.81712, 'accuracy-SBM': 0.81712, 'auc': 0.97459}
2025-07-11 14:44:10,368 - INFO - val: {'epoch': 95, 'time_epoch': 3.90572, 'loss': 0.63003258, 'lr': 0, 'params': 541238, 'time_iter': 0.062, 'accuracy': 0.78522, 'f1': 0.78514, 'accuracy-SBM': 0.78513, 'auc': 0.96101}
2025-07-11 14:44:14,312 - INFO - test: {'epoch': 95, 'time_epoch': 3.90988, 'loss': 0.62649159, 'lr': 0, 'params': 541238, 'time_iter': 0.06206, 'accuracy': 0.78342, 'f1': 0.78341, 'accuracy-SBM': 0.78343, 'auc': 0.9615}
2025-07-11 14:44:14,315 - INFO - > Epoch 95: took 89.2s (avg 90.0s) | Best so far: epoch 94	train_loss: 0.5042 train_accuracy-SBM: 0.8168	val_loss: 0.6296 val_accuracy-SBM: 0.7860	test_loss: 0.6318 test_accuracy-SBM: 0.7823
2025-07-11 14:45:35,559 - INFO - train: {'epoch': 96, 'time_epoch': 80.9091, 'eta': 244.95741, 'eta_hours': 0.06804, 'loss': 0.50438651, 'lr': 4.37e-06, 'params': 541238, 'time_iter': 0.12945, 'accuracy': 0.81673, 'f1': 0.81673, 'accuracy-SBM': 0.81673, 'auc': 0.97455}
2025-07-11 14:45:39,570 - INFO - val: {'epoch': 96, 'time_epoch': 3.96467, 'loss': 0.62957768, 'lr': 0, 'params': 541238, 'time_iter': 0.06293, 'accuracy': 0.78591, 'f1': 0.78582, 'accuracy-SBM': 0.78581, 'auc': 0.96124}
2025-07-11 14:45:43,537 - INFO - test: {'epoch': 96, 'time_epoch': 3.93496, 'loss': 0.62902195, 'lr': 0, 'params': 541238, 'time_iter': 0.06246, 'accuracy': 0.7835, 'f1': 0.78349, 'accuracy-SBM': 0.7835, 'auc': 0.96137}
2025-07-11 14:45:43,539 - INFO - > Epoch 96: took 89.2s (avg 90.0s) | Best so far: epoch 94	train_loss: 0.5042 train_accuracy-SBM: 0.8168	val_loss: 0.6296 val_accuracy-SBM: 0.7860	test_loss: 0.6318 test_accuracy-SBM: 0.7823
2025-07-11 14:47:05,149 - INFO - train: {'epoch': 97, 'time_epoch': 81.37442, 'eta': 163.29926, 'eta_hours': 0.04536, 'loss': 0.50234719, 'lr': 2.46e-06, 'params': 541238, 'time_iter': 0.1302, 'accuracy': 0.81798, 'f1': 0.81798, 'accuracy-SBM': 0.81798, 'auc': 0.97475}
2025-07-11 14:47:09,141 - INFO - val: {'epoch': 97, 'time_epoch': 3.94748, 'loss': 0.6297978, 'lr': 0, 'params': 541238, 'time_iter': 0.06266, 'accuracy': 0.78608, 'f1': 0.78599, 'accuracy-SBM': 0.78598, 'auc': 0.96127}
2025-07-11 14:47:13,105 - INFO - test: {'epoch': 97, 'time_epoch': 3.93081, 'loss': 0.62906463, 'lr': 0, 'params': 541238, 'time_iter': 0.06239, 'accuracy': 0.78336, 'f1': 0.78334, 'accuracy-SBM': 0.78336, 'auc': 0.96141}
2025-07-11 14:47:13,107 - INFO - > Epoch 97: took 89.6s (avg 90.0s) | Best so far: epoch 94	train_loss: 0.5042 train_accuracy-SBM: 0.8168	val_loss: 0.6296 val_accuracy-SBM: 0.7860	test_loss: 0.6318 test_accuracy-SBM: 0.7823
2025-07-11 14:48:32,960 - INFO - train: {'epoch': 98, 'time_epoch': 79.51628, 'eta': 81.62808, 'eta_hours': 0.02267, 'loss': 0.50309898, 'lr': 1.09e-06, 'params': 541238, 'time_iter': 0.12723, 'accuracy': 0.81742, 'f1': 0.81741, 'accuracy-SBM': 0.81741, 'auc': 0.97468}
2025-07-11 14:48:36,798 - INFO - val: {'epoch': 98, 'time_epoch': 3.78521, 'loss': 0.62962335, 'lr': 0, 'params': 541238, 'time_iter': 0.06008, 'accuracy': 0.78523, 'f1': 0.78515, 'accuracy-SBM': 0.78515, 'auc': 0.9611}
2025-07-11 14:48:40,622 - INFO - test: {'epoch': 98, 'time_epoch': 3.78974, 'loss': 0.62718683, 'lr': 0, 'params': 541238, 'time_iter': 0.06015, 'accuracy': 0.78328, 'f1': 0.78327, 'accuracy-SBM': 0.78328, 'auc': 0.96146}
2025-07-11 14:48:40,624 - INFO - > Epoch 98: took 87.5s (avg 89.9s) | Best so far: epoch 94	train_loss: 0.5042 train_accuracy-SBM: 0.8168	val_loss: 0.6296 val_accuracy-SBM: 0.7860	test_loss: 0.6318 test_accuracy-SBM: 0.7823
2025-07-11 14:49:59,976 - INFO - train: {'epoch': 99, 'time_epoch': 79.10816, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.50462368, 'lr': 2.7e-07, 'params': 541238, 'time_iter': 0.12657, 'accuracy': 0.81722, 'f1': 0.81722, 'accuracy-SBM': 0.81722, 'auc': 0.97452}
2025-07-11 14:50:03,997 - INFO - val: {'epoch': 99, 'time_epoch': 3.97479, 'loss': 0.63070217, 'lr': 0, 'params': 541238, 'time_iter': 0.06309, 'accuracy': 0.7865, 'f1': 0.78641, 'accuracy-SBM': 0.78641, 'auc': 0.96106}
2025-07-11 14:50:08,006 - INFO - test: {'epoch': 99, 'time_epoch': 3.97609, 'loss': 0.62903405, 'lr': 0, 'params': 541238, 'time_iter': 0.06311, 'accuracy': 0.78373, 'f1': 0.7837, 'accuracy-SBM': 0.78373, 'auc': 0.96131}
2025-07-11 14:50:08,258 - INFO - > Epoch 99: took 87.4s (avg 89.9s) | Best so far: epoch 99	train_loss: 0.5046 train_accuracy-SBM: 0.8172	val_loss: 0.6307 val_accuracy-SBM: 0.7864	test_loss: 0.6290 test_accuracy-SBM: 0.7837
2025-07-11 14:50:08,259 - INFO - Avg time per epoch: 89.91s
2025-07-11 14:50:08,259 - INFO - Total train loop time: 2.50h
2025-07-11 14:50:08,260 - INFO - Task done, results saved in results/Cluster/Cluster-GATEDGCN-47
2025-07-11 14:50:08,260 - INFO - Total time: 9055.00s (2.52h)
2025-07-11 14:50:08,361 - INFO - Results aggregated across runs saved in results/Cluster/Cluster-GATEDGCN-47/agg
2025-07-11 14:50:08,361 - INFO - === OPTIMIZED TRAINING COMPLETED SUCCESSFULLY! ===
2025-07-11 14:50:08,361 - INFO - Results saved in: results/Cluster/Cluster-GATEDGCN-47
2025-07-11 14:50:08,361 - INFO - Test results JSON files saved in: results/Cluster/Cluster-GATEDGCN-47/test_results/
Completed seed 47. Results saved in results/Cluster/Cluster-GATEDGCN-47
----------------------------------------
All experiments completed!
