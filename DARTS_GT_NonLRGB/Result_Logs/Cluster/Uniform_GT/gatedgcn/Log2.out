Lmod has detected the following error: The following module(s) are unknown:
"cuda/12.0"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore-cache load "cuda/12.0"

Also make sure that all modulefiles written in TCL start with the string
#%Module



              total        used        free      shared  buff/cache   available
Mem:          376Gi        15Gi       302Gi       2.1Gi        59Gi       356Gi
Swap:         1.9Gi       3.0Mi       1.9Gi
Fri Jul 11 09:42:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:1A:00.0 Off |                    0 |
| N/A   41C    P0             43W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Running experiment with seed: 45
Starting training for seed 45...
Changed working directory to: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GATEDGCN
Loading config from: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GATEDGCN/confignas.yaml
Using device: cuda
2025-07-11 09:43:56,639 - INFO - GPU Mem: 34.1GB
2025-07-11 09:43:56,639 - INFO - Run directory: results/Cluster/Cluster-GATEDGCN-45
2025-07-11 09:43:56,639 - INFO - Seed: 45
2025-07-11 09:43:56,640 - INFO - === OPTIMIZED MOE CONFIGURATION ===
2025-07-11 09:43:56,640 - INFO - Routing mode: none
2025-07-11 09:43:56,640 - INFO - Expert types: ['GINE', 'CustomGatedGCN', 'GATV2']
2025-07-11 09:43:56,640 - INFO - Number of layers: 16
2025-07-11 09:43:56,640 - INFO - Uncertainty enabled: False
2025-07-11 09:43:56,640 - INFO - Training mode: custom
2025-07-11 09:43:56,640 - INFO - Optimization: Uncertainty only for last layer + test/val
2025-07-11 09:43:56,640 - INFO - Additional features: Router weights logging + JSON export
2025-07-11 09:44:10,562 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 09:44:10,564 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 09:44:10,857 - INFO -   undirected: True
2025-07-11 09:44:10,857 - INFO -   num graphs: 12000
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 09:44:10,857 - INFO -   avg num_nodes/graph: 117
2025-07-11 09:44:10,857 - INFO -   num node features: 7
2025-07-11 09:44:10,857 - INFO -   num edge features: 0
2025-07-11 09:44:10,859 - INFO -   num classes: 6
2025-07-11 09:44:10,859 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 09:44:10,859 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 09:44:10,867 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s] 17%|█▋        | 2074/12000 [00:10<00:47, 207.33it/s] 34%|███▍      | 4094/12000 [00:20<00:38, 204.16it/s] 51%|█████▏    | 6160/12000 [00:30<00:28, 205.26it/s] 69%|██████▊   | 8229/12000 [00:40<00:18, 205.89it/s] 85%|████████▌ | 10241/12000 [00:50<00:08, 204.16it/s]100%|██████████| 12000/12000 [00:59<00:00, 202.27it/s]
2025-07-11 09:45:10,930 - INFO - Done! Took 00:01:00.07
2025-07-11 09:45:10,951 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 09:45:11,265 - INFO - Created model type: <class 'torch_geometric.graphgym.model_builder.GraphGymModule'>
2025-07-11 09:45:11,265 - INFO - Inner model type: <class 'graphgps.network.uniform_gt_model_edge.UNIFORM_GTModelEdge'>
2025-07-11 09:45:11,265 - INFO - Inner model has get_darts_model: False
2025-07-11 09:45:11,270 - INFO - GraphGymModule(
  (model): UNIFORM_GTModelEdge(
    (encoder): FeatureEncoder(
      (node_encoder): LapPENodeEncoder(
        (linear_x): Linear(in_features=7, out_features=32, bias=True)
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (edge_encoder): DummyEdgeEncoder(
        (encoder): Embedding(1, 48)
      )
    )
    (layers): Sequential(
      (0): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (1): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (2): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (3): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (4): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (5): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (6): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (7): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (8): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (9): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (10): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (11): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (12): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (13): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (14): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (15): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (post_mp): GNNInductiveNodeHead(
      (layer_post_mp): MLP(
        (model): Sequential(
          (0): GeneralMultiLayer(
            (Layer_0): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
            (Layer_1): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
          )
          (1): Linear(
            (model): Linear(48, 6, bias=True)
          )
        )
      )
    )
  )
)
2025-07-11 09:45:11,275 - INFO - Number of parameters: 541,238
2025-07-11 09:45:11,275 - INFO - Starting optimized training: 2025-07-11 09:45:11.275197
2025-07-11 09:45:16,949 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
2025-07-11 09:45:16,949 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 09:45:16,950 - INFO -   undirected: True
2025-07-11 09:45:16,950 - INFO -   num graphs: 12000
2025-07-11 09:45:16,950 - INFO -   avg num_nodes/graph: 117
2025-07-11 09:45:16,951 - INFO -   num node features: 7
2025-07-11 09:45:16,951 - INFO -   num edge features: 0
2025-07-11 09:45:16,952 - INFO -   num classes: 6
2025-07-11 09:45:16,952 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 09:45:16,952 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 09:45:16,960 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s] 17%|█▋        | 2055/12000 [00:10<00:48, 205.46it/s] 34%|███▍      | 4105/12000 [00:20<00:38, 205.17it/s] 51%|█████▏    | 6163/12000 [00:30<00:28, 205.44it/s] 68%|██████▊   | 8182/12000 [00:40<00:18, 204.02it/s] 85%|████████▌ | 10253/12000 [00:50<00:08, 205.11it/s]100%|██████████| 12000/12000 [00:58<00:00, 205.62it/s]
2025-07-11 09:46:16,025 - INFO - Done! Took 00:00:59.07
2025-07-11 09:46:16,048 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 09:46:16,053 - INFO - === STARTING OPTIMIZED TRAINING ===
2025-07-11 09:46:16,053 - INFO - Start from epoch 0
2025-07-11 09:47:44,495 - INFO - train: {'epoch': 0, 'time_epoch': 87.83146, 'eta': 8695.31487, 'eta_hours': 2.41537, 'loss': 1.79843745, 'lr': 0.0, 'params': 541238, 'time_iter': 0.14053, 'accuracy': 0.16663, 'f1': 0.04769, 'accuracy-SBM': 0.16665, 'auc': 0.50857}
2025-07-11 09:47:44,501 - INFO - ...computing epoch stats took: 0.60s
2025-07-11 09:47:48,706 - INFO - val: {'epoch': 0, 'time_epoch': 4.16927, 'loss': 1.79887856, 'lr': 0, 'params': 541238, 'time_iter': 0.06618, 'accuracy': 0.16714, 'f1': 0.04774, 'accuracy-SBM': 0.16667, 'auc': 0.50783}
2025-07-11 09:47:48,708 - INFO - ...computing epoch stats took: 0.03s
2025-07-11 09:47:52,874 - INFO - test: {'epoch': 0, 'time_epoch': 4.13865, 'loss': 1.79902201, 'lr': 0, 'params': 541238, 'time_iter': 0.06569, 'accuracy': 0.16728, 'f1': 0.04777, 'accuracy-SBM': 0.16667, 'auc': 0.50643}
2025-07-11 09:47:52,877 - INFO - ...computing epoch stats took: 0.03s
2025-07-11 09:47:52,877 - INFO - > Epoch 0: took 96.8s (avg 96.8s) | Best so far: epoch 0	train_loss: 1.7984 train_accuracy-SBM: 0.1666	val_loss: 1.7989 val_accuracy-SBM: 0.1667	test_loss: 1.7990 test_accuracy-SBM: 0.1667
2025-07-11 09:49:16,687 - INFO - train: {'epoch': 1, 'time_epoch': 83.49592, 'eta': 8395.04186, 'eta_hours': 2.33196, 'loss': 1.6519798, 'lr': 0.0002, 'params': 541238, 'time_iter': 0.13359, 'accuracy': 0.374, 'f1': 0.34219, 'accuracy-SBM': 0.37388, 'auc': 0.70415}
2025-07-11 09:49:16,692 - INFO - ...computing epoch stats took: 0.30s
2025-07-11 09:49:20,795 - INFO - val: {'epoch': 1, 'time_epoch': 4.05716, 'loss': 1.72035848, 'lr': 0, 'params': 541238, 'time_iter': 0.0644, 'accuracy': 0.27515, 'f1': 0.20602, 'accuracy-SBM': 0.27578, 'auc': 0.69155}
2025-07-11 09:49:20,797 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 09:49:24,884 - INFO - test: {'epoch': 1, 'time_epoch': 4.03873, 'loss': 1.71642851, 'lr': 0, 'params': 541238, 'time_iter': 0.06411, 'accuracy': 0.27881, 'f1': 0.20968, 'accuracy-SBM': 0.27868, 'auc': 0.69175}
2025-07-11 09:49:24,886 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 09:49:24,886 - INFO - > Epoch 1: took 92.0s (avg 94.4s) | Best so far: epoch 1	train_loss: 1.6520 train_accuracy-SBM: 0.3739	val_loss: 1.7204 val_accuracy-SBM: 0.2758	test_loss: 1.7164 test_accuracy-SBM: 0.2787
2025-07-11 09:50:48,643 - INFO - train: {'epoch': 2, 'time_epoch': 83.50981, 'eta': 8239.73598, 'eta_hours': 2.28882, 'loss': 1.39216111, 'lr': 0.0004, 'params': 541238, 'time_iter': 0.13362, 'accuracy': 0.50476, 'f1': 0.4941, 'accuracy-SBM': 0.50464, 'auc': 0.80776}
2025-07-11 09:50:48,648 - INFO - ...computing epoch stats took: 0.23s
2025-07-11 09:50:52,744 - INFO - val: {'epoch': 2, 'time_epoch': 4.04361, 'loss': 1.51487416, 'lr': 0, 'params': 541238, 'time_iter': 0.06418, 'accuracy': 0.41097, 'f1': 0.36561, 'accuracy-SBM': 0.41302, 'auc': 0.78853}
2025-07-11 09:50:52,746 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 09:50:56,821 - INFO - test: {'epoch': 2, 'time_epoch': 4.03024, 'loss': 1.50034335, 'lr': 0, 'params': 541238, 'time_iter': 0.06397, 'accuracy': 0.42028, 'f1': 0.37433, 'accuracy-SBM': 0.4196, 'auc': 0.79153}
2025-07-11 09:50:56,822 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 09:50:56,823 - INFO - > Epoch 2: took 91.9s (avg 93.6s) | Best so far: epoch 2	train_loss: 1.3922 train_accuracy-SBM: 0.5046	val_loss: 1.5149 val_accuracy-SBM: 0.4130	test_loss: 1.5003 test_accuracy-SBM: 0.4196
2025-07-11 09:52:22,065 - INFO - train: {'epoch': 3, 'time_epoch': 84.99832, 'eta': 8156.05234, 'eta_hours': 2.26557, 'loss': 1.18876147, 'lr': 0.0006, 'params': 541238, 'time_iter': 0.136, 'accuracy': 0.5497, 'f1': 0.5395, 'accuracy-SBM': 0.54957, 'auc': 0.85565}
2025-07-11 09:52:26,224 - INFO - val: {'epoch': 3, 'time_epoch': 4.09449, 'loss': 1.36569041, 'lr': 0, 'params': 541238, 'time_iter': 0.06499, 'accuracy': 0.45527, 'f1': 0.44154, 'accuracy-SBM': 0.4554, 'auc': 0.81974}
2025-07-11 09:52:30,338 - INFO - test: {'epoch': 3, 'time_epoch': 4.07692, 'loss': 1.35937265, 'lr': 0, 'params': 541238, 'time_iter': 0.06471, 'accuracy': 0.45846, 'f1': 0.44508, 'accuracy-SBM': 0.4582, 'auc': 0.81984}
2025-07-11 09:52:30,341 - INFO - > Epoch 3: took 93.5s (avg 93.6s) | Best so far: epoch 3	train_loss: 1.1888 train_accuracy-SBM: 0.5496	val_loss: 1.3657 val_accuracy-SBM: 0.4554	test_loss: 1.3594 test_accuracy-SBM: 0.4582
2025-07-11 09:53:54,995 - INFO - train: {'epoch': 4, 'time_epoch': 84.4073, 'eta': 8060.61341, 'eta_hours': 2.23906, 'loss': 1.08168636, 'lr': 0.0008, 'params': 541238, 'time_iter': 0.13505, 'accuracy': 0.5712, 'f1': 0.5596, 'accuracy-SBM': 0.57107, 'auc': 0.87671}
2025-07-11 09:53:59,097 - INFO - val: {'epoch': 4, 'time_epoch': 4.05442, 'loss': 1.06780109, 'lr': 0, 'params': 541238, 'time_iter': 0.06436, 'accuracy': 0.57231, 'f1': 0.55442, 'accuracy-SBM': 0.57223, 'auc': 0.88056}
2025-07-11 09:54:03,189 - INFO - test: {'epoch': 4, 'time_epoch': 4.05599, 'loss': 1.05388347, 'lr': 0, 'params': 541238, 'time_iter': 0.06438, 'accuracy': 0.58089, 'f1': 0.56458, 'accuracy-SBM': 0.58098, 'auc': 0.88338}
2025-07-11 09:54:03,191 - INFO - > Epoch 4: took 92.8s (avg 93.4s) | Best so far: epoch 4	train_loss: 1.0817 train_accuracy-SBM: 0.5711	val_loss: 1.0678 val_accuracy-SBM: 0.5722	test_loss: 1.0539 test_accuracy-SBM: 0.5810
2025-07-11 09:55:27,640 - INFO - train: {'epoch': 5, 'time_epoch': 84.18859, 'eta': 7965.42533, 'eta_hours': 2.21262, 'loss': 1.02637779, 'lr': 0.001, 'params': 541238, 'time_iter': 0.1347, 'accuracy': 0.58679, 'f1': 0.57567, 'accuracy-SBM': 0.58666, 'auc': 0.88703}
2025-07-11 09:55:31,825 - INFO - val: {'epoch': 5, 'time_epoch': 4.13825, 'loss': 1.03150983, 'lr': 0, 'params': 541238, 'time_iter': 0.06569, 'accuracy': 0.58452, 'f1': 0.56809, 'accuracy-SBM': 0.58475, 'auc': 0.8894}
2025-07-11 09:55:35,933 - INFO - test: {'epoch': 5, 'time_epoch': 4.07295, 'loss': 1.01388412, 'lr': 0, 'params': 541238, 'time_iter': 0.06465, 'accuracy': 0.59321, 'f1': 0.57605, 'accuracy-SBM': 0.59299, 'auc': 0.89307}
2025-07-11 09:55:35,935 - INFO - > Epoch 5: took 92.7s (avg 93.3s) | Best so far: epoch 5	train_loss: 1.0264 train_accuracy-SBM: 0.5867	val_loss: 1.0315 val_accuracy-SBM: 0.5847	test_loss: 1.0139 test_accuracy-SBM: 0.5930
2025-07-11 09:57:00,261 - INFO - train: {'epoch': 6, 'time_epoch': 84.06557, 'eta': 7871.74557, 'eta_hours': 2.1866, 'loss': 0.98832252, 'lr': 0.00099973, 'params': 541238, 'time_iter': 0.1345, 'accuracy': 0.59611, 'f1': 0.58446, 'accuracy-SBM': 0.59597, 'auc': 0.894}
2025-07-11 09:57:04,380 - INFO - val: {'epoch': 6, 'time_epoch': 4.06451, 'loss': 0.9578784, 'lr': 0, 'params': 541238, 'time_iter': 0.06452, 'accuracy': 0.6043, 'f1': 0.57629, 'accuracy-SBM': 0.60509, 'auc': 0.90028}
2025-07-11 09:57:08,463 - INFO - test: {'epoch': 6, 'time_epoch': 4.04516, 'loss': 0.93803722, 'lr': 0, 'params': 541238, 'time_iter': 0.06421, 'accuracy': 0.61562, 'f1': 0.58922, 'accuracy-SBM': 0.61697, 'auc': 0.90369}
2025-07-11 09:57:08,465 - INFO - > Epoch 6: took 92.5s (avg 93.2s) | Best so far: epoch 6	train_loss: 0.9883 train_accuracy-SBM: 0.5960	val_loss: 0.9579 val_accuracy-SBM: 0.6051	test_loss: 0.9380 test_accuracy-SBM: 0.6170
2025-07-11 09:58:32,644 - INFO - train: {'epoch': 7, 'time_epoch': 83.93938, 'eta': 7779.01814, 'eta_hours': 2.16084, 'loss': 0.96440149, 'lr': 0.00099891, 'params': 541238, 'time_iter': 0.1343, 'accuracy': 0.60508, 'f1': 0.59428, 'accuracy-SBM': 0.60494, 'auc': 0.8983}
2025-07-11 09:58:36,741 - INFO - val: {'epoch': 7, 'time_epoch': 4.05168, 'loss': 0.98617635, 'lr': 0, 'params': 541238, 'time_iter': 0.06431, 'accuracy': 0.59929, 'f1': 0.569, 'accuracy-SBM': 0.59922, 'auc': 0.89539}
2025-07-11 09:58:40,818 - INFO - test: {'epoch': 7, 'time_epoch': 4.04333, 'loss': 0.96999925, 'lr': 0, 'params': 541238, 'time_iter': 0.06418, 'accuracy': 0.60594, 'f1': 0.57444, 'accuracy-SBM': 0.60492, 'auc': 0.89793}
2025-07-11 09:58:40,820 - INFO - > Epoch 7: took 92.4s (avg 93.1s) | Best so far: epoch 6	train_loss: 0.9883 train_accuracy-SBM: 0.5960	val_loss: 0.9579 val_accuracy-SBM: 0.6051	test_loss: 0.9380 test_accuracy-SBM: 0.6170
2025-07-11 10:00:05,631 - INFO - train: {'epoch': 8, 'time_epoch': 84.55571, 'eta': 7694.47542, 'eta_hours': 2.13735, 'loss': 0.95195544, 'lr': 0.00099754, 'params': 541238, 'time_iter': 0.13529, 'accuracy': 0.60785, 'f1': 0.59398, 'accuracy-SBM': 0.60769, 'auc': 0.90048}
2025-07-11 10:00:09,976 - INFO - val: {'epoch': 8, 'time_epoch': 4.29369, 'loss': 0.91838397, 'lr': 0, 'params': 541238, 'time_iter': 0.06815, 'accuracy': 0.61627, 'f1': 0.5698, 'accuracy-SBM': 0.617, 'auc': 0.90744}
2025-07-11 10:00:14,086 - INFO - test: {'epoch': 8, 'time_epoch': 4.07387, 'loss': 0.9111602, 'lr': 0, 'params': 541238, 'time_iter': 0.06466, 'accuracy': 0.62059, 'f1': 0.57535, 'accuracy-SBM': 0.62237, 'auc': 0.9084}
2025-07-11 10:00:14,089 - INFO - > Epoch 8: took 93.3s (avg 93.1s) | Best so far: epoch 8	train_loss: 0.9520 train_accuracy-SBM: 0.6077	val_loss: 0.9184 val_accuracy-SBM: 0.6170	test_loss: 0.9112 test_accuracy-SBM: 0.6224
2025-07-11 10:01:38,189 - INFO - train: {'epoch': 9, 'time_epoch': 83.85327, 'eta': 7603.60811, 'eta_hours': 2.11211, 'loss': 0.9388673, 'lr': 0.00099563, 'params': 541238, 'time_iter': 0.13417, 'accuracy': 0.61235, 'f1': 0.60119, 'accuracy-SBM': 0.61221, 'auc': 0.90273}
2025-07-11 10:01:42,283 - INFO - val: {'epoch': 9, 'time_epoch': 4.04892, 'loss': 0.93849947, 'lr': 0, 'params': 541238, 'time_iter': 0.06427, 'accuracy': 0.61352, 'f1': 0.57203, 'accuracy-SBM': 0.61331, 'auc': 0.90335}
2025-07-11 10:01:46,370 - INFO - test: {'epoch': 9, 'time_epoch': 4.04992, 'loss': 0.93758051, 'lr': 0, 'params': 541238, 'time_iter': 0.06428, 'accuracy': 0.61386, 'f1': 0.57193, 'accuracy-SBM': 0.61253, 'auc': 0.9034}
2025-07-11 10:01:46,372 - INFO - > Epoch 9: took 92.3s (avg 93.0s) | Best so far: epoch 8	train_loss: 0.9520 train_accuracy-SBM: 0.6077	val_loss: 0.9184 val_accuracy-SBM: 0.6170	test_loss: 0.9112 test_accuracy-SBM: 0.6224
2025-07-11 10:03:10,782 - INFO - train: {'epoch': 10, 'time_epoch': 84.08118, 'eta': 7515.86007, 'eta_hours': 2.08774, 'loss': 0.93258694, 'lr': 0.00099318, 'params': 541238, 'time_iter': 0.13453, 'accuracy': 0.61301, 'f1': 0.60194, 'accuracy-SBM': 0.61287, 'auc': 0.90381}
2025-07-11 10:03:14,896 - INFO - val: {'epoch': 10, 'time_epoch': 4.06805, 'loss': 0.90649107, 'lr': 0, 'params': 541238, 'time_iter': 0.06457, 'accuracy': 0.62266, 'f1': 0.60509, 'accuracy-SBM': 0.62263, 'auc': 0.90905}
2025-07-11 10:03:18,987 - INFO - test: {'epoch': 10, 'time_epoch': 4.057, 'loss': 0.89811378, 'lr': 0, 'params': 541238, 'time_iter': 0.0644, 'accuracy': 0.62379, 'f1': 0.60714, 'accuracy-SBM': 0.62362, 'auc': 0.91034}
2025-07-11 10:03:18,989 - INFO - > Epoch 10: took 92.6s (avg 93.0s) | Best so far: epoch 10	train_loss: 0.9326 train_accuracy-SBM: 0.6129	val_loss: 0.9065 val_accuracy-SBM: 0.6226	test_loss: 0.8981 test_accuracy-SBM: 0.6236
2025-07-11 10:04:43,630 - INFO - train: {'epoch': 11, 'time_epoch': 84.30721, 'eta': 7430.38074, 'eta_hours': 2.06399, 'loss': 0.92363419, 'lr': 0.00099019, 'params': 541238, 'time_iter': 0.13489, 'accuracy': 0.6149, 'f1': 0.60418, 'accuracy-SBM': 0.61476, 'auc': 0.90544}
2025-07-11 10:04:47,743 - INFO - val: {'epoch': 11, 'time_epoch': 4.05632, 'loss': 0.88821374, 'lr': 0, 'params': 541238, 'time_iter': 0.06439, 'accuracy': 0.62663, 'f1': 0.60454, 'accuracy-SBM': 0.62734, 'auc': 0.91203}
2025-07-11 10:04:51,820 - INFO - test: {'epoch': 11, 'time_epoch': 4.03852, 'loss': 0.87763363, 'lr': 0, 'params': 541238, 'time_iter': 0.0641, 'accuracy': 0.63215, 'f1': 0.61083, 'accuracy-SBM': 0.63338, 'auc': 0.91352}
2025-07-11 10:04:51,822 - INFO - > Epoch 11: took 92.8s (avg 93.0s) | Best so far: epoch 11	train_loss: 0.9236 train_accuracy-SBM: 0.6148	val_loss: 0.8882 val_accuracy-SBM: 0.6273	test_loss: 0.8776 test_accuracy-SBM: 0.6334
2025-07-11 10:06:16,190 - INFO - train: {'epoch': 12, 'time_epoch': 84.12214, 'eta': 7343.84314, 'eta_hours': 2.03996, 'loss': 0.91657714, 'lr': 0.00098666, 'params': 541238, 'time_iter': 0.1346, 'accuracy': 0.61896, 'f1': 0.60828, 'accuracy-SBM': 0.61881, 'auc': 0.90663}
2025-07-11 10:06:20,306 - INFO - val: {'epoch': 12, 'time_epoch': 4.06677, 'loss': 0.90597438, 'lr': 0, 'params': 541238, 'time_iter': 0.06455, 'accuracy': 0.62024, 'f1': 0.5761, 'accuracy-SBM': 0.62103, 'auc': 0.90892}
2025-07-11 10:06:24,391 - INFO - test: {'epoch': 12, 'time_epoch': 4.04766, 'loss': 0.89365471, 'lr': 0, 'params': 541238, 'time_iter': 0.06425, 'accuracy': 0.62501, 'f1': 0.58189, 'accuracy-SBM': 0.62674, 'auc': 0.91063}
2025-07-11 10:06:24,393 - INFO - > Epoch 12: took 92.6s (avg 92.9s) | Best so far: epoch 11	train_loss: 0.9236 train_accuracy-SBM: 0.6148	val_loss: 0.8882 val_accuracy-SBM: 0.6273	test_loss: 0.8776 test_accuracy-SBM: 0.6334
2025-07-11 10:07:51,244 - INFO - train: {'epoch': 13, 'time_epoch': 86.59508, 'eta': 7272.84159, 'eta_hours': 2.02023, 'loss': 0.91139091, 'lr': 0.0009826, 'params': 541238, 'time_iter': 0.13855, 'accuracy': 0.61956, 'f1': 0.6092, 'accuracy-SBM': 0.61942, 'auc': 0.90753}
2025-07-11 10:07:55,345 - INFO - val: {'epoch': 13, 'time_epoch': 4.05566, 'loss': 0.90244457, 'lr': 0, 'params': 541238, 'time_iter': 0.06438, 'accuracy': 0.62283, 'f1': 0.61247, 'accuracy-SBM': 0.62317, 'auc': 0.90992}
2025-07-11 10:07:59,438 - INFO - test: {'epoch': 13, 'time_epoch': 4.05131, 'loss': 0.88701302, 'lr': 0, 'params': 541238, 'time_iter': 0.06431, 'accuracy': 0.62698, 'f1': 0.61829, 'accuracy-SBM': 0.6276, 'auc': 0.91202}
2025-07-11 10:07:59,440 - INFO - > Epoch 13: took 95.0s (avg 93.1s) | Best so far: epoch 11	train_loss: 0.9236 train_accuracy-SBM: 0.6148	val_loss: 0.8882 val_accuracy-SBM: 0.6273	test_loss: 0.8776 test_accuracy-SBM: 0.6334
2025-07-11 10:09:23,825 - INFO - train: {'epoch': 14, 'time_epoch': 84.14458, 'eta': 7185.8747, 'eta_hours': 1.99608, 'loss': 0.90296316, 'lr': 0.00097802, 'params': 541238, 'time_iter': 0.13463, 'accuracy': 0.6226, 'f1': 0.61162, 'accuracy-SBM': 0.62248, 'auc': 0.90893}
2025-07-11 10:09:27,928 - INFO - val: {'epoch': 14, 'time_epoch': 4.05301, 'loss': 0.9043744, 'lr': 0, 'params': 541238, 'time_iter': 0.06433, 'accuracy': 0.62426, 'f1': 0.58092, 'accuracy-SBM': 0.62415, 'auc': 0.91013}
2025-07-11 10:09:32,042 - INFO - test: {'epoch': 14, 'time_epoch': 4.06685, 'loss': 0.89142456, 'lr': 0, 'params': 541238, 'time_iter': 0.06455, 'accuracy': 0.62792, 'f1': 0.58464, 'accuracy-SBM': 0.62659, 'auc': 0.91181}
2025-07-11 10:09:32,044 - INFO - > Epoch 14: took 92.6s (avg 93.1s) | Best so far: epoch 11	train_loss: 0.9236 train_accuracy-SBM: 0.6148	val_loss: 0.8882 val_accuracy-SBM: 0.6273	test_loss: 0.8776 test_accuracy-SBM: 0.6334
2025-07-11 10:10:56,301 - INFO - train: {'epoch': 15, 'time_epoch': 84.01712, 'eta': 7098.59145, 'eta_hours': 1.97183, 'loss': 0.89970046, 'lr': 0.00097291, 'params': 541238, 'time_iter': 0.13443, 'accuracy': 0.62268, 'f1': 0.61256, 'accuracy-SBM': 0.62255, 'auc': 0.90954}
2025-07-11 10:11:00,403 - INFO - val: {'epoch': 15, 'time_epoch': 4.05304, 'loss': 0.88528178, 'lr': 0, 'params': 541238, 'time_iter': 0.06433, 'accuracy': 0.62899, 'f1': 0.57746, 'accuracy-SBM': 0.62834, 'auc': 0.91263}
2025-07-11 10:11:04,479 - INFO - test: {'epoch': 15, 'time_epoch': 4.03854, 'loss': 0.88207065, 'lr': 0, 'params': 541238, 'time_iter': 0.0641, 'accuracy': 0.63204, 'f1': 0.58066, 'accuracy-SBM': 0.63073, 'auc': 0.91356}
2025-07-11 10:11:04,481 - INFO - > Epoch 15: took 92.4s (avg 93.0s) | Best so far: epoch 15	train_loss: 0.8997 train_accuracy-SBM: 0.6226	val_loss: 0.8853 val_accuracy-SBM: 0.6283	test_loss: 0.8821 test_accuracy-SBM: 0.6307
2025-07-11 10:12:28,594 - INFO - train: {'epoch': 16, 'time_epoch': 83.86879, 'eta': 7010.96823, 'eta_hours': 1.94749, 'loss': 0.8943267, 'lr': 0.00096728, 'params': 541238, 'time_iter': 0.13419, 'accuracy': 0.62484, 'f1': 0.61511, 'accuracy-SBM': 0.6247, 'auc': 0.91046}
2025-07-11 10:12:32,693 - INFO - val: {'epoch': 16, 'time_epoch': 4.05318, 'loss': 0.87151253, 'lr': 0, 'params': 541238, 'time_iter': 0.06434, 'accuracy': 0.63474, 'f1': 0.62491, 'accuracy-SBM': 0.63466, 'auc': 0.9149}
2025-07-11 10:12:36,757 - INFO - test: {'epoch': 16, 'time_epoch': 4.03052, 'loss': 0.85949985, 'lr': 0, 'params': 541238, 'time_iter': 0.06398, 'accuracy': 0.63769, 'f1': 0.6281, 'accuracy-SBM': 0.63765, 'auc': 0.91644}
2025-07-11 10:12:36,759 - INFO - > Epoch 16: took 92.3s (avg 93.0s) | Best so far: epoch 16	train_loss: 0.8943 train_accuracy-SBM: 0.6247	val_loss: 0.8715 val_accuracy-SBM: 0.6347	test_loss: 0.8595 test_accuracy-SBM: 0.6377
2025-07-11 10:14:00,683 - INFO - train: {'epoch': 17, 'time_epoch': 83.67922, 'eta': 6922.8986, 'eta_hours': 1.92303, 'loss': 0.88779201, 'lr': 0.00096114, 'params': 541238, 'time_iter': 0.13389, 'accuracy': 0.62736, 'f1': 0.61752, 'accuracy-SBM': 0.62721, 'auc': 0.91165}
2025-07-11 10:14:04,779 - INFO - val: {'epoch': 17, 'time_epoch': 4.04688, 'loss': 0.85584554, 'lr': 0, 'params': 541238, 'time_iter': 0.06424, 'accuracy': 0.63677, 'f1': 0.59814, 'accuracy-SBM': 0.63628, 'auc': 0.91736}
2025-07-11 10:14:08,848 - INFO - test: {'epoch': 17, 'time_epoch': 4.03225, 'loss': 0.8500548, 'lr': 0, 'params': 541238, 'time_iter': 0.064, 'accuracy': 0.64016, 'f1': 0.60218, 'accuracy-SBM': 0.63897, 'auc': 0.91821}
2025-07-11 10:14:08,850 - INFO - > Epoch 17: took 92.1s (avg 92.9s) | Best so far: epoch 17	train_loss: 0.8878 train_accuracy-SBM: 0.6272	val_loss: 0.8558 val_accuracy-SBM: 0.6363	test_loss: 0.8501 test_accuracy-SBM: 0.6390
2025-07-11 10:15:33,651 - INFO - train: {'epoch': 18, 'time_epoch': 84.55818, 'eta': 6839.03826, 'eta_hours': 1.89973, 'loss': 0.88314724, 'lr': 0.0009545, 'params': 541238, 'time_iter': 0.13529, 'accuracy': 0.62896, 'f1': 0.61908, 'accuracy-SBM': 0.62882, 'auc': 0.91244}
2025-07-11 10:15:37,809 - INFO - val: {'epoch': 18, 'time_epoch': 4.11093, 'loss': 0.87799926, 'lr': 0, 'params': 541238, 'time_iter': 0.06525, 'accuracy': 0.63486, 'f1': 0.62655, 'accuracy-SBM': 0.6347, 'auc': 0.91401}
2025-07-11 10:15:41,924 - INFO - test: {'epoch': 18, 'time_epoch': 4.08005, 'loss': 0.86875859, 'lr': 0, 'params': 541238, 'time_iter': 0.06476, 'accuracy': 0.63456, 'f1': 0.62627, 'accuracy-SBM': 0.63453, 'auc': 0.91548}
2025-07-11 10:15:41,926 - INFO - > Epoch 18: took 93.1s (avg 92.9s) | Best so far: epoch 17	train_loss: 0.8878 train_accuracy-SBM: 0.6272	val_loss: 0.8558 val_accuracy-SBM: 0.6363	test_loss: 0.8501 test_accuracy-SBM: 0.6390
2025-07-11 10:17:06,280 - INFO - train: {'epoch': 19, 'time_epoch': 84.10003, 'eta': 6753.27553, 'eta_hours': 1.87591, 'loss': 0.87890362, 'lr': 0.00094736, 'params': 541238, 'time_iter': 0.13456, 'accuracy': 0.62949, 'f1': 0.61955, 'accuracy-SBM': 0.62935, 'auc': 0.91311}
2025-07-11 10:17:10,397 - INFO - val: {'epoch': 19, 'time_epoch': 4.07049, 'loss': 0.86663837, 'lr': 0, 'params': 541238, 'time_iter': 0.06461, 'accuracy': 0.63692, 'f1': 0.62867, 'accuracy-SBM': 0.63702, 'auc': 0.91578}
2025-07-11 10:17:14,493 - INFO - test: {'epoch': 19, 'time_epoch': 4.06154, 'loss': 0.85291723, 'lr': 0, 'params': 541238, 'time_iter': 0.06447, 'accuracy': 0.64133, 'f1': 0.63347, 'accuracy-SBM': 0.64119, 'auc': 0.91793}
2025-07-11 10:17:14,495 - INFO - > Epoch 19: took 92.6s (avg 92.9s) | Best so far: epoch 19	train_loss: 0.8789 train_accuracy-SBM: 0.6293	val_loss: 0.8666 val_accuracy-SBM: 0.6370	test_loss: 0.8529 test_accuracy-SBM: 0.6412
2025-07-11 10:18:38,795 - INFO - train: {'epoch': 20, 'time_epoch': 84.05036, 'eta': 6667.48429, 'eta_hours': 1.85208, 'loss': 0.87140891, 'lr': 0.00093974, 'params': 541238, 'time_iter': 0.13448, 'accuracy': 0.63223, 'f1': 0.62184, 'accuracy-SBM': 0.63208, 'auc': 0.91441}
2025-07-11 10:18:42,909 - INFO - val: {'epoch': 20, 'time_epoch': 4.06772, 'loss': 0.85468008, 'lr': 0, 'params': 541238, 'time_iter': 0.06457, 'accuracy': 0.64072, 'f1': 0.62116, 'accuracy-SBM': 0.64069, 'auc': 0.91778}
2025-07-11 10:18:47,005 - INFO - test: {'epoch': 20, 'time_epoch': 4.05308, 'loss': 0.85655814, 'lr': 0, 'params': 541238, 'time_iter': 0.06433, 'accuracy': 0.64112, 'f1': 0.62196, 'accuracy-SBM': 0.64067, 'auc': 0.91755}
2025-07-11 10:18:47,007 - INFO - > Epoch 20: took 92.5s (avg 92.9s) | Best so far: epoch 20	train_loss: 0.8714 train_accuracy-SBM: 0.6321	val_loss: 0.8547 val_accuracy-SBM: 0.6407	test_loss: 0.8566 test_accuracy-SBM: 0.6407
2025-07-11 10:20:11,558 - INFO - train: {'epoch': 21, 'time_epoch': 84.30606, 'eta': 6582.75788, 'eta_hours': 1.82854, 'loss': 0.86840399, 'lr': 0.00093163, 'params': 541238, 'time_iter': 0.13489, 'accuracy': 0.63369, 'f1': 0.62409, 'accuracy-SBM': 0.63354, 'auc': 0.91501}
2025-07-11 10:20:15,682 - INFO - val: {'epoch': 21, 'time_epoch': 4.07286, 'loss': 0.86595567, 'lr': 0, 'params': 541238, 'time_iter': 0.06465, 'accuracy': 0.63699, 'f1': 0.61424, 'accuracy-SBM': 0.63685, 'auc': 0.91633}
2025-07-11 10:20:19,769 - INFO - test: {'epoch': 21, 'time_epoch': 4.05281, 'loss': 0.86278727, 'lr': 0, 'params': 541238, 'time_iter': 0.06433, 'accuracy': 0.63408, 'f1': 0.61094, 'accuracy-SBM': 0.63336, 'auc': 0.91686}
2025-07-11 10:20:19,771 - INFO - > Epoch 21: took 92.8s (avg 92.9s) | Best so far: epoch 20	train_loss: 0.8714 train_accuracy-SBM: 0.6321	val_loss: 0.8547 val_accuracy-SBM: 0.6407	test_loss: 0.8566 test_accuracy-SBM: 0.6407
2025-07-11 10:21:44,666 - INFO - train: {'epoch': 22, 'time_epoch': 84.65079, 'eta': 6499.22213, 'eta_hours': 1.80534, 'loss': 0.86292445, 'lr': 0.00092305, 'params': 541238, 'time_iter': 0.13544, 'accuracy': 0.63462, 'f1': 0.62512, 'accuracy-SBM': 0.63447, 'auc': 0.91586}
2025-07-11 10:21:48,759 - INFO - val: {'epoch': 22, 'time_epoch': 4.04133, 'loss': 0.88080062, 'lr': 0, 'params': 541238, 'time_iter': 0.06415, 'accuracy': 0.63102, 'f1': 0.61636, 'accuracy-SBM': 0.6317, 'auc': 0.91387}
2025-07-11 10:21:52,821 - INFO - test: {'epoch': 22, 'time_epoch': 4.02878, 'loss': 0.86742657, 'lr': 0, 'params': 541238, 'time_iter': 0.06395, 'accuracy': 0.63379, 'f1': 0.61957, 'accuracy-SBM': 0.63454, 'auc': 0.91589}
2025-07-11 10:21:52,830 - INFO - > Epoch 22: took 93.1s (avg 92.9s) | Best so far: epoch 20	train_loss: 0.8714 train_accuracy-SBM: 0.6321	val_loss: 0.8547 val_accuracy-SBM: 0.6407	test_loss: 0.8566 test_accuracy-SBM: 0.6407
2025-07-11 10:23:16,971 - INFO - train: {'epoch': 23, 'time_epoch': 83.89647, 'eta': 6413.20476, 'eta_hours': 1.78145, 'loss': 0.86121801, 'lr': 0.000914, 'params': 541238, 'time_iter': 0.13423, 'accuracy': 0.6353, 'f1': 0.6261, 'accuracy-SBM': 0.63516, 'auc': 0.91622}
2025-07-11 10:23:21,051 - INFO - val: {'epoch': 23, 'time_epoch': 4.03576, 'loss': 0.84879268, 'lr': 0, 'params': 541238, 'time_iter': 0.06406, 'accuracy': 0.64427, 'f1': 0.61964, 'accuracy-SBM': 0.64388, 'auc': 0.91885}
2025-07-11 10:23:25,108 - INFO - test: {'epoch': 23, 'time_epoch': 4.02282, 'loss': 0.84701343, 'lr': 0, 'params': 541238, 'time_iter': 0.06385, 'accuracy': 0.64416, 'f1': 0.61843, 'accuracy-SBM': 0.64323, 'auc': 0.91909}
2025-07-11 10:23:25,110 - INFO - > Epoch 23: took 92.3s (avg 92.9s) | Best so far: epoch 23	train_loss: 0.8612 train_accuracy-SBM: 0.6352	val_loss: 0.8488 val_accuracy-SBM: 0.6439	test_loss: 0.8470 test_accuracy-SBM: 0.6432
2025-07-11 10:24:49,061 - INFO - train: {'epoch': 24, 'time_epoch': 83.70712, 'eta': 6326.78903, 'eta_hours': 1.75744, 'loss': 0.85399177, 'lr': 0.00090451, 'params': 541238, 'time_iter': 0.13393, 'accuracy': 0.63675, 'f1': 0.62587, 'accuracy-SBM': 0.63659, 'auc': 0.91742}
2025-07-11 10:24:53,157 - INFO - val: {'epoch': 24, 'time_epoch': 4.04685, 'loss': 0.84940439, 'lr': 0, 'params': 541238, 'time_iter': 0.06424, 'accuracy': 0.63447, 'f1': 0.58527, 'accuracy-SBM': 0.63526, 'auc': 0.91867}
2025-07-11 10:24:57,223 - INFO - test: {'epoch': 24, 'time_epoch': 4.02916, 'loss': 0.84649493, 'lr': 0, 'params': 541238, 'time_iter': 0.06395, 'accuracy': 0.6379, 'f1': 0.589, 'accuracy-SBM': 0.63978, 'auc': 0.91918}
2025-07-11 10:24:57,225 - INFO - > Epoch 24: took 92.1s (avg 92.8s) | Best so far: epoch 23	train_loss: 0.8612 train_accuracy-SBM: 0.6352	val_loss: 0.8488 val_accuracy-SBM: 0.6439	test_loss: 0.8470 test_accuracy-SBM: 0.6432
2025-07-11 10:26:21,227 - INFO - train: {'epoch': 25, 'time_epoch': 83.76364, 'eta': 6240.74251, 'eta_hours': 1.73354, 'loss': 0.85153703, 'lr': 0.00089457, 'params': 541238, 'time_iter': 0.13402, 'accuracy': 0.63803, 'f1': 0.62904, 'accuracy-SBM': 0.6379, 'auc': 0.91783}
2025-07-11 10:26:25,330 - INFO - val: {'epoch': 25, 'time_epoch': 4.05757, 'loss': 0.85068266, 'lr': 0, 'params': 541238, 'time_iter': 0.06441, 'accuracy': 0.6411, 'f1': 0.60761, 'accuracy-SBM': 0.64091, 'auc': 0.91904}
2025-07-11 10:26:29,393 - INFO - test: {'epoch': 25, 'time_epoch': 4.02926, 'loss': 0.83526283, 'lr': 0, 'params': 541238, 'time_iter': 0.06396, 'accuracy': 0.64325, 'f1': 0.60907, 'accuracy-SBM': 0.64221, 'auc': 0.92096}
2025-07-11 10:26:29,395 - INFO - > Epoch 25: took 92.2s (avg 92.8s) | Best so far: epoch 23	train_loss: 0.8612 train_accuracy-SBM: 0.6352	val_loss: 0.8488 val_accuracy-SBM: 0.6439	test_loss: 0.8470 test_accuracy-SBM: 0.6432
2025-07-11 10:27:53,616 - INFO - train: {'epoch': 26, 'time_epoch': 83.97234, 'eta': 6155.42934, 'eta_hours': 1.70984, 'loss': 0.84695564, 'lr': 0.0008842, 'params': 541238, 'time_iter': 0.13436, 'accuracy': 0.63968, 'f1': 0.63031, 'accuracy-SBM': 0.63954, 'auc': 0.91857}
2025-07-11 10:27:57,710 - INFO - val: {'epoch': 26, 'time_epoch': 4.04823, 'loss': 0.83526966, 'lr': 0, 'params': 541238, 'time_iter': 0.06426, 'accuracy': 0.64095, 'f1': 0.62752, 'accuracy-SBM': 0.64144, 'auc': 0.92076}
2025-07-11 10:28:01,812 - INFO - test: {'epoch': 26, 'time_epoch': 4.06536, 'loss': 0.83195769, 'lr': 0, 'params': 541238, 'time_iter': 0.06453, 'accuracy': 0.6426, 'f1': 0.62998, 'accuracy-SBM': 0.64348, 'auc': 0.92121}
2025-07-11 10:28:01,814 - INFO - > Epoch 26: took 92.4s (avg 92.8s) | Best so far: epoch 23	train_loss: 0.8612 train_accuracy-SBM: 0.6352	val_loss: 0.8488 val_accuracy-SBM: 0.6439	test_loss: 0.8470 test_accuracy-SBM: 0.6432
2025-07-11 10:29:26,709 - INFO - train: {'epoch': 27, 'time_epoch': 84.65791, 'eta': 6071.97487, 'eta_hours': 1.68666, 'loss': 0.84343732, 'lr': 0.00087341, 'params': 541238, 'time_iter': 0.13545, 'accuracy': 0.63971, 'f1': 0.63081, 'accuracy-SBM': 0.63958, 'auc': 0.91913}
2025-07-11 10:29:30,633 - INFO - val: {'epoch': 27, 'time_epoch': 3.8743, 'loss': 0.83713676, 'lr': 0, 'params': 541238, 'time_iter': 0.0615, 'accuracy': 0.64622, 'f1': 0.61861, 'accuracy-SBM': 0.64593, 'auc': 0.92051}
2025-07-11 10:29:34,557 - INFO - test: {'epoch': 27, 'time_epoch': 3.88783, 'loss': 0.82533898, 'lr': 0, 'params': 541238, 'time_iter': 0.06171, 'accuracy': 0.64662, 'f1': 0.61912, 'accuracy-SBM': 0.64566, 'auc': 0.92213}
2025-07-11 10:29:34,560 - INFO - > Epoch 27: took 92.7s (avg 92.8s) | Best so far: epoch 27	train_loss: 0.8434 train_accuracy-SBM: 0.6396	val_loss: 0.8371 val_accuracy-SBM: 0.6459	test_loss: 0.8253 test_accuracy-SBM: 0.6457
2025-07-11 10:30:59,625 - INFO - train: {'epoch': 28, 'time_epoch': 84.7327, 'eta': 5988.62049, 'eta_hours': 1.66351, 'loss': 0.8402637, 'lr': 0.00086221, 'params': 541238, 'time_iter': 0.13557, 'accuracy': 0.64202, 'f1': 0.6332, 'accuracy-SBM': 0.64189, 'auc': 0.91963}
2025-07-11 10:31:03,652 - INFO - val: {'epoch': 28, 'time_epoch': 3.9699, 'loss': 0.82643156, 'lr': 0, 'params': 541238, 'time_iter': 0.06301, 'accuracy': 0.64377, 'f1': 0.62752, 'accuracy-SBM': 0.64444, 'auc': 0.92206}
2025-07-11 10:31:07,640 - INFO - test: {'epoch': 28, 'time_epoch': 3.94879, 'loss': 0.82248205, 'lr': 0, 'params': 541238, 'time_iter': 0.06268, 'accuracy': 0.64629, 'f1': 0.63022, 'accuracy-SBM': 0.64732, 'auc': 0.92264}
2025-07-11 10:31:07,642 - INFO - > Epoch 28: took 93.1s (avg 92.8s) | Best so far: epoch 27	train_loss: 0.8434 train_accuracy-SBM: 0.6396	val_loss: 0.8371 val_accuracy-SBM: 0.6459	test_loss: 0.8253 test_accuracy-SBM: 0.6457
2025-07-11 10:32:29,496 - INFO - train: {'epoch': 29, 'time_epoch': 81.61185, 'eta': 5897.89225, 'eta_hours': 1.6383, 'loss': 0.83611851, 'lr': 0.00085062, 'params': 541238, 'time_iter': 0.13058, 'accuracy': 0.64232, 'f1': 0.63354, 'accuracy-SBM': 0.64218, 'auc': 0.9203}
2025-07-11 10:32:33,495 - INFO - val: {'epoch': 29, 'time_epoch': 3.95477, 'loss': 0.82265627, 'lr': 0, 'params': 541238, 'time_iter': 0.06277, 'accuracy': 0.64555, 'f1': 0.63588, 'accuracy-SBM': 0.64565, 'auc': 0.92245}
2025-07-11 10:32:37,474 - INFO - test: {'epoch': 29, 'time_epoch': 3.94607, 'loss': 0.83096095, 'lr': 0, 'params': 541238, 'time_iter': 0.06264, 'accuracy': 0.64448, 'f1': 0.63539, 'accuracy-SBM': 0.64455, 'auc': 0.92131}
2025-07-11 10:32:37,476 - INFO - > Epoch 29: took 89.8s (avg 92.7s) | Best so far: epoch 27	train_loss: 0.8434 train_accuracy-SBM: 0.6396	val_loss: 0.8371 val_accuracy-SBM: 0.6459	test_loss: 0.8253 test_accuracy-SBM: 0.6457
2025-07-11 10:33:58,976 - INFO - train: {'epoch': 30, 'time_epoch': 81.25954, 'eta': 5806.96798, 'eta_hours': 1.61305, 'loss': 0.83285031, 'lr': 0.00083864, 'params': 541238, 'time_iter': 0.13002, 'accuracy': 0.64347, 'f1': 0.63465, 'accuracy-SBM': 0.64333, 'auc': 0.92075}
2025-07-11 10:34:02,932 - INFO - val: {'epoch': 30, 'time_epoch': 3.91221, 'loss': 0.82799998, 'lr': 0, 'params': 541238, 'time_iter': 0.0621, 'accuracy': 0.64456, 'f1': 0.63577, 'accuracy-SBM': 0.64449, 'auc': 0.92206}
2025-07-11 10:34:06,823 - INFO - test: {'epoch': 30, 'time_epoch': 3.85692, 'loss': 0.82117984, 'lr': 0, 'params': 541238, 'time_iter': 0.06122, 'accuracy': 0.64616, 'f1': 0.6377, 'accuracy-SBM': 0.64606, 'auc': 0.92291}
2025-07-11 10:34:06,825 - INFO - > Epoch 30: took 89.3s (avg 92.6s) | Best so far: epoch 27	train_loss: 0.8434 train_accuracy-SBM: 0.6396	val_loss: 0.8371 val_accuracy-SBM: 0.6459	test_loss: 0.8253 test_accuracy-SBM: 0.6457
2025-07-11 10:35:28,583 - INFO - train: {'epoch': 31, 'time_epoch': 81.51361, 'eta': 5717.18766, 'eta_hours': 1.58811, 'loss': 0.83027958, 'lr': 0.00082629, 'params': 541238, 'time_iter': 0.13042, 'accuracy': 0.64515, 'f1': 0.63639, 'accuracy-SBM': 0.64502, 'auc': 0.92128}
2025-07-11 10:35:32,599 - INFO - val: {'epoch': 31, 'time_epoch': 3.96641, 'loss': 0.82256281, 'lr': 0, 'params': 541238, 'time_iter': 0.06296, 'accuracy': 0.64884, 'f1': 0.63104, 'accuracy-SBM': 0.64869, 'auc': 0.92264}
2025-07-11 10:35:36,574 - INFO - test: {'epoch': 31, 'time_epoch': 3.9415, 'loss': 0.82050041, 'lr': 0, 'params': 541238, 'time_iter': 0.06256, 'accuracy': 0.64896, 'f1': 0.63132, 'accuracy-SBM': 0.64837, 'auc': 0.92291}
2025-07-11 10:35:36,576 - INFO - > Epoch 31: took 89.8s (avg 92.5s) | Best so far: epoch 31	train_loss: 0.8303 train_accuracy-SBM: 0.6450	val_loss: 0.8226 val_accuracy-SBM: 0.6487	test_loss: 0.8205 test_accuracy-SBM: 0.6484
2025-07-11 10:37:03,223 - INFO - train: {'epoch': 32, 'time_epoch': 86.18501, 'eta': 5637.39271, 'eta_hours': 1.56594, 'loss': 0.8296067, 'lr': 0.00081359, 'params': 541238, 'time_iter': 0.1379, 'accuracy': 0.64428, 'f1': 0.63543, 'accuracy-SBM': 0.64414, 'auc': 0.92129}
2025-07-11 10:37:07,334 - INFO - val: {'epoch': 32, 'time_epoch': 4.0516, 'loss': 0.82591347, 'lr': 0, 'params': 541238, 'time_iter': 0.06431, 'accuracy': 0.64529, 'f1': 0.62414, 'accuracy-SBM': 0.64496, 'auc': 0.92208}
2025-07-11 10:37:11,387 - INFO - test: {'epoch': 32, 'time_epoch': 4.01824, 'loss': 0.82343535, 'lr': 0, 'params': 541238, 'time_iter': 0.06378, 'accuracy': 0.64986, 'f1': 0.62978, 'accuracy-SBM': 0.64901, 'auc': 0.92258}
2025-07-11 10:37:11,389 - INFO - > Epoch 32: took 94.8s (avg 92.6s) | Best so far: epoch 31	train_loss: 0.8303 train_accuracy-SBM: 0.6450	val_loss: 0.8226 val_accuracy-SBM: 0.6487	test_loss: 0.8205 test_accuracy-SBM: 0.6484
2025-07-11 10:38:35,540 - INFO - train: {'epoch': 33, 'time_epoch': 83.89219, 'eta': 5552.77111, 'eta_hours': 1.54244, 'loss': 0.82643021, 'lr': 0.00080054, 'params': 541238, 'time_iter': 0.13423, 'accuracy': 0.64582, 'f1': 0.63694, 'accuracy-SBM': 0.64568, 'auc': 0.92179}
2025-07-11 10:38:39,617 - INFO - val: {'epoch': 33, 'time_epoch': 4.02696, 'loss': 0.82419478, 'lr': 0, 'params': 541238, 'time_iter': 0.06392, 'accuracy': 0.6435, 'f1': 0.61095, 'accuracy-SBM': 0.64429, 'auc': 0.92266}
2025-07-11 10:38:43,656 - INFO - test: {'epoch': 33, 'time_epoch': 4.0058, 'loss': 0.82012306, 'lr': 0, 'params': 541238, 'time_iter': 0.06358, 'accuracy': 0.64623, 'f1': 0.61489, 'accuracy-SBM': 0.64788, 'auc': 0.92333}
2025-07-11 10:38:43,658 - INFO - > Epoch 33: took 92.3s (avg 92.6s) | Best so far: epoch 31	train_loss: 0.8303 train_accuracy-SBM: 0.6450	val_loss: 0.8226 val_accuracy-SBM: 0.6487	test_loss: 0.8205 test_accuracy-SBM: 0.6484
2025-07-11 10:40:07,471 - INFO - train: {'epoch': 34, 'time_epoch': 83.5704, 'eta': 5467.59357, 'eta_hours': 1.51878, 'loss': 0.82124259, 'lr': 0.00078716, 'params': 541238, 'time_iter': 0.13371, 'accuracy': 0.64731, 'f1': 0.6387, 'accuracy-SBM': 0.64719, 'auc': 0.92264}
2025-07-11 10:40:11,511 - INFO - val: {'epoch': 34, 'time_epoch': 3.9957, 'loss': 0.82227136, 'lr': 0, 'params': 541238, 'time_iter': 0.06342, 'accuracy': 0.64451, 'f1': 0.61527, 'accuracy-SBM': 0.64524, 'auc': 0.92284}
2025-07-11 10:40:15,529 - INFO - test: {'epoch': 34, 'time_epoch': 3.98065, 'loss': 0.81693479, 'lr': 0, 'params': 541238, 'time_iter': 0.06318, 'accuracy': 0.64569, 'f1': 0.61705, 'accuracy-SBM': 0.64728, 'auc': 0.92351}
2025-07-11 10:40:15,531 - INFO - > Epoch 34: took 91.9s (avg 92.6s) | Best so far: epoch 31	train_loss: 0.8303 train_accuracy-SBM: 0.6450	val_loss: 0.8226 val_accuracy-SBM: 0.6487	test_loss: 0.8205 test_accuracy-SBM: 0.6484
2025-07-11 10:41:38,600 - INFO - train: {'epoch': 35, 'time_epoch': 82.82576, 'eta': 5381.18152, 'eta_hours': 1.49477, 'loss': 0.82020939, 'lr': 0.00077347, 'params': 541238, 'time_iter': 0.13252, 'accuracy': 0.64749, 'f1': 0.6385, 'accuracy-SBM': 0.64737, 'auc': 0.92275}
2025-07-11 10:41:42,552 - INFO - val: {'epoch': 35, 'time_epoch': 3.89287, 'loss': 0.82715621, 'lr': 0, 'params': 541238, 'time_iter': 0.06179, 'accuracy': 0.6471, 'f1': 0.6045, 'accuracy-SBM': 0.64678, 'auc': 0.92227}
2025-07-11 10:41:46,554 - INFO - test: {'epoch': 35, 'time_epoch': 3.95538, 'loss': 0.81649326, 'lr': 0, 'params': 541238, 'time_iter': 0.06278, 'accuracy': 0.64961, 'f1': 0.60636, 'accuracy-SBM': 0.64828, 'auc': 0.92363}
2025-07-11 10:41:46,556 - INFO - > Epoch 35: took 91.0s (avg 92.5s) | Best so far: epoch 31	train_loss: 0.8303 train_accuracy-SBM: 0.6450	val_loss: 0.8226 val_accuracy-SBM: 0.6487	test_loss: 0.8205 test_accuracy-SBM: 0.6484
2025-07-11 10:43:08,988 - INFO - train: {'epoch': 36, 'time_epoch': 82.18431, 'eta': 5293.87113, 'eta_hours': 1.47052, 'loss': 0.8158768, 'lr': 0.00075948, 'params': 541238, 'time_iter': 0.13149, 'accuracy': 0.64888, 'f1': 0.6405, 'accuracy-SBM': 0.64874, 'auc': 0.92341}
2025-07-11 10:43:13,002 - INFO - val: {'epoch': 36, 'time_epoch': 3.9702, 'loss': 0.81608041, 'lr': 0, 'params': 541238, 'time_iter': 0.06302, 'accuracy': 0.64726, 'f1': 0.63554, 'accuracy-SBM': 0.64785, 'auc': 0.92397}
2025-07-11 10:43:16,979 - INFO - test: {'epoch': 36, 'time_epoch': 3.94365, 'loss': 0.80620566, 'lr': 0, 'params': 541238, 'time_iter': 0.0626, 'accuracy': 0.65037, 'f1': 0.6392, 'accuracy-SBM': 0.65137, 'auc': 0.92517}
2025-07-11 10:43:16,981 - INFO - > Epoch 36: took 90.4s (avg 92.5s) | Best so far: epoch 31	train_loss: 0.8303 train_accuracy-SBM: 0.6450	val_loss: 0.8226 val_accuracy-SBM: 0.6487	test_loss: 0.8205 test_accuracy-SBM: 0.6484
2025-07-11 10:44:39,437 - INFO - train: {'epoch': 37, 'time_epoch': 82.10216, 'eta': 5206.6965, 'eta_hours': 1.4463, 'loss': 0.81214409, 'lr': 0.00074521, 'params': 541238, 'time_iter': 0.13136, 'accuracy': 0.64989, 'f1': 0.64126, 'accuracy-SBM': 0.64977, 'auc': 0.92401}
2025-07-11 10:44:43,433 - INFO - val: {'epoch': 37, 'time_epoch': 3.95167, 'loss': 0.81570372, 'lr': 0, 'params': 541238, 'time_iter': 0.06272, 'accuracy': 0.6504, 'f1': 0.63869, 'accuracy-SBM': 0.65039, 'auc': 0.92378}
2025-07-11 10:44:47,422 - INFO - test: {'epoch': 37, 'time_epoch': 3.95546, 'loss': 0.80442991, 'lr': 0, 'params': 541238, 'time_iter': 0.06279, 'accuracy': 0.65331, 'f1': 0.64227, 'accuracy-SBM': 0.65291, 'auc': 0.92533}
2025-07-11 10:44:47,424 - INFO - > Epoch 37: took 90.4s (avg 92.4s) | Best so far: epoch 37	train_loss: 0.8121 train_accuracy-SBM: 0.6498	val_loss: 0.8157 val_accuracy-SBM: 0.6504	test_loss: 0.8044 test_accuracy-SBM: 0.6529
2025-07-11 10:46:10,088 - INFO - train: {'epoch': 38, 'time_epoch': 82.42364, 'eta': 5120.28482, 'eta_hours': 1.4223, 'loss': 0.81200045, 'lr': 0.00073067, 'params': 541238, 'time_iter': 0.13188, 'accuracy': 0.6503, 'f1': 0.64145, 'accuracy-SBM': 0.65018, 'auc': 0.92399}
2025-07-11 10:46:14,112 - INFO - val: {'epoch': 38, 'time_epoch': 3.97224, 'loss': 0.81012916, 'lr': 0, 'params': 541238, 'time_iter': 0.06305, 'accuracy': 0.6504, 'f1': 0.63957, 'accuracy-SBM': 0.65044, 'auc': 0.92497}
2025-07-11 10:46:18,106 - INFO - test: {'epoch': 38, 'time_epoch': 3.95999, 'loss': 0.8123578, 'lr': 0, 'params': 541238, 'time_iter': 0.06286, 'accuracy': 0.65014, 'f1': 0.6396, 'accuracy-SBM': 0.65016, 'auc': 0.9246}
2025-07-11 10:46:18,108 - INFO - > Epoch 38: took 90.7s (avg 92.4s) | Best so far: epoch 38	train_loss: 0.8120 train_accuracy-SBM: 0.6502	val_loss: 0.8101 val_accuracy-SBM: 0.6504	test_loss: 0.8124 test_accuracy-SBM: 0.6502
2025-07-11 10:47:40,303 - INFO - train: {'epoch': 39, 'time_epoch': 81.85793, 'eta': 5033.22397, 'eta_hours': 1.39812, 'loss': 0.81035746, 'lr': 0.00071588, 'params': 541238, 'time_iter': 0.13097, 'accuracy': 0.65088, 'f1': 0.64244, 'accuracy-SBM': 0.65074, 'auc': 0.92422}
2025-07-11 10:47:44,319 - INFO - val: {'epoch': 39, 'time_epoch': 3.97247, 'loss': 0.81014066, 'lr': 0, 'params': 541238, 'time_iter': 0.06306, 'accuracy': 0.65258, 'f1': 0.63944, 'accuracy-SBM': 0.6524, 'auc': 0.92454}
2025-07-11 10:47:48,308 - INFO - test: {'epoch': 39, 'time_epoch': 3.95582, 'loss': 0.81531739, 'lr': 0, 'params': 541238, 'time_iter': 0.06279, 'accuracy': 0.6492, 'f1': 0.63643, 'accuracy-SBM': 0.64866, 'auc': 0.92365}
2025-07-11 10:47:48,310 - INFO - > Epoch 39: took 90.2s (avg 92.3s) | Best so far: epoch 39	train_loss: 0.8104 train_accuracy-SBM: 0.6507	val_loss: 0.8101 val_accuracy-SBM: 0.6524	test_loss: 0.8153 test_accuracy-SBM: 0.6487
2025-07-11 10:49:10,648 - INFO - train: {'epoch': 40, 'time_epoch': 82.00533, 'eta': 4946.62904, 'eta_hours': 1.37406, 'loss': 0.80549574, 'lr': 0.00070085, 'params': 541238, 'time_iter': 0.13121, 'accuracy': 0.6517, 'f1': 0.64327, 'accuracy-SBM': 0.65156, 'auc': 0.92498}
2025-07-11 10:49:14,665 - INFO - val: {'epoch': 40, 'time_epoch': 3.97273, 'loss': 0.81096326, 'lr': 0, 'params': 541238, 'time_iter': 0.06306, 'accuracy': 0.64982, 'f1': 0.63601, 'accuracy-SBM': 0.65052, 'auc': 0.92478}
2025-07-11 10:49:18,667 - INFO - test: {'epoch': 40, 'time_epoch': 3.96023, 'loss': 0.80040507, 'lr': 0, 'params': 541238, 'time_iter': 0.06286, 'accuracy': 0.6536, 'f1': 0.64041, 'accuracy-SBM': 0.65466, 'auc': 0.92611}
2025-07-11 10:49:18,669 - INFO - > Epoch 40: took 90.4s (avg 92.3s) | Best so far: epoch 39	train_loss: 0.8104 train_accuracy-SBM: 0.6507	val_loss: 0.8101 val_accuracy-SBM: 0.6524	test_loss: 0.8153 test_accuracy-SBM: 0.6487
2025-07-11 10:50:41,191 - INFO - train: {'epoch': 41, 'time_epoch': 82.17402, 'eta': 4860.48561, 'eta_hours': 1.35013, 'loss': 0.80256784, 'lr': 0.0006856, 'params': 541238, 'time_iter': 0.13148, 'accuracy': 0.65279, 'f1': 0.64436, 'accuracy-SBM': 0.65266, 'auc': 0.92543}
2025-07-11 10:50:45,209 - INFO - val: {'epoch': 41, 'time_epoch': 3.97339, 'loss': 0.80846934, 'lr': 0, 'params': 541238, 'time_iter': 0.06307, 'accuracy': 0.65417, 'f1': 0.63752, 'accuracy-SBM': 0.65393, 'auc': 0.92502}
2025-07-11 10:50:49,203 - INFO - test: {'epoch': 41, 'time_epoch': 3.95985, 'loss': 0.80916188, 'lr': 0, 'params': 541238, 'time_iter': 0.06285, 'accuracy': 0.65076, 'f1': 0.6335, 'accuracy-SBM': 0.65007, 'auc': 0.92481}
2025-07-11 10:50:49,205 - INFO - > Epoch 41: took 90.5s (avg 92.2s) | Best so far: epoch 41	train_loss: 0.8026 train_accuracy-SBM: 0.6527	val_loss: 0.8085 val_accuracy-SBM: 0.6539	test_loss: 0.8092 test_accuracy-SBM: 0.6501
2025-07-11 10:52:11,662 - INFO - train: {'epoch': 42, 'time_epoch': 82.21761, 'eta': 4774.58459, 'eta_hours': 1.32627, 'loss': 0.79939945, 'lr': 0.00067015, 'params': 541238, 'time_iter': 0.13155, 'accuracy': 0.65307, 'f1': 0.64459, 'accuracy-SBM': 0.65294, 'auc': 0.92585}
2025-07-11 10:52:15,600 - INFO - val: {'epoch': 42, 'time_epoch': 3.89435, 'loss': 0.81161756, 'lr': 0, 'params': 541238, 'time_iter': 0.06182, 'accuracy': 0.6511, 'f1': 0.64214, 'accuracy-SBM': 0.65119, 'auc': 0.92459}
2025-07-11 10:52:19,419 - INFO - test: {'epoch': 42, 'time_epoch': 3.7868, 'loss': 0.79868137, 'lr': 0, 'params': 541238, 'time_iter': 0.06011, 'accuracy': 0.65638, 'f1': 0.64744, 'accuracy-SBM': 0.6563, 'auc': 0.92626}
2025-07-11 10:52:19,421 - INFO - > Epoch 42: took 90.2s (avg 92.2s) | Best so far: epoch 41	train_loss: 0.8026 train_accuracy-SBM: 0.6527	val_loss: 0.8085 val_accuracy-SBM: 0.6539	test_loss: 0.8092 test_accuracy-SBM: 0.6501
2025-07-11 10:53:38,733 - INFO - train: {'epoch': 43, 'time_epoch': 79.0797, 'eta': 4684.85729, 'eta_hours': 1.30135, 'loss': 0.79858637, 'lr': 0.00065451, 'params': 541238, 'time_iter': 0.12653, 'accuracy': 0.65349, 'f1': 0.64518, 'accuracy-SBM': 0.65336, 'auc': 0.92602}
2025-07-11 10:53:42,499 - INFO - val: {'epoch': 43, 'time_epoch': 3.71563, 'loss': 0.80648399, 'lr': 0, 'params': 541238, 'time_iter': 0.05898, 'accuracy': 0.65241, 'f1': 0.64416, 'accuracy-SBM': 0.65261, 'auc': 0.92501}
2025-07-11 10:53:46,221 - INFO - test: {'epoch': 43, 'time_epoch': 3.69017, 'loss': 0.80013919, 'lr': 0, 'params': 541238, 'time_iter': 0.05857, 'accuracy': 0.65246, 'f1': 0.64474, 'accuracy-SBM': 0.65274, 'auc': 0.92605}
2025-07-11 10:53:46,224 - INFO - > Epoch 43: took 86.8s (avg 92.0s) | Best so far: epoch 41	train_loss: 0.8026 train_accuracy-SBM: 0.6527	val_loss: 0.8085 val_accuracy-SBM: 0.6539	test_loss: 0.8092 test_accuracy-SBM: 0.6501
2025-07-11 10:55:05,366 - INFO - train: {'epoch': 44, 'time_epoch': 78.89585, 'eta': 4595.37852, 'eta_hours': 1.27649, 'loss': 0.79690412, 'lr': 0.0006387, 'params': 541238, 'time_iter': 0.12623, 'accuracy': 0.65569, 'f1': 0.64736, 'accuracy-SBM': 0.65556, 'auc': 0.92626}
2025-07-11 10:55:09,131 - INFO - val: {'epoch': 44, 'time_epoch': 3.72177, 'loss': 0.83082892, 'lr': 0, 'params': 541238, 'time_iter': 0.05908, 'accuracy': 0.64697, 'f1': 0.634, 'accuracy-SBM': 0.64748, 'auc': 0.92216}
2025-07-11 10:55:12,863 - INFO - test: {'epoch': 44, 'time_epoch': 3.69389, 'loss': 0.80696365, 'lr': 0, 'params': 541238, 'time_iter': 0.05863, 'accuracy': 0.65329, 'f1': 0.64018, 'accuracy-SBM': 0.65408, 'auc': 0.92527}
2025-07-11 10:55:12,865 - INFO - > Epoch 44: took 86.6s (avg 91.9s) | Best so far: epoch 41	train_loss: 0.8026 train_accuracy-SBM: 0.6527	val_loss: 0.8085 val_accuracy-SBM: 0.6539	test_loss: 0.8092 test_accuracy-SBM: 0.6501
2025-07-11 10:56:32,320 - INFO - train: {'epoch': 45, 'time_epoch': 79.12298, 'eta': 4506.6265, 'eta_hours': 1.25184, 'loss': 0.79242133, 'lr': 0.00062274, 'params': 541238, 'time_iter': 0.1266, 'accuracy': 0.6566, 'f1': 0.64832, 'accuracy-SBM': 0.65646, 'auc': 0.92689}
2025-07-11 10:56:36,148 - INFO - val: {'epoch': 45, 'time_epoch': 3.78539, 'loss': 0.80361438, 'lr': 0, 'params': 541238, 'time_iter': 0.06009, 'accuracy': 0.65023, 'f1': 0.63287, 'accuracy-SBM': 0.65088, 'auc': 0.92553}
2025-07-11 10:56:39,943 - INFO - test: {'epoch': 45, 'time_epoch': 3.75785, 'loss': 0.79662686, 'lr': 0, 'params': 541238, 'time_iter': 0.05965, 'accuracy': 0.65599, 'f1': 0.64028, 'accuracy-SBM': 0.65717, 'auc': 0.92648}
2025-07-11 10:56:39,945 - INFO - > Epoch 45: took 87.1s (avg 91.8s) | Best so far: epoch 41	train_loss: 0.8026 train_accuracy-SBM: 0.6527	val_loss: 0.8085 val_accuracy-SBM: 0.6539	test_loss: 0.8092 test_accuracy-SBM: 0.6501
2025-07-11 10:57:59,706 - INFO - train: {'epoch': 46, 'time_epoch': 79.4283, 'eta': 4418.62853, 'eta_hours': 1.2274, 'loss': 0.78917145, 'lr': 0.00060665, 'params': 541238, 'time_iter': 0.12709, 'accuracy': 0.65822, 'f1': 0.65, 'accuracy-SBM': 0.65809, 'auc': 0.9274}
2025-07-11 10:58:03,553 - INFO - val: {'epoch': 46, 'time_epoch': 3.80021, 'loss': 0.80953974, 'lr': 0, 'params': 541238, 'time_iter': 0.06032, 'accuracy': 0.6508, 'f1': 0.62035, 'accuracy-SBM': 0.65151, 'auc': 0.92502}
2025-07-11 10:58:07,310 - INFO - test: {'epoch': 46, 'time_epoch': 3.72016, 'loss': 0.8057534, 'lr': 0, 'params': 541238, 'time_iter': 0.05905, 'accuracy': 0.65001, 'f1': 0.62012, 'accuracy-SBM': 0.65166, 'auc': 0.9253}
2025-07-11 10:58:07,312 - INFO - > Epoch 46: took 87.4s (avg 91.7s) | Best so far: epoch 41	train_loss: 0.8026 train_accuracy-SBM: 0.6527	val_loss: 0.8085 val_accuracy-SBM: 0.6539	test_loss: 0.8092 test_accuracy-SBM: 0.6501
2025-07-11 10:59:26,898 - INFO - train: {'epoch': 47, 'time_epoch': 79.34351, 'eta': 4330.89578, 'eta_hours': 1.20303, 'loss': 0.78647595, 'lr': 0.00059044, 'params': 541238, 'time_iter': 0.12695, 'accuracy': 0.65923, 'f1': 0.65114, 'accuracy-SBM': 0.6591, 'auc': 0.92777}
2025-07-11 10:59:30,938 - INFO - val: {'epoch': 47, 'time_epoch': 3.99308, 'loss': 0.80768907, 'lr': 0, 'params': 541238, 'time_iter': 0.06338, 'accuracy': 0.65056, 'f1': 0.61089, 'accuracy-SBM': 0.65139, 'auc': 0.92507}
2025-07-11 10:59:34,942 - INFO - test: {'epoch': 47, 'time_epoch': 3.96593, 'loss': 0.79448355, 'lr': 0, 'params': 541238, 'time_iter': 0.06295, 'accuracy': 0.65374, 'f1': 0.61392, 'accuracy-SBM': 0.65572, 'auc': 0.92686}
2025-07-11 10:59:34,944 - INFO - > Epoch 47: took 87.6s (avg 91.6s) | Best so far: epoch 41	train_loss: 0.8026 train_accuracy-SBM: 0.6527	val_loss: 0.8085 val_accuracy-SBM: 0.6539	test_loss: 0.8092 test_accuracy-SBM: 0.6501
2025-07-11 11:00:57,869 - INFO - train: {'epoch': 48, 'time_epoch': 82.67729, 'eta': 4246.97529, 'eta_hours': 1.17972, 'loss': 0.785072, 'lr': 0.00057413, 'params': 541238, 'time_iter': 0.13228, 'accuracy': 0.65986, 'f1': 0.65174, 'accuracy-SBM': 0.65973, 'auc': 0.92794}
2025-07-11 11:01:01,928 - INFO - val: {'epoch': 48, 'time_epoch': 4.00742, 'loss': 0.80234333, 'lr': 0, 'params': 541238, 'time_iter': 0.06361, 'accuracy': 0.6547, 'f1': 0.63818, 'accuracy-SBM': 0.65453, 'auc': 0.92598}
2025-07-11 11:01:05,932 - INFO - test: {'epoch': 48, 'time_epoch': 3.97142, 'loss': 0.79936408, 'lr': 0, 'params': 541238, 'time_iter': 0.06304, 'accuracy': 0.6553, 'f1': 0.63764, 'accuracy-SBM': 0.65466, 'auc': 0.92621}
2025-07-11 11:01:05,934 - INFO - > Epoch 48: took 91.0s (avg 91.6s) | Best so far: epoch 48	train_loss: 0.7851 train_accuracy-SBM: 0.6597	val_loss: 0.8023 val_accuracy-SBM: 0.6545	test_loss: 0.7994 test_accuracy-SBM: 0.6547
2025-07-11 11:02:28,416 - INFO - train: {'epoch': 49, 'time_epoch': 82.23669, 'eta': 4162.66393, 'eta_hours': 1.1563, 'loss': 0.78335036, 'lr': 0.00055774, 'params': 541238, 'time_iter': 0.13158, 'accuracy': 0.66049, 'f1': 0.65245, 'accuracy-SBM': 0.66037, 'auc': 0.92821}
2025-07-11 11:02:32,469 - INFO - val: {'epoch': 49, 'time_epoch': 4.0051, 'loss': 0.81334357, 'lr': 0, 'params': 541238, 'time_iter': 0.06357, 'accuracy': 0.65339, 'f1': 0.61692, 'accuracy-SBM': 0.65293, 'auc': 0.9247}
2025-07-11 11:02:36,490 - INFO - test: {'epoch': 49, 'time_epoch': 3.98435, 'loss': 0.79775331, 'lr': 0, 'params': 541238, 'time_iter': 0.06324, 'accuracy': 0.65852, 'f1': 0.62299, 'accuracy-SBM': 0.65727, 'auc': 0.92658}
2025-07-11 11:02:36,492 - INFO - > Epoch 49: took 90.6s (avg 91.6s) | Best so far: epoch 48	train_loss: 0.7851 train_accuracy-SBM: 0.6597	val_loss: 0.8023 val_accuracy-SBM: 0.6545	test_loss: 0.7994 test_accuracy-SBM: 0.6547
2025-07-11 11:03:59,485 - INFO - train: {'epoch': 50, 'time_epoch': 82.63837, 'eta': 4078.81986, 'eta_hours': 1.13301, 'loss': 0.77739844, 'lr': 0.00054129, 'params': 541238, 'time_iter': 0.13222, 'accuracy': 0.66147, 'f1': 0.65336, 'accuracy-SBM': 0.66134, 'auc': 0.92903}
2025-07-11 11:04:03,539 - INFO - val: {'epoch': 50, 'time_epoch': 4.00296, 'loss': 0.81405334, 'lr': 0, 'params': 541238, 'time_iter': 0.06354, 'accuracy': 0.65262, 'f1': 0.63492, 'accuracy-SBM': 0.65312, 'auc': 0.92471}
2025-07-11 11:04:07,560 - INFO - test: {'epoch': 50, 'time_epoch': 3.97534, 'loss': 0.80230424, 'lr': 0, 'params': 541238, 'time_iter': 0.0631, 'accuracy': 0.65357, 'f1': 0.63552, 'accuracy-SBM': 0.65477, 'auc': 0.92611}
2025-07-11 11:04:07,562 - INFO - > Epoch 50: took 91.1s (avg 91.6s) | Best so far: epoch 48	train_loss: 0.7851 train_accuracy-SBM: 0.6597	val_loss: 0.8023 val_accuracy-SBM: 0.6545	test_loss: 0.7994 test_accuracy-SBM: 0.6547
2025-07-11 11:05:30,192 - INFO - train: {'epoch': 51, 'time_epoch': 82.28059, 'eta': 3994.6919, 'eta_hours': 1.10964, 'loss': 0.77783749, 'lr': 0.00052479, 'params': 541238, 'time_iter': 0.13165, 'accuracy': 0.66125, 'f1': 0.65308, 'accuracy-SBM': 0.66111, 'auc': 0.92889}
2025-07-11 11:05:34,231 - INFO - val: {'epoch': 51, 'time_epoch': 3.98755, 'loss': 0.80215451, 'lr': 0, 'params': 541238, 'time_iter': 0.06329, 'accuracy': 0.65508, 'f1': 0.64395, 'accuracy-SBM': 0.65543, 'auc': 0.92599}
2025-07-11 11:05:38,236 - INFO - test: {'epoch': 51, 'time_epoch': 3.97178, 'loss': 0.78986428, 'lr': 0, 'params': 541238, 'time_iter': 0.06304, 'accuracy': 0.65686, 'f1': 0.64577, 'accuracy-SBM': 0.65745, 'auc': 0.9276}
2025-07-11 11:05:38,238 - INFO - > Epoch 51: took 90.7s (avg 91.6s) | Best so far: epoch 51	train_loss: 0.7778 train_accuracy-SBM: 0.6611	val_loss: 0.8022 val_accuracy-SBM: 0.6554	test_loss: 0.7899 test_accuracy-SBM: 0.6574
2025-07-11 11:07:01,225 - INFO - train: {'epoch': 52, 'time_epoch': 82.63319, 'eta': 3910.94634, 'eta_hours': 1.08637, 'loss': 0.77609143, 'lr': 0.00050827, 'params': 541238, 'time_iter': 0.13221, 'accuracy': 0.66158, 'f1': 0.65339, 'accuracy-SBM': 0.66145, 'auc': 0.92917}
2025-07-11 11:07:05,273 - INFO - val: {'epoch': 52, 'time_epoch': 4.00384, 'loss': 0.79885687, 'lr': 0, 'params': 541238, 'time_iter': 0.06355, 'accuracy': 0.656, 'f1': 0.62634, 'accuracy-SBM': 0.65554, 'auc': 0.92642}
2025-07-11 11:07:09,308 - INFO - test: {'epoch': 52, 'time_epoch': 3.99807, 'loss': 0.8008918, 'lr': 0, 'params': 541238, 'time_iter': 0.06346, 'accuracy': 0.65462, 'f1': 0.62532, 'accuracy-SBM': 0.65352, 'auc': 0.92601}
2025-07-11 11:07:09,310 - INFO - > Epoch 52: took 91.1s (avg 91.6s) | Best so far: epoch 52	train_loss: 0.7761 train_accuracy-SBM: 0.6614	val_loss: 0.7989 val_accuracy-SBM: 0.6555	test_loss: 0.8009 test_accuracy-SBM: 0.6535
2025-07-11 11:08:32,528 - INFO - train: {'epoch': 53, 'time_epoch': 82.9774, 'eta': 3827.53518, 'eta_hours': 1.0632, 'loss': 0.77284427, 'lr': 0.00049173, 'params': 541238, 'time_iter': 0.13276, 'accuracy': 0.6642, 'f1': 0.65626, 'accuracy-SBM': 0.66406, 'auc': 0.92969}
2025-07-11 11:08:36,500 - INFO - val: {'epoch': 53, 'time_epoch': 3.92737, 'loss': 0.8031695, 'lr': 0, 'params': 541238, 'time_iter': 0.06234, 'accuracy': 0.65438, 'f1': 0.62264, 'accuracy-SBM': 0.65396, 'auc': 0.92583}
2025-07-11 11:08:40,414 - INFO - test: {'epoch': 53, 'time_epoch': 3.87998, 'loss': 0.78736365, 'lr': 0, 'params': 541238, 'time_iter': 0.06159, 'accuracy': 0.65919, 'f1': 0.6274, 'accuracy-SBM': 0.65795, 'auc': 0.92782}
2025-07-11 11:08:40,417 - INFO - > Epoch 53: took 91.1s (avg 91.6s) | Best so far: epoch 52	train_loss: 0.7761 train_accuracy-SBM: 0.6614	val_loss: 0.7989 val_accuracy-SBM: 0.6555	test_loss: 0.8009 test_accuracy-SBM: 0.6535
2025-07-11 11:10:02,781 - INFO - train: {'epoch': 54, 'time_epoch': 82.11066, 'eta': 3743.43066, 'eta_hours': 1.03984, 'loss': 0.76936034, 'lr': 0.00047521, 'params': 541238, 'time_iter': 0.13138, 'accuracy': 0.66432, 'f1': 0.65645, 'accuracy-SBM': 0.66418, 'auc': 0.93019}
2025-07-11 11:10:06,841 - INFO - val: {'epoch': 54, 'time_epoch': 4.0156, 'loss': 0.80561301, 'lr': 0, 'params': 541238, 'time_iter': 0.06374, 'accuracy': 0.65515, 'f1': 0.63782, 'accuracy-SBM': 0.65503, 'auc': 0.92566}
2025-07-11 11:10:10,835 - INFO - test: {'epoch': 54, 'time_epoch': 3.96202, 'loss': 0.78962073, 'lr': 0, 'params': 541238, 'time_iter': 0.06289, 'accuracy': 0.65885, 'f1': 0.64138, 'accuracy-SBM': 0.65812, 'auc': 0.92745}
2025-07-11 11:10:10,837 - INFO - > Epoch 54: took 90.4s (avg 91.5s) | Best so far: epoch 52	train_loss: 0.7761 train_accuracy-SBM: 0.6614	val_loss: 0.7989 val_accuracy-SBM: 0.6555	test_loss: 0.8009 test_accuracy-SBM: 0.6535
2025-07-11 11:11:33,070 - INFO - train: {'epoch': 55, 'time_epoch': 81.89173, 'eta': 3659.22532, 'eta_hours': 1.01645, 'loss': 0.76609222, 'lr': 0.00045871, 'params': 541238, 'time_iter': 0.13103, 'accuracy': 0.66571, 'f1': 0.65778, 'accuracy-SBM': 0.66557, 'auc': 0.93058}
2025-07-11 11:11:36,861 - INFO - val: {'epoch': 55, 'time_epoch': 3.74241, 'loss': 0.81348036, 'lr': 0, 'params': 541238, 'time_iter': 0.0594, 'accuracy': 0.65693, 'f1': 0.63166, 'accuracy-SBM': 0.65665, 'auc': 0.92535}
2025-07-11 11:11:40,590 - INFO - test: {'epoch': 55, 'time_epoch': 3.69625, 'loss': 0.794415, 'lr': 0, 'params': 541238, 'time_iter': 0.05867, 'accuracy': 0.65947, 'f1': 0.6345, 'accuracy-SBM': 0.65851, 'auc': 0.92738}
2025-07-11 11:11:40,592 - INFO - > Epoch 55: took 89.8s (avg 91.5s) | Best so far: epoch 55	train_loss: 0.7661 train_accuracy-SBM: 0.6656	val_loss: 0.8135 val_accuracy-SBM: 0.6566	test_loss: 0.7944 test_accuracy-SBM: 0.6585
2025-07-11 11:12:59,444 - INFO - train: {'epoch': 56, 'time_epoch': 78.61468, 'eta': 3572.629, 'eta_hours': 0.9924, 'loss': 0.76532707, 'lr': 0.00044226, 'params': 541238, 'time_iter': 0.12578, 'accuracy': 0.66595, 'f1': 0.65794, 'accuracy-SBM': 0.66581, 'auc': 0.93068}
2025-07-11 11:13:03,234 - INFO - val: {'epoch': 56, 'time_epoch': 3.74744, 'loss': 0.8020354, 'lr': 0, 'params': 541238, 'time_iter': 0.05948, 'accuracy': 0.65445, 'f1': 0.64852, 'accuracy-SBM': 0.65461, 'auc': 0.92624}
2025-07-11 11:13:06,969 - INFO - test: {'epoch': 56, 'time_epoch': 3.70233, 'loss': 0.78308995, 'lr': 0, 'params': 541238, 'time_iter': 0.05877, 'accuracy': 0.66002, 'f1': 0.65385, 'accuracy-SBM': 0.66003, 'auc': 0.92855}
2025-07-11 11:13:06,971 - INFO - > Epoch 56: took 86.4s (avg 91.4s) | Best so far: epoch 55	train_loss: 0.7661 train_accuracy-SBM: 0.6656	val_loss: 0.8135 val_accuracy-SBM: 0.6566	test_loss: 0.7944 test_accuracy-SBM: 0.6585
2025-07-11 11:14:28,613 - INFO - train: {'epoch': 57, 'time_epoch': 81.39451, 'eta': 3488.3209, 'eta_hours': 0.96898, 'loss': 0.76218417, 'lr': 0.00042587, 'params': 541238, 'time_iter': 0.13023, 'accuracy': 0.66734, 'f1': 0.65952, 'accuracy-SBM': 0.66721, 'auc': 0.93113}
2025-07-11 11:14:32,664 - INFO - val: {'epoch': 57, 'time_epoch': 4.00641, 'loss': 0.80245353, 'lr': 0, 'params': 541238, 'time_iter': 0.06359, 'accuracy': 0.65523, 'f1': 0.63716, 'accuracy-SBM': 0.65599, 'auc': 0.92603}
2025-07-11 11:14:36,689 - INFO - test: {'epoch': 57, 'time_epoch': 3.98692, 'loss': 0.79078895, 'lr': 0, 'params': 541238, 'time_iter': 0.06328, 'accuracy': 0.65461, 'f1': 0.63613, 'accuracy-SBM': 0.65599, 'auc': 0.92729}
2025-07-11 11:14:36,691 - INFO - > Epoch 57: took 89.7s (avg 91.4s) | Best so far: epoch 55	train_loss: 0.7661 train_accuracy-SBM: 0.6656	val_loss: 0.8135 val_accuracy-SBM: 0.6566	test_loss: 0.7944 test_accuracy-SBM: 0.6585
2025-07-11 11:15:59,537 - INFO - train: {'epoch': 58, 'time_epoch': 82.49612, 'eta': 3404.87708, 'eta_hours': 0.9458, 'loss': 0.76050278, 'lr': 0.00040956, 'params': 541238, 'time_iter': 0.13199, 'accuracy': 0.66716, 'f1': 0.65886, 'accuracy-SBM': 0.66704, 'auc': 0.93139}
2025-07-11 11:16:03,595 - INFO - val: {'epoch': 58, 'time_epoch': 4.0129, 'loss': 0.79959606, 'lr': 0, 'params': 541238, 'time_iter': 0.0637, 'accuracy': 0.65729, 'f1': 0.64914, 'accuracy-SBM': 0.65762, 'auc': 0.92682}
2025-07-11 11:16:07,620 - INFO - test: {'epoch': 58, 'time_epoch': 3.98489, 'loss': 0.78808389, 'lr': 0, 'params': 541238, 'time_iter': 0.06325, 'accuracy': 0.65785, 'f1': 0.64956, 'accuracy-SBM': 0.65839, 'auc': 0.92807}
2025-07-11 11:16:07,622 - INFO - > Epoch 58: took 90.9s (avg 91.4s) | Best so far: epoch 58	train_loss: 0.7605 train_accuracy-SBM: 0.6670	val_loss: 0.7996 val_accuracy-SBM: 0.6576	test_loss: 0.7881 test_accuracy-SBM: 0.6584
2025-07-11 11:17:30,197 - INFO - train: {'epoch': 59, 'time_epoch': 82.32486, 'eta': 3321.35069, 'eta_hours': 0.9226, 'loss': 0.75834548, 'lr': 0.00039335, 'params': 541238, 'time_iter': 0.13172, 'accuracy': 0.66757, 'f1': 0.65982, 'accuracy-SBM': 0.66744, 'auc': 0.93169}
2025-07-11 11:17:34,221 - INFO - val: {'epoch': 59, 'time_epoch': 3.9778, 'loss': 0.80145996, 'lr': 0, 'params': 541238, 'time_iter': 0.06314, 'accuracy': 0.65701, 'f1': 0.64963, 'accuracy-SBM': 0.65721, 'auc': 0.92641}
2025-07-11 11:17:38,224 - INFO - test: {'epoch': 59, 'time_epoch': 3.97003, 'loss': 0.78937069, 'lr': 0, 'params': 541238, 'time_iter': 0.06302, 'accuracy': 0.65866, 'f1': 0.65079, 'accuracy-SBM': 0.65883, 'auc': 0.92773}
2025-07-11 11:17:38,226 - INFO - > Epoch 59: took 90.6s (avg 91.4s) | Best so far: epoch 58	train_loss: 0.7605 train_accuracy-SBM: 0.6670	val_loss: 0.7996 val_accuracy-SBM: 0.6576	test_loss: 0.7881 test_accuracy-SBM: 0.6584
2025-07-11 11:19:00,815 - INFO - train: {'epoch': 60, 'time_epoch': 82.34597, 'eta': 3237.87718, 'eta_hours': 0.89941, 'loss': 0.75611271, 'lr': 0.00037726, 'params': 541238, 'time_iter': 0.13175, 'accuracy': 0.66855, 'f1': 0.66081, 'accuracy-SBM': 0.66842, 'auc': 0.93202}
2025-07-11 11:19:04,889 - INFO - val: {'epoch': 60, 'time_epoch': 4.02878, 'loss': 0.80669596, 'lr': 0, 'params': 541238, 'time_iter': 0.06395, 'accuracy': 0.65622, 'f1': 0.64719, 'accuracy-SBM': 0.65615, 'auc': 0.92579}
2025-07-11 11:19:08,898 - INFO - test: {'epoch': 60, 'time_epoch': 3.96857, 'loss': 0.78334796, 'lr': 0, 'params': 541238, 'time_iter': 0.06299, 'accuracy': 0.66099, 'f1': 0.65129, 'accuracy-SBM': 0.66068, 'auc': 0.92854}
2025-07-11 11:19:08,900 - INFO - > Epoch 60: took 90.7s (avg 91.4s) | Best so far: epoch 58	train_loss: 0.7605 train_accuracy-SBM: 0.6670	val_loss: 0.7996 val_accuracy-SBM: 0.6576	test_loss: 0.7881 test_accuracy-SBM: 0.6584
2025-07-11 11:20:32,083 - INFO - train: {'epoch': 61, 'time_epoch': 82.93571, 'eta': 3154.8015, 'eta_hours': 0.87633, 'loss': 0.75194586, 'lr': 0.0003613, 'params': 541238, 'time_iter': 0.1327, 'accuracy': 0.67046, 'f1': 0.6628, 'accuracy-SBM': 0.67033, 'auc': 0.93254}
2025-07-11 11:20:36,154 - INFO - val: {'epoch': 61, 'time_epoch': 4.02659, 'loss': 0.80475585, 'lr': 0, 'params': 541238, 'time_iter': 0.06391, 'accuracy': 0.65522, 'f1': 0.64671, 'accuracy-SBM': 0.65549, 'auc': 0.92609}
2025-07-11 11:20:40,205 - INFO - test: {'epoch': 61, 'time_epoch': 4.0173, 'loss': 0.78430374, 'lr': 0, 'params': 541238, 'time_iter': 0.06377, 'accuracy': 0.65879, 'f1': 0.64961, 'accuracy-SBM': 0.65903, 'auc': 0.92836}
2025-07-11 11:20:40,207 - INFO - > Epoch 61: took 91.3s (avg 91.4s) | Best so far: epoch 58	train_loss: 0.7605 train_accuracy-SBM: 0.6670	val_loss: 0.7996 val_accuracy-SBM: 0.6576	test_loss: 0.7881 test_accuracy-SBM: 0.6584
2025-07-11 11:22:03,414 - INFO - train: {'epoch': 62, 'time_epoch': 82.96303, 'eta': 3071.74631, 'eta_hours': 0.85326, 'loss': 0.75004628, 'lr': 0.00034549, 'params': 541238, 'time_iter': 0.13274, 'accuracy': 0.67092, 'f1': 0.66337, 'accuracy-SBM': 0.67079, 'auc': 0.93283}
2025-07-11 11:22:07,440 - INFO - val: {'epoch': 62, 'time_epoch': 3.98073, 'loss': 0.80619787, 'lr': 0, 'params': 541238, 'time_iter': 0.06319, 'accuracy': 0.65673, 'f1': 0.6461, 'accuracy-SBM': 0.65676, 'auc': 0.92605}
2025-07-11 11:22:11,469 - INFO - test: {'epoch': 62, 'time_epoch': 3.99579, 'loss': 0.79362892, 'lr': 0, 'params': 541238, 'time_iter': 0.06343, 'accuracy': 0.65769, 'f1': 0.64629, 'accuracy-SBM': 0.65746, 'auc': 0.9274}
2025-07-11 11:22:11,472 - INFO - > Epoch 62: took 91.3s (avg 91.4s) | Best so far: epoch 58	train_loss: 0.7605 train_accuracy-SBM: 0.6670	val_loss: 0.7996 val_accuracy-SBM: 0.6576	test_loss: 0.7881 test_accuracy-SBM: 0.6584
2025-07-11 11:23:34,152 - INFO - train: {'epoch': 63, 'time_epoch': 82.43281, 'eta': 2988.39575, 'eta_hours': 0.83011, 'loss': 0.7474207, 'lr': 0.00032985, 'params': 541238, 'time_iter': 0.13189, 'accuracy': 0.67276, 'f1': 0.66512, 'accuracy-SBM': 0.67263, 'auc': 0.9332}
2025-07-11 11:23:38,183 - INFO - val: {'epoch': 63, 'time_epoch': 3.98687, 'loss': 0.80982419, 'lr': 0, 'params': 541238, 'time_iter': 0.06328, 'accuracy': 0.65491, 'f1': 0.64575, 'accuracy-SBM': 0.65493, 'auc': 0.92552}
2025-07-11 11:23:42,179 - INFO - test: {'epoch': 63, 'time_epoch': 3.96239, 'loss': 0.79021562, 'lr': 0, 'params': 541238, 'time_iter': 0.0629, 'accuracy': 0.65851, 'f1': 0.64911, 'accuracy-SBM': 0.6582, 'auc': 0.92781}
2025-07-11 11:23:42,181 - INFO - > Epoch 63: took 90.7s (avg 91.3s) | Best so far: epoch 58	train_loss: 0.7605 train_accuracy-SBM: 0.6670	val_loss: 0.7996 val_accuracy-SBM: 0.6576	test_loss: 0.7881 test_accuracy-SBM: 0.6584
2025-07-11 11:25:04,414 - INFO - train: {'epoch': 64, 'time_epoch': 81.98107, 'eta': 2904.83019, 'eta_hours': 0.8069, 'loss': 0.74634807, 'lr': 0.0003144, 'params': 541238, 'time_iter': 0.13117, 'accuracy': 0.67273, 'f1': 0.66519, 'accuracy-SBM': 0.67259, 'auc': 0.93332}
2025-07-11 11:25:08,435 - INFO - val: {'epoch': 64, 'time_epoch': 3.97695, 'loss': 0.79883345, 'lr': 0, 'params': 541238, 'time_iter': 0.06313, 'accuracy': 0.65871, 'f1': 0.65078, 'accuracy-SBM': 0.65875, 'auc': 0.9268}
2025-07-11 11:25:12,424 - INFO - test: {'epoch': 64, 'time_epoch': 3.95644, 'loss': 0.78697983, 'lr': 0, 'params': 541238, 'time_iter': 0.0628, 'accuracy': 0.66055, 'f1': 0.65255, 'accuracy-SBM': 0.66056, 'auc': 0.92811}
2025-07-11 11:25:12,426 - INFO - > Epoch 64: took 90.2s (avg 91.3s) | Best so far: epoch 64	train_loss: 0.7463 train_accuracy-SBM: 0.6726	val_loss: 0.7988 val_accuracy-SBM: 0.6587	test_loss: 0.7870 test_accuracy-SBM: 0.6606
2025-07-11 11:26:34,705 - INFO - train: {'epoch': 65, 'time_epoch': 82.03696, 'eta': 2821.34142, 'eta_hours': 0.78371, 'loss': 0.74189674, 'lr': 0.00029915, 'params': 541238, 'time_iter': 0.13126, 'accuracy': 0.67448, 'f1': 0.6669, 'accuracy-SBM': 0.67434, 'auc': 0.93392}
2025-07-11 11:26:38,730 - INFO - val: {'epoch': 65, 'time_epoch': 3.98128, 'loss': 0.80438974, 'lr': 0, 'params': 541238, 'time_iter': 0.06319, 'accuracy': 0.65993, 'f1': 0.65107, 'accuracy-SBM': 0.66034, 'auc': 0.92645}
2025-07-11 11:26:42,732 - INFO - test: {'epoch': 65, 'time_epoch': 3.96262, 'loss': 0.79469575, 'lr': 0, 'params': 541238, 'time_iter': 0.0629, 'accuracy': 0.65936, 'f1': 0.64958, 'accuracy-SBM': 0.66005, 'auc': 0.92739}
2025-07-11 11:26:42,734 - INFO - > Epoch 65: took 90.3s (avg 91.3s) | Best so far: epoch 65	train_loss: 0.7419 train_accuracy-SBM: 0.6743	val_loss: 0.8044 val_accuracy-SBM: 0.6603	test_loss: 0.7947 test_accuracy-SBM: 0.6601
2025-07-11 11:28:05,222 - INFO - train: {'epoch': 66, 'time_epoch': 82.24902, 'eta': 2738.00045, 'eta_hours': 0.76056, 'loss': 0.74188059, 'lr': 0.00028412, 'params': 541238, 'time_iter': 0.1316, 'accuracy': 0.67477, 'f1': 0.66698, 'accuracy-SBM': 0.67464, 'auc': 0.93396}
2025-07-11 11:28:09,240 - INFO - val: {'epoch': 66, 'time_epoch': 3.97394, 'loss': 0.80572594, 'lr': 0, 'params': 541238, 'time_iter': 0.06308, 'accuracy': 0.65826, 'f1': 0.65084, 'accuracy-SBM': 0.65842, 'auc': 0.92651}
2025-07-11 11:28:13,231 - INFO - test: {'epoch': 66, 'time_epoch': 3.9573, 'loss': 0.79519009, 'lr': 0, 'params': 541238, 'time_iter': 0.06281, 'accuracy': 0.65913, 'f1': 0.65103, 'accuracy-SBM': 0.65932, 'auc': 0.92755}
2025-07-11 11:28:13,233 - INFO - > Epoch 66: took 90.5s (avg 91.3s) | Best so far: epoch 65	train_loss: 0.7419 train_accuracy-SBM: 0.6743	val_loss: 0.8044 val_accuracy-SBM: 0.6603	test_loss: 0.7947 test_accuracy-SBM: 0.6601
2025-07-11 11:29:35,729 - INFO - train: {'epoch': 67, 'time_epoch': 82.25588, 'eta': 2654.69482, 'eta_hours': 0.73742, 'loss': 0.74029266, 'lr': 0.00026933, 'params': 541238, 'time_iter': 0.13161, 'accuracy': 0.67554, 'f1': 0.66766, 'accuracy-SBM': 0.67541, 'auc': 0.93417}
2025-07-11 11:29:39,749 - INFO - val: {'epoch': 67, 'time_epoch': 3.97522, 'loss': 0.80947777, 'lr': 0, 'params': 541238, 'time_iter': 0.0631, 'accuracy': 0.65678, 'f1': 0.64534, 'accuracy-SBM': 0.65679, 'auc': 0.92585}
2025-07-11 11:29:43,752 - INFO - test: {'epoch': 67, 'time_epoch': 3.9695, 'loss': 0.78599883, 'lr': 0, 'params': 541238, 'time_iter': 0.06301, 'accuracy': 0.66303, 'f1': 0.65079, 'accuracy-SBM': 0.66272, 'auc': 0.92872}
2025-07-11 11:29:43,754 - INFO - > Epoch 67: took 90.5s (avg 91.3s) | Best so far: epoch 65	train_loss: 0.7419 train_accuracy-SBM: 0.6743	val_loss: 0.8044 val_accuracy-SBM: 0.6603	test_loss: 0.7947 test_accuracy-SBM: 0.6601
2025-07-11 11:31:04,867 - INFO - train: {'epoch': 68, 'time_epoch': 80.7635, 'eta': 2570.74913, 'eta_hours': 0.7141, 'loss': 0.73698745, 'lr': 0.00025479, 'params': 541238, 'time_iter': 0.12922, 'accuracy': 0.67666, 'f1': 0.66925, 'accuracy-SBM': 0.67653, 'auc': 0.93466}
2025-07-11 11:31:08,642 - INFO - val: {'epoch': 68, 'time_epoch': 3.7318, 'loss': 0.80787301, 'lr': 0, 'params': 541238, 'time_iter': 0.05923, 'accuracy': 0.65974, 'f1': 0.65362, 'accuracy-SBM': 0.65994, 'auc': 0.92601}
2025-07-11 11:31:12,382 - INFO - test: {'epoch': 68, 'time_epoch': 3.70757, 'loss': 0.78953614, 'lr': 0, 'params': 541238, 'time_iter': 0.05885, 'accuracy': 0.66206, 'f1': 0.65548, 'accuracy-SBM': 0.66244, 'auc': 0.9281}
2025-07-11 11:31:12,384 - INFO - > Epoch 68: took 88.6s (avg 91.3s) | Best so far: epoch 65	train_loss: 0.7419 train_accuracy-SBM: 0.6743	val_loss: 0.8044 val_accuracy-SBM: 0.6603	test_loss: 0.7947 test_accuracy-SBM: 0.6601
2025-07-11 11:32:31,673 - INFO - train: {'epoch': 69, 'time_epoch': 79.0334, 'eta': 2486.15289, 'eta_hours': 0.6906, 'loss': 0.73485932, 'lr': 0.00024052, 'params': 541238, 'time_iter': 0.12645, 'accuracy': 0.67711, 'f1': 0.66977, 'accuracy-SBM': 0.67697, 'auc': 0.93487}
2025-07-11 11:32:35,448 - INFO - val: {'epoch': 69, 'time_epoch': 3.73171, 'loss': 0.80122855, 'lr': 0, 'params': 541238, 'time_iter': 0.05923, 'accuracy': 0.65853, 'f1': 0.64873, 'accuracy-SBM': 0.65887, 'auc': 0.92649}
2025-07-11 11:32:39,219 - INFO - test: {'epoch': 69, 'time_epoch': 3.73983, 'loss': 0.78355912, 'lr': 0, 'params': 541238, 'time_iter': 0.05936, 'accuracy': 0.66153, 'f1': 0.65127, 'accuracy-SBM': 0.66228, 'auc': 0.92859}
2025-07-11 11:32:39,221 - INFO - > Epoch 69: took 86.8s (avg 91.2s) | Best so far: epoch 65	train_loss: 0.7419 train_accuracy-SBM: 0.6743	val_loss: 0.8044 val_accuracy-SBM: 0.6603	test_loss: 0.7947 test_accuracy-SBM: 0.6601
2025-07-11 11:33:59,027 - INFO - train: {'epoch': 70, 'time_epoch': 79.46717, 'eta': 2401.89051, 'eta_hours': 0.66719, 'loss': 0.7330373, 'lr': 0.00022653, 'params': 541238, 'time_iter': 0.12715, 'accuracy': 0.67959, 'f1': 0.67215, 'accuracy-SBM': 0.67946, 'auc': 0.93517}
2025-07-11 11:34:02,881 - INFO - val: {'epoch': 70, 'time_epoch': 3.80968, 'loss': 0.80949341, 'lr': 0, 'params': 541238, 'time_iter': 0.06047, 'accuracy': 0.65804, 'f1': 0.64455, 'accuracy-SBM': 0.6585, 'auc': 0.92597}
2025-07-11 11:34:06,696 - INFO - test: {'epoch': 70, 'time_epoch': 3.77719, 'loss': 0.78855546, 'lr': 0, 'params': 541238, 'time_iter': 0.05996, 'accuracy': 0.66174, 'f1': 0.6473, 'accuracy-SBM': 0.6628, 'auc': 0.9283}
2025-07-11 11:34:06,698 - INFO - > Epoch 70: took 87.5s (avg 91.1s) | Best so far: epoch 65	train_loss: 0.7419 train_accuracy-SBM: 0.6743	val_loss: 0.8044 val_accuracy-SBM: 0.6603	test_loss: 0.7947 test_accuracy-SBM: 0.6601
2025-07-11 11:35:26,653 - INFO - train: {'epoch': 71, 'time_epoch': 79.59668, 'eta': 2317.81171, 'eta_hours': 0.64384, 'loss': 0.73031384, 'lr': 0.00021284, 'params': 541238, 'time_iter': 0.12735, 'accuracy': 0.67972, 'f1': 0.6722, 'accuracy-SBM': 0.67959, 'auc': 0.9355}
2025-07-11 11:35:30,465 - INFO - val: {'epoch': 71, 'time_epoch': 3.76916, 'loss': 0.80453643, 'lr': 0, 'params': 541238, 'time_iter': 0.05983, 'accuracy': 0.65931, 'f1': 0.65208, 'accuracy-SBM': 0.65958, 'auc': 0.92672}
2025-07-11 11:35:34,177 - INFO - test: {'epoch': 71, 'time_epoch': 3.67926, 'loss': 0.79205028, 'lr': 0, 'params': 541238, 'time_iter': 0.0584, 'accuracy': 0.66122, 'f1': 0.65357, 'accuracy-SBM': 0.66158, 'auc': 0.92808}
2025-07-11 11:35:34,179 - INFO - > Epoch 71: took 87.5s (avg 91.1s) | Best so far: epoch 65	train_loss: 0.7419 train_accuracy-SBM: 0.6743	val_loss: 0.8044 val_accuracy-SBM: 0.6603	test_loss: 0.7947 test_accuracy-SBM: 0.6601
2025-07-11 11:36:54,518 - INFO - train: {'epoch': 72, 'time_epoch': 80.09776, 'eta': 2234.04103, 'eta_hours': 0.62057, 'loss': 0.72908411, 'lr': 0.00019946, 'params': 541238, 'time_iter': 0.12816, 'accuracy': 0.6803, 'f1': 0.6728, 'accuracy-SBM': 0.68018, 'auc': 0.93572}
2025-07-11 11:36:58,386 - INFO - val: {'epoch': 72, 'time_epoch': 3.82388, 'loss': 0.80683415, 'lr': 0, 'params': 541238, 'time_iter': 0.0607, 'accuracy': 0.66189, 'f1': 0.65473, 'accuracy-SBM': 0.66215, 'auc': 0.92658}
2025-07-11 11:37:02,289 - INFO - test: {'epoch': 72, 'time_epoch': 3.87097, 'loss': 0.78755293, 'lr': 0, 'params': 541238, 'time_iter': 0.06144, 'accuracy': 0.66425, 'f1': 0.65684, 'accuracy-SBM': 0.66455, 'auc': 0.92859}
2025-07-11 11:37:02,291 - INFO - > Epoch 72: took 88.1s (avg 91.0s) | Best so far: epoch 72	train_loss: 0.7291 train_accuracy-SBM: 0.6802	val_loss: 0.8068 val_accuracy-SBM: 0.6622	test_loss: 0.7876 test_accuracy-SBM: 0.6645
2025-07-11 11:38:21,885 - INFO - train: {'epoch': 73, 'time_epoch': 79.35863, 'eta': 2150.10993, 'eta_hours': 0.59725, 'loss': 0.72724077, 'lr': 0.00018641, 'params': 541238, 'time_iter': 0.12697, 'accuracy': 0.68048, 'f1': 0.6731, 'accuracy-SBM': 0.68035, 'auc': 0.93595}
2025-07-11 11:38:25,672 - INFO - val: {'epoch': 73, 'time_epoch': 3.74396, 'loss': 0.80732091, 'lr': 0, 'params': 541238, 'time_iter': 0.05943, 'accuracy': 0.65879, 'f1': 0.65245, 'accuracy-SBM': 0.659, 'auc': 0.92635}
2025-07-11 11:38:29,410 - INFO - test: {'epoch': 73, 'time_epoch': 3.70599, 'loss': 0.78718061, 'lr': 0, 'params': 541238, 'time_iter': 0.05883, 'accuracy': 0.66484, 'f1': 0.65792, 'accuracy-SBM': 0.66517, 'auc': 0.92847}
2025-07-11 11:38:29,412 - INFO - > Epoch 73: took 87.1s (avg 91.0s) | Best so far: epoch 72	train_loss: 0.7291 train_accuracy-SBM: 0.6802	val_loss: 0.8068 val_accuracy-SBM: 0.6622	test_loss: 0.7876 test_accuracy-SBM: 0.6645
2025-07-11 11:39:48,535 - INFO - train: {'epoch': 74, 'time_epoch': 78.88428, 'eta': 2066.14264, 'eta_hours': 0.57393, 'loss': 0.72600387, 'lr': 0.00017371, 'params': 541238, 'time_iter': 0.12621, 'accuracy': 0.6809, 'f1': 0.67375, 'accuracy-SBM': 0.68077, 'auc': 0.93612}
2025-07-11 11:39:52,321 - INFO - val: {'epoch': 74, 'time_epoch': 3.7427, 'loss': 0.80066438, 'lr': 0, 'params': 541238, 'time_iter': 0.05941, 'accuracy': 0.6635, 'f1': 0.65299, 'accuracy-SBM': 0.66395, 'auc': 0.92718}
2025-07-11 11:39:56,068 - INFO - test: {'epoch': 74, 'time_epoch': 3.71379, 'loss': 0.78544505, 'lr': 0, 'params': 541238, 'time_iter': 0.05895, 'accuracy': 0.66395, 'f1': 0.65238, 'accuracy-SBM': 0.66482, 'auc': 0.92862}
2025-07-11 11:39:56,071 - INFO - > Epoch 74: took 86.7s (avg 90.9s) | Best so far: epoch 74	train_loss: 0.7260 train_accuracy-SBM: 0.6808	val_loss: 0.8007 val_accuracy-SBM: 0.6640	test_loss: 0.7854 test_accuracy-SBM: 0.6648
2025-07-11 11:41:14,857 - INFO - train: {'epoch': 75, 'time_epoch': 78.55253, 'eta': 1982.20435, 'eta_hours': 0.55061, 'loss': 0.7230494, 'lr': 0.00016136, 'params': 541238, 'time_iter': 0.12568, 'accuracy': 0.68342, 'f1': 0.67613, 'accuracy-SBM': 0.6833, 'auc': 0.93656}
2025-07-11 11:41:18,692 - INFO - val: {'epoch': 75, 'time_epoch': 3.78995, 'loss': 0.80218594, 'lr': 0, 'params': 541238, 'time_iter': 0.06016, 'accuracy': 0.66231, 'f1': 0.65505, 'accuracy-SBM': 0.66256, 'auc': 0.92703}
2025-07-11 11:41:22,419 - INFO - test: {'epoch': 75, 'time_epoch': 3.69501, 'loss': 0.78455022, 'lr': 0, 'params': 541238, 'time_iter': 0.05865, 'accuracy': 0.66348, 'f1': 0.6559, 'accuracy-SBM': 0.66406, 'auc': 0.92874}
2025-07-11 11:41:22,422 - INFO - > Epoch 75: took 86.4s (avg 90.9s) | Best so far: epoch 74	train_loss: 0.7260 train_accuracy-SBM: 0.6808	val_loss: 0.8007 val_accuracy-SBM: 0.6640	test_loss: 0.7854 test_accuracy-SBM: 0.6648
2025-07-11 11:42:41,904 - INFO - train: {'epoch': 76, 'time_epoch': 78.94025, 'eta': 1898.52177, 'eta_hours': 0.52737, 'loss': 0.72277218, 'lr': 0.00014938, 'params': 541238, 'time_iter': 0.1263, 'accuracy': 0.68316, 'f1': 0.67589, 'accuracy-SBM': 0.68303, 'auc': 0.93659}
2025-07-11 11:42:45,705 - INFO - val: {'epoch': 76, 'time_epoch': 3.75854, 'loss': 0.80800464, 'lr': 0, 'params': 541238, 'time_iter': 0.05966, 'accuracy': 0.66312, 'f1': 0.65557, 'accuracy-SBM': 0.66335, 'auc': 0.92669}
2025-07-11 11:42:49,453 - INFO - test: {'epoch': 76, 'time_epoch': 3.71546, 'loss': 0.79722831, 'lr': 0, 'params': 541238, 'time_iter': 0.05898, 'accuracy': 0.66481, 'f1': 0.65689, 'accuracy-SBM': 0.66526, 'auc': 0.92764}
2025-07-11 11:42:49,455 - INFO - > Epoch 76: took 87.0s (avg 90.8s) | Best so far: epoch 74	train_loss: 0.7260 train_accuracy-SBM: 0.6808	val_loss: 0.8007 val_accuracy-SBM: 0.6640	test_loss: 0.7854 test_accuracy-SBM: 0.6648
2025-07-11 11:44:08,582 - INFO - train: {'epoch': 77, 'time_epoch': 78.79042, 'eta': 1814.91852, 'eta_hours': 0.50414, 'loss': 0.72146583, 'lr': 0.00013779, 'params': 541238, 'time_iter': 0.12606, 'accuracy': 0.68283, 'f1': 0.67523, 'accuracy-SBM': 0.68271, 'auc': 0.93673}
2025-07-11 11:44:12,366 - INFO - val: {'epoch': 77, 'time_epoch': 3.74158, 'loss': 0.80556822, 'lr': 0, 'params': 541238, 'time_iter': 0.05939, 'accuracy': 0.6629, 'f1': 0.65568, 'accuracy-SBM': 0.66307, 'auc': 0.92697}
2025-07-11 11:44:16,140 - INFO - test: {'epoch': 77, 'time_epoch': 3.73398, 'loss': 0.79184806, 'lr': 0, 'params': 541238, 'time_iter': 0.05927, 'accuracy': 0.66315, 'f1': 0.65495, 'accuracy-SBM': 0.66336, 'auc': 0.92808}
2025-07-11 11:44:16,142 - INFO - > Epoch 77: took 86.7s (avg 90.8s) | Best so far: epoch 74	train_loss: 0.7260 train_accuracy-SBM: 0.6808	val_loss: 0.8007 val_accuracy-SBM: 0.6640	test_loss: 0.7854 test_accuracy-SBM: 0.6648
2025-07-11 11:45:35,681 - INFO - train: {'epoch': 78, 'time_epoch': 79.3006, 'eta': 1731.57274, 'eta_hours': 0.48099, 'loss': 0.7202172, 'lr': 0.00012659, 'params': 541238, 'time_iter': 0.12688, 'accuracy': 0.68385, 'f1': 0.67658, 'accuracy-SBM': 0.68372, 'auc': 0.93691}
2025-07-11 11:45:39,504 - INFO - val: {'epoch': 78, 'time_epoch': 3.77948, 'loss': 0.80436555, 'lr': 0, 'params': 541238, 'time_iter': 0.05999, 'accuracy': 0.66113, 'f1': 0.654, 'accuracy-SBM': 0.66149, 'auc': 0.92694}
2025-07-11 11:45:43,272 - INFO - test: {'epoch': 78, 'time_epoch': 3.72762, 'loss': 0.78623586, 'lr': 0, 'params': 541238, 'time_iter': 0.05917, 'accuracy': 0.66232, 'f1': 0.65442, 'accuracy-SBM': 0.66296, 'auc': 0.92856}
2025-07-11 11:45:43,274 - INFO - > Epoch 78: took 87.1s (avg 90.7s) | Best so far: epoch 74	train_loss: 0.7260 train_accuracy-SBM: 0.6808	val_loss: 0.8007 val_accuracy-SBM: 0.6640	test_loss: 0.7854 test_accuracy-SBM: 0.6648
2025-07-11 11:47:02,446 - INFO - train: {'epoch': 79, 'time_epoch': 78.93603, 'eta': 1648.23694, 'eta_hours': 0.45784, 'loss': 0.71926818, 'lr': 0.0001158, 'params': 541238, 'time_iter': 0.1263, 'accuracy': 0.68371, 'f1': 0.67607, 'accuracy-SBM': 0.68359, 'auc': 0.93702}
2025-07-11 11:47:06,206 - INFO - val: {'epoch': 79, 'time_epoch': 3.71755, 'loss': 0.80949226, 'lr': 0, 'params': 541238, 'time_iter': 0.05901, 'accuracy': 0.66116, 'f1': 0.65437, 'accuracy-SBM': 0.66133, 'auc': 0.92641}
2025-07-11 11:47:09,945 - INFO - test: {'epoch': 79, 'time_epoch': 3.70616, 'loss': 0.79166354, 'lr': 0, 'params': 541238, 'time_iter': 0.05883, 'accuracy': 0.66481, 'f1': 0.65742, 'accuracy-SBM': 0.66494, 'auc': 0.92808}
2025-07-11 11:47:09,947 - INFO - > Epoch 79: took 86.7s (avg 90.7s) | Best so far: epoch 74	train_loss: 0.7260 train_accuracy-SBM: 0.6808	val_loss: 0.8007 val_accuracy-SBM: 0.6640	test_loss: 0.7854 test_accuracy-SBM: 0.6648
2025-07-11 11:48:29,263 - INFO - train: {'epoch': 80, 'time_epoch': 79.07486, 'eta': 1565.04234, 'eta_hours': 0.43473, 'loss': 0.71684199, 'lr': 0.00010543, 'params': 541238, 'time_iter': 0.12652, 'accuracy': 0.68516, 'f1': 0.67784, 'accuracy-SBM': 0.68503, 'auc': 0.93736}
2025-07-11 11:48:33,084 - INFO - val: {'epoch': 80, 'time_epoch': 3.76946, 'loss': 0.80462449, 'lr': 0, 'params': 541238, 'time_iter': 0.05983, 'accuracy': 0.66206, 'f1': 0.65565, 'accuracy-SBM': 0.66222, 'auc': 0.92694}
2025-07-11 11:48:36,941 - INFO - test: {'epoch': 80, 'time_epoch': 3.81725, 'loss': 0.78610156, 'lr': 0, 'params': 541238, 'time_iter': 0.06059, 'accuracy': 0.66736, 'f1': 0.66066, 'accuracy-SBM': 0.66764, 'auc': 0.92881}
2025-07-11 11:48:36,944 - INFO - > Epoch 80: took 87.0s (avg 90.6s) | Best so far: epoch 74	train_loss: 0.7260 train_accuracy-SBM: 0.6808	val_loss: 0.8007 val_accuracy-SBM: 0.6640	test_loss: 0.7854 test_accuracy-SBM: 0.6648
2025-07-11 11:49:56,384 - INFO - train: {'epoch': 81, 'time_epoch': 79.19426, 'eta': 1481.97444, 'eta_hours': 0.41166, 'loss': 0.71542434, 'lr': 9.549e-05, 'params': 541238, 'time_iter': 0.12671, 'accuracy': 0.68601, 'f1': 0.67865, 'accuracy-SBM': 0.68589, 'auc': 0.93759}
2025-07-11 11:50:00,199 - INFO - val: {'epoch': 81, 'time_epoch': 3.77254, 'loss': 0.80821828, 'lr': 0, 'params': 541238, 'time_iter': 0.05988, 'accuracy': 0.6601, 'f1': 0.65353, 'accuracy-SBM': 0.66038, 'auc': 0.92641}
2025-07-11 11:50:04,051 - INFO - test: {'epoch': 81, 'time_epoch': 3.81789, 'loss': 0.7864132, 'lr': 0, 'params': 541238, 'time_iter': 0.0606, 'accuracy': 0.66428, 'f1': 0.65722, 'accuracy-SBM': 0.66479, 'auc': 0.92861}
2025-07-11 11:50:04,053 - INFO - > Epoch 81: took 87.1s (avg 90.6s) | Best so far: epoch 74	train_loss: 0.7260 train_accuracy-SBM: 0.6808	val_loss: 0.8007 val_accuracy-SBM: 0.6640	test_loss: 0.7854 test_accuracy-SBM: 0.6648
2025-07-11 11:51:24,429 - INFO - train: {'epoch': 82, 'time_epoch': 80.12748, 'eta': 1399.19101, 'eta_hours': 0.38866, 'loss': 0.71311057, 'lr': 8.6e-05, 'params': 541238, 'time_iter': 0.1282, 'accuracy': 0.68647, 'f1': 0.67897, 'accuracy-SBM': 0.68635, 'auc': 0.93783}
2025-07-11 11:51:28,383 - INFO - val: {'epoch': 82, 'time_epoch': 3.90876, 'loss': 0.80572684, 'lr': 0, 'params': 541238, 'time_iter': 0.06204, 'accuracy': 0.66271, 'f1': 0.65547, 'accuracy-SBM': 0.66298, 'auc': 0.92711}
2025-07-11 11:51:32,310 - INFO - test: {'epoch': 82, 'time_epoch': 3.89219, 'loss': 0.78964733, 'lr': 0, 'params': 541238, 'time_iter': 0.06178, 'accuracy': 0.66461, 'f1': 0.65688, 'accuracy-SBM': 0.66511, 'auc': 0.92856}
2025-07-11 11:51:32,313 - INFO - > Epoch 82: took 88.3s (avg 90.6s) | Best so far: epoch 74	train_loss: 0.7260 train_accuracy-SBM: 0.6808	val_loss: 0.8007 val_accuracy-SBM: 0.6640	test_loss: 0.7854 test_accuracy-SBM: 0.6648
2025-07-11 11:52:54,265 - INFO - train: {'epoch': 83, 'time_epoch': 81.70769, 'eta': 1316.77182, 'eta_hours': 0.36577, 'loss': 0.71217855, 'lr': 7.695e-05, 'params': 541238, 'time_iter': 0.13073, 'accuracy': 0.68759, 'f1': 0.68002, 'accuracy-SBM': 0.68747, 'auc': 0.93804}
2025-07-11 11:52:58,199 - INFO - val: {'epoch': 83, 'time_epoch': 3.88992, 'loss': 0.80129579, 'lr': 0, 'params': 541238, 'time_iter': 0.06174, 'accuracy': 0.6641, 'f1': 0.6577, 'accuracy-SBM': 0.66431, 'auc': 0.92731}
2025-07-11 11:53:02,098 - INFO - test: {'epoch': 83, 'time_epoch': 3.86711, 'loss': 0.78797595, 'lr': 0, 'params': 541238, 'time_iter': 0.06138, 'accuracy': 0.6656, 'f1': 0.65869, 'accuracy-SBM': 0.66593, 'auc': 0.92848}
2025-07-11 11:53:02,101 - INFO - > Epoch 83: took 89.8s (avg 90.5s) | Best so far: epoch 83	train_loss: 0.7122 train_accuracy-SBM: 0.6875	val_loss: 0.8013 val_accuracy-SBM: 0.6643	test_loss: 0.7880 test_accuracy-SBM: 0.6659
2025-07-11 11:54:23,293 - INFO - train: {'epoch': 84, 'time_epoch': 80.9275, 'eta': 1234.23169, 'eta_hours': 0.34284, 'loss': 0.71275093, 'lr': 6.837e-05, 'params': 541238, 'time_iter': 0.12948, 'accuracy': 0.68778, 'f1': 0.68052, 'accuracy-SBM': 0.68766, 'auc': 0.93799}
2025-07-11 11:54:27,192 - INFO - val: {'epoch': 84, 'time_epoch': 3.85423, 'loss': 0.80353092, 'lr': 0, 'params': 541238, 'time_iter': 0.06118, 'accuracy': 0.66266, 'f1': 0.65445, 'accuracy-SBM': 0.66301, 'auc': 0.92715}
2025-07-11 11:54:31,038 - INFO - test: {'epoch': 84, 'time_epoch': 3.80797, 'loss': 0.78810398, 'lr': 0, 'params': 541238, 'time_iter': 0.06044, 'accuracy': 0.66378, 'f1': 0.65508, 'accuracy-SBM': 0.66442, 'auc': 0.92851}
2025-07-11 11:54:31,041 - INFO - > Epoch 84: took 88.9s (avg 90.5s) | Best so far: epoch 83	train_loss: 0.7122 train_accuracy-SBM: 0.6875	val_loss: 0.8013 val_accuracy-SBM: 0.6643	test_loss: 0.7880 test_accuracy-SBM: 0.6659
2025-07-11 11:55:52,252 - INFO - train: {'epoch': 85, 'time_epoch': 80.96051, 'eta': 1151.73443, 'eta_hours': 0.31993, 'loss': 0.71193819, 'lr': 6.026e-05, 'params': 541238, 'time_iter': 0.12954, 'accuracy': 0.68734, 'f1': 0.67964, 'accuracy-SBM': 0.68722, 'auc': 0.93808}
2025-07-11 11:55:56,166 - INFO - val: {'epoch': 85, 'time_epoch': 3.8703, 'loss': 0.8061476, 'lr': 0, 'params': 541238, 'time_iter': 0.06143, 'accuracy': 0.66441, 'f1': 0.65679, 'accuracy-SBM': 0.66473, 'auc': 0.92718}
2025-07-11 11:56:00,063 - INFO - test: {'epoch': 85, 'time_epoch': 3.86094, 'loss': 0.79163307, 'lr': 0, 'params': 541238, 'time_iter': 0.06128, 'accuracy': 0.66474, 'f1': 0.65664, 'accuracy-SBM': 0.66536, 'auc': 0.92837}
2025-07-11 11:56:00,066 - INFO - > Epoch 85: took 89.0s (avg 90.5s) | Best so far: epoch 85	train_loss: 0.7119 train_accuracy-SBM: 0.6872	val_loss: 0.8061 val_accuracy-SBM: 0.6647	test_loss: 0.7916 test_accuracy-SBM: 0.6654
2025-07-11 11:57:21,359 - INFO - train: {'epoch': 86, 'time_epoch': 81.04782, 'eta': 1069.28555, 'eta_hours': 0.29702, 'loss': 0.70963872, 'lr': 5.264e-05, 'params': 541238, 'time_iter': 0.12968, 'accuracy': 0.68838, 'f1': 0.68037, 'accuracy-SBM': 0.68827, 'auc': 0.93836}
2025-07-11 11:57:25,283 - INFO - val: {'epoch': 86, 'time_epoch': 3.88009, 'loss': 0.80642938, 'lr': 0, 'params': 541238, 'time_iter': 0.06159, 'accuracy': 0.66354, 'f1': 0.6563, 'accuracy-SBM': 0.66378, 'auc': 0.92701}
2025-07-11 11:57:29,180 - INFO - test: {'epoch': 86, 'time_epoch': 3.86319, 'loss': 0.7891001, 'lr': 0, 'params': 541238, 'time_iter': 0.06132, 'accuracy': 0.66686, 'f1': 0.65918, 'accuracy-SBM': 0.6673, 'auc': 0.92865}
2025-07-11 11:57:29,182 - INFO - > Epoch 86: took 89.1s (avg 90.5s) | Best so far: epoch 85	train_loss: 0.7119 train_accuracy-SBM: 0.6872	val_loss: 0.8061 val_accuracy-SBM: 0.6647	test_loss: 0.7916 test_accuracy-SBM: 0.6654
2025-07-11 11:58:50,770 - INFO - train: {'epoch': 87, 'time_epoch': 81.34541, 'eta': 986.90908, 'eta_hours': 0.27414, 'loss': 0.70879152, 'lr': 4.55e-05, 'params': 541238, 'time_iter': 0.13015, 'accuracy': 0.68919, 'f1': 0.68206, 'accuracy-SBM': 0.68907, 'auc': 0.93854}
2025-07-11 11:58:54,712 - INFO - val: {'epoch': 87, 'time_epoch': 3.88959, 'loss': 0.80423254, 'lr': 0, 'params': 541238, 'time_iter': 0.06174, 'accuracy': 0.66524, 'f1': 0.65775, 'accuracy-SBM': 0.66554, 'auc': 0.92744}
2025-07-11 11:58:58,628 - INFO - test: {'epoch': 87, 'time_epoch': 3.88297, 'loss': 0.78904034, 'lr': 0, 'params': 541238, 'time_iter': 0.06163, 'accuracy': 0.66644, 'f1': 0.65806, 'accuracy-SBM': 0.66695, 'auc': 0.92876}
2025-07-11 11:58:58,630 - INFO - > Epoch 87: took 89.4s (avg 90.5s) | Best so far: epoch 87	train_loss: 0.7088 train_accuracy-SBM: 0.6891	val_loss: 0.8042 val_accuracy-SBM: 0.6655	test_loss: 0.7890 test_accuracy-SBM: 0.6670
2025-07-11 12:00:20,520 - INFO - train: {'epoch': 88, 'time_epoch': 81.63545, 'eta': 904.59164, 'eta_hours': 0.25128, 'loss': 0.70874633, 'lr': 3.886e-05, 'params': 541238, 'time_iter': 0.13062, 'accuracy': 0.69018, 'f1': 0.6827, 'accuracy-SBM': 0.69006, 'auc': 0.93863}
2025-07-11 12:00:24,439 - INFO - val: {'epoch': 88, 'time_epoch': 3.87631, 'loss': 0.80335395, 'lr': 0, 'params': 541238, 'time_iter': 0.06153, 'accuracy': 0.66509, 'f1': 0.65801, 'accuracy-SBM': 0.66537, 'auc': 0.92752}
2025-07-11 12:00:28,348 - INFO - test: {'epoch': 88, 'time_epoch': 3.87604, 'loss': 0.7878402, 'lr': 0, 'params': 541238, 'time_iter': 0.06152, 'accuracy': 0.66746, 'f1': 0.65971, 'accuracy-SBM': 0.66798, 'auc': 0.92885}
2025-07-11 12:00:28,350 - INFO - > Epoch 88: took 89.7s (avg 90.5s) | Best so far: epoch 87	train_loss: 0.7088 train_accuracy-SBM: 0.6891	val_loss: 0.8042 val_accuracy-SBM: 0.6655	test_loss: 0.7890 test_accuracy-SBM: 0.6670
2025-07-11 12:01:49,976 - INFO - train: {'epoch': 89, 'time_epoch': 81.37424, 'eta': 822.26033, 'eta_hours': 0.22841, 'loss': 0.70758412, 'lr': 3.272e-05, 'params': 541238, 'time_iter': 0.1302, 'accuracy': 0.6896, 'f1': 0.68199, 'accuracy-SBM': 0.68948, 'auc': 0.93875}
2025-07-11 12:01:53,904 - INFO - val: {'epoch': 89, 'time_epoch': 3.87618, 'loss': 0.80237234, 'lr': 0, 'params': 541238, 'time_iter': 0.06153, 'accuracy': 0.66458, 'f1': 0.65773, 'accuracy-SBM': 0.66486, 'auc': 0.92754}
2025-07-11 12:01:57,821 - INFO - test: {'epoch': 89, 'time_epoch': 3.88432, 'loss': 0.79112452, 'lr': 0, 'params': 541238, 'time_iter': 0.06166, 'accuracy': 0.66582, 'f1': 0.65848, 'accuracy-SBM': 0.6663, 'auc': 0.92841}
2025-07-11 12:01:57,824 - INFO - > Epoch 89: took 89.5s (avg 90.5s) | Best so far: epoch 87	train_loss: 0.7088 train_accuracy-SBM: 0.6891	val_loss: 0.8042 val_accuracy-SBM: 0.6655	test_loss: 0.7890 test_accuracy-SBM: 0.6670
2025-07-11 12:03:19,105 - INFO - train: {'epoch': 90, 'time_epoch': 80.84679, 'eta': 739.89789, 'eta_hours': 0.20553, 'loss': 0.70808484, 'lr': 2.709e-05, 'params': 541238, 'time_iter': 0.12935, 'accuracy': 0.68923, 'f1': 0.68183, 'accuracy-SBM': 0.68911, 'auc': 0.93864}
2025-07-11 12:03:22,989 - INFO - val: {'epoch': 90, 'time_epoch': 3.84111, 'loss': 0.80520032, 'lr': 0, 'params': 541238, 'time_iter': 0.06097, 'accuracy': 0.66408, 'f1': 0.65695, 'accuracy-SBM': 0.66435, 'auc': 0.92713}
2025-07-11 12:03:26,853 - INFO - test: {'epoch': 90, 'time_epoch': 3.83031, 'loss': 0.78989767, 'lr': 0, 'params': 541238, 'time_iter': 0.0608, 'accuracy': 0.66631, 'f1': 0.65868, 'accuracy-SBM': 0.66676, 'auc': 0.92852}
2025-07-11 12:03:26,855 - INFO - > Epoch 90: took 89.0s (avg 90.4s) | Best so far: epoch 87	train_loss: 0.7088 train_accuracy-SBM: 0.6891	val_loss: 0.8042 val_accuracy-SBM: 0.6655	test_loss: 0.7890 test_accuracy-SBM: 0.6670
2025-07-11 12:04:48,024 - INFO - train: {'epoch': 91, 'time_epoch': 80.82986, 'eta': 657.56692, 'eta_hours': 0.18266, 'loss': 0.70741469, 'lr': 2.198e-05, 'params': 541238, 'time_iter': 0.12933, 'accuracy': 0.68952, 'f1': 0.68201, 'accuracy-SBM': 0.6894, 'auc': 0.93872}
2025-07-11 12:04:51,923 - INFO - val: {'epoch': 91, 'time_epoch': 3.855, 'loss': 0.8052755, 'lr': 0, 'params': 541238, 'time_iter': 0.06119, 'accuracy': 0.66532, 'f1': 0.65859, 'accuracy-SBM': 0.6656, 'auc': 0.92726}
2025-07-11 12:04:55,785 - INFO - test: {'epoch': 91, 'time_epoch': 3.83015, 'loss': 0.78910302, 'lr': 0, 'params': 541238, 'time_iter': 0.0608, 'accuracy': 0.66686, 'f1': 0.65954, 'accuracy-SBM': 0.66735, 'auc': 0.92863}
2025-07-11 12:04:55,787 - INFO - > Epoch 91: took 88.9s (avg 90.4s) | Best so far: epoch 91	train_loss: 0.7074 train_accuracy-SBM: 0.6894	val_loss: 0.8053 val_accuracy-SBM: 0.6656	test_loss: 0.7891 test_accuracy-SBM: 0.6673
2025-07-11 12:06:16,485 - INFO - train: {'epoch': 92, 'time_epoch': 80.45395, 'eta': 575.23995, 'eta_hours': 0.15979, 'loss': 0.70497668, 'lr': 1.74e-05, 'params': 541238, 'time_iter': 0.12873, 'accuracy': 0.69023, 'f1': 0.6829, 'accuracy-SBM': 0.69011, 'auc': 0.93901}
2025-07-11 12:06:20,375 - INFO - val: {'epoch': 92, 'time_epoch': 3.84628, 'loss': 0.80451961, 'lr': 0, 'params': 541238, 'time_iter': 0.06105, 'accuracy': 0.66515, 'f1': 0.6583, 'accuracy-SBM': 0.66541, 'auc': 0.9275}
2025-07-11 12:06:24,216 - INFO - test: {'epoch': 92, 'time_epoch': 3.80814, 'loss': 0.78867862, 'lr': 0, 'params': 541238, 'time_iter': 0.06045, 'accuracy': 0.66648, 'f1': 0.65903, 'accuracy-SBM': 0.66695, 'auc': 0.92878}
2025-07-11 12:06:24,218 - INFO - > Epoch 92: took 88.4s (avg 90.4s) | Best so far: epoch 91	train_loss: 0.7074 train_accuracy-SBM: 0.6894	val_loss: 0.8053 val_accuracy-SBM: 0.6656	test_loss: 0.7891 test_accuracy-SBM: 0.6673
2025-07-11 12:07:48,226 - INFO - train: {'epoch': 93, 'time_epoch': 83.73165, 'eta': 493.16204, 'eta_hours': 0.13699, 'loss': 0.70580013, 'lr': 1.334e-05, 'params': 541238, 'time_iter': 0.13397, 'accuracy': 0.69148, 'f1': 0.68417, 'accuracy-SBM': 0.69136, 'auc': 0.93904}
2025-07-11 12:07:52,028 - INFO - val: {'epoch': 93, 'time_epoch': 3.75738, 'loss': 0.80560099, 'lr': 0, 'params': 541238, 'time_iter': 0.05964, 'accuracy': 0.66529, 'f1': 0.65784, 'accuracy-SBM': 0.66559, 'auc': 0.92735}
2025-07-11 12:07:55,721 - INFO - test: {'epoch': 93, 'time_epoch': 3.65212, 'loss': 0.79224056, 'lr': 0, 'params': 541238, 'time_iter': 0.05797, 'accuracy': 0.66622, 'f1': 0.65825, 'accuracy-SBM': 0.66675, 'auc': 0.92835}
2025-07-11 12:07:55,723 - INFO - > Epoch 93: took 91.5s (avg 90.4s) | Best so far: epoch 91	train_loss: 0.7074 train_accuracy-SBM: 0.6894	val_loss: 0.8053 val_accuracy-SBM: 0.6656	test_loss: 0.7891 test_accuracy-SBM: 0.6673
2025-07-11 12:09:13,527 - INFO - train: {'epoch': 94, 'time_epoch': 77.57271, 'eta': 410.72516, 'eta_hours': 0.11409, 'loss': 0.70484845, 'lr': 9.81e-06, 'params': 541238, 'time_iter': 0.12412, 'accuracy': 0.69134, 'f1': 0.68391, 'accuracy-SBM': 0.69122, 'auc': 0.9391}
2025-07-11 12:09:17,282 - INFO - val: {'epoch': 94, 'time_epoch': 3.71285, 'loss': 0.80691744, 'lr': 0, 'params': 541238, 'time_iter': 0.05893, 'accuracy': 0.66486, 'f1': 0.65774, 'accuracy-SBM': 0.66517, 'auc': 0.92715}
2025-07-11 12:09:20,990 - INFO - test: {'epoch': 94, 'time_epoch': 3.67568, 'loss': 0.79018315, 'lr': 0, 'params': 541238, 'time_iter': 0.05834, 'accuracy': 0.66687, 'f1': 0.65913, 'accuracy-SBM': 0.66741, 'auc': 0.92855}
2025-07-11 12:09:20,993 - INFO - > Epoch 94: took 85.3s (avg 90.4s) | Best so far: epoch 91	train_loss: 0.7074 train_accuracy-SBM: 0.6894	val_loss: 0.8053 val_accuracy-SBM: 0.6656	test_loss: 0.7891 test_accuracy-SBM: 0.6673
2025-07-11 12:10:38,738 - INFO - train: {'epoch': 95, 'time_epoch': 77.51126, 'eta': 328.38705, 'eta_hours': 0.09122, 'loss': 0.70452303, 'lr': 6.82e-06, 'params': 541238, 'time_iter': 0.12402, 'accuracy': 0.69113, 'f1': 0.68359, 'accuracy-SBM': 0.69101, 'auc': 0.93908}
2025-07-11 12:10:42,452 - INFO - val: {'epoch': 95, 'time_epoch': 3.66614, 'loss': 0.80434939, 'lr': 0, 'params': 541238, 'time_iter': 0.05819, 'accuracy': 0.66488, 'f1': 0.65787, 'accuracy-SBM': 0.66518, 'auc': 0.9274}
2025-07-11 12:10:46,123 - INFO - test: {'epoch': 95, 'time_epoch': 3.63929, 'loss': 0.78878491, 'lr': 0, 'params': 541238, 'time_iter': 0.05777, 'accuracy': 0.6666, 'f1': 0.65907, 'accuracy-SBM': 0.66711, 'auc': 0.92867}
2025-07-11 12:10:46,126 - INFO - > Epoch 95: took 85.1s (avg 90.3s) | Best so far: epoch 91	train_loss: 0.7074 train_accuracy-SBM: 0.6894	val_loss: 0.8053 val_accuracy-SBM: 0.6656	test_loss: 0.7891 test_accuracy-SBM: 0.6673
2025-07-11 12:12:03,729 - INFO - train: {'epoch': 96, 'time_epoch': 77.36779, 'eta': 246.14403, 'eta_hours': 0.06837, 'loss': 0.70533527, 'lr': 4.37e-06, 'params': 541238, 'time_iter': 0.12379, 'accuracy': 0.69057, 'f1': 0.68309, 'accuracy-SBM': 0.69046, 'auc': 0.93904}
2025-07-11 12:12:07,462 - INFO - val: {'epoch': 96, 'time_epoch': 3.69025, 'loss': 0.80595086, 'lr': 0, 'params': 541238, 'time_iter': 0.05858, 'accuracy': 0.66422, 'f1': 0.65754, 'accuracy-SBM': 0.66448, 'auc': 0.92725}
2025-07-11 12:12:11,182 - INFO - test: {'epoch': 96, 'time_epoch': 3.68153, 'loss': 0.79027748, 'lr': 0, 'params': 541238, 'time_iter': 0.05844, 'accuracy': 0.6662, 'f1': 0.65907, 'accuracy-SBM': 0.66667, 'auc': 0.92856}
2025-07-11 12:12:11,185 - INFO - > Epoch 96: took 85.1s (avg 90.3s) | Best so far: epoch 91	train_loss: 0.7074 train_accuracy-SBM: 0.6894	val_loss: 0.8053 val_accuracy-SBM: 0.6656	test_loss: 0.7891 test_accuracy-SBM: 0.6673
2025-07-11 12:13:28,977 - INFO - train: {'epoch': 97, 'time_epoch': 77.31026, 'eta': 163.99933, 'eta_hours': 0.04556, 'loss': 0.70410245, 'lr': 2.46e-06, 'params': 541238, 'time_iter': 0.1237, 'accuracy': 0.69065, 'f1': 0.68322, 'accuracy-SBM': 0.69053, 'auc': 0.93915}
2025-07-11 12:13:32,671 - INFO - val: {'epoch': 97, 'time_epoch': 3.65083, 'loss': 0.80397682, 'lr': 0, 'params': 541238, 'time_iter': 0.05795, 'accuracy': 0.66491, 'f1': 0.65766, 'accuracy-SBM': 0.6652, 'auc': 0.9274}
2025-07-11 12:13:36,399 - INFO - test: {'epoch': 97, 'time_epoch': 3.69502, 'loss': 0.78985943, 'lr': 0, 'params': 541238, 'time_iter': 0.05865, 'accuracy': 0.66625, 'f1': 0.65859, 'accuracy-SBM': 0.66677, 'auc': 0.92852}
2025-07-11 12:13:36,401 - INFO - > Epoch 97: took 85.2s (avg 90.2s) | Best so far: epoch 91	train_loss: 0.7074 train_accuracy-SBM: 0.6894	val_loss: 0.8053 val_accuracy-SBM: 0.6656	test_loss: 0.7891 test_accuracy-SBM: 0.6673
2025-07-11 12:14:52,033 - INFO - train: {'epoch': 98, 'time_epoch': 75.30929, 'eta': 81.93209, 'eta_hours': 0.02276, 'loss': 0.7048154, 'lr': 1.09e-06, 'params': 541238, 'time_iter': 0.12049, 'accuracy': 0.69076, 'f1': 0.6833, 'accuracy-SBM': 0.69064, 'auc': 0.93908}
2025-07-11 12:14:55,690 - INFO - val: {'epoch': 98, 'time_epoch': 3.61533, 'loss': 0.80688168, 'lr': 0, 'params': 541238, 'time_iter': 0.05739, 'accuracy': 0.66408, 'f1': 0.65686, 'accuracy-SBM': 0.66438, 'auc': 0.92718}
2025-07-11 12:14:59,325 - INFO - test: {'epoch': 98, 'time_epoch': 3.60294, 'loss': 0.79130802, 'lr': 0, 'params': 541238, 'time_iter': 0.05719, 'accuracy': 0.666, 'f1': 0.65834, 'accuracy-SBM': 0.66648, 'auc': 0.92845}
2025-07-11 12:14:59,327 - INFO - > Epoch 98: took 82.9s (avg 90.1s) | Best so far: epoch 91	train_loss: 0.7074 train_accuracy-SBM: 0.6894	val_loss: 0.8053 val_accuracy-SBM: 0.6656	test_loss: 0.7891 test_accuracy-SBM: 0.6673
2025-07-11 12:16:16,050 - INFO - train: {'epoch': 99, 'time_epoch': 76.39434, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.70437261, 'lr': 2.7e-07, 'params': 541238, 'time_iter': 0.12223, 'accuracy': 0.6908, 'f1': 0.68327, 'accuracy-SBM': 0.69068, 'auc': 0.93914}
2025-07-11 12:16:19,760 - INFO - val: {'epoch': 99, 'time_epoch': 3.66662, 'loss': 0.80364601, 'lr': 0, 'params': 541238, 'time_iter': 0.0582, 'accuracy': 0.66485, 'f1': 0.65772, 'accuracy-SBM': 0.66514, 'auc': 0.92738}
2025-07-11 12:16:23,415 - INFO - test: {'epoch': 99, 'time_epoch': 3.62405, 'loss': 0.78846418, 'lr': 0, 'params': 541238, 'time_iter': 0.05752, 'accuracy': 0.66615, 'f1': 0.65853, 'accuracy-SBM': 0.66667, 'auc': 0.92862}
2025-07-11 12:16:23,694 - INFO - > Epoch 99: took 84.1s (avg 90.1s) | Best so far: epoch 91	train_loss: 0.7074 train_accuracy-SBM: 0.6894	val_loss: 0.8053 val_accuracy-SBM: 0.6656	test_loss: 0.7891 test_accuracy-SBM: 0.6673
2025-07-11 12:16:23,694 - INFO - Avg time per epoch: 90.07s
2025-07-11 12:16:23,694 - INFO - Total train loop time: 2.50h
2025-07-11 12:16:23,709 - INFO - Task done, results saved in results/Cluster/Cluster-GATEDGCN-45
2025-07-11 12:16:23,710 - INFO - Total time: 9072.43s (2.52h)
2025-07-11 12:16:23,727 - INFO - Results aggregated across runs saved in results/Cluster/Cluster-GATEDGCN-45/agg
2025-07-11 12:16:23,727 - INFO - === OPTIMIZED TRAINING COMPLETED SUCCESSFULLY! ===
2025-07-11 12:16:23,727 - INFO - Results saved in: results/Cluster/Cluster-GATEDGCN-45
2025-07-11 12:16:23,727 - INFO - Test results JSON files saved in: results/Cluster/Cluster-GATEDGCN-45/test_results/
Completed seed 45. Results saved in results/Cluster/Cluster-GATEDGCN-45
----------------------------------------
Submitting next job for seed 47
Submitted batch job 5349005
