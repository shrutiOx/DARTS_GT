Lmod has detected the following error: The following module(s) are unknown:
"cuda/12.0"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore-cache load "cuda/12.0"

Also make sure that all modulefiles written in TCL start with the string
#%Module



              total        used        free      shared  buff/cache   available
Mem:          376Gi        19Gi       295Gi       2.9Gi        61Gi       351Gi
Swap:         1.9Gi       3.0Mi       1.9Gi
Fri Jul 11 09:27:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:1D:00.0 Off |                    0 |
| N/A   32C    P0             42W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Running experiment with seed: 45
Starting training for seed 45...
Changed working directory to: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GATV2
Loading config from: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GATV2/confignas.yaml
Using device: cuda
2025-07-11 09:27:54,983 - INFO - GPU Mem: 34.1GB
2025-07-11 09:27:54,984 - INFO - Run directory: results/Cluster/Cluster-GATV2-45
2025-07-11 09:27:54,984 - INFO - Seed: 45
2025-07-11 09:27:54,984 - INFO - === OPTIMIZED MOE CONFIGURATION ===
2025-07-11 09:27:54,984 - INFO - Routing mode: none
2025-07-11 09:27:54,984 - INFO - Expert types: ['GINE', 'CustomGatedGCN', 'GATV2']
2025-07-11 09:27:54,984 - INFO - Number of layers: 16
2025-07-11 09:27:54,984 - INFO - Uncertainty enabled: False
2025-07-11 09:27:54,984 - INFO - Training mode: custom
2025-07-11 09:27:54,984 - INFO - Optimization: Uncertainty only for last layer + test/val
2025-07-11 09:27:54,984 - INFO - Additional features: Router weights logging + JSON export
2025-07-11 09:28:08,301 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 09:28:08,302 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 09:28:08,304 - INFO -   undirected: True
2025-07-11 09:28:08,304 - INFO -   num graphs: 12000
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 09:28:08,304 - INFO -   avg num_nodes/graph: 117
2025-07-11 09:28:08,304 - INFO -   num node features: 7
2025-07-11 09:28:08,305 - INFO -   num edge features: 0
2025-07-11 09:28:08,306 - INFO -   num classes: 6
2025-07-11 09:28:08,306 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 09:28:08,306 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 09:28:08,314 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s] 17%|█▋        | 2058/12000 [00:10<00:48, 205.74it/s] 34%|███▍      | 4095/12000 [00:20<00:38, 204.49it/s] 51%|█████▏    | 6170/12000 [00:30<00:28, 205.84it/s] 69%|██████▊   | 8233/12000 [00:40<00:18, 206.01it/s] 85%|████████▌ | 10223/12000 [00:50<00:08, 203.45it/s]100%|██████████| 12000/12000 [00:59<00:00, 203.10it/s]
2025-07-11 09:29:08,110 - INFO - Done! Took 00:00:59.80
2025-07-11 09:29:08,131 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 09:29:08,379 - INFO - Created model type: <class 'torch_geometric.graphgym.model_builder.GraphGymModule'>
2025-07-11 09:29:08,380 - INFO - Inner model type: <class 'graphgps.network.uniform_gt_model_edge.UNIFORM_GTModelEdge'>
2025-07-11 09:29:08,380 - INFO - Inner model has get_darts_model: False
2025-07-11 09:29:08,383 - INFO - GraphGymModule(
  (model): UNIFORM_GTModelEdge(
    (encoder): FeatureEncoder(
      (node_encoder): LapPENodeEncoder(
        (linear_x): Linear(in_features=7, out_features=32, bias=True)
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (edge_encoder): DummyEdgeEncoder(
        (encoder): Embedding(1, 48)
      )
    )
    (layers): Sequential(
      (0): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (1): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (2): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (3): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (4): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (5): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (6): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (7): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (8): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (9): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (10): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (11): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (12): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (13): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (14): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (15): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (post_mp): GNNInductiveNodeHead(
      (layer_post_mp): MLP(
        (model): Sequential(
          (0): GeneralMultiLayer(
            (Layer_0): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
            (Layer_1): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
          )
          (1): Linear(
            (model): Linear(48, 6, bias=True)
          )
        )
      )
    )
  )
)
2025-07-11 09:29:08,388 - INFO - Number of parameters: 463,670
2025-07-11 09:29:08,388 - INFO - Starting optimized training: 2025-07-11 09:29:08.388623
2025-07-11 09:29:13,822 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
2025-07-11 09:29:13,822 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 09:29:13,823 - INFO -   undirected: True
2025-07-11 09:29:13,823 - INFO -   num graphs: 12000
2025-07-11 09:29:13,823 - INFO -   avg num_nodes/graph: 117
2025-07-11 09:29:13,823 - INFO -   num node features: 7
2025-07-11 09:29:13,824 - INFO -   num edge features: 0
2025-07-11 09:29:13,825 - INFO -   num classes: 6
2025-07-11 09:29:13,825 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 09:29:13,825 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 09:29:13,833 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s] 17%|█▋        | 2072/12000 [00:10<00:47, 207.15it/s] 34%|███▍      | 4111/12000 [00:20<00:38, 205.22it/s] 51%|█████     | 6136/12000 [00:30<00:28, 203.95it/s] 68%|██████▊   | 8193/12000 [00:40<00:18, 204.61it/s] 85%|████████▌ | 10241/12000 [00:50<00:08, 204.65it/s]100%|██████████| 12000/12000 [00:58<00:00, 205.00it/s]
2025-07-11 09:30:13,066 - INFO - Done! Took 00:00:59.24
2025-07-11 09:30:13,089 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 09:30:13,095 - INFO - === STARTING OPTIMIZED TRAINING ===
2025-07-11 09:30:13,095 - INFO - Start from epoch 0
2025-07-11 09:31:42,661 - INFO - train: {'epoch': 0, 'time_epoch': 88.71183, 'eta': 8782.47142, 'eta_hours': 2.43958, 'loss': 1.79851826, 'lr': 0.0, 'params': 463670, 'time_iter': 0.14194, 'accuracy': 0.1655, 'f1': 0.09753, 'accuracy-SBM': 0.16556, 'auc': 0.49833}
2025-07-11 09:31:42,667 - INFO - ...computing epoch stats took: 0.84s
2025-07-11 09:31:46,986 - INFO - val: {'epoch': 0, 'time_epoch': 4.27831, 'loss': 1.79968127, 'lr': 0, 'params': 463670, 'time_iter': 0.06791, 'accuracy': 0.16248, 'f1': 0.08477, 'accuracy-SBM': 0.1655, 'auc': 0.49996}
2025-07-11 09:31:46,988 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 09:31:51,319 - INFO - test: {'epoch': 0, 'time_epoch': 4.28243, 'loss': 1.79975554, 'lr': 0, 'params': 463670, 'time_iter': 0.06798, 'accuracy': 0.16487, 'f1': 0.0882, 'accuracy-SBM': 0.16546, 'auc': 0.49875}
2025-07-11 09:31:51,321 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 09:31:51,321 - INFO - > Epoch 0: took 98.2s (avg 98.2s) | Best so far: epoch 0	train_loss: 1.7985 train_accuracy-SBM: 0.1656	val_loss: 1.7997 val_accuracy-SBM: 0.1655	test_loss: 1.7998 test_accuracy-SBM: 0.1655
2025-07-11 09:33:20,959 - INFO - train: {'epoch': 1, 'time_epoch': 89.39803, 'eta': 8727.38318, 'eta_hours': 2.42427, 'loss': 1.63734033, 'lr': 0.0002, 'params': 463670, 'time_iter': 0.14304, 'accuracy': 0.37297, 'f1': 0.36862, 'accuracy-SBM': 0.37303, 'auc': 0.71187}
2025-07-11 09:33:20,965 - INFO - ...computing epoch stats took: 0.23s
2025-07-11 09:33:25,193 - INFO - val: {'epoch': 1, 'time_epoch': 4.18374, 'loss': 1.59076891, 'lr': 0, 'params': 463670, 'time_iter': 0.06641, 'accuracy': 0.41963, 'f1': 0.40028, 'accuracy-SBM': 0.41951, 'auc': 0.77445}
2025-07-11 09:33:25,194 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 09:33:29,458 - INFO - test: {'epoch': 1, 'time_epoch': 4.22521, 'loss': 1.58399554, 'lr': 0, 'params': 463670, 'time_iter': 0.06707, 'accuracy': 0.42667, 'f1': 0.40931, 'accuracy-SBM': 0.42659, 'auc': 0.77805}
2025-07-11 09:33:29,460 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 09:33:29,460 - INFO - > Epoch 1: took 98.1s (avg 98.2s) | Best so far: epoch 1	train_loss: 1.6373 train_accuracy-SBM: 0.3730	val_loss: 1.5908 val_accuracy-SBM: 0.4195	test_loss: 1.5840 test_accuracy-SBM: 0.4266
2025-07-11 09:34:59,046 - INFO - train: {'epoch': 2, 'time_epoch': 89.34938, 'eta': 8647.84888, 'eta_hours': 2.40218, 'loss': 1.26416882, 'lr': 0.0004, 'params': 463670, 'time_iter': 0.14296, 'accuracy': 0.58577, 'f1': 0.58561, 'accuracy-SBM': 0.58578, 'auc': 0.85039}
2025-07-11 09:34:59,052 - INFO - ...computing epoch stats took: 0.22s
2025-07-11 09:35:03,261 - INFO - val: {'epoch': 2, 'time_epoch': 4.16783, 'loss': 1.51775344, 'lr': 0, 'params': 463670, 'time_iter': 0.06616, 'accuracy': 0.40576, 'f1': 0.39047, 'accuracy-SBM': 0.40343, 'auc': 0.82794}
2025-07-11 09:35:03,263 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 09:35:07,468 - INFO - test: {'epoch': 2, 'time_epoch': 4.17469, 'loss': 1.50445257, 'lr': 0, 'params': 463670, 'time_iter': 0.06626, 'accuracy': 0.41216, 'f1': 0.401, 'accuracy-SBM': 0.41174, 'auc': 0.83064}
2025-07-11 09:35:07,470 - INFO - ...computing epoch stats took: 0.03s
2025-07-11 09:35:07,471 - INFO - > Epoch 2: took 98.0s (avg 98.1s) | Best so far: epoch 1	train_loss: 1.6373 train_accuracy-SBM: 0.3730	val_loss: 1.5908 val_accuracy-SBM: 0.4195	test_loss: 1.5840 test_accuracy-SBM: 0.4266
2025-07-11 09:36:37,825 - INFO - train: {'epoch': 3, 'time_epoch': 90.11698, 'eta': 8581.82933, 'eta_hours': 2.38384, 'loss': 1.04110788, 'lr': 0.0006, 'params': 463670, 'time_iter': 0.14419, 'accuracy': 0.64456, 'f1': 0.64457, 'accuracy-SBM': 0.64457, 'auc': 0.89792}
2025-07-11 09:36:42,170 - INFO - val: {'epoch': 3, 'time_epoch': 4.29357, 'loss': 1.23042955, 'lr': 0, 'params': 463670, 'time_iter': 0.06815, 'accuracy': 0.53729, 'f1': 0.53999, 'accuracy-SBM': 0.53832, 'auc': 0.87764}
2025-07-11 09:36:46,339 - INFO - test: {'epoch': 3, 'time_epoch': 4.10999, 'loss': 1.21737756, 'lr': 0, 'params': 463670, 'time_iter': 0.06524, 'accuracy': 0.54165, 'f1': 0.54453, 'accuracy-SBM': 0.54133, 'auc': 0.88006}
2025-07-11 09:36:46,343 - INFO - > Epoch 3: took 98.9s (avg 98.3s) | Best so far: epoch 3	train_loss: 1.0411 train_accuracy-SBM: 0.6446	val_loss: 1.2304 val_accuracy-SBM: 0.5383	test_loss: 1.2174 test_accuracy-SBM: 0.5413
2025-07-11 09:38:18,524 - INFO - train: {'epoch': 4, 'time_epoch': 91.83597, 'eta': 8538.83175, 'eta_hours': 2.3719, 'loss': 0.92593398, 'lr': 0.0008, 'params': 463670, 'time_iter': 0.14694, 'accuracy': 0.67386, 'f1': 0.67386, 'accuracy-SBM': 0.67386, 'auc': 0.91745}
2025-07-11 09:38:22,728 - INFO - val: {'epoch': 4, 'time_epoch': 4.15853, 'loss': 0.95029231, 'lr': 0, 'params': 463670, 'time_iter': 0.06601, 'accuracy': 0.65531, 'f1': 0.6523, 'accuracy-SBM': 0.65463, 'auc': 0.91866}
2025-07-11 09:38:26,908 - INFO - test: {'epoch': 4, 'time_epoch': 4.14664, 'loss': 0.94322154, 'lr': 0, 'params': 463670, 'time_iter': 0.06582, 'accuracy': 0.65892, 'f1': 0.65619, 'accuracy-SBM': 0.65923, 'auc': 0.9202}
2025-07-11 09:38:26,910 - INFO - > Epoch 4: took 100.6s (avg 98.8s) | Best so far: epoch 4	train_loss: 0.9259 train_accuracy-SBM: 0.6739	val_loss: 0.9503 val_accuracy-SBM: 0.6546	test_loss: 0.9432 test_accuracy-SBM: 0.6592
2025-07-11 09:39:56,054 - INFO - train: {'epoch': 5, 'time_epoch': 88.90305, 'eta': 8433.60548, 'eta_hours': 2.34267, 'loss': 0.86819929, 'lr': 0.001, 'params': 463670, 'time_iter': 0.14224, 'accuracy': 0.68983, 'f1': 0.68983, 'accuracy-SBM': 0.68983, 'auc': 0.92605}
2025-07-11 09:40:00,259 - INFO - val: {'epoch': 5, 'time_epoch': 4.15223, 'loss': 0.91669677, 'lr': 0, 'params': 463670, 'time_iter': 0.06591, 'accuracy': 0.67024, 'f1': 0.67009, 'accuracy-SBM': 0.66966, 'auc': 0.92109}
2025-07-11 09:40:04,454 - INFO - test: {'epoch': 5, 'time_epoch': 4.1634, 'loss': 0.89790687, 'lr': 0, 'params': 463670, 'time_iter': 0.06609, 'accuracy': 0.67478, 'f1': 0.67511, 'accuracy-SBM': 0.67466, 'auc': 0.92429}
2025-07-11 09:40:04,456 - INFO - > Epoch 5: took 97.5s (avg 98.6s) | Best so far: epoch 5	train_loss: 0.8682 train_accuracy-SBM: 0.6898	val_loss: 0.9167 val_accuracy-SBM: 0.6697	test_loss: 0.8979 test_accuracy-SBM: 0.6747
2025-07-11 09:41:34,987 - INFO - train: {'epoch': 6, 'time_epoch': 90.17136, 'eta': 8349.89338, 'eta_hours': 2.31941, 'loss': 0.82796026, 'lr': 0.00099973, 'params': 463670, 'time_iter': 0.14427, 'accuracy': 0.70298, 'f1': 0.70298, 'accuracy-SBM': 0.70298, 'auc': 0.93217}
2025-07-11 09:41:39,149 - INFO - val: {'epoch': 6, 'time_epoch': 4.10956, 'loss': 0.80933616, 'lr': 0, 'params': 463670, 'time_iter': 0.06523, 'accuracy': 0.70869, 'f1': 0.70805, 'accuracy-SBM': 0.70825, 'auc': 0.93652}
2025-07-11 09:41:43,233 - INFO - test: {'epoch': 6, 'time_epoch': 4.04706, 'loss': 0.80295734, 'lr': 0, 'params': 463670, 'time_iter': 0.06424, 'accuracy': 0.71259, 'f1': 0.71222, 'accuracy-SBM': 0.71242, 'auc': 0.93772}
2025-07-11 09:41:43,381 - INFO - > Epoch 6: took 98.9s (avg 98.6s) | Best so far: epoch 6	train_loss: 0.8280 train_accuracy-SBM: 0.7030	val_loss: 0.8093 val_accuracy-SBM: 0.7083	test_loss: 0.8030 test_accuracy-SBM: 0.7124
2025-07-11 09:43:11,326 - INFO - train: {'epoch': 7, 'time_epoch': 87.69587, 'eta': 8236.09835, 'eta_hours': 2.28781, 'loss': 0.7964286, 'lr': 0.00099891, 'params': 463670, 'time_iter': 0.14031, 'accuracy': 0.71353, 'f1': 0.71353, 'accuracy-SBM': 0.71353, 'auc': 0.937}
2025-07-11 09:43:15,252 - INFO - val: {'epoch': 7, 'time_epoch': 3.86922, 'loss': 0.78429663, 'lr': 0, 'params': 463670, 'time_iter': 0.06142, 'accuracy': 0.72043, 'f1': 0.72019, 'accuracy-SBM': 0.72032, 'auc': 0.9401}
2025-07-11 09:43:19,178 - INFO - test: {'epoch': 7, 'time_epoch': 3.8923, 'loss': 0.77502075, 'lr': 0, 'params': 463670, 'time_iter': 0.06178, 'accuracy': 0.72065, 'f1': 0.72054, 'accuracy-SBM': 0.72048, 'auc': 0.94161}
2025-07-11 09:43:19,181 - INFO - > Epoch 7: took 95.8s (avg 98.3s) | Best so far: epoch 7	train_loss: 0.7964 train_accuracy-SBM: 0.7135	val_loss: 0.7843 val_accuracy-SBM: 0.7203	test_loss: 0.7750 test_accuracy-SBM: 0.7205
2025-07-11 09:44:44,007 - INFO - train: {'epoch': 8, 'time_epoch': 84.50041, 'eta': 8095.79353, 'eta_hours': 2.24883, 'loss': 0.78080005, 'lr': 0.00099754, 'params': 463670, 'time_iter': 0.1352, 'accuracy': 0.71793, 'f1': 0.71793, 'accuracy-SBM': 0.71793, 'auc': 0.93934}
2025-07-11 09:44:47,929 - INFO - val: {'epoch': 8, 'time_epoch': 3.87875, 'loss': 0.7611547, 'lr': 0, 'params': 463670, 'time_iter': 0.06157, 'accuracy': 0.72734, 'f1': 0.72725, 'accuracy-SBM': 0.72721, 'auc': 0.94422}
2025-07-11 09:44:51,820 - INFO - test: {'epoch': 8, 'time_epoch': 3.85788, 'loss': 0.75291269, 'lr': 0, 'params': 463670, 'time_iter': 0.06124, 'accuracy': 0.72806, 'f1': 0.72818, 'accuracy-SBM': 0.7281, 'auc': 0.94568}
2025-07-11 09:44:51,822 - INFO - > Epoch 8: took 92.6s (avg 97.6s) | Best so far: epoch 8	train_loss: 0.7808 train_accuracy-SBM: 0.7179	val_loss: 0.7612 val_accuracy-SBM: 0.7272	test_loss: 0.7529 test_accuracy-SBM: 0.7281
2025-07-11 09:46:16,865 - INFO - train: {'epoch': 9, 'time_epoch': 84.81299, 'eta': 7969.4628, 'eta_hours': 2.21374, 'loss': 0.76705497, 'lr': 0.00099563, 'params': 463670, 'time_iter': 0.1357, 'accuracy': 0.72316, 'f1': 0.72316, 'accuracy-SBM': 0.72316, 'auc': 0.94134}
2025-07-11 09:46:20,778 - INFO - val: {'epoch': 9, 'time_epoch': 3.86623, 'loss': 0.7396101, 'lr': 0, 'params': 463670, 'time_iter': 0.06137, 'accuracy': 0.7332, 'f1': 0.7332, 'accuracy-SBM': 0.73309, 'auc': 0.94593}
2025-07-11 09:46:24,640 - INFO - test: {'epoch': 9, 'time_epoch': 3.83126, 'loss': 0.73261824, 'lr': 0, 'params': 463670, 'time_iter': 0.06081, 'accuracy': 0.73416, 'f1': 0.7341, 'accuracy-SBM': 0.73413, 'auc': 0.94712}
2025-07-11 09:46:24,642 - INFO - > Epoch 9: took 92.8s (avg 97.2s) | Best so far: epoch 9	train_loss: 0.7671 train_accuracy-SBM: 0.7232	val_loss: 0.7396 val_accuracy-SBM: 0.7331	test_loss: 0.7326 test_accuracy-SBM: 0.7341
2025-07-11 09:47:48,771 - INFO - train: {'epoch': 10, 'time_epoch': 83.89839, 'eta': 7843.28079, 'eta_hours': 2.17869, 'loss': 0.75438338, 'lr': 0.00099318, 'params': 463670, 'time_iter': 0.13424, 'accuracy': 0.72847, 'f1': 0.72847, 'accuracy-SBM': 0.72847, 'auc': 0.94317}
2025-07-11 09:47:52,740 - INFO - val: {'epoch': 10, 'time_epoch': 3.92598, 'loss': 0.74311452, 'lr': 0, 'params': 463670, 'time_iter': 0.06232, 'accuracy': 0.73271, 'f1': 0.73254, 'accuracy-SBM': 0.73249, 'auc': 0.9458}
2025-07-11 09:47:56,707 - INFO - test: {'epoch': 10, 'time_epoch': 3.93515, 'loss': 0.73889928, 'lr': 0, 'params': 463670, 'time_iter': 0.06246, 'accuracy': 0.73322, 'f1': 0.73303, 'accuracy-SBM': 0.73295, 'auc': 0.9464}
2025-07-11 09:47:56,709 - INFO - > Epoch 10: took 92.1s (avg 96.7s) | Best so far: epoch 9	train_loss: 0.7671 train_accuracy-SBM: 0.7232	val_loss: 0.7396 val_accuracy-SBM: 0.7331	test_loss: 0.7326 test_accuracy-SBM: 0.7341
2025-07-11 09:49:21,567 - INFO - train: {'epoch': 11, 'time_epoch': 84.5362, 'eta': 7728.82334, 'eta_hours': 2.1469, 'loss': 0.74784259, 'lr': 0.00099019, 'params': 463670, 'time_iter': 0.13526, 'accuracy': 0.73, 'f1': 0.73001, 'accuracy-SBM': 0.73, 'auc': 0.94415}
2025-07-11 09:49:25,585 - INFO - val: {'epoch': 11, 'time_epoch': 3.96587, 'loss': 0.74916049, 'lr': 0, 'params': 463670, 'time_iter': 0.06295, 'accuracy': 0.73086, 'f1': 0.73133, 'accuracy-SBM': 0.73117, 'auc': 0.94606}
2025-07-11 09:49:29,628 - INFO - test: {'epoch': 11, 'time_epoch': 4.01162, 'loss': 0.73821004, 'lr': 0, 'params': 463670, 'time_iter': 0.06368, 'accuracy': 0.73375, 'f1': 0.73382, 'accuracy-SBM': 0.73388, 'auc': 0.94802}
2025-07-11 09:49:29,631 - INFO - > Epoch 11: took 92.9s (avg 96.4s) | Best so far: epoch 9	train_loss: 0.7671 train_accuracy-SBM: 0.7232	val_loss: 0.7396 val_accuracy-SBM: 0.7331	test_loss: 0.7326 test_accuracy-SBM: 0.7341
2025-07-11 09:50:54,363 - INFO - train: {'epoch': 12, 'time_epoch': 84.4976, 'eta': 7618.71081, 'eta_hours': 2.11631, 'loss': 0.73634316, 'lr': 0.00098666, 'params': 463670, 'time_iter': 0.1352, 'accuracy': 0.73445, 'f1': 0.73445, 'accuracy-SBM': 0.73445, 'auc': 0.94579}
2025-07-11 09:50:58,414 - INFO - val: {'epoch': 12, 'time_epoch': 4.00684, 'loss': 0.71032369, 'lr': 0, 'params': 463670, 'time_iter': 0.0636, 'accuracy': 0.74393, 'f1': 0.74388, 'accuracy-SBM': 0.74356, 'auc': 0.95057}
2025-07-11 09:51:02,335 - INFO - test: {'epoch': 12, 'time_epoch': 3.89023, 'loss': 0.70299723, 'lr': 0, 'params': 463670, 'time_iter': 0.06175, 'accuracy': 0.74547, 'f1': 0.74558, 'accuracy-SBM': 0.74546, 'auc': 0.95206}
2025-07-11 09:51:02,337 - INFO - > Epoch 12: took 92.7s (avg 96.1s) | Best so far: epoch 12	train_loss: 0.7363 train_accuracy-SBM: 0.7345	val_loss: 0.7103 val_accuracy-SBM: 0.7436	test_loss: 0.7030 test_accuracy-SBM: 0.7455
2025-07-11 09:52:29,072 - INFO - train: {'epoch': 13, 'time_epoch': 86.30667, 'eta': 7523.37042, 'eta_hours': 2.08983, 'loss': 0.73130698, 'lr': 0.0009826, 'params': 463670, 'time_iter': 0.13809, 'accuracy': 0.73595, 'f1': 0.73595, 'accuracy-SBM': 0.73595, 'auc': 0.94651}
2025-07-11 09:52:33,079 - INFO - val: {'epoch': 13, 'time_epoch': 3.96352, 'loss': 0.7106841, 'lr': 0, 'params': 463670, 'time_iter': 0.06291, 'accuracy': 0.74569, 'f1': 0.74547, 'accuracy-SBM': 0.74545, 'auc': 0.95074}
2025-07-11 09:52:37,074 - INFO - test: {'epoch': 13, 'time_epoch': 3.96173, 'loss': 0.70935197, 'lr': 0, 'params': 463670, 'time_iter': 0.06288, 'accuracy': 0.74463, 'f1': 0.74462, 'accuracy-SBM': 0.74448, 'auc': 0.95084}
2025-07-11 09:52:37,076 - INFO - > Epoch 13: took 94.7s (avg 96.0s) | Best so far: epoch 13	train_loss: 0.7313 train_accuracy-SBM: 0.7359	val_loss: 0.7107 val_accuracy-SBM: 0.7454	test_loss: 0.7094 test_accuracy-SBM: 0.7445
2025-07-11 09:54:01,849 - INFO - train: {'epoch': 14, 'time_epoch': 84.51848, 'eta': 7419.10144, 'eta_hours': 2.06086, 'loss': 0.7232926, 'lr': 0.00097802, 'params': 463670, 'time_iter': 0.13523, 'accuracy': 0.73827, 'f1': 0.73827, 'accuracy-SBM': 0.73827, 'auc': 0.94769}
2025-07-11 09:54:05,850 - INFO - val: {'epoch': 14, 'time_epoch': 3.95829, 'loss': 0.703926, 'lr': 0, 'params': 463670, 'time_iter': 0.06283, 'accuracy': 0.74823, 'f1': 0.74838, 'accuracy-SBM': 0.7482, 'auc': 0.95146}
2025-07-11 09:54:09,806 - INFO - test: {'epoch': 14, 'time_epoch': 3.92285, 'loss': 0.69965782, 'lr': 0, 'params': 463670, 'time_iter': 0.06227, 'accuracy': 0.74838, 'f1': 0.74838, 'accuracy-SBM': 0.74849, 'auc': 0.95227}
2025-07-11 09:54:09,808 - INFO - > Epoch 14: took 92.7s (avg 95.8s) | Best so far: epoch 14	train_loss: 0.7233 train_accuracy-SBM: 0.7383	val_loss: 0.7039 val_accuracy-SBM: 0.7482	test_loss: 0.6997 test_accuracy-SBM: 0.7485
2025-07-11 09:55:35,002 - INFO - train: {'epoch': 15, 'time_epoch': 84.95411, 'eta': 7319.58838, 'eta_hours': 2.03322, 'loss': 0.72068355, 'lr': 0.00097291, 'params': 463670, 'time_iter': 0.13593, 'accuracy': 0.7386, 'f1': 0.7386, 'accuracy-SBM': 0.7386, 'auc': 0.94806}
2025-07-11 09:55:39,024 - INFO - val: {'epoch': 15, 'time_epoch': 3.9788, 'loss': 0.717144, 'lr': 0, 'params': 463670, 'time_iter': 0.06316, 'accuracy': 0.74208, 'f1': 0.74206, 'accuracy-SBM': 0.74194, 'auc': 0.949}
2025-07-11 09:55:43,033 - INFO - test: {'epoch': 15, 'time_epoch': 3.96784, 'loss': 0.70665436, 'lr': 0, 'params': 463670, 'time_iter': 0.06298, 'accuracy': 0.74548, 'f1': 0.74559, 'accuracy-SBM': 0.74547, 'auc': 0.95057}
2025-07-11 09:55:43,035 - INFO - > Epoch 15: took 93.2s (avg 95.6s) | Best so far: epoch 14	train_loss: 0.7233 train_accuracy-SBM: 0.7383	val_loss: 0.7039 val_accuracy-SBM: 0.7482	test_loss: 0.6997 test_accuracy-SBM: 0.7485
2025-07-11 09:57:08,244 - INFO - train: {'epoch': 16, 'time_epoch': 84.97863, 'eta': 7221.9078, 'eta_hours': 2.00609, 'loss': 0.7121593, 'lr': 0.00096728, 'params': 463670, 'time_iter': 0.13597, 'accuracy': 0.74224, 'f1': 0.74224, 'accuracy-SBM': 0.74223, 'auc': 0.94928}
2025-07-11 09:57:12,248 - INFO - val: {'epoch': 16, 'time_epoch': 3.95964, 'loss': 0.68653826, 'lr': 0, 'params': 463670, 'time_iter': 0.06285, 'accuracy': 0.75319, 'f1': 0.75324, 'accuracy-SBM': 0.75314, 'auc': 0.95339}
2025-07-11 09:57:16,242 - INFO - test: {'epoch': 16, 'time_epoch': 3.95418, 'loss': 0.68314825, 'lr': 0, 'params': 463670, 'time_iter': 0.06276, 'accuracy': 0.75162, 'f1': 0.75177, 'accuracy-SBM': 0.75169, 'auc': 0.95413}
2025-07-11 09:57:16,244 - INFO - > Epoch 16: took 93.2s (avg 95.5s) | Best so far: epoch 16	train_loss: 0.7122 train_accuracy-SBM: 0.7422	val_loss: 0.6865 val_accuracy-SBM: 0.7531	test_loss: 0.6831 test_accuracy-SBM: 0.7517
2025-07-11 09:58:40,751 - INFO - train: {'epoch': 17, 'time_epoch': 84.2732, 'eta': 7122.42494, 'eta_hours': 1.97845, 'loss': 0.70796303, 'lr': 0.00096114, 'params': 463670, 'time_iter': 0.13484, 'accuracy': 0.7439, 'f1': 0.7439, 'accuracy-SBM': 0.7439, 'auc': 0.94986}
2025-07-11 09:58:44,720 - INFO - val: {'epoch': 17, 'time_epoch': 3.90377, 'loss': 0.69195839, 'lr': 0, 'params': 463670, 'time_iter': 0.06196, 'accuracy': 0.75238, 'f1': 0.75231, 'accuracy-SBM': 0.75226, 'auc': 0.95235}
2025-07-11 09:58:48,645 - INFO - test: {'epoch': 17, 'time_epoch': 3.8929, 'loss': 0.68315948, 'lr': 0, 'params': 463670, 'time_iter': 0.06179, 'accuracy': 0.75271, 'f1': 0.75267, 'accuracy-SBM': 0.75275, 'auc': 0.95364}
2025-07-11 09:58:48,647 - INFO - > Epoch 17: took 92.4s (avg 95.3s) | Best so far: epoch 16	train_loss: 0.7122 train_accuracy-SBM: 0.7422	val_loss: 0.6865 val_accuracy-SBM: 0.7531	test_loss: 0.6831 test_accuracy-SBM: 0.7517
2025-07-11 10:00:13,788 - INFO - train: {'epoch': 18, 'time_epoch': 84.91273, 'eta': 7027.26954, 'eta_hours': 1.95202, 'loss': 0.70313226, 'lr': 0.0009545, 'params': 463670, 'time_iter': 0.13586, 'accuracy': 0.74609, 'f1': 0.74609, 'accuracy-SBM': 0.74609, 'auc': 0.95051}
2025-07-11 10:00:17,820 - INFO - val: {'epoch': 18, 'time_epoch': 3.98744, 'loss': 0.67313604, 'lr': 0, 'params': 463670, 'time_iter': 0.06329, 'accuracy': 0.75666, 'f1': 0.75655, 'accuracy-SBM': 0.7566, 'auc': 0.95507}
2025-07-11 10:00:21,815 - INFO - test: {'epoch': 18, 'time_epoch': 3.96231, 'loss': 0.67343616, 'lr': 0, 'params': 463670, 'time_iter': 0.06289, 'accuracy': 0.75608, 'f1': 0.75612, 'accuracy-SBM': 0.75609, 'auc': 0.95498}
2025-07-11 10:00:21,817 - INFO - > Epoch 18: took 93.2s (avg 95.2s) | Best so far: epoch 18	train_loss: 0.7031 train_accuracy-SBM: 0.7461	val_loss: 0.6731 val_accuracy-SBM: 0.7566	test_loss: 0.6734 test_accuracy-SBM: 0.7561
2025-07-11 10:01:46,379 - INFO - train: {'epoch': 19, 'time_epoch': 84.32806, 'eta': 6930.7997, 'eta_hours': 1.92522, 'loss': 0.70221458, 'lr': 0.00094736, 'params': 463670, 'time_iter': 0.13492, 'accuracy': 0.74594, 'f1': 0.74594, 'accuracy-SBM': 0.74594, 'auc': 0.95064}
2025-07-11 10:01:50,349 - INFO - val: {'epoch': 19, 'time_epoch': 3.92543, 'loss': 0.6848261, 'lr': 0, 'params': 463670, 'time_iter': 0.06231, 'accuracy': 0.75615, 'f1': 0.75603, 'accuracy-SBM': 0.75601, 'auc': 0.95343}
2025-07-11 10:01:54,290 - INFO - test: {'epoch': 19, 'time_epoch': 3.90972, 'loss': 0.67472523, 'lr': 0, 'params': 463670, 'time_iter': 0.06206, 'accuracy': 0.7571, 'f1': 0.75696, 'accuracy-SBM': 0.75698, 'auc': 0.95483}
2025-07-11 10:01:54,293 - INFO - > Epoch 19: took 92.5s (avg 95.1s) | Best so far: epoch 18	train_loss: 0.7031 train_accuracy-SBM: 0.7461	val_loss: 0.6731 val_accuracy-SBM: 0.7566	test_loss: 0.6734 test_accuracy-SBM: 0.7561
2025-07-11 10:03:18,683 - INFO - train: {'epoch': 20, 'time_epoch': 84.16156, 'eta': 6834.85989, 'eta_hours': 1.89857, 'loss': 0.69566207, 'lr': 0.00093974, 'params': 463670, 'time_iter': 0.13466, 'accuracy': 0.74808, 'f1': 0.74808, 'accuracy-SBM': 0.74808, 'auc': 0.95156}
2025-07-11 10:03:22,632 - INFO - val: {'epoch': 20, 'time_epoch': 3.90592, 'loss': 0.68148873, 'lr': 0, 'params': 463670, 'time_iter': 0.062, 'accuracy': 0.75715, 'f1': 0.75683, 'accuracy-SBM': 0.75683, 'auc': 0.95408}
2025-07-11 10:03:26,541 - INFO - test: {'epoch': 20, 'time_epoch': 3.87663, 'loss': 0.67158994, 'lr': 0, 'params': 463670, 'time_iter': 0.06153, 'accuracy': 0.75786, 'f1': 0.75785, 'accuracy-SBM': 0.75769, 'auc': 0.95535}
2025-07-11 10:03:26,543 - INFO - > Epoch 20: took 92.3s (avg 94.9s) | Best so far: epoch 20	train_loss: 0.6957 train_accuracy-SBM: 0.7481	val_loss: 0.6815 val_accuracy-SBM: 0.7568	test_loss: 0.6716 test_accuracy-SBM: 0.7577
2025-07-11 10:04:51,013 - INFO - train: {'epoch': 21, 'time_epoch': 84.22093, 'eta': 6740.2013, 'eta_hours': 1.87228, 'loss': 0.69458213, 'lr': 0.00093163, 'params': 463670, 'time_iter': 0.13475, 'accuracy': 0.74942, 'f1': 0.74942, 'accuracy-SBM': 0.74942, 'auc': 0.9517}
2025-07-11 10:04:54,988 - INFO - val: {'epoch': 21, 'time_epoch': 3.93075, 'loss': 0.68425283, 'lr': 0, 'params': 463670, 'time_iter': 0.06239, 'accuracy': 0.75388, 'f1': 0.75373, 'accuracy-SBM': 0.75382, 'auc': 0.95334}
2025-07-11 10:04:58,957 - INFO - test: {'epoch': 21, 'time_epoch': 3.93638, 'loss': 0.67996656, 'lr': 0, 'params': 463670, 'time_iter': 0.06248, 'accuracy': 0.75413, 'f1': 0.75417, 'accuracy-SBM': 0.75399, 'auc': 0.95406}
2025-07-11 10:04:58,959 - INFO - > Epoch 21: took 92.4s (avg 94.8s) | Best so far: epoch 20	train_loss: 0.6957 train_accuracy-SBM: 0.7481	val_loss: 0.6815 val_accuracy-SBM: 0.7568	test_loss: 0.6716 test_accuracy-SBM: 0.7577
2025-07-11 10:06:23,783 - INFO - train: {'epoch': 22, 'time_epoch': 84.59563, 'eta': 6647.70478, 'eta_hours': 1.84658, 'loss': 0.68960938, 'lr': 0.00092305, 'params': 463670, 'time_iter': 0.13535, 'accuracy': 0.75082, 'f1': 0.75082, 'accuracy-SBM': 0.75082, 'auc': 0.9524}
2025-07-11 10:06:27,837 - INFO - val: {'epoch': 22, 'time_epoch': 3.99787, 'loss': 0.69478824, 'lr': 0, 'params': 463670, 'time_iter': 0.06346, 'accuracy': 0.75035, 'f1': 0.75028, 'accuracy-SBM': 0.75001, 'auc': 0.95225}
2025-07-11 10:06:31,850 - INFO - test: {'epoch': 22, 'time_epoch': 3.98105, 'loss': 0.68016686, 'lr': 0, 'params': 463670, 'time_iter': 0.06319, 'accuracy': 0.75233, 'f1': 0.75256, 'accuracy-SBM': 0.75222, 'auc': 0.95425}
2025-07-11 10:06:31,853 - INFO - > Epoch 22: took 92.9s (avg 94.7s) | Best so far: epoch 20	train_loss: 0.6957 train_accuracy-SBM: 0.7481	val_loss: 0.6815 val_accuracy-SBM: 0.7568	test_loss: 0.6716 test_accuracy-SBM: 0.7577
2025-07-11 10:07:58,226 - INFO - train: {'epoch': 23, 'time_epoch': 86.14401, 'eta': 6560.76987, 'eta_hours': 1.82244, 'loss': 0.68467558, 'lr': 0.000914, 'params': 463670, 'time_iter': 0.13783, 'accuracy': 0.75222, 'f1': 0.75222, 'accuracy-SBM': 0.75222, 'auc': 0.95309}
2025-07-11 10:08:02,230 - INFO - val: {'epoch': 23, 'time_epoch': 3.96035, 'loss': 0.66550768, 'lr': 0, 'params': 463670, 'time_iter': 0.06286, 'accuracy': 0.76081, 'f1': 0.76081, 'accuracy-SBM': 0.76072, 'auc': 0.95634}
2025-07-11 10:08:06,116 - INFO - test: {'epoch': 23, 'time_epoch': 3.85419, 'loss': 0.67085419, 'lr': 0, 'params': 463670, 'time_iter': 0.06118, 'accuracy': 0.75701, 'f1': 0.75693, 'accuracy-SBM': 0.75699, 'auc': 0.9557}
2025-07-11 10:08:06,118 - INFO - > Epoch 23: took 94.3s (avg 94.7s) | Best so far: epoch 23	train_loss: 0.6847 train_accuracy-SBM: 0.7522	val_loss: 0.6655 val_accuracy-SBM: 0.7607	test_loss: 0.6709 test_accuracy-SBM: 0.7570
2025-07-11 10:09:30,273 - INFO - train: {'epoch': 24, 'time_epoch': 83.92252, 'eta': 6467.23376, 'eta_hours': 1.79645, 'loss': 0.68424514, 'lr': 0.00090451, 'params': 463670, 'time_iter': 0.13428, 'accuracy': 0.75288, 'f1': 0.75288, 'accuracy-SBM': 0.75287, 'auc': 0.9531}
2025-07-11 10:09:34,319 - INFO - val: {'epoch': 24, 'time_epoch': 4.00215, 'loss': 0.6756275, 'lr': 0, 'params': 463670, 'time_iter': 0.06353, 'accuracy': 0.75512, 'f1': 0.75516, 'accuracy-SBM': 0.75519, 'auc': 0.95462}
2025-07-11 10:09:38,293 - INFO - test: {'epoch': 24, 'time_epoch': 3.94202, 'loss': 0.66807712, 'lr': 0, 'params': 463670, 'time_iter': 0.06257, 'accuracy': 0.75769, 'f1': 0.7577, 'accuracy-SBM': 0.75781, 'auc': 0.95575}
2025-07-11 10:09:38,296 - INFO - > Epoch 24: took 92.2s (avg 94.6s) | Best so far: epoch 23	train_loss: 0.6847 train_accuracy-SBM: 0.7522	val_loss: 0.6655 val_accuracy-SBM: 0.7607	test_loss: 0.6709 test_accuracy-SBM: 0.7570
2025-07-11 10:11:02,706 - INFO - train: {'epoch': 25, 'time_epoch': 84.18323, 'eta': 6375.17917, 'eta_hours': 1.77088, 'loss': 0.6788136, 'lr': 0.00089457, 'params': 463670, 'time_iter': 0.13469, 'accuracy': 0.75422, 'f1': 0.75422, 'accuracy-SBM': 0.75422, 'auc': 0.95387}
2025-07-11 10:11:06,622 - INFO - val: {'epoch': 25, 'time_epoch': 3.87224, 'loss': 0.66369464, 'lr': 0, 'params': 463670, 'time_iter': 0.06146, 'accuracy': 0.76226, 'f1': 0.76228, 'accuracy-SBM': 0.76216, 'auc': 0.95618}
2025-07-11 10:11:10,509 - INFO - test: {'epoch': 25, 'time_epoch': 3.85587, 'loss': 0.66277933, 'lr': 0, 'params': 463670, 'time_iter': 0.0612, 'accuracy': 0.75894, 'f1': 0.75904, 'accuracy-SBM': 0.75908, 'auc': 0.95649}
2025-07-11 10:11:10,511 - INFO - > Epoch 25: took 92.2s (avg 94.5s) | Best so far: epoch 25	train_loss: 0.6788 train_accuracy-SBM: 0.7542	val_loss: 0.6637 val_accuracy-SBM: 0.7622	test_loss: 0.6628 test_accuracy-SBM: 0.7591
2025-07-11 10:12:34,994 - INFO - train: {'epoch': 26, 'time_epoch': 84.25131, 'eta': 6283.8917, 'eta_hours': 1.74553, 'loss': 0.67394639, 'lr': 0.0008842, 'params': 463670, 'time_iter': 0.1348, 'accuracy': 0.75635, 'f1': 0.75635, 'accuracy-SBM': 0.75635, 'auc': 0.95451}
2025-07-11 10:12:39,014 - INFO - val: {'epoch': 26, 'time_epoch': 3.97624, 'loss': 0.66338122, 'lr': 0, 'params': 463670, 'time_iter': 0.06311, 'accuracy': 0.761, 'f1': 0.76068, 'accuracy-SBM': 0.76067, 'auc': 0.95623}
2025-07-11 10:12:42,986 - INFO - test: {'epoch': 26, 'time_epoch': 3.93981, 'loss': 0.66562089, 'lr': 0, 'params': 463670, 'time_iter': 0.06254, 'accuracy': 0.75934, 'f1': 0.75913, 'accuracy-SBM': 0.75915, 'auc': 0.95608}
2025-07-11 10:12:42,988 - INFO - > Epoch 26: took 92.5s (avg 94.4s) | Best so far: epoch 25	train_loss: 0.6788 train_accuracy-SBM: 0.7542	val_loss: 0.6637 val_accuracy-SBM: 0.7622	test_loss: 0.6628 test_accuracy-SBM: 0.7591
2025-07-11 10:14:07,852 - INFO - train: {'epoch': 27, 'time_epoch': 84.54155, 'eta': 6193.85316, 'eta_hours': 1.72051, 'loss': 0.6740005, 'lr': 0.00087341, 'params': 463670, 'time_iter': 0.13527, 'accuracy': 0.75625, 'f1': 0.75625, 'accuracy-SBM': 0.75625, 'auc': 0.9545}
2025-07-11 10:14:11,807 - INFO - val: {'epoch': 27, 'time_epoch': 3.91166, 'loss': 0.66449021, 'lr': 0, 'params': 463670, 'time_iter': 0.06209, 'accuracy': 0.76122, 'f1': 0.76136, 'accuracy-SBM': 0.76122, 'auc': 0.9561}
2025-07-11 10:14:15,754 - INFO - test: {'epoch': 27, 'time_epoch': 3.91612, 'loss': 0.66157339, 'lr': 0, 'params': 463670, 'time_iter': 0.06216, 'accuracy': 0.75878, 'f1': 0.75882, 'accuracy-SBM': 0.75898, 'auc': 0.95674}
2025-07-11 10:14:15,757 - INFO - > Epoch 27: took 92.8s (avg 94.4s) | Best so far: epoch 25	train_loss: 0.6788 train_accuracy-SBM: 0.7542	val_loss: 0.6637 val_accuracy-SBM: 0.7622	test_loss: 0.6628 test_accuracy-SBM: 0.7591
2025-07-11 10:15:40,538 - INFO - train: {'epoch': 28, 'time_epoch': 84.55718, 'eta': 6104.23197, 'eta_hours': 1.69562, 'loss': 0.67206263, 'lr': 0.00086221, 'params': 463670, 'time_iter': 0.13529, 'accuracy': 0.75656, 'f1': 0.75656, 'accuracy-SBM': 0.75656, 'auc': 0.95479}
2025-07-11 10:15:44,451 - INFO - val: {'epoch': 28, 'time_epoch': 3.86994, 'loss': 0.65186474, 'lr': 0, 'params': 463670, 'time_iter': 0.06143, 'accuracy': 0.7672, 'f1': 0.7671, 'accuracy-SBM': 0.76692, 'auc': 0.95759}
2025-07-11 10:15:48,344 - INFO - test: {'epoch': 28, 'time_epoch': 3.86224, 'loss': 0.64601673, 'lr': 0, 'params': 463670, 'time_iter': 0.06131, 'accuracy': 0.76611, 'f1': 0.76612, 'accuracy-SBM': 0.76613, 'auc': 0.95851}
2025-07-11 10:15:48,346 - INFO - > Epoch 28: took 92.6s (avg 94.3s) | Best so far: epoch 28	train_loss: 0.6721 train_accuracy-SBM: 0.7566	val_loss: 0.6519 val_accuracy-SBM: 0.7669	test_loss: 0.6460 test_accuracy-SBM: 0.7661
2025-07-11 10:17:11,905 - INFO - train: {'epoch': 29, 'time_epoch': 83.2325, 'eta': 6011.85748, 'eta_hours': 1.66996, 'loss': 0.66674303, 'lr': 0.00085062, 'params': 463670, 'time_iter': 0.13317, 'accuracy': 0.75817, 'f1': 0.75817, 'accuracy-SBM': 0.75817, 'auc': 0.95548}
2025-07-11 10:17:15,812 - INFO - val: {'epoch': 29, 'time_epoch': 3.86376, 'loss': 0.65975255, 'lr': 0, 'params': 463670, 'time_iter': 0.06133, 'accuracy': 0.76378, 'f1': 0.76393, 'accuracy-SBM': 0.7639, 'auc': 0.95689}
2025-07-11 10:17:19,742 - INFO - test: {'epoch': 29, 'time_epoch': 3.8985, 'loss': 0.6649378, 'lr': 0, 'params': 463670, 'time_iter': 0.06188, 'accuracy': 0.75915, 'f1': 0.75916, 'accuracy-SBM': 0.75928, 'auc': 0.95627}
2025-07-11 10:17:19,744 - INFO - > Epoch 29: took 91.4s (avg 94.2s) | Best so far: epoch 28	train_loss: 0.6721 train_accuracy-SBM: 0.7566	val_loss: 0.6519 val_accuracy-SBM: 0.7669	test_loss: 0.6460 test_accuracy-SBM: 0.7661
2025-07-11 10:18:43,198 - INFO - train: {'epoch': 30, 'time_epoch': 83.22801, 'eta': 5920.06281, 'eta_hours': 1.64446, 'loss': 0.66621921, 'lr': 0.00083864, 'params': 463670, 'time_iter': 0.13316, 'accuracy': 0.75894, 'f1': 0.75894, 'accuracy-SBM': 0.75894, 'auc': 0.95555}
2025-07-11 10:18:47,075 - INFO - val: {'epoch': 30, 'time_epoch': 3.83396, 'loss': 0.66518236, 'lr': 0, 'params': 463670, 'time_iter': 0.06086, 'accuracy': 0.76269, 'f1': 0.76274, 'accuracy-SBM': 0.76278, 'auc': 0.95594}
2025-07-11 10:18:50,926 - INFO - test: {'epoch': 30, 'time_epoch': 3.82031, 'loss': 0.65380156, 'lr': 0, 'params': 463670, 'time_iter': 0.06064, 'accuracy': 0.76401, 'f1': 0.76401, 'accuracy-SBM': 0.76407, 'auc': 0.95738}
2025-07-11 10:18:50,928 - INFO - > Epoch 30: took 91.2s (avg 94.1s) | Best so far: epoch 28	train_loss: 0.6721 train_accuracy-SBM: 0.7566	val_loss: 0.6519 val_accuracy-SBM: 0.7669	test_loss: 0.6460 test_accuracy-SBM: 0.7661
2025-07-11 10:20:14,396 - INFO - train: {'epoch': 31, 'time_epoch': 83.23869, 'eta': 5828.82624, 'eta_hours': 1.61912, 'loss': 0.66080633, 'lr': 0.00082629, 'params': 463670, 'time_iter': 0.13318, 'accuracy': 0.76036, 'f1': 0.76036, 'accuracy-SBM': 0.76036, 'auc': 0.95629}
2025-07-11 10:20:18,312 - INFO - val: {'epoch': 31, 'time_epoch': 3.87351, 'loss': 0.64479257, 'lr': 0, 'params': 463670, 'time_iter': 0.06148, 'accuracy': 0.76747, 'f1': 0.76737, 'accuracy-SBM': 0.76739, 'auc': 0.95844}
2025-07-11 10:20:22,178 - INFO - test: {'epoch': 31, 'time_epoch': 3.83433, 'loss': 0.65157373, 'lr': 0, 'params': 463670, 'time_iter': 0.06086, 'accuracy': 0.76559, 'f1': 0.76558, 'accuracy-SBM': 0.76561, 'auc': 0.95768}
2025-07-11 10:20:22,180 - INFO - > Epoch 31: took 91.3s (avg 94.0s) | Best so far: epoch 31	train_loss: 0.6608 train_accuracy-SBM: 0.7604	val_loss: 0.6448 val_accuracy-SBM: 0.7674	test_loss: 0.6516 test_accuracy-SBM: 0.7656
2025-07-11 10:21:46,843 - INFO - train: {'epoch': 32, 'time_epoch': 84.42974, 'eta': 5740.49259, 'eta_hours': 1.59458, 'loss': 0.65455742, 'lr': 0.00081359, 'params': 463670, 'time_iter': 0.13509, 'accuracy': 0.76265, 'f1': 0.76264, 'accuracy-SBM': 0.76265, 'auc': 0.9571}
2025-07-11 10:21:50,697 - INFO - val: {'epoch': 32, 'time_epoch': 3.80982, 'loss': 0.65024352, 'lr': 0, 'params': 463670, 'time_iter': 0.06047, 'accuracy': 0.76461, 'f1': 0.76438, 'accuracy-SBM': 0.76443, 'auc': 0.95805}
2025-07-11 10:21:54,498 - INFO - test: {'epoch': 32, 'time_epoch': 3.77092, 'loss': 0.65381232, 'lr': 0, 'params': 463670, 'time_iter': 0.05986, 'accuracy': 0.76494, 'f1': 0.76497, 'accuracy-SBM': 0.76481, 'auc': 0.95758}
2025-07-11 10:21:54,500 - INFO - > Epoch 32: took 92.3s (avg 94.0s) | Best so far: epoch 31	train_loss: 0.6608 train_accuracy-SBM: 0.7604	val_loss: 0.6448 val_accuracy-SBM: 0.7674	test_loss: 0.6516 test_accuracy-SBM: 0.7656
2025-07-11 10:23:16,937 - INFO - train: {'epoch': 33, 'time_epoch': 82.21475, 'eta': 5648.08888, 'eta_hours': 1.56891, 'loss': 0.65613575, 'lr': 0.00080054, 'params': 463670, 'time_iter': 0.13154, 'accuracy': 0.762, 'f1': 0.762, 'accuracy-SBM': 0.762, 'auc': 0.95689}
2025-07-11 10:23:20,794 - INFO - val: {'epoch': 33, 'time_epoch': 3.81447, 'loss': 0.6496756, 'lr': 0, 'params': 463670, 'time_iter': 0.06055, 'accuracy': 0.76496, 'f1': 0.76502, 'accuracy-SBM': 0.76505, 'auc': 0.95799}
2025-07-11 10:23:24,645 - INFO - test: {'epoch': 33, 'time_epoch': 3.82098, 'loss': 0.65059318, 'lr': 0, 'params': 463670, 'time_iter': 0.06065, 'accuracy': 0.76426, 'f1': 0.76433, 'accuracy-SBM': 0.76441, 'auc': 0.95799}
2025-07-11 10:23:24,647 - INFO - > Epoch 33: took 90.1s (avg 93.9s) | Best so far: epoch 31	train_loss: 0.6608 train_accuracy-SBM: 0.7604	val_loss: 0.6448 val_accuracy-SBM: 0.7674	test_loss: 0.6516 test_accuracy-SBM: 0.7656
2025-07-11 10:24:47,060 - INFO - train: {'epoch': 34, 'time_epoch': 82.09854, 'eta': 5556.05159, 'eta_hours': 1.54335, 'loss': 0.65058634, 'lr': 0.00078716, 'params': 463670, 'time_iter': 0.13136, 'accuracy': 0.76417, 'f1': 0.76417, 'accuracy-SBM': 0.76417, 'auc': 0.95763}
2025-07-11 10:24:50,933 - INFO - val: {'epoch': 34, 'time_epoch': 3.82838, 'loss': 0.65560897, 'lr': 0, 'params': 463670, 'time_iter': 0.06077, 'accuracy': 0.76403, 'f1': 0.7639, 'accuracy-SBM': 0.76406, 'auc': 0.95716}
2025-07-11 10:24:54,764 - INFO - test: {'epoch': 34, 'time_epoch': 3.79251, 'loss': 0.6482663, 'lr': 0, 'params': 463670, 'time_iter': 0.0602, 'accuracy': 0.76641, 'f1': 0.76644, 'accuracy-SBM': 0.76644, 'auc': 0.95825}
2025-07-11 10:24:54,766 - INFO - > Epoch 34: took 90.1s (avg 93.8s) | Best so far: epoch 31	train_loss: 0.6608 train_accuracy-SBM: 0.7604	val_loss: 0.6448 val_accuracy-SBM: 0.7674	test_loss: 0.6516 test_accuracy-SBM: 0.7656
2025-07-11 10:26:17,059 - INFO - train: {'epoch': 35, 'time_epoch': 82.07418, 'eta': 5464.52314, 'eta_hours': 1.51792, 'loss': 0.65004551, 'lr': 0.00077347, 'params': 463670, 'time_iter': 0.13132, 'accuracy': 0.76455, 'f1': 0.76455, 'accuracy-SBM': 0.76455, 'auc': 0.95769}
2025-07-11 10:26:20,912 - INFO - val: {'epoch': 35, 'time_epoch': 3.81099, 'loss': 0.66594695, 'lr': 0, 'params': 463670, 'time_iter': 0.06049, 'accuracy': 0.76163, 'f1': 0.76164, 'accuracy-SBM': 0.76165, 'auc': 0.95586}
2025-07-11 10:26:24,711 - INFO - test: {'epoch': 35, 'time_epoch': 3.76139, 'loss': 0.66431008, 'lr': 0, 'params': 463670, 'time_iter': 0.0597, 'accuracy': 0.76218, 'f1': 0.76216, 'accuracy-SBM': 0.76227, 'auc': 0.95614}
2025-07-11 10:26:24,713 - INFO - > Epoch 35: took 89.9s (avg 93.7s) | Best so far: epoch 31	train_loss: 0.6608 train_accuracy-SBM: 0.7604	val_loss: 0.6448 val_accuracy-SBM: 0.7674	test_loss: 0.6516 test_accuracy-SBM: 0.7656
2025-07-11 10:27:46,716 - INFO - train: {'epoch': 36, 'time_epoch': 81.7848, 'eta': 5373.01301, 'eta_hours': 1.4925, 'loss': 0.64584271, 'lr': 0.00075948, 'params': 463670, 'time_iter': 0.13086, 'accuracy': 0.76546, 'f1': 0.76546, 'accuracy-SBM': 0.76546, 'auc': 0.95824}
2025-07-11 10:27:50,565 - INFO - val: {'epoch': 36, 'time_epoch': 3.80636, 'loss': 0.65680977, 'lr': 0, 'params': 463670, 'time_iter': 0.06042, 'accuracy': 0.76363, 'f1': 0.76364, 'accuracy-SBM': 0.76363, 'auc': 0.95698}
2025-07-11 10:27:54,384 - INFO - test: {'epoch': 36, 'time_epoch': 3.78772, 'loss': 0.64991875, 'lr': 0, 'params': 463670, 'time_iter': 0.06012, 'accuracy': 0.76583, 'f1': 0.76582, 'accuracy-SBM': 0.76586, 'auc': 0.95798}
2025-07-11 10:27:54,386 - INFO - > Epoch 36: took 89.7s (avg 93.5s) | Best so far: epoch 31	train_loss: 0.6608 train_accuracy-SBM: 0.7604	val_loss: 0.6448 val_accuracy-SBM: 0.7674	test_loss: 0.6516 test_accuracy-SBM: 0.7656
2025-07-11 10:29:16,222 - INFO - train: {'epoch': 37, 'time_epoch': 81.61563, 'eta': 5281.73871, 'eta_hours': 1.46715, 'loss': 0.64806262, 'lr': 0.00074521, 'params': 463670, 'time_iter': 0.13059, 'accuracy': 0.76483, 'f1': 0.76483, 'accuracy-SBM': 0.76483, 'auc': 0.95795}
2025-07-11 10:29:20,113 - INFO - val: {'epoch': 37, 'time_epoch': 3.84931, 'loss': 0.64974989, 'lr': 0, 'params': 463670, 'time_iter': 0.0611, 'accuracy': 0.76547, 'f1': 0.76541, 'accuracy-SBM': 0.76538, 'auc': 0.95801}
2025-07-11 10:29:25,035 - INFO - test: {'epoch': 37, 'time_epoch': 4.78477, 'loss': 0.64773669, 'lr': 0, 'params': 463670, 'time_iter': 0.07595, 'accuracy': 0.76489, 'f1': 0.765, 'accuracy-SBM': 0.76503, 'auc': 0.95835}
2025-07-11 10:29:25,042 - INFO - > Epoch 37: took 90.7s (avg 93.5s) | Best so far: epoch 31	train_loss: 0.6608 train_accuracy-SBM: 0.7604	val_loss: 0.6448 val_accuracy-SBM: 0.7674	test_loss: 0.6516 test_accuracy-SBM: 0.7656
2025-07-11 10:30:47,547 - INFO - train: {'epoch': 38, 'time_epoch': 82.2866, 'eta': 5192.0092, 'eta_hours': 1.44222, 'loss': 0.64217153, 'lr': 0.00073067, 'params': 463670, 'time_iter': 0.13166, 'accuracy': 0.76727, 'f1': 0.76727, 'accuracy-SBM': 0.76727, 'auc': 0.95873}
2025-07-11 10:30:51,403 - INFO - val: {'epoch': 38, 'time_epoch': 3.77576, 'loss': 0.6486025, 'lr': 0, 'params': 463670, 'time_iter': 0.05993, 'accuracy': 0.76928, 'f1': 0.76926, 'accuracy-SBM': 0.76918, 'auc': 0.95791}
2025-07-11 10:30:55,189 - INFO - test: {'epoch': 38, 'time_epoch': 3.75473, 'loss': 0.64060328, 'lr': 0, 'params': 463670, 'time_iter': 0.0596, 'accuracy': 0.7676, 'f1': 0.76763, 'accuracy-SBM': 0.76761, 'auc': 0.95912}
2025-07-11 10:30:55,191 - INFO - > Epoch 38: took 90.1s (avg 93.4s) | Best so far: epoch 38	train_loss: 0.6422 train_accuracy-SBM: 0.7673	val_loss: 0.6486 val_accuracy-SBM: 0.7692	test_loss: 0.6406 test_accuracy-SBM: 0.7676
2025-07-11 10:32:16,965 - INFO - train: {'epoch': 39, 'time_epoch': 81.55175, 'eta': 5101.54956, 'eta_hours': 1.4171, 'loss': 0.6396136, 'lr': 0.00071588, 'params': 463670, 'time_iter': 0.13048, 'accuracy': 0.76788, 'f1': 0.76788, 'accuracy-SBM': 0.76788, 'auc': 0.95905}
2025-07-11 10:32:20,874 - INFO - val: {'epoch': 39, 'time_epoch': 3.85476, 'loss': 0.64198186, 'lr': 0, 'params': 463670, 'time_iter': 0.06119, 'accuracy': 0.76784, 'f1': 0.76773, 'accuracy-SBM': 0.76753, 'auc': 0.95893}
2025-07-11 10:32:24,705 - INFO - test: {'epoch': 39, 'time_epoch': 3.79776, 'loss': 0.63990095, 'lr': 0, 'params': 463670, 'time_iter': 0.06028, 'accuracy': 0.76897, 'f1': 0.769, 'accuracy-SBM': 0.76891, 'auc': 0.95924}
2025-07-11 10:32:24,707 - INFO - > Epoch 39: took 89.5s (avg 93.3s) | Best so far: epoch 38	train_loss: 0.6422 train_accuracy-SBM: 0.7673	val_loss: 0.6486 val_accuracy-SBM: 0.7692	test_loss: 0.6406 test_accuracy-SBM: 0.7676
2025-07-11 10:33:46,606 - INFO - train: {'epoch': 40, 'time_epoch': 81.67202, 'eta': 5011.69753, 'eta_hours': 1.39214, 'loss': 0.63734307, 'lr': 0.00070085, 'params': 463670, 'time_iter': 0.13068, 'accuracy': 0.76922, 'f1': 0.76922, 'accuracy-SBM': 0.76922, 'auc': 0.95932}
2025-07-11 10:33:50,474 - INFO - val: {'epoch': 40, 'time_epoch': 3.81454, 'loss': 0.63971117, 'lr': 0, 'params': 463670, 'time_iter': 0.06055, 'accuracy': 0.77104, 'f1': 0.77095, 'accuracy-SBM': 0.77094, 'auc': 0.95935}
2025-07-11 10:33:54,300 - INFO - test: {'epoch': 40, 'time_epoch': 3.793, 'loss': 0.6418683, 'lr': 0, 'params': 463670, 'time_iter': 0.06021, 'accuracy': 0.76984, 'f1': 0.76983, 'accuracy-SBM': 0.76989, 'auc': 0.95907}
2025-07-11 10:33:54,303 - INFO - > Epoch 40: took 89.6s (avg 93.2s) | Best so far: epoch 40	train_loss: 0.6373 train_accuracy-SBM: 0.7692	val_loss: 0.6397 val_accuracy-SBM: 0.7709	test_loss: 0.6419 test_accuracy-SBM: 0.7699
2025-07-11 10:35:16,508 - INFO - train: {'epoch': 41, 'time_epoch': 81.98215, 'eta': 4922.6633, 'eta_hours': 1.36741, 'loss': 0.63656833, 'lr': 0.0006856, 'params': 463670, 'time_iter': 0.13117, 'accuracy': 0.76869, 'f1': 0.76869, 'accuracy-SBM': 0.76869, 'auc': 0.95943}
2025-07-11 10:35:20,418 - INFO - val: {'epoch': 41, 'time_epoch': 3.86817, 'loss': 0.63852734, 'lr': 0, 'params': 463670, 'time_iter': 0.0614, 'accuracy': 0.76966, 'f1': 0.76958, 'accuracy-SBM': 0.76952, 'auc': 0.95921}
2025-07-11 10:35:24,268 - INFO - test: {'epoch': 41, 'time_epoch': 3.81908, 'loss': 0.63741849, 'lr': 0, 'params': 463670, 'time_iter': 0.06062, 'accuracy': 0.77105, 'f1': 0.77105, 'accuracy-SBM': 0.77103, 'auc': 0.95943}
2025-07-11 10:35:24,271 - INFO - > Epoch 41: took 90.0s (avg 93.1s) | Best so far: epoch 40	train_loss: 0.6373 train_accuracy-SBM: 0.7692	val_loss: 0.6397 val_accuracy-SBM: 0.7709	test_loss: 0.6419 test_accuracy-SBM: 0.7699
2025-07-11 10:36:48,669 - INFO - train: {'epoch': 42, 'time_epoch': 84.1687, 'eta': 4836.85551, 'eta_hours': 1.34357, 'loss': 0.63174263, 'lr': 0.00067015, 'params': 463670, 'time_iter': 0.13467, 'accuracy': 0.77075, 'f1': 0.77075, 'accuracy-SBM': 0.77075, 'auc': 0.96004}
2025-07-11 10:36:52,719 - INFO - val: {'epoch': 42, 'time_epoch': 4.00515, 'loss': 0.64748796, 'lr': 0, 'params': 463670, 'time_iter': 0.06357, 'accuracy': 0.76997, 'f1': 0.76995, 'accuracy-SBM': 0.76989, 'auc': 0.95804}
2025-07-11 10:36:56,773 - INFO - test: {'epoch': 42, 'time_epoch': 4.01263, 'loss': 0.64046853, 'lr': 0, 'params': 463670, 'time_iter': 0.06369, 'accuracy': 0.76912, 'f1': 0.7691, 'accuracy-SBM': 0.76919, 'auc': 0.95915}
2025-07-11 10:36:56,775 - INFO - > Epoch 42: took 92.5s (avg 93.1s) | Best so far: epoch 40	train_loss: 0.6373 train_accuracy-SBM: 0.7692	val_loss: 0.6397 val_accuracy-SBM: 0.7709	test_loss: 0.6419 test_accuracy-SBM: 0.7699
2025-07-11 10:38:24,357 - INFO - train: {'epoch': 43, 'time_epoch': 87.35007, 'eta': 4755.17125, 'eta_hours': 1.32088, 'loss': 0.62925668, 'lr': 0.00065451, 'params': 463670, 'time_iter': 0.13976, 'accuracy': 0.7714, 'f1': 0.7714, 'accuracy-SBM': 0.7714, 'auc': 0.96037}
2025-07-11 10:38:28,478 - INFO - val: {'epoch': 43, 'time_epoch': 4.07665, 'loss': 0.64466991, 'lr': 0, 'params': 463670, 'time_iter': 0.06471, 'accuracy': 0.76968, 'f1': 0.76961, 'accuracy-SBM': 0.76967, 'auc': 0.95869}
2025-07-11 10:38:32,609 - INFO - test: {'epoch': 43, 'time_epoch': 4.09171, 'loss': 0.64809855, 'lr': 0, 'params': 463670, 'time_iter': 0.06495, 'accuracy': 0.76936, 'f1': 0.76929, 'accuracy-SBM': 0.7694, 'auc': 0.95827}
2025-07-11 10:38:32,611 - INFO - > Epoch 43: took 95.8s (avg 93.2s) | Best so far: epoch 40	train_loss: 0.6373 train_accuracy-SBM: 0.7692	val_loss: 0.6397 val_accuracy-SBM: 0.7709	test_loss: 0.6419 test_accuracy-SBM: 0.7699
2025-07-11 10:40:00,218 - INFO - train: {'epoch': 44, 'time_epoch': 87.37257, 'eta': 4673.26267, 'eta_hours': 1.29813, 'loss': 0.6279303, 'lr': 0.0006387, 'params': 463670, 'time_iter': 0.1398, 'accuracy': 0.77248, 'f1': 0.77248, 'accuracy-SBM': 0.77248, 'auc': 0.96052}
2025-07-11 10:40:04,353 - INFO - val: {'epoch': 44, 'time_epoch': 4.09134, 'loss': 0.6478699, 'lr': 0, 'params': 463670, 'time_iter': 0.06494, 'accuracy': 0.76846, 'f1': 0.76827, 'accuracy-SBM': 0.76832, 'auc': 0.95802}
2025-07-11 10:40:08,458 - INFO - test: {'epoch': 44, 'time_epoch': 4.07191, 'loss': 0.64996752, 'lr': 0, 'params': 463670, 'time_iter': 0.06463, 'accuracy': 0.76791, 'f1': 0.76789, 'accuracy-SBM': 0.76783, 'auc': 0.95778}
2025-07-11 10:40:08,461 - INFO - > Epoch 44: took 95.8s (avg 93.2s) | Best so far: epoch 40	train_loss: 0.6373 train_accuracy-SBM: 0.7692	val_loss: 0.6397 val_accuracy-SBM: 0.7709	test_loss: 0.6419 test_accuracy-SBM: 0.7699
2025-07-11 10:41:35,888 - INFO - train: {'epoch': 45, 'time_epoch': 87.09587, 'eta': 4590.79171, 'eta_hours': 1.27522, 'loss': 0.62207455, 'lr': 0.00062274, 'params': 463670, 'time_iter': 0.13935, 'accuracy': 0.77478, 'f1': 0.77478, 'accuracy-SBM': 0.77478, 'auc': 0.96126}
2025-07-11 10:41:39,945 - INFO - val: {'epoch': 45, 'time_epoch': 4.01376, 'loss': 0.63925463, 'lr': 0, 'params': 463670, 'time_iter': 0.06371, 'accuracy': 0.76976, 'f1': 0.76975, 'accuracy-SBM': 0.76979, 'auc': 0.95916}
2025-07-11 10:41:43,978 - INFO - test: {'epoch': 45, 'time_epoch': 4.00151, 'loss': 0.63874086, 'lr': 0, 'params': 463670, 'time_iter': 0.06352, 'accuracy': 0.7711, 'f1': 0.77106, 'accuracy-SBM': 0.77108, 'auc': 0.95924}
2025-07-11 10:41:43,980 - INFO - > Epoch 45: took 95.5s (avg 93.3s) | Best so far: epoch 40	train_loss: 0.6373 train_accuracy-SBM: 0.7692	val_loss: 0.6397 val_accuracy-SBM: 0.7709	test_loss: 0.6419 test_accuracy-SBM: 0.7699
2025-07-11 10:43:11,434 - INFO - train: {'epoch': 46, 'time_epoch': 87.22727, 'eta': 4508.27213, 'eta_hours': 1.2523, 'loss': 0.62171533, 'lr': 0.00060665, 'params': 463670, 'time_iter': 0.13956, 'accuracy': 0.77404, 'f1': 0.77404, 'accuracy-SBM': 0.77404, 'auc': 0.96131}
2025-07-11 10:43:15,529 - INFO - val: {'epoch': 46, 'time_epoch': 4.05178, 'loss': 0.64262286, 'lr': 0, 'params': 463670, 'time_iter': 0.06431, 'accuracy': 0.76928, 'f1': 0.76928, 'accuracy-SBM': 0.7693, 'auc': 0.9589}
2025-07-11 10:43:19,619 - INFO - test: {'epoch': 46, 'time_epoch': 4.05856, 'loss': 0.64236832, 'lr': 0, 'params': 463670, 'time_iter': 0.06442, 'accuracy': 0.76872, 'f1': 0.76869, 'accuracy-SBM': 0.76876, 'auc': 0.95888}
2025-07-11 10:43:19,621 - INFO - > Epoch 46: took 95.6s (avg 93.3s) | Best so far: epoch 40	train_loss: 0.6373 train_accuracy-SBM: 0.7692	val_loss: 0.6397 val_accuracy-SBM: 0.7709	test_loss: 0.6419 test_accuracy-SBM: 0.7699
2025-07-11 10:44:47,606 - INFO - train: {'epoch': 47, 'time_epoch': 87.65941, 'eta': 4426.02453, 'eta_hours': 1.22945, 'loss': 0.6187208, 'lr': 0.00059044, 'params': 463670, 'time_iter': 0.14026, 'accuracy': 0.77541, 'f1': 0.77541, 'accuracy-SBM': 0.77541, 'auc': 0.96169}
2025-07-11 10:44:51,750 - INFO - val: {'epoch': 47, 'time_epoch': 4.09858, 'loss': 0.62919001, 'lr': 0, 'params': 463670, 'time_iter': 0.06506, 'accuracy': 0.77503, 'f1': 0.77484, 'accuracy-SBM': 0.77478, 'auc': 0.96035}
2025-07-11 10:44:55,871 - INFO - test: {'epoch': 47, 'time_epoch': 4.08599, 'loss': 0.64321459, 'lr': 0, 'params': 463670, 'time_iter': 0.06486, 'accuracy': 0.76918, 'f1': 0.76925, 'accuracy-SBM': 0.76913, 'auc': 0.95867}
2025-07-11 10:44:55,873 - INFO - > Epoch 47: took 96.3s (avg 93.4s) | Best so far: epoch 47	train_loss: 0.6187 train_accuracy-SBM: 0.7754	val_loss: 0.6292 val_accuracy-SBM: 0.7748	test_loss: 0.6432 test_accuracy-SBM: 0.7691
2025-07-11 10:46:23,515 - INFO - train: {'epoch': 48, 'time_epoch': 87.40528, 'eta': 4343.29155, 'eta_hours': 1.20647, 'loss': 0.61552107, 'lr': 0.00057413, 'params': 463670, 'time_iter': 0.13985, 'accuracy': 0.77685, 'f1': 0.77685, 'accuracy-SBM': 0.77685, 'auc': 0.96209}
2025-07-11 10:46:27,648 - INFO - val: {'epoch': 48, 'time_epoch': 4.08882, 'loss': 0.63921723, 'lr': 0, 'params': 463670, 'time_iter': 0.0649, 'accuracy': 0.7724, 'f1': 0.7723, 'accuracy-SBM': 0.77237, 'auc': 0.95929}
2025-07-11 10:46:31,765 - INFO - test: {'epoch': 48, 'time_epoch': 4.08402, 'loss': 0.64478183, 'lr': 0, 'params': 463670, 'time_iter': 0.06483, 'accuracy': 0.76967, 'f1': 0.76966, 'accuracy-SBM': 0.76957, 'auc': 0.95858}
2025-07-11 10:46:31,767 - INFO - > Epoch 48: took 95.9s (avg 93.4s) | Best so far: epoch 47	train_loss: 0.6187 train_accuracy-SBM: 0.7754	val_loss: 0.6292 val_accuracy-SBM: 0.7748	test_loss: 0.6432 test_accuracy-SBM: 0.7691
2025-07-11 10:47:59,542 - INFO - train: {'epoch': 49, 'time_epoch': 87.54573, 'eta': 4260.51212, 'eta_hours': 1.18348, 'loss': 0.61414129, 'lr': 0.00055774, 'params': 463670, 'time_iter': 0.14007, 'accuracy': 0.77787, 'f1': 0.77787, 'accuracy-SBM': 0.77787, 'auc': 0.96222}
2025-07-11 10:48:03,663 - INFO - val: {'epoch': 49, 'time_epoch': 4.07627, 'loss': 0.63186939, 'lr': 0, 'params': 463670, 'time_iter': 0.0647, 'accuracy': 0.77485, 'f1': 0.7748, 'accuracy-SBM': 0.7748, 'auc': 0.96019}
2025-07-11 10:48:07,756 - INFO - test: {'epoch': 49, 'time_epoch': 4.06127, 'loss': 0.63136375, 'lr': 0, 'params': 463670, 'time_iter': 0.06446, 'accuracy': 0.77238, 'f1': 0.77243, 'accuracy-SBM': 0.77243, 'auc': 0.96029}
2025-07-11 10:48:07,758 - INFO - > Epoch 49: took 96.0s (avg 93.5s) | Best so far: epoch 49	train_loss: 0.6141 train_accuracy-SBM: 0.7779	val_loss: 0.6319 val_accuracy-SBM: 0.7748	test_loss: 0.6314 test_accuracy-SBM: 0.7724
2025-07-11 10:49:35,872 - INFO - train: {'epoch': 50, 'time_epoch': 87.87383, 'eta': 4177.86101, 'eta_hours': 1.16052, 'loss': 0.61395635, 'lr': 0.00054129, 'params': 463670, 'time_iter': 0.1406, 'accuracy': 0.77677, 'f1': 0.77677, 'accuracy-SBM': 0.77677, 'auc': 0.96229}
2025-07-11 10:49:40,066 - INFO - val: {'epoch': 50, 'time_epoch': 4.14849, 'loss': 0.6323289, 'lr': 0, 'params': 463670, 'time_iter': 0.06585, 'accuracy': 0.77571, 'f1': 0.77562, 'accuracy-SBM': 0.77559, 'auc': 0.9599}
2025-07-11 10:49:44,225 - INFO - test: {'epoch': 50, 'time_epoch': 4.12713, 'loss': 0.61945965, 'lr': 0, 'params': 463670, 'time_iter': 0.06551, 'accuracy': 0.77632, 'f1': 0.77637, 'accuracy-SBM': 0.77638, 'auc': 0.96164}
2025-07-11 10:49:44,228 - INFO - > Epoch 50: took 96.5s (avg 93.6s) | Best so far: epoch 50	train_loss: 0.6140 train_accuracy-SBM: 0.7768	val_loss: 0.6323 val_accuracy-SBM: 0.7756	test_loss: 0.6195 test_accuracy-SBM: 0.7764
2025-07-11 10:51:11,733 - INFO - train: {'epoch': 51, 'time_epoch': 87.27452, 'eta': 4094.45581, 'eta_hours': 1.13735, 'loss': 0.60558662, 'lr': 0.00052479, 'params': 463670, 'time_iter': 0.13964, 'accuracy': 0.78028, 'f1': 0.78028, 'accuracy-SBM': 0.78028, 'auc': 0.96329}
2025-07-11 10:51:15,799 - INFO - val: {'epoch': 51, 'time_epoch': 4.02177, 'loss': 0.63197437, 'lr': 0, 'params': 463670, 'time_iter': 0.06384, 'accuracy': 0.77344, 'f1': 0.77336, 'accuracy-SBM': 0.77345, 'auc': 0.96026}
2025-07-11 10:51:19,889 - INFO - test: {'epoch': 51, 'time_epoch': 4.05816, 'loss': 0.63484833, 'lr': 0, 'params': 463670, 'time_iter': 0.06442, 'accuracy': 0.77387, 'f1': 0.77386, 'accuracy-SBM': 0.7738, 'auc': 0.95979}
2025-07-11 10:51:19,892 - INFO - > Epoch 51: took 95.7s (avg 93.6s) | Best so far: epoch 50	train_loss: 0.6140 train_accuracy-SBM: 0.7768	val_loss: 0.6323 val_accuracy-SBM: 0.7756	test_loss: 0.6195 test_accuracy-SBM: 0.7764
2025-07-11 10:52:46,931 - INFO - train: {'epoch': 52, 'time_epoch': 86.806, 'eta': 4010.48913, 'eta_hours': 1.11402, 'loss': 0.6051263, 'lr': 0.00050827, 'params': 463670, 'time_iter': 0.13889, 'accuracy': 0.78068, 'f1': 0.78068, 'accuracy-SBM': 0.78068, 'auc': 0.96334}
2025-07-11 10:52:50,880 - INFO - val: {'epoch': 52, 'time_epoch': 3.90278, 'loss': 0.63552155, 'lr': 0, 'params': 463670, 'time_iter': 0.06195, 'accuracy': 0.77484, 'f1': 0.77463, 'accuracy-SBM': 0.77461, 'auc': 0.95959}
2025-07-11 10:52:54,773 - INFO - test: {'epoch': 52, 'time_epoch': 3.8608, 'loss': 0.62320919, 'lr': 0, 'params': 463670, 'time_iter': 0.06128, 'accuracy': 0.77713, 'f1': 0.77708, 'accuracy-SBM': 0.77705, 'auc': 0.96113}
2025-07-11 10:52:54,775 - INFO - > Epoch 52: took 94.9s (avg 93.6s) | Best so far: epoch 50	train_loss: 0.6140 train_accuracy-SBM: 0.7768	val_loss: 0.6323 val_accuracy-SBM: 0.7756	test_loss: 0.6195 test_accuracy-SBM: 0.7764
2025-07-11 10:54:19,661 - INFO - train: {'epoch': 53, 'time_epoch': 84.66129, 'eta': 3924.59031, 'eta_hours': 1.09016, 'loss': 0.60239239, 'lr': 0.00049173, 'params': 463670, 'time_iter': 0.13546, 'accuracy': 0.78109, 'f1': 0.78109, 'accuracy-SBM': 0.78109, 'auc': 0.96367}
2025-07-11 10:54:23,570 - INFO - val: {'epoch': 53, 'time_epoch': 3.86576, 'loss': 0.63446235, 'lr': 0, 'params': 463670, 'time_iter': 0.06136, 'accuracy': 0.77347, 'f1': 0.77339, 'accuracy-SBM': 0.77329, 'auc': 0.95969}
2025-07-11 10:54:27,441 - INFO - test: {'epoch': 53, 'time_epoch': 3.83737, 'loss': 0.62683139, 'lr': 0, 'params': 463670, 'time_iter': 0.06091, 'accuracy': 0.77433, 'f1': 0.77432, 'accuracy-SBM': 0.77438, 'auc': 0.96071}
2025-07-11 10:54:27,443 - INFO - > Epoch 53: took 92.7s (avg 93.6s) | Best so far: epoch 50	train_loss: 0.6140 train_accuracy-SBM: 0.7768	val_loss: 0.6323 val_accuracy-SBM: 0.7756	test_loss: 0.6195 test_accuracy-SBM: 0.7764
2025-07-11 10:55:52,665 - INFO - train: {'epoch': 54, 'time_epoch': 84.99362, 'eta': 3839.00839, 'eta_hours': 1.06639, 'loss': 0.60183993, 'lr': 0.00047521, 'params': 463670, 'time_iter': 0.13599, 'accuracy': 0.78145, 'f1': 0.78144, 'accuracy-SBM': 0.78145, 'auc': 0.96376}
2025-07-11 10:55:56,618 - INFO - val: {'epoch': 54, 'time_epoch': 3.90894, 'loss': 0.63685538, 'lr': 0, 'params': 463670, 'time_iter': 0.06205, 'accuracy': 0.77439, 'f1': 0.7743, 'accuracy-SBM': 0.77421, 'auc': 0.95948}
2025-07-11 10:56:00,562 - INFO - test: {'epoch': 54, 'time_epoch': 3.91276, 'loss': 0.62968881, 'lr': 0, 'params': 463670, 'time_iter': 0.06211, 'accuracy': 0.77491, 'f1': 0.77485, 'accuracy-SBM': 0.77489, 'auc': 0.96034}
2025-07-11 10:56:00,564 - INFO - > Epoch 54: took 93.1s (avg 93.6s) | Best so far: epoch 50	train_loss: 0.6140 train_accuracy-SBM: 0.7768	val_loss: 0.6323 val_accuracy-SBM: 0.7756	test_loss: 0.6195 test_accuracy-SBM: 0.7764
2025-07-11 10:57:25,902 - INFO - train: {'epoch': 55, 'time_epoch': 85.00358, 'eta': 3753.45532, 'eta_hours': 1.04263, 'loss': 0.59856351, 'lr': 0.00045871, 'params': 463670, 'time_iter': 0.13601, 'accuracy': 0.78264, 'f1': 0.78264, 'accuracy-SBM': 0.78264, 'auc': 0.96414}
2025-07-11 10:57:29,850 - INFO - val: {'epoch': 55, 'time_epoch': 3.90374, 'loss': 0.63571486, 'lr': 0, 'params': 463670, 'time_iter': 0.06196, 'accuracy': 0.77358, 'f1': 0.77348, 'accuracy-SBM': 0.77336, 'auc': 0.95976}
2025-07-11 10:57:33,777 - INFO - test: {'epoch': 55, 'time_epoch': 3.89534, 'loss': 0.63740897, 'lr': 0, 'params': 463670, 'time_iter': 0.06183, 'accuracy': 0.7737, 'f1': 0.77365, 'accuracy-SBM': 0.77372, 'auc': 0.95955}
2025-07-11 10:57:33,779 - INFO - > Epoch 55: took 93.2s (avg 93.6s) | Best so far: epoch 50	train_loss: 0.6140 train_accuracy-SBM: 0.7768	val_loss: 0.6323 val_accuracy-SBM: 0.7756	test_loss: 0.6195 test_accuracy-SBM: 0.7764
2025-07-11 10:58:58,811 - INFO - train: {'epoch': 56, 'time_epoch': 84.79881, 'eta': 3667.76705, 'eta_hours': 1.01882, 'loss': 0.59526768, 'lr': 0.00044226, 'params': 463670, 'time_iter': 0.13568, 'accuracy': 0.7833, 'f1': 0.7833, 'accuracy-SBM': 0.7833, 'auc': 0.96455}
2025-07-11 10:59:02,775 - INFO - val: {'epoch': 56, 'time_epoch': 3.9211, 'loss': 0.63449065, 'lr': 0, 'params': 463670, 'time_iter': 0.06224, 'accuracy': 0.77533, 'f1': 0.77525, 'accuracy-SBM': 0.77531, 'auc': 0.95983}
2025-07-11 10:59:06,684 - INFO - test: {'epoch': 56, 'time_epoch': 3.87743, 'loss': 0.6312161, 'lr': 0, 'params': 463670, 'time_iter': 0.06155, 'accuracy': 0.77436, 'f1': 0.77436, 'accuracy-SBM': 0.77439, 'auc': 0.96031}
2025-07-11 10:59:06,686 - INFO - > Epoch 56: took 92.9s (avg 93.6s) | Best so far: epoch 50	train_loss: 0.6140 train_accuracy-SBM: 0.7768	val_loss: 0.6323 val_accuracy-SBM: 0.7756	test_loss: 0.6195 test_accuracy-SBM: 0.7764
2025-07-11 11:00:34,620 - INFO - train: {'epoch': 57, 'time_epoch': 87.60493, 'eta': 3584.14146, 'eta_hours': 0.99559, 'loss': 0.59429821, 'lr': 0.00042587, 'params': 463670, 'time_iter': 0.14017, 'accuracy': 0.78436, 'f1': 0.78436, 'accuracy-SBM': 0.78436, 'auc': 0.96464}
2025-07-11 11:00:38,791 - INFO - val: {'epoch': 57, 'time_epoch': 4.11918, 'loss': 0.63558037, 'lr': 0, 'params': 463670, 'time_iter': 0.06538, 'accuracy': 0.77326, 'f1': 0.77319, 'accuracy-SBM': 0.77322, 'auc': 0.95958}
2025-07-11 11:00:42,930 - INFO - test: {'epoch': 57, 'time_epoch': 4.10707, 'loss': 0.62089729, 'lr': 0, 'params': 463670, 'time_iter': 0.06519, 'accuracy': 0.7768, 'f1': 0.77678, 'accuracy-SBM': 0.77679, 'auc': 0.96142}
2025-07-11 11:00:42,932 - INFO - > Epoch 57: took 96.2s (avg 93.6s) | Best so far: epoch 50	train_loss: 0.6140 train_accuracy-SBM: 0.7768	val_loss: 0.6323 val_accuracy-SBM: 0.7756	test_loss: 0.6195 test_accuracy-SBM: 0.7764
2025-07-11 11:02:10,990 - INFO - train: {'epoch': 58, 'time_epoch': 87.8287, 'eta': 3500.53649, 'eta_hours': 0.97237, 'loss': 0.59078864, 'lr': 0.00040956, 'params': 463670, 'time_iter': 0.14053, 'accuracy': 0.78547, 'f1': 0.78547, 'accuracy-SBM': 0.78547, 'auc': 0.96507}
2025-07-11 11:02:15,129 - INFO - val: {'epoch': 58, 'time_epoch': 4.08839, 'loss': 0.63073886, 'lr': 0, 'params': 463670, 'time_iter': 0.0649, 'accuracy': 0.77641, 'f1': 0.77638, 'accuracy-SBM': 0.7763, 'auc': 0.9603}
2025-07-11 11:02:19,246 - INFO - test: {'epoch': 58, 'time_epoch': 4.08503, 'loss': 0.62710589, 'lr': 0, 'params': 463670, 'time_iter': 0.06484, 'accuracy': 0.77679, 'f1': 0.77677, 'accuracy-SBM': 0.77687, 'auc': 0.96083}
2025-07-11 11:02:19,248 - INFO - > Epoch 58: took 96.3s (avg 93.7s) | Best so far: epoch 58	train_loss: 0.5908 train_accuracy-SBM: 0.7855	val_loss: 0.6307 val_accuracy-SBM: 0.7763	test_loss: 0.6271 test_accuracy-SBM: 0.7769
2025-07-11 11:03:46,522 - INFO - train: {'epoch': 59, 'time_epoch': 87.04294, 'eta': 3416.26688, 'eta_hours': 0.94896, 'loss': 0.58967633, 'lr': 0.00039335, 'params': 463670, 'time_iter': 0.13927, 'accuracy': 0.78518, 'f1': 0.78518, 'accuracy-SBM': 0.78518, 'auc': 0.96522}
2025-07-11 11:03:50,639 - INFO - val: {'epoch': 59, 'time_epoch': 4.07198, 'loss': 0.63549706, 'lr': 0, 'params': 463670, 'time_iter': 0.06463, 'accuracy': 0.77636, 'f1': 0.77625, 'accuracy-SBM': 0.77623, 'auc': 0.95986}
2025-07-11 11:03:54,728 - INFO - test: {'epoch': 59, 'time_epoch': 4.05789, 'loss': 0.62733219, 'lr': 0, 'params': 463670, 'time_iter': 0.06441, 'accuracy': 0.77595, 'f1': 0.77596, 'accuracy-SBM': 0.77596, 'auc': 0.96083}
2025-07-11 11:03:54,730 - INFO - > Epoch 59: took 95.5s (avg 93.7s) | Best so far: epoch 58	train_loss: 0.5908 train_accuracy-SBM: 0.7855	val_loss: 0.6307 val_accuracy-SBM: 0.7763	test_loss: 0.6271 test_accuracy-SBM: 0.7769
2025-07-11 11:05:22,279 - INFO - train: {'epoch': 60, 'time_epoch': 87.31565, 'eta': 3332.08071, 'eta_hours': 0.92558, 'loss': 0.58607348, 'lr': 0.00037726, 'params': 463670, 'time_iter': 0.13971, 'accuracy': 0.78727, 'f1': 0.78728, 'accuracy-SBM': 0.78728, 'auc': 0.96563}
2025-07-11 11:05:26,392 - INFO - val: {'epoch': 60, 'time_epoch': 4.06875, 'loss': 0.63662361, 'lr': 0, 'params': 463670, 'time_iter': 0.06458, 'accuracy': 0.77459, 'f1': 0.77446, 'accuracy-SBM': 0.77438, 'auc': 0.95954}
2025-07-11 11:05:30,488 - INFO - test: {'epoch': 60, 'time_epoch': 4.06343, 'loss': 0.6348457, 'lr': 0, 'params': 463670, 'time_iter': 0.0645, 'accuracy': 0.77343, 'f1': 0.77347, 'accuracy-SBM': 0.77342, 'auc': 0.95987}
2025-07-11 11:05:30,490 - INFO - > Epoch 60: took 95.8s (avg 93.7s) | Best so far: epoch 58	train_loss: 0.5908 train_accuracy-SBM: 0.7855	val_loss: 0.6307 val_accuracy-SBM: 0.7763	test_loss: 0.6271 test_accuracy-SBM: 0.7769
2025-07-11 11:06:58,249 - INFO - train: {'epoch': 61, 'time_epoch': 87.52841, 'eta': 3247.92398, 'eta_hours': 0.9022, 'loss': 0.58316643, 'lr': 0.0003613, 'params': 463670, 'time_iter': 0.14005, 'accuracy': 0.78835, 'f1': 0.78835, 'accuracy-SBM': 0.78835, 'auc': 0.96597}
2025-07-11 11:07:02,441 - INFO - val: {'epoch': 61, 'time_epoch': 4.14553, 'loss': 0.63646496, 'lr': 0, 'params': 463670, 'time_iter': 0.0658, 'accuracy': 0.77503, 'f1': 0.77488, 'accuracy-SBM': 0.77487, 'auc': 0.95979}
2025-07-11 11:07:06,510 - INFO - test: {'epoch': 61, 'time_epoch': 4.03508, 'loss': 0.62494986, 'lr': 0, 'params': 463670, 'time_iter': 0.06405, 'accuracy': 0.77784, 'f1': 0.77786, 'accuracy-SBM': 0.77784, 'auc': 0.96113}
2025-07-11 11:07:06,512 - INFO - > Epoch 61: took 96.0s (avg 93.8s) | Best so far: epoch 58	train_loss: 0.5908 train_accuracy-SBM: 0.7855	val_loss: 0.6307 val_accuracy-SBM: 0.7763	test_loss: 0.6271 test_accuracy-SBM: 0.7769
2025-07-11 11:08:34,685 - INFO - train: {'epoch': 62, 'time_epoch': 87.94312, 'eta': 3163.90378, 'eta_hours': 0.87886, 'loss': 0.57924417, 'lr': 0.00034549, 'params': 463670, 'time_iter': 0.14071, 'accuracy': 0.78954, 'f1': 0.78954, 'accuracy-SBM': 0.78954, 'auc': 0.96643}
2025-07-11 11:08:38,755 - INFO - val: {'epoch': 62, 'time_epoch': 4.02521, 'loss': 0.62881007, 'lr': 0, 'params': 463670, 'time_iter': 0.06389, 'accuracy': 0.77913, 'f1': 0.77899, 'accuracy-SBM': 0.77897, 'auc': 0.96037}
2025-07-11 11:08:42,825 - INFO - test: {'epoch': 62, 'time_epoch': 4.03547, 'loss': 0.62060716, 'lr': 0, 'params': 463670, 'time_iter': 0.06406, 'accuracy': 0.77775, 'f1': 0.77771, 'accuracy-SBM': 0.7777, 'auc': 0.96138}
2025-07-11 11:08:42,827 - INFO - > Epoch 62: took 96.3s (avg 93.8s) | Best so far: epoch 62	train_loss: 0.5792 train_accuracy-SBM: 0.7895	val_loss: 0.6288 val_accuracy-SBM: 0.7790	test_loss: 0.6206 test_accuracy-SBM: 0.7777
2025-07-11 11:10:10,509 - INFO - train: {'epoch': 63, 'time_epoch': 87.45453, 'eta': 3079.48615, 'eta_hours': 0.85541, 'loss': 0.57834073, 'lr': 0.00032985, 'params': 463670, 'time_iter': 0.13993, 'accuracy': 0.78925, 'f1': 0.78925, 'accuracy-SBM': 0.78925, 'auc': 0.96654}
2025-07-11 11:10:14,648 - INFO - val: {'epoch': 63, 'time_epoch': 4.09248, 'loss': 0.63246629, 'lr': 0, 'params': 463670, 'time_iter': 0.06496, 'accuracy': 0.77751, 'f1': 0.77741, 'accuracy-SBM': 0.77745, 'auc': 0.96031}
2025-07-11 11:10:18,769 - INFO - test: {'epoch': 63, 'time_epoch': 4.08028, 'loss': 0.62773327, 'lr': 0, 'params': 463670, 'time_iter': 0.06477, 'accuracy': 0.77804, 'f1': 0.77803, 'accuracy-SBM': 0.77806, 'auc': 0.96086}
2025-07-11 11:10:18,771 - INFO - > Epoch 63: took 95.9s (avg 93.8s) | Best so far: epoch 62	train_loss: 0.5792 train_accuracy-SBM: 0.7895	val_loss: 0.6288 val_accuracy-SBM: 0.7790	test_loss: 0.6206 test_accuracy-SBM: 0.7777
2025-07-11 11:11:45,464 - INFO - train: {'epoch': 64, 'time_epoch': 86.46869, 'eta': 2994.44424, 'eta_hours': 0.83179, 'loss': 0.57427144, 'lr': 0.0003144, 'params': 463670, 'time_iter': 0.13835, 'accuracy': 0.7911, 'f1': 0.7911, 'accuracy-SBM': 0.7911, 'auc': 0.96701}
2025-07-11 11:11:49,350 - INFO - val: {'epoch': 64, 'time_epoch': 3.84227, 'loss': 0.62231068, 'lr': 0, 'params': 463670, 'time_iter': 0.06099, 'accuracy': 0.78036, 'f1': 0.78026, 'accuracy-SBM': 0.78027, 'auc': 0.96114}
2025-07-11 11:11:53,205 - INFO - test: {'epoch': 64, 'time_epoch': 3.82345, 'loss': 0.61920385, 'lr': 0, 'params': 463670, 'time_iter': 0.06069, 'accuracy': 0.77862, 'f1': 0.77863, 'accuracy-SBM': 0.77863, 'auc': 0.96163}
2025-07-11 11:11:53,207 - INFO - > Epoch 64: took 94.4s (avg 93.8s) | Best so far: epoch 64	train_loss: 0.5743 train_accuracy-SBM: 0.7911	val_loss: 0.6223 val_accuracy-SBM: 0.7803	test_loss: 0.6192 test_accuracy-SBM: 0.7786
2025-07-11 11:13:17,290 - INFO - train: {'epoch': 65, 'time_epoch': 83.85816, 'eta': 2908.01428, 'eta_hours': 0.80778, 'loss': 0.57464996, 'lr': 0.00029915, 'params': 463670, 'time_iter': 0.13417, 'accuracy': 0.79122, 'f1': 0.79122, 'accuracy-SBM': 0.79122, 'auc': 0.96695}
2025-07-11 11:13:21,199 - INFO - val: {'epoch': 65, 'time_epoch': 3.86517, 'loss': 0.62960735, 'lr': 0, 'params': 463670, 'time_iter': 0.06135, 'accuracy': 0.7787, 'f1': 0.77864, 'accuracy-SBM': 0.77868, 'auc': 0.9603}
2025-07-11 11:13:25,093 - INFO - test: {'epoch': 65, 'time_epoch': 3.86275, 'loss': 0.62466167, 'lr': 0, 'params': 463670, 'time_iter': 0.06131, 'accuracy': 0.77865, 'f1': 0.77864, 'accuracy-SBM': 0.77869, 'auc': 0.96096}
2025-07-11 11:13:25,095 - INFO - > Epoch 65: took 91.9s (avg 93.8s) | Best so far: epoch 64	train_loss: 0.5743 train_accuracy-SBM: 0.7911	val_loss: 0.6223 val_accuracy-SBM: 0.7803	test_loss: 0.6192 test_accuracy-SBM: 0.7786
2025-07-11 11:14:52,387 - INFO - train: {'epoch': 66, 'time_epoch': 87.06512, 'eta': 2823.24063, 'eta_hours': 0.78423, 'loss': 0.57264784, 'lr': 0.00028412, 'params': 463670, 'time_iter': 0.1393, 'accuracy': 0.79156, 'f1': 0.79156, 'accuracy-SBM': 0.79156, 'auc': 0.9672}
2025-07-11 11:14:56,512 - INFO - val: {'epoch': 66, 'time_epoch': 4.07922, 'loss': 0.62851325, 'lr': 0, 'params': 463670, 'time_iter': 0.06475, 'accuracy': 0.77988, 'f1': 0.77974, 'accuracy-SBM': 0.77969, 'auc': 0.9606}
2025-07-11 11:15:00,637 - INFO - test: {'epoch': 66, 'time_epoch': 4.09152, 'loss': 0.62811384, 'lr': 0, 'params': 463670, 'time_iter': 0.06494, 'accuracy': 0.77857, 'f1': 0.77852, 'accuracy-SBM': 0.77852, 'auc': 0.96068}
2025-07-11 11:15:00,639 - INFO - > Epoch 66: took 95.5s (avg 93.8s) | Best so far: epoch 64	train_loss: 0.5743 train_accuracy-SBM: 0.7911	val_loss: 0.6223 val_accuracy-SBM: 0.7803	test_loss: 0.6192 test_accuracy-SBM: 0.7786
2025-07-11 11:16:27,994 - INFO - train: {'epoch': 67, 'time_epoch': 87.12808, 'eta': 2738.42922, 'eta_hours': 0.76067, 'loss': 0.56965292, 'lr': 0.00026933, 'params': 463670, 'time_iter': 0.1394, 'accuracy': 0.79297, 'f1': 0.79297, 'accuracy-SBM': 0.79297, 'auc': 0.96752}
2025-07-11 11:16:32,123 - INFO - val: {'epoch': 67, 'time_epoch': 4.08449, 'loss': 0.62257748, 'lr': 0, 'params': 463670, 'time_iter': 0.06483, 'accuracy': 0.77972, 'f1': 0.77965, 'accuracy-SBM': 0.77964, 'auc': 0.96123}
2025-07-11 11:16:36,230 - INFO - test: {'epoch': 67, 'time_epoch': 4.06491, 'loss': 0.62247585, 'lr': 0, 'params': 463670, 'time_iter': 0.06452, 'accuracy': 0.77887, 'f1': 0.77886, 'accuracy-SBM': 0.7789, 'auc': 0.96127}
2025-07-11 11:16:36,254 - INFO - > Epoch 67: took 95.6s (avg 93.9s) | Best so far: epoch 64	train_loss: 0.5743 train_accuracy-SBM: 0.7911	val_loss: 0.6223 val_accuracy-SBM: 0.7803	test_loss: 0.6192 test_accuracy-SBM: 0.7786
2025-07-11 11:18:03,973 - INFO - train: {'epoch': 68, 'time_epoch': 87.4796, 'eta': 2653.70859, 'eta_hours': 0.73714, 'loss': 0.56745723, 'lr': 0.00025479, 'params': 463670, 'time_iter': 0.13997, 'accuracy': 0.79336, 'f1': 0.79336, 'accuracy-SBM': 0.79336, 'auc': 0.96778}
2025-07-11 11:18:08,092 - INFO - val: {'epoch': 68, 'time_epoch': 4.07527, 'loss': 0.63234615, 'lr': 0, 'params': 463670, 'time_iter': 0.06469, 'accuracy': 0.77865, 'f1': 0.77857, 'accuracy-SBM': 0.77849, 'auc': 0.96051}
2025-07-11 11:18:12,191 - INFO - test: {'epoch': 68, 'time_epoch': 4.06644, 'loss': 0.63043487, 'lr': 0, 'params': 463670, 'time_iter': 0.06455, 'accuracy': 0.77718, 'f1': 0.77717, 'accuracy-SBM': 0.77723, 'auc': 0.96073}
2025-07-11 11:18:12,193 - INFO - > Epoch 68: took 95.9s (avg 93.9s) | Best so far: epoch 64	train_loss: 0.5743 train_accuracy-SBM: 0.7911	val_loss: 0.6223 val_accuracy-SBM: 0.7803	test_loss: 0.6192 test_accuracy-SBM: 0.7786
2025-07-11 11:19:39,798 - INFO - train: {'epoch': 69, 'time_epoch': 87.37799, 'eta': 2568.86558, 'eta_hours': 0.71357, 'loss': 0.56597806, 'lr': 0.00024052, 'params': 463670, 'time_iter': 0.1398, 'accuracy': 0.79388, 'f1': 0.79388, 'accuracy-SBM': 0.79388, 'auc': 0.96795}
2025-07-11 11:19:43,923 - INFO - val: {'epoch': 69, 'time_epoch': 4.08065, 'loss': 0.63106625, 'lr': 0, 'params': 463670, 'time_iter': 0.06477, 'accuracy': 0.77821, 'f1': 0.77807, 'accuracy-SBM': 0.77805, 'auc': 0.96042}
2025-07-11 11:19:48,021 - INFO - test: {'epoch': 69, 'time_epoch': 4.06584, 'loss': 0.61605146, 'lr': 0, 'params': 463670, 'time_iter': 0.06454, 'accuracy': 0.78095, 'f1': 0.78094, 'accuracy-SBM': 0.78097, 'auc': 0.96221}
2025-07-11 11:19:48,023 - INFO - > Epoch 69: took 95.8s (avg 93.9s) | Best so far: epoch 64	train_loss: 0.5743 train_accuracy-SBM: 0.7911	val_loss: 0.6223 val_accuracy-SBM: 0.7803	test_loss: 0.6192 test_accuracy-SBM: 0.7786
2025-07-11 11:21:15,377 - INFO - train: {'epoch': 70, 'time_epoch': 87.02924, 'eta': 2483.80871, 'eta_hours': 0.68995, 'loss': 0.56298658, 'lr': 0.00022653, 'params': 463670, 'time_iter': 0.13925, 'accuracy': 0.79571, 'f1': 0.79571, 'accuracy-SBM': 0.79571, 'auc': 0.96828}
2025-07-11 11:21:19,547 - INFO - val: {'epoch': 70, 'time_epoch': 4.1126, 'loss': 0.6285291, 'lr': 0, 'params': 463670, 'time_iter': 0.06528, 'accuracy': 0.77968, 'f1': 0.77957, 'accuracy-SBM': 0.77953, 'auc': 0.96059}
2025-07-11 11:21:23,633 - INFO - test: {'epoch': 70, 'time_epoch': 4.05434, 'loss': 0.61946216, 'lr': 0, 'params': 463670, 'time_iter': 0.06435, 'accuracy': 0.77946, 'f1': 0.77942, 'accuracy-SBM': 0.77943, 'auc': 0.96169}
2025-07-11 11:21:23,635 - INFO - > Epoch 70: took 95.6s (avg 94.0s) | Best so far: epoch 64	train_loss: 0.5743 train_accuracy-SBM: 0.7911	val_loss: 0.6223 val_accuracy-SBM: 0.7803	test_loss: 0.6192 test_accuracy-SBM: 0.7786
2025-07-11 11:22:51,823 - INFO - train: {'epoch': 71, 'time_epoch': 87.95987, 'eta': 2399.05897, 'eta_hours': 0.66641, 'loss': 0.55983717, 'lr': 0.00021284, 'params': 463670, 'time_iter': 0.14074, 'accuracy': 0.79682, 'f1': 0.79682, 'accuracy-SBM': 0.79682, 'auc': 0.96863}
2025-07-11 11:22:55,956 - INFO - val: {'epoch': 71, 'time_epoch': 4.08898, 'loss': 0.62615443, 'lr': 0, 'params': 463670, 'time_iter': 0.0649, 'accuracy': 0.7811, 'f1': 0.78097, 'accuracy-SBM': 0.78093, 'auc': 0.96103}
2025-07-11 11:23:00,078 - INFO - test: {'epoch': 71, 'time_epoch': 4.08697, 'loss': 0.6237607, 'lr': 0, 'params': 463670, 'time_iter': 0.06487, 'accuracy': 0.78009, 'f1': 0.78007, 'accuracy-SBM': 0.78009, 'auc': 0.96129}
2025-07-11 11:23:00,080 - INFO - > Epoch 71: took 96.4s (avg 94.0s) | Best so far: epoch 71	train_loss: 0.5598 train_accuracy-SBM: 0.7968	val_loss: 0.6262 val_accuracy-SBM: 0.7809	test_loss: 0.6238 test_accuracy-SBM: 0.7801
2025-07-11 11:24:27,767 - INFO - train: {'epoch': 72, 'time_epoch': 87.46118, 'eta': 2314.03684, 'eta_hours': 0.64279, 'loss': 0.55820956, 'lr': 0.00019946, 'params': 463670, 'time_iter': 0.13994, 'accuracy': 0.79698, 'f1': 0.79698, 'accuracy-SBM': 0.79699, 'auc': 0.96883}
2025-07-11 11:24:31,896 - INFO - val: {'epoch': 72, 'time_epoch': 4.08396, 'loss': 0.63048935, 'lr': 0, 'params': 463670, 'time_iter': 0.06482, 'accuracy': 0.77902, 'f1': 0.77896, 'accuracy-SBM': 0.77894, 'auc': 0.96066}
2025-07-11 11:24:35,986 - INFO - test: {'epoch': 72, 'time_epoch': 4.05709, 'loss': 0.62787377, 'lr': 0, 'params': 463670, 'time_iter': 0.0644, 'accuracy': 0.77908, 'f1': 0.77911, 'accuracy-SBM': 0.77915, 'auc': 0.96098}
2025-07-11 11:24:35,988 - INFO - > Epoch 72: took 95.9s (avg 94.0s) | Best so far: epoch 71	train_loss: 0.5598 train_accuracy-SBM: 0.7968	val_loss: 0.6262 val_accuracy-SBM: 0.7809	test_loss: 0.6238 test_accuracy-SBM: 0.7801
2025-07-11 11:26:03,369 - INFO - train: {'epoch': 73, 'time_epoch': 87.15749, 'eta': 2228.84208, 'eta_hours': 0.61912, 'loss': 0.55571098, 'lr': 0.00018641, 'params': 463670, 'time_iter': 0.13945, 'accuracy': 0.79811, 'f1': 0.79811, 'accuracy-SBM': 0.79811, 'auc': 0.9691}
2025-07-11 11:26:07,474 - INFO - val: {'epoch': 73, 'time_epoch': 4.06118, 'loss': 0.62221999, 'lr': 0, 'params': 463670, 'time_iter': 0.06446, 'accuracy': 0.77905, 'f1': 0.77893, 'accuracy-SBM': 0.77892, 'auc': 0.96134}
2025-07-11 11:26:11,543 - INFO - test: {'epoch': 73, 'time_epoch': 4.02753, 'loss': 0.62532564, 'lr': 0, 'params': 463670, 'time_iter': 0.06393, 'accuracy': 0.77937, 'f1': 0.7794, 'accuracy-SBM': 0.77939, 'auc': 0.96097}
2025-07-11 11:26:11,545 - INFO - > Epoch 73: took 95.6s (avg 94.0s) | Best so far: epoch 71	train_loss: 0.5598 train_accuracy-SBM: 0.7968	val_loss: 0.6262 val_accuracy-SBM: 0.7809	test_loss: 0.6238 test_accuracy-SBM: 0.7801
2025-07-11 11:27:39,085 - INFO - train: {'epoch': 74, 'time_epoch': 87.30929, 'eta': 2143.64558, 'eta_hours': 0.59546, 'loss': 0.55559821, 'lr': 0.00017371, 'params': 463670, 'time_iter': 0.13969, 'accuracy': 0.79776, 'f1': 0.79776, 'accuracy-SBM': 0.79777, 'auc': 0.96912}
2025-07-11 11:27:43,199 - INFO - val: {'epoch': 74, 'time_epoch': 4.06999, 'loss': 0.62787007, 'lr': 0, 'params': 463670, 'time_iter': 0.0646, 'accuracy': 0.78178, 'f1': 0.78164, 'accuracy-SBM': 0.78159, 'auc': 0.96085}
2025-07-11 11:27:47,295 - INFO - test: {'epoch': 74, 'time_epoch': 4.06394, 'loss': 0.62086301, 'lr': 0, 'params': 463670, 'time_iter': 0.06451, 'accuracy': 0.78221, 'f1': 0.78217, 'accuracy-SBM': 0.78218, 'auc': 0.96167}
2025-07-11 11:27:47,297 - INFO - > Epoch 74: took 95.8s (avg 94.1s) | Best so far: epoch 74	train_loss: 0.5556 train_accuracy-SBM: 0.7978	val_loss: 0.6279 val_accuracy-SBM: 0.7816	test_loss: 0.6209 test_accuracy-SBM: 0.7822
2025-07-11 11:29:14,862 - INFO - train: {'epoch': 75, 'time_epoch': 87.33725, 'eta': 2058.40232, 'eta_hours': 0.57178, 'loss': 0.55432106, 'lr': 0.00016136, 'params': 463670, 'time_iter': 0.13974, 'accuracy': 0.79844, 'f1': 0.79844, 'accuracy-SBM': 0.79844, 'auc': 0.96926}
2025-07-11 11:29:18,980 - INFO - val: {'epoch': 75, 'time_epoch': 4.075, 'loss': 0.6315998, 'lr': 0, 'params': 463670, 'time_iter': 0.06468, 'accuracy': 0.78021, 'f1': 0.78011, 'accuracy-SBM': 0.78012, 'auc': 0.96062}
2025-07-11 11:29:23,079 - INFO - test: {'epoch': 75, 'time_epoch': 4.0648, 'loss': 0.62042529, 'lr': 0, 'params': 463670, 'time_iter': 0.06452, 'accuracy': 0.78164, 'f1': 0.78164, 'accuracy-SBM': 0.78163, 'auc': 0.96183}
2025-07-11 11:29:23,081 - INFO - > Epoch 75: took 95.8s (avg 94.1s) | Best so far: epoch 74	train_loss: 0.5556 train_accuracy-SBM: 0.7978	val_loss: 0.6279 val_accuracy-SBM: 0.7816	test_loss: 0.6209 test_accuracy-SBM: 0.7822
2025-07-11 11:30:49,926 - INFO - train: {'epoch': 76, 'time_epoch': 86.62004, 'eta': 1972.89043, 'eta_hours': 0.54803, 'loss': 0.55031732, 'lr': 0.00014938, 'params': 463670, 'time_iter': 0.13859, 'accuracy': 0.80019, 'f1': 0.80019, 'accuracy-SBM': 0.80019, 'auc': 0.9697}
2025-07-11 11:30:53,824 - INFO - val: {'epoch': 76, 'time_epoch': 3.85547, 'loss': 0.629941, 'lr': 0, 'params': 463670, 'time_iter': 0.0612, 'accuracy': 0.78162, 'f1': 0.78157, 'accuracy-SBM': 0.78153, 'auc': 0.96064}
2025-07-11 11:30:57,719 - INFO - test: {'epoch': 76, 'time_epoch': 3.86244, 'loss': 0.62219059, 'lr': 0, 'params': 463670, 'time_iter': 0.06131, 'accuracy': 0.78103, 'f1': 0.78101, 'accuracy-SBM': 0.78107, 'auc': 0.96152}
2025-07-11 11:30:57,721 - INFO - > Epoch 76: took 94.6s (avg 94.1s) | Best so far: epoch 74	train_loss: 0.5556 train_accuracy-SBM: 0.7978	val_loss: 0.6279 val_accuracy-SBM: 0.7816	test_loss: 0.6209 test_accuracy-SBM: 0.7822
2025-07-11 11:32:22,334 - INFO - train: {'epoch': 77, 'time_epoch': 84.38392, 'eta': 1886.71943, 'eta_hours': 0.52409, 'loss': 0.55032818, 'lr': 0.00013779, 'params': 463670, 'time_iter': 0.13501, 'accuracy': 0.79982, 'f1': 0.79982, 'accuracy-SBM': 0.79982, 'auc': 0.96971}
2025-07-11 11:32:26,241 - INFO - val: {'epoch': 77, 'time_epoch': 3.86517, 'loss': 0.62868195, 'lr': 0, 'params': 463670, 'time_iter': 0.06135, 'accuracy': 0.78171, 'f1': 0.78159, 'accuracy-SBM': 0.78158, 'auc': 0.96086}
2025-07-11 11:32:30,126 - INFO - test: {'epoch': 77, 'time_epoch': 3.85222, 'loss': 0.62573878, 'lr': 0, 'params': 463670, 'time_iter': 0.06115, 'accuracy': 0.78105, 'f1': 0.78103, 'accuracy-SBM': 0.78104, 'auc': 0.96114}
2025-07-11 11:32:30,128 - INFO - > Epoch 77: took 92.4s (avg 94.1s) | Best so far: epoch 74	train_loss: 0.5556 train_accuracy-SBM: 0.7978	val_loss: 0.6279 val_accuracy-SBM: 0.7816	test_loss: 0.6209 test_accuracy-SBM: 0.7822
2025-07-11 11:33:54,263 - INFO - train: {'epoch': 78, 'time_epoch': 83.91291, 'eta': 1800.46846, 'eta_hours': 0.50013, 'loss': 0.54882413, 'lr': 0.00012659, 'params': 463670, 'time_iter': 0.13426, 'accuracy': 0.80002, 'f1': 0.80002, 'accuracy-SBM': 0.80002, 'auc': 0.96987}
2025-07-11 11:33:58,181 - INFO - val: {'epoch': 78, 'time_epoch': 3.87532, 'loss': 0.63296612, 'lr': 0, 'params': 463670, 'time_iter': 0.06151, 'accuracy': 0.78041, 'f1': 0.78031, 'accuracy-SBM': 0.78029, 'auc': 0.96032}
2025-07-11 11:34:02,080 - INFO - test: {'epoch': 78, 'time_epoch': 3.86727, 'loss': 0.62094557, 'lr': 0, 'params': 463670, 'time_iter': 0.06139, 'accuracy': 0.78219, 'f1': 0.78217, 'accuracy-SBM': 0.7822, 'auc': 0.96168}
2025-07-11 11:34:02,082 - INFO - > Epoch 78: took 92.0s (avg 94.0s) | Best so far: epoch 74	train_loss: 0.5556 train_accuracy-SBM: 0.7978	val_loss: 0.6279 val_accuracy-SBM: 0.7816	test_loss: 0.6209 test_accuracy-SBM: 0.7822
2025-07-11 11:35:26,447 - INFO - train: {'epoch': 79, 'time_epoch': 84.04265, 'eta': 1714.30838, 'eta_hours': 0.4762, 'loss': 0.54853321, 'lr': 0.0001158, 'params': 463670, 'time_iter': 0.13447, 'accuracy': 0.80054, 'f1': 0.80054, 'accuracy-SBM': 0.80054, 'auc': 0.9699}
2025-07-11 11:35:30,337 - INFO - val: {'epoch': 79, 'time_epoch': 3.84725, 'loss': 0.62881465, 'lr': 0, 'params': 463670, 'time_iter': 0.06107, 'accuracy': 0.78066, 'f1': 0.78059, 'accuracy-SBM': 0.78056, 'auc': 0.96054}
2025-07-11 11:35:34,135 - INFO - test: {'epoch': 79, 'time_epoch': 3.76695, 'loss': 0.61816964, 'lr': 0, 'params': 463670, 'time_iter': 0.05979, 'accuracy': 0.78213, 'f1': 0.78209, 'accuracy-SBM': 0.78216, 'auc': 0.96183}
2025-07-11 11:35:34,137 - INFO - > Epoch 79: took 92.1s (avg 94.0s) | Best so far: epoch 74	train_loss: 0.5556 train_accuracy-SBM: 0.7978	val_loss: 0.6279 val_accuracy-SBM: 0.7816	test_loss: 0.6209 test_accuracy-SBM: 0.7822
2025-07-11 11:36:59,199 - INFO - train: {'epoch': 80, 'time_epoch': 84.82816, 'eta': 1628.38484, 'eta_hours': 0.45233, 'loss': 0.54523586, 'lr': 0.00010543, 'params': 463670, 'time_iter': 0.13573, 'accuracy': 0.80156, 'f1': 0.80156, 'accuracy-SBM': 0.80156, 'auc': 0.97027}
2025-07-11 11:37:03,216 - INFO - val: {'epoch': 80, 'time_epoch': 3.97522, 'loss': 0.62936389, 'lr': 0, 'params': 463670, 'time_iter': 0.0631, 'accuracy': 0.78211, 'f1': 0.78198, 'accuracy-SBM': 0.78194, 'auc': 0.96091}
2025-07-11 11:37:07,094 - INFO - test: {'epoch': 80, 'time_epoch': 3.84585, 'loss': 0.62252416, 'lr': 0, 'params': 463670, 'time_iter': 0.06105, 'accuracy': 0.78235, 'f1': 0.78234, 'accuracy-SBM': 0.78235, 'auc': 0.96166}
2025-07-11 11:37:07,096 - INFO - > Epoch 80: took 93.0s (avg 94.0s) | Best so far: epoch 80	train_loss: 0.5452 train_accuracy-SBM: 0.8016	val_loss: 0.6294 val_accuracy-SBM: 0.7819	test_loss: 0.6225 test_accuracy-SBM: 0.7823
2025-07-11 11:38:31,344 - INFO - train: {'epoch': 81, 'time_epoch': 84.02094, 'eta': 1542.31082, 'eta_hours': 0.42842, 'loss': 0.54380504, 'lr': 9.549e-05, 'params': 463670, 'time_iter': 0.13443, 'accuracy': 0.80212, 'f1': 0.80212, 'accuracy-SBM': 0.80212, 'auc': 0.97043}
2025-07-11 11:38:35,299 - INFO - val: {'epoch': 81, 'time_epoch': 3.91227, 'loss': 0.6300974, 'lr': 0, 'params': 463670, 'time_iter': 0.0621, 'accuracy': 0.78072, 'f1': 0.78062, 'accuracy-SBM': 0.7806, 'auc': 0.96058}
2025-07-11 11:38:39,137 - INFO - test: {'epoch': 81, 'time_epoch': 3.8068, 'loss': 0.62379239, 'lr': 0, 'params': 463670, 'time_iter': 0.06043, 'accuracy': 0.78047, 'f1': 0.78046, 'accuracy-SBM': 0.7805, 'auc': 0.96131}
2025-07-11 11:38:39,139 - INFO - > Epoch 81: took 92.0s (avg 94.0s) | Best so far: epoch 80	train_loss: 0.5452 train_accuracy-SBM: 0.8016	val_loss: 0.6294 val_accuracy-SBM: 0.7819	test_loss: 0.6225 test_accuracy-SBM: 0.7823
2025-07-11 11:40:03,919 - INFO - train: {'epoch': 82, 'time_epoch': 84.55002, 'eta': 1456.39464, 'eta_hours': 0.40455, 'loss': 0.54336811, 'lr': 8.6e-05, 'params': 463670, 'time_iter': 0.13528, 'accuracy': 0.80241, 'f1': 0.80241, 'accuracy-SBM': 0.80241, 'auc': 0.97046}
2025-07-11 11:40:07,819 - INFO - val: {'epoch': 82, 'time_epoch': 3.84999, 'loss': 0.6274083, 'lr': 0, 'params': 463670, 'time_iter': 0.06111, 'accuracy': 0.78275, 'f1': 0.78263, 'accuracy-SBM': 0.78262, 'auc': 0.96092}
2025-07-11 11:40:11,680 - INFO - test: {'epoch': 82, 'time_epoch': 3.82885, 'loss': 0.62343534, 'lr': 0, 'params': 463670, 'time_iter': 0.06078, 'accuracy': 0.78176, 'f1': 0.78173, 'accuracy-SBM': 0.78172, 'auc': 0.96136}
2025-07-11 11:40:11,682 - INFO - > Epoch 82: took 92.5s (avg 94.0s) | Best so far: epoch 82	train_loss: 0.5434 train_accuracy-SBM: 0.8024	val_loss: 0.6274 val_accuracy-SBM: 0.7826	test_loss: 0.6234 test_accuracy-SBM: 0.7817
2025-07-11 11:41:36,416 - INFO - train: {'epoch': 83, 'time_epoch': 84.40053, 'eta': 1370.48251, 'eta_hours': 0.38069, 'loss': 0.54113241, 'lr': 7.695e-05, 'params': 463670, 'time_iter': 0.13504, 'accuracy': 0.80336, 'f1': 0.80336, 'accuracy-SBM': 0.80336, 'auc': 0.9707}
2025-07-11 11:41:40,332 - INFO - val: {'epoch': 83, 'time_epoch': 3.86567, 'loss': 0.62609395, 'lr': 0, 'params': 463670, 'time_iter': 0.06136, 'accuracy': 0.78247, 'f1': 0.78233, 'accuracy-SBM': 0.78228, 'auc': 0.96112}
2025-07-11 11:41:44,225 - INFO - test: {'epoch': 83, 'time_epoch': 3.85338, 'loss': 0.6204463, 'lr': 0, 'params': 463670, 'time_iter': 0.06116, 'accuracy': 0.78282, 'f1': 0.7828, 'accuracy-SBM': 0.78281, 'auc': 0.96174}
2025-07-11 11:41:44,227 - INFO - > Epoch 83: took 92.5s (avg 93.9s) | Best so far: epoch 82	train_loss: 0.5434 train_accuracy-SBM: 0.8024	val_loss: 0.6274 val_accuracy-SBM: 0.7826	test_loss: 0.6234 test_accuracy-SBM: 0.7817
2025-07-11 11:43:08,533 - INFO - train: {'epoch': 84, 'time_epoch': 84.08012, 'eta': 1284.54941, 'eta_hours': 0.35682, 'loss': 0.5394956, 'lr': 6.837e-05, 'params': 463670, 'time_iter': 0.13453, 'accuracy': 0.80325, 'f1': 0.80325, 'accuracy-SBM': 0.80325, 'auc': 0.9709}
2025-07-11 11:43:12,375 - INFO - val: {'epoch': 84, 'time_epoch': 3.80011, 'loss': 0.63037169, 'lr': 0, 'params': 463670, 'time_iter': 0.06032, 'accuracy': 0.78136, 'f1': 0.78129, 'accuracy-SBM': 0.78128, 'auc': 0.96071}
2025-07-11 11:43:16,198 - INFO - test: {'epoch': 84, 'time_epoch': 3.79076, 'loss': 0.62573553, 'lr': 0, 'params': 463670, 'time_iter': 0.06017, 'accuracy': 0.78227, 'f1': 0.78226, 'accuracy-SBM': 0.78229, 'auc': 0.9612}
2025-07-11 11:43:16,200 - INFO - > Epoch 84: took 92.0s (avg 93.9s) | Best so far: epoch 82	train_loss: 0.5434 train_accuracy-SBM: 0.8024	val_loss: 0.6274 val_accuracy-SBM: 0.7826	test_loss: 0.6234 test_accuracy-SBM: 0.7817
2025-07-11 11:44:40,524 - INFO - train: {'epoch': 85, 'time_epoch': 84.08777, 'eta': 1198.66064, 'eta_hours': 0.33296, 'loss': 0.53881936, 'lr': 6.026e-05, 'params': 463670, 'time_iter': 0.13454, 'accuracy': 0.80371, 'f1': 0.80371, 'accuracy-SBM': 0.80371, 'auc': 0.97096}
2025-07-11 11:44:44,503 - INFO - val: {'epoch': 85, 'time_epoch': 3.93637, 'loss': 0.62950923, 'lr': 0, 'params': 463670, 'time_iter': 0.06248, 'accuracy': 0.78107, 'f1': 0.78099, 'accuracy-SBM': 0.78098, 'auc': 0.96066}
2025-07-11 11:44:48,334 - INFO - test: {'epoch': 85, 'time_epoch': 3.79877, 'loss': 0.62286954, 'lr': 0, 'params': 463670, 'time_iter': 0.0603, 'accuracy': 0.78234, 'f1': 0.78233, 'accuracy-SBM': 0.78236, 'auc': 0.96146}
2025-07-11 11:44:48,336 - INFO - > Epoch 85: took 92.1s (avg 93.9s) | Best so far: epoch 82	train_loss: 0.5434 train_accuracy-SBM: 0.8024	val_loss: 0.6274 val_accuracy-SBM: 0.7826	test_loss: 0.6234 test_accuracy-SBM: 0.7817
2025-07-11 11:46:13,098 - INFO - train: {'epoch': 86, 'time_epoch': 84.52612, 'eta': 1112.87878, 'eta_hours': 0.30913, 'loss': 0.53697836, 'lr': 5.264e-05, 'params': 463670, 'time_iter': 0.13524, 'accuracy': 0.80478, 'f1': 0.80478, 'accuracy-SBM': 0.80478, 'auc': 0.97115}
2025-07-11 11:46:17,000 - INFO - val: {'epoch': 86, 'time_epoch': 3.85924, 'loss': 0.63021445, 'lr': 0, 'params': 463670, 'time_iter': 0.06126, 'accuracy': 0.78221, 'f1': 0.78209, 'accuracy-SBM': 0.78205, 'auc': 0.96075}
2025-07-11 11:46:20,856 - INFO - test: {'epoch': 86, 'time_epoch': 3.82423, 'loss': 0.62355747, 'lr': 0, 'params': 463670, 'time_iter': 0.0607, 'accuracy': 0.78226, 'f1': 0.78223, 'accuracy-SBM': 0.78224, 'auc': 0.96152}
2025-07-11 11:46:20,859 - INFO - > Epoch 86: took 92.5s (avg 93.9s) | Best so far: epoch 82	train_loss: 0.5434 train_accuracy-SBM: 0.8024	val_loss: 0.6274 val_accuracy-SBM: 0.7826	test_loss: 0.6234 test_accuracy-SBM: 0.7817
2025-07-11 11:47:44,911 - INFO - train: {'epoch': 87, 'time_epoch': 83.72899, 'eta': 1027.01675, 'eta_hours': 0.28528, 'loss': 0.53595525, 'lr': 4.55e-05, 'params': 463670, 'time_iter': 0.13397, 'accuracy': 0.8054, 'f1': 0.8054, 'accuracy-SBM': 0.8054, 'auc': 0.97126}
2025-07-11 11:47:48,804 - INFO - val: {'epoch': 87, 'time_epoch': 3.85007, 'loss': 0.62490198, 'lr': 0, 'params': 463670, 'time_iter': 0.06111, 'accuracy': 0.78182, 'f1': 0.7817, 'accuracy-SBM': 0.78169, 'auc': 0.96126}
2025-07-11 11:47:52,665 - INFO - test: {'epoch': 87, 'time_epoch': 3.82936, 'loss': 0.6235727, 'lr': 0, 'params': 463670, 'time_iter': 0.06078, 'accuracy': 0.78274, 'f1': 0.78272, 'accuracy-SBM': 0.78273, 'auc': 0.96141}
2025-07-11 11:47:52,667 - INFO - > Epoch 87: took 91.8s (avg 93.9s) | Best so far: epoch 82	train_loss: 0.5434 train_accuracy-SBM: 0.8024	val_loss: 0.6274 val_accuracy-SBM: 0.7826	test_loss: 0.6234 test_accuracy-SBM: 0.7817
2025-07-11 11:49:17,168 - INFO - train: {'epoch': 88, 'time_epoch': 84.11958, 'eta': 941.25094, 'eta_hours': 0.26146, 'loss': 0.53700538, 'lr': 3.886e-05, 'params': 463670, 'time_iter': 0.13459, 'accuracy': 0.80451, 'f1': 0.80451, 'accuracy-SBM': 0.80451, 'auc': 0.97115}
2025-07-11 11:49:21,060 - INFO - val: {'epoch': 88, 'time_epoch': 3.8263, 'loss': 0.62819355, 'lr': 0, 'params': 463670, 'time_iter': 0.06073, 'accuracy': 0.78156, 'f1': 0.78146, 'accuracy-SBM': 0.78142, 'auc': 0.96096}
2025-07-11 11:49:24,922 - INFO - test: {'epoch': 88, 'time_epoch': 3.83051, 'loss': 0.62211974, 'lr': 0, 'params': 463670, 'time_iter': 0.0608, 'accuracy': 0.78258, 'f1': 0.78256, 'accuracy-SBM': 0.78258, 'auc': 0.96163}
2025-07-11 11:49:24,924 - INFO - > Epoch 88: took 92.3s (avg 93.8s) | Best so far: epoch 82	train_loss: 0.5434 train_accuracy-SBM: 0.8024	val_loss: 0.6274 val_accuracy-SBM: 0.7826	test_loss: 0.6234 test_accuracy-SBM: 0.7817
2025-07-11 11:50:49,297 - INFO - train: {'epoch': 89, 'time_epoch': 84.15214, 'eta': 855.52532, 'eta_hours': 0.23765, 'loss': 0.53741233, 'lr': 3.272e-05, 'params': 463670, 'time_iter': 0.13464, 'accuracy': 0.80439, 'f1': 0.80439, 'accuracy-SBM': 0.80439, 'auc': 0.97111}
2025-07-11 11:50:53,221 - INFO - val: {'epoch': 89, 'time_epoch': 3.88157, 'loss': 0.62911684, 'lr': 0, 'params': 463670, 'time_iter': 0.06161, 'accuracy': 0.78229, 'f1': 0.78217, 'accuracy-SBM': 0.78215, 'auc': 0.9609}
2025-07-11 11:50:57,226 - INFO - test: {'epoch': 89, 'time_epoch': 3.97294, 'loss': 0.62473329, 'lr': 0, 'params': 463670, 'time_iter': 0.06306, 'accuracy': 0.78317, 'f1': 0.78316, 'accuracy-SBM': 0.78317, 'auc': 0.96136}
2025-07-11 11:50:57,228 - INFO - > Epoch 89: took 92.3s (avg 93.8s) | Best so far: epoch 82	train_loss: 0.5434 train_accuracy-SBM: 0.8024	val_loss: 0.6274 val_accuracy-SBM: 0.7826	test_loss: 0.6234 test_accuracy-SBM: 0.7817
2025-07-11 11:52:24,462 - INFO - train: {'epoch': 90, 'time_epoch': 86.9809, 'eta': 770.11406, 'eta_hours': 0.21392, 'loss': 0.53497401, 'lr': 2.709e-05, 'params': 463670, 'time_iter': 0.13917, 'accuracy': 0.80552, 'f1': 0.80552, 'accuracy-SBM': 0.80552, 'auc': 0.97137}
2025-07-11 11:52:28,488 - INFO - val: {'epoch': 90, 'time_epoch': 3.97641, 'loss': 0.62574674, 'lr': 0, 'params': 463670, 'time_iter': 0.06312, 'accuracy': 0.78291, 'f1': 0.78281, 'accuracy-SBM': 0.7828, 'auc': 0.96111}
2025-07-11 11:52:32,494 - INFO - test: {'epoch': 90, 'time_epoch': 3.97431, 'loss': 0.62392361, 'lr': 0, 'params': 463670, 'time_iter': 0.06308, 'accuracy': 0.7825, 'f1': 0.7825, 'accuracy-SBM': 0.78253, 'auc': 0.96133}
2025-07-11 11:52:32,496 - INFO - > Epoch 90: took 95.3s (avg 93.8s) | Best so far: epoch 90	train_loss: 0.5350 train_accuracy-SBM: 0.8055	val_loss: 0.6257 val_accuracy-SBM: 0.7828	test_loss: 0.6239 test_accuracy-SBM: 0.7825
2025-07-11 11:53:58,842 - INFO - train: {'epoch': 91, 'time_epoch': 86.02326, 'eta': 684.5854, 'eta_hours': 0.19016, 'loss': 0.53400763, 'lr': 2.198e-05, 'params': 463670, 'time_iter': 0.13764, 'accuracy': 0.80548, 'f1': 0.80548, 'accuracy-SBM': 0.80549, 'auc': 0.97149}
2025-07-11 11:54:02,834 - INFO - val: {'epoch': 91, 'time_epoch': 3.94768, 'loss': 0.63042306, 'lr': 0, 'params': 463670, 'time_iter': 0.06266, 'accuracy': 0.7824, 'f1': 0.78231, 'accuracy-SBM': 0.78228, 'auc': 0.96072}
2025-07-11 11:54:06,806 - INFO - test: {'epoch': 91, 'time_epoch': 3.9407, 'loss': 0.62599692, 'lr': 0, 'params': 463670, 'time_iter': 0.06255, 'accuracy': 0.78184, 'f1': 0.78184, 'accuracy-SBM': 0.78188, 'auc': 0.96126}
2025-07-11 11:54:06,808 - INFO - > Epoch 91: took 94.3s (avg 93.8s) | Best so far: epoch 90	train_loss: 0.5350 train_accuracy-SBM: 0.8055	val_loss: 0.6257 val_accuracy-SBM: 0.7828	test_loss: 0.6239 test_accuracy-SBM: 0.7825
2025-07-11 11:55:32,781 - INFO - train: {'epoch': 92, 'time_epoch': 85.55277, 'eta': 599.01069, 'eta_hours': 0.16639, 'loss': 0.53364188, 'lr': 1.74e-05, 'params': 463670, 'time_iter': 0.13688, 'accuracy': 0.80532, 'f1': 0.80532, 'accuracy-SBM': 0.80532, 'auc': 0.97152}
2025-07-11 11:55:36,794 - INFO - val: {'epoch': 92, 'time_epoch': 3.96047, 'loss': 0.62851676, 'lr': 0, 'params': 463670, 'time_iter': 0.06286, 'accuracy': 0.78285, 'f1': 0.78277, 'accuracy-SBM': 0.78275, 'auc': 0.96092}
2025-07-11 11:55:40,751 - INFO - test: {'epoch': 92, 'time_epoch': 3.9259, 'loss': 0.62534569, 'lr': 0, 'params': 463670, 'time_iter': 0.06232, 'accuracy': 0.78259, 'f1': 0.78259, 'accuracy-SBM': 0.78262, 'auc': 0.96129}
2025-07-11 11:55:40,753 - INFO - > Epoch 92: took 93.9s (avg 93.8s) | Best so far: epoch 90	train_loss: 0.5350 train_accuracy-SBM: 0.8055	val_loss: 0.6257 val_accuracy-SBM: 0.7828	test_loss: 0.6239 test_accuracy-SBM: 0.7825
2025-07-11 11:57:06,854 - INFO - train: {'epoch': 93, 'time_epoch': 85.86825, 'eta': 513.45658, 'eta_hours': 0.14263, 'loss': 0.53302432, 'lr': 1.334e-05, 'params': 463670, 'time_iter': 0.13739, 'accuracy': 0.80648, 'f1': 0.80648, 'accuracy-SBM': 0.80648, 'auc': 0.97157}
2025-07-11 11:57:10,835 - INFO - val: {'epoch': 93, 'time_epoch': 3.93824, 'loss': 0.62811916, 'lr': 0, 'params': 463670, 'time_iter': 0.06251, 'accuracy': 0.78319, 'f1': 0.7831, 'accuracy-SBM': 0.78308, 'auc': 0.96093}
2025-07-11 11:57:14,858 - INFO - test: {'epoch': 93, 'time_epoch': 3.99078, 'loss': 0.62513937, 'lr': 0, 'params': 463670, 'time_iter': 0.06335, 'accuracy': 0.78311, 'f1': 0.78309, 'accuracy-SBM': 0.78312, 'auc': 0.96128}
2025-07-11 11:57:14,859 - INFO - > Epoch 93: took 94.1s (avg 93.8s) | Best so far: epoch 93	train_loss: 0.5330 train_accuracy-SBM: 0.8065	val_loss: 0.6281 val_accuracy-SBM: 0.7831	test_loss: 0.6251 test_accuracy-SBM: 0.7831
2025-07-11 11:58:41,201 - INFO - train: {'epoch': 94, 'time_epoch': 86.10852, 'eta': 427.90851, 'eta_hours': 0.11886, 'loss': 0.5325471, 'lr': 9.81e-06, 'params': 463670, 'time_iter': 0.13777, 'accuracy': 0.8061, 'f1': 0.8061, 'accuracy-SBM': 0.8061, 'auc': 0.97164}
2025-07-11 11:58:45,217 - INFO - val: {'epoch': 94, 'time_epoch': 3.97144, 'loss': 0.62838834, 'lr': 0, 'params': 463670, 'time_iter': 0.06304, 'accuracy': 0.78331, 'f1': 0.7832, 'accuracy-SBM': 0.78318, 'auc': 0.96095}
2025-07-11 11:58:49,216 - INFO - test: {'epoch': 94, 'time_epoch': 3.96745, 'loss': 0.6249388, 'lr': 0, 'params': 463670, 'time_iter': 0.06298, 'accuracy': 0.78282, 'f1': 0.78282, 'accuracy-SBM': 0.78284, 'auc': 0.96136}
2025-07-11 11:58:49,218 - INFO - > Epoch 94: took 94.4s (avg 93.9s) | Best so far: epoch 94	train_loss: 0.5325 train_accuracy-SBM: 0.8061	val_loss: 0.6284 val_accuracy-SBM: 0.7832	test_loss: 0.6249 test_accuracy-SBM: 0.7828
2025-07-11 12:00:16,087 - INFO - train: {'epoch': 95, 'time_epoch': 86.6381, 'eta': 342.37082, 'eta_hours': 0.0951, 'loss': 0.53238993, 'lr': 6.82e-06, 'params': 463670, 'time_iter': 0.13862, 'accuracy': 0.80623, 'f1': 0.80623, 'accuracy-SBM': 0.80623, 'auc': 0.97165}
2025-07-11 12:00:20,120 - INFO - val: {'epoch': 95, 'time_epoch': 3.98805, 'loss': 0.62771, 'lr': 0, 'params': 463670, 'time_iter': 0.0633, 'accuracy': 0.78295, 'f1': 0.78285, 'accuracy-SBM': 0.78283, 'auc': 0.961}
2025-07-11 12:00:24,131 - INFO - test: {'epoch': 95, 'time_epoch': 3.96419, 'loss': 0.62410418, 'lr': 0, 'params': 463670, 'time_iter': 0.06292, 'accuracy': 0.78296, 'f1': 0.78295, 'accuracy-SBM': 0.78298, 'auc': 0.96142}
2025-07-11 12:00:24,133 - INFO - > Epoch 95: took 94.9s (avg 93.9s) | Best so far: epoch 94	train_loss: 0.5325 train_accuracy-SBM: 0.8061	val_loss: 0.6284 val_accuracy-SBM: 0.7832	test_loss: 0.6249 test_accuracy-SBM: 0.7828
2025-07-11 12:01:50,807 - INFO - train: {'epoch': 96, 'time_epoch': 86.43833, 'eta': 256.80427, 'eta_hours': 0.07133, 'loss': 0.53202223, 'lr': 4.37e-06, 'params': 463670, 'time_iter': 0.1383, 'accuracy': 0.80618, 'f1': 0.80618, 'accuracy-SBM': 0.80618, 'auc': 0.9717}
2025-07-11 12:01:54,841 - INFO - val: {'epoch': 96, 'time_epoch': 3.98967, 'loss': 0.6303572, 'lr': 0, 'params': 463670, 'time_iter': 0.06333, 'accuracy': 0.78246, 'f1': 0.78235, 'accuracy-SBM': 0.78233, 'auc': 0.9607}
2025-07-11 12:01:58,884 - INFO - test: {'epoch': 96, 'time_epoch': 4.01106, 'loss': 0.623681, 'lr': 0, 'params': 463670, 'time_iter': 0.06367, 'accuracy': 0.78301, 'f1': 0.783, 'accuracy-SBM': 0.78303, 'auc': 0.96147}
2025-07-11 12:01:58,886 - INFO - > Epoch 96: took 94.8s (avg 93.9s) | Best so far: epoch 94	train_loss: 0.5325 train_accuracy-SBM: 0.8061	val_loss: 0.6284 val_accuracy-SBM: 0.7832	test_loss: 0.6249 test_accuracy-SBM: 0.7828
2025-07-11 12:03:25,727 - INFO - train: {'epoch': 97, 'time_epoch': 86.6107, 'eta': 171.22344, 'eta_hours': 0.04756, 'loss': 0.5305417, 'lr': 2.46e-06, 'params': 463670, 'time_iter': 0.13858, 'accuracy': 0.80697, 'f1': 0.80697, 'accuracy-SBM': 0.80697, 'auc': 0.97184}
2025-07-11 12:03:29,742 - INFO - val: {'epoch': 97, 'time_epoch': 3.97238, 'loss': 0.62867098, 'lr': 0, 'params': 463670, 'time_iter': 0.06305, 'accuracy': 0.78247, 'f1': 0.78237, 'accuracy-SBM': 0.78236, 'auc': 0.96083}
2025-07-11 12:03:33,734 - INFO - test: {'epoch': 97, 'time_epoch': 3.9587, 'loss': 0.62340493, 'lr': 0, 'params': 463670, 'time_iter': 0.06284, 'accuracy': 0.78305, 'f1': 0.78304, 'accuracy-SBM': 0.78307, 'auc': 0.96145}
2025-07-11 12:03:33,736 - INFO - > Epoch 97: took 94.8s (avg 93.9s) | Best so far: epoch 94	train_loss: 0.5325 train_accuracy-SBM: 0.8061	val_loss: 0.6284 val_accuracy-SBM: 0.7832	test_loss: 0.6249 test_accuracy-SBM: 0.7828
2025-07-11 12:05:00,297 - INFO - train: {'epoch': 98, 'time_epoch': 86.31677, 'eta': 85.61884, 'eta_hours': 0.02378, 'loss': 0.53065373, 'lr': 1.09e-06, 'params': 463670, 'time_iter': 0.13811, 'accuracy': 0.80707, 'f1': 0.80707, 'accuracy-SBM': 0.80707, 'auc': 0.97183}
2025-07-11 12:05:06,398 - INFO - val: {'epoch': 98, 'time_epoch': 3.98574, 'loss': 0.62946083, 'lr': 0, 'params': 463670, 'time_iter': 0.06327, 'accuracy': 0.78296, 'f1': 0.78284, 'accuracy-SBM': 0.78281, 'auc': 0.96082}
2025-07-11 12:05:10,378 - INFO - test: {'epoch': 98, 'time_epoch': 3.94742, 'loss': 0.62456351, 'lr': 0, 'params': 463670, 'time_iter': 0.06266, 'accuracy': 0.78308, 'f1': 0.78307, 'accuracy-SBM': 0.78309, 'auc': 0.96138}
2025-07-11 12:05:10,380 - INFO - > Epoch 98: took 96.6s (avg 93.9s) | Best so far: epoch 94	train_loss: 0.5325 train_accuracy-SBM: 0.8061	val_loss: 0.6284 val_accuracy-SBM: 0.7832	test_loss: 0.6249 test_accuracy-SBM: 0.7828
2025-07-11 12:06:37,120 - INFO - train: {'epoch': 99, 'time_epoch': 86.40451, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.53155923, 'lr': 2.7e-07, 'params': 463670, 'time_iter': 0.13825, 'accuracy': 0.80632, 'f1': 0.80632, 'accuracy-SBM': 0.80632, 'auc': 0.97175}
2025-07-11 12:06:41,298 - INFO - val: {'epoch': 99, 'time_epoch': 4.13256, 'loss': 0.6306151, 'lr': 0, 'params': 463670, 'time_iter': 0.0656, 'accuracy': 0.78255, 'f1': 0.78245, 'accuracy-SBM': 0.78244, 'auc': 0.96081}
2025-07-11 12:06:45,437 - INFO - test: {'epoch': 99, 'time_epoch': 4.10538, 'loss': 0.62624277, 'lr': 0, 'params': 463670, 'time_iter': 0.06516, 'accuracy': 0.78305, 'f1': 0.78304, 'accuracy-SBM': 0.78306, 'auc': 0.96129}
2025-07-11 12:06:45,638 - INFO - > Epoch 99: took 95.1s (avg 93.9s) | Best so far: epoch 94	train_loss: 0.5325 train_accuracy-SBM: 0.8061	val_loss: 0.6284 val_accuracy-SBM: 0.7832	test_loss: 0.6249 test_accuracy-SBM: 0.7828
2025-07-11 12:06:45,639 - INFO - Avg time per epoch: 93.92s
2025-07-11 12:06:45,639 - INFO - Total train loop time: 2.61h
2025-07-11 12:06:45,640 - INFO - Task done, results saved in results/Cluster/Cluster-GATV2-45
2025-07-11 12:06:45,640 - INFO - Total time: 9457.25s (2.63h)
2025-07-11 12:06:45,641 - INFO - Results aggregated across runs saved in results/Cluster/Cluster-GATV2-45/agg
2025-07-11 12:06:45,641 - INFO - === OPTIMIZED TRAINING COMPLETED SUCCESSFULLY! ===
2025-07-11 12:06:45,641 - INFO - Results saved in: results/Cluster/Cluster-GATV2-45
2025-07-11 12:06:45,641 - INFO - Test results JSON files saved in: results/Cluster/Cluster-GATV2-45/test_results/
Completed seed 45. Results saved in results/Cluster/Cluster-GATV2-45
----------------------------------------
Submitting next job for seed 47
Submitted batch job 5348995
