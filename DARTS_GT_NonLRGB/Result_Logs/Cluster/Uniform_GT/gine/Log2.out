Lmod has detected the following error: The following module(s) are unknown:
"cuda/12.0"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore-cache load "cuda/12.0"

Also make sure that all modulefiles written in TCL start with the string
#%Module



              total        used        free      shared  buff/cache   available
Mem:          376Gi        18Gi       283Gi       2.8Gi        74Gi       352Gi
Swap:         1.9Gi       2.0Mi       1.9Gi
Fri Jul 11 09:09:18 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:1A:00.0 Off |                    0 |
| N/A   39C    P0             43W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Running experiment with seed: 45
Starting training for seed 45...
Changed working directory to: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GINE
Loading config from: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GINE/confignas.yaml
Using device: cuda
2025-07-11 09:11:17,446 - INFO - GPU Mem: 34.1GB
2025-07-11 09:11:17,446 - INFO - Run directory: results/Cluster/Cluster-GINE-45
2025-07-11 09:11:17,446 - INFO - Seed: 45
2025-07-11 09:11:17,446 - INFO - === OPTIMIZED MOE CONFIGURATION ===
2025-07-11 09:11:17,446 - INFO - Routing mode: none
2025-07-11 09:11:17,446 - INFO - Expert types: ['GINE', 'CustomGatedGCN', 'GATV2']
2025-07-11 09:11:17,446 - INFO - Number of layers: 16
2025-07-11 09:11:17,446 - INFO - Uncertainty enabled: False
2025-07-11 09:11:17,446 - INFO - Training mode: custom
2025-07-11 09:11:17,446 - INFO - Optimization: Uncertainty only for last layer + test/val
2025-07-11 09:11:17,446 - INFO - Additional features: Router weights logging + JSON export
2025-07-11 09:11:37,627 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 09:11:37,630 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 09:11:37,691 - INFO -   undirected: True
2025-07-11 09:11:37,691 - INFO -   num graphs: 12000
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 09:11:37,692 - INFO -   avg num_nodes/graph: 117
2025-07-11 09:11:37,692 - INFO -   num node features: 7
2025-07-11 09:11:37,692 - INFO -   num edge features: 0
2025-07-11 09:11:37,694 - INFO -   num classes: 6
2025-07-11 09:11:37,694 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 09:11:37,694 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 09:11:37,702 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s] 17%|█▋        | 2017/12000 [00:10<00:49, 201.68it/s] 33%|███▎      | 4010/12000 [00:20<00:39, 200.21it/s] 49%|████▉     | 5911/12000 [00:30<00:31, 195.58it/s] 66%|██████▌   | 7926/12000 [00:40<00:20, 197.90it/s] 82%|████████▏ | 9860/12000 [00:50<00:10, 196.24it/s] 98%|█████████▊| 11797/12000 [01:00<00:01, 195.36it/s]100%|██████████| 12000/12000 [01:01<00:00, 196.56it/s]
2025-07-11 09:12:39,484 - INFO - Done! Took 00:01:01.79
2025-07-11 09:12:39,506 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 09:12:40,055 - INFO - Created model type: <class 'torch_geometric.graphgym.model_builder.GraphGymModule'>
2025-07-11 09:12:40,055 - INFO - Inner model type: <class 'graphgps.network.uniform_gt_model_edge.UNIFORM_GTModelEdge'>
2025-07-11 09:12:40,055 - INFO - Inner model has get_darts_model: False
2025-07-11 09:12:40,059 - INFO - GraphGymModule(
  (model): UNIFORM_GTModelEdge(
    (encoder): FeatureEncoder(
      (node_encoder): LapPENodeEncoder(
        (linear_x): Linear(in_features=7, out_features=32, bias=True)
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (edge_encoder): DummyEdgeEncoder(
        (encoder): Embedding(1, 48)
      )
    )
    (layers): Sequential(
      (0): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (1): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (2): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (3): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (4): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (5): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (6): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (7): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (8): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (9): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (10): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (11): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (12): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (13): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (14): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (15): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (post_mp): GNNInductiveNodeHead(
      (layer_post_mp): MLP(
        (model): Sequential(
          (0): GeneralMultiLayer(
            (Layer_0): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
            (Layer_1): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
          )
          (1): Linear(
            (model): Linear(48, 6, bias=True)
          )
        )
      )
    )
  )
)
2025-07-11 09:12:40,064 - INFO - Number of parameters: 425,270
2025-07-11 09:12:40,065 - INFO - Starting optimized training: 2025-07-11 09:12:40.065035
2025-07-11 09:12:45,723 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
2025-07-11 09:12:45,723 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 09:12:45,724 - INFO -   undirected: True
2025-07-11 09:12:45,725 - INFO -   num graphs: 12000
2025-07-11 09:12:45,725 - INFO -   avg num_nodes/graph: 117
2025-07-11 09:12:45,725 - INFO -   num node features: 7
2025-07-11 09:12:45,725 - INFO -   num edge features: 0
2025-07-11 09:12:45,727 - INFO -   num classes: 6
2025-07-11 09:12:45,727 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 09:12:45,727 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 09:12:45,735 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s] 16%|█▋        | 1957/12000 [00:10<00:51, 195.69it/s] 32%|███▏      | 3859/12000 [00:20<00:42, 192.42it/s] 48%|████▊     | 5717/12000 [00:30<00:33, 189.38it/s] 65%|██████▍   | 7783/12000 [00:40<00:21, 196.15it/s] 82%|████████▏ | 9793/12000 [00:50<00:11, 197.88it/s] 99%|█████████▉| 11870/12000 [01:00<00:00, 201.20it/s]100%|██████████| 12000/12000 [01:00<00:00, 197.93it/s]
2025-07-11 09:13:47,057 - INFO - Done! Took 00:01:01.33
2025-07-11 09:13:47,080 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 09:13:47,126 - INFO - === STARTING OPTIMIZED TRAINING ===
2025-07-11 09:13:47,126 - INFO - Start from epoch 0
2025-07-11 09:14:58,286 - INFO - train: {'epoch': 0, 'time_epoch': 70.75905, 'eta': 7005.14599, 'eta_hours': 1.94587, 'loss': 1.79375631, 'lr': 0.0, 'params': 425270, 'time_iter': 0.11321, 'accuracy': 0.16737, 'f1': 0.11979, 'accuracy-SBM': 0.1673, 'auc': 0.49875}
2025-07-11 09:14:58,293 - INFO - ...computing epoch stats took: 0.39s
2025-07-11 09:15:02,035 - INFO - val: {'epoch': 0, 'time_epoch': 3.67803, 'loss': 1.7935256, 'lr': 0, 'params': 425270, 'time_iter': 0.05838, 'accuracy': 0.16758, 'f1': 0.11095, 'accuracy-SBM': 0.16621, 'auc': 0.49932}
2025-07-11 09:15:02,037 - INFO - ...computing epoch stats took: 0.06s
2025-07-11 09:15:06,161 - INFO - test: {'epoch': 0, 'time_epoch': 4.07852, 'loss': 1.79377433, 'lr': 0, 'params': 425270, 'time_iter': 0.06474, 'accuracy': 0.16884, 'f1': 0.10997, 'accuracy-SBM': 0.16852, 'auc': 0.49962}
2025-07-11 09:15:06,163 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 09:15:06,163 - INFO - > Epoch 0: took 79.0s (avg 79.0s) | Best so far: epoch 0	train_loss: 1.7938 train_accuracy-SBM: 0.1673	val_loss: 1.7935 val_accuracy-SBM: 0.1662	test_loss: 1.7938 test_accuracy-SBM: 0.1685
2025-07-11 09:16:16,313 - INFO - train: {'epoch': 1, 'time_epoch': 69.91835, 'eta': 6893.19285, 'eta_hours': 1.91478, 'loss': 1.7524672, 'lr': 0.0002, 'params': 425270, 'time_iter': 0.11187, 'accuracy': 0.23579, 'f1': 0.23089, 'accuracy-SBM': 0.23576, 'auc': 0.58916}
2025-07-11 09:16:16,319 - INFO - ...computing epoch stats took: 0.22s
2025-07-11 09:16:19,859 - INFO - val: {'epoch': 1, 'time_epoch': 3.48332, 'loss': 1.82680155, 'lr': 0, 'params': 425270, 'time_iter': 0.05529, 'accuracy': 0.17398, 'f1': 0.09182, 'accuracy-SBM': 0.17657, 'auc': 0.52578}
2025-07-11 09:16:19,861 - INFO - ...computing epoch stats took: 0.06s
2025-07-11 09:16:23,459 - INFO - test: {'epoch': 1, 'time_epoch': 3.54974, 'loss': 1.82258113, 'lr': 0, 'params': 425270, 'time_iter': 0.05635, 'accuracy': 0.17776, 'f1': 0.09619, 'accuracy-SBM': 0.17861, 'auc': 0.53091}
2025-07-11 09:16:23,461 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 09:16:23,461 - INFO - > Epoch 1: took 77.3s (avg 78.2s) | Best so far: epoch 1	train_loss: 1.7525 train_accuracy-SBM: 0.2358	val_loss: 1.8268 val_accuracy-SBM: 0.1766	test_loss: 1.8226 test_accuracy-SBM: 0.1786
2025-07-11 09:17:31,830 - INFO - train: {'epoch': 2, 'time_epoch': 68.13421, 'eta': 6751.57541, 'eta_hours': 1.87544, 'loss': 1.4932303, 'lr': 0.0004, 'params': 425270, 'time_iter': 0.10901, 'accuracy': 0.41435, 'f1': 0.40251, 'accuracy-SBM': 0.41426, 'auc': 0.76463}
2025-07-11 09:17:31,835 - INFO - ...computing epoch stats took: 0.22s
2025-07-11 09:17:35,328 - INFO - val: {'epoch': 2, 'time_epoch': 3.44962, 'loss': 1.76266469, 'lr': 0, 'params': 425270, 'time_iter': 0.05476, 'accuracy': 0.23748, 'f1': 0.21513, 'accuracy-SBM': 0.23797, 'auc': 0.60098}
2025-07-11 09:17:35,330 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 09:17:38,796 - INFO - test: {'epoch': 2, 'time_epoch': 3.43238, 'loss': 1.76046986, 'lr': 0, 'params': 425270, 'time_iter': 0.05448, 'accuracy': 0.2388, 'f1': 0.22095, 'accuracy-SBM': 0.2395, 'auc': 0.60512}
2025-07-11 09:17:38,797 - INFO - ...computing epoch stats took: 0.03s
2025-07-11 09:17:38,798 - INFO - > Epoch 2: took 75.3s (avg 77.2s) | Best so far: epoch 2	train_loss: 1.4932 train_accuracy-SBM: 0.4143	val_loss: 1.7627 val_accuracy-SBM: 0.2380	test_loss: 1.7605 test_accuracy-SBM: 0.2395
2025-07-11 09:18:47,196 - INFO - train: {'epoch': 3, 'time_epoch': 68.15923, 'eta': 6647.30006, 'eta_hours': 1.84647, 'loss': 1.33640792, 'lr': 0.0006, 'params': 425270, 'time_iter': 0.10905, 'accuracy': 0.47243, 'f1': 0.46638, 'accuracy-SBM': 0.47237, 'auc': 0.81323}
2025-07-11 09:18:50,714 - INFO - val: {'epoch': 3, 'time_epoch': 3.46813, 'loss': 1.57296509, 'lr': 0, 'params': 425270, 'time_iter': 0.05505, 'accuracy': 0.35542, 'f1': 0.34305, 'accuracy-SBM': 0.35362, 'auc': 0.74593}
2025-07-11 09:18:54,225 - INFO - test: {'epoch': 3, 'time_epoch': 3.47831, 'loss': 1.55198219, 'lr': 0, 'params': 425270, 'time_iter': 0.05521, 'accuracy': 0.36795, 'f1': 0.35817, 'accuracy-SBM': 0.36745, 'auc': 0.7538}
2025-07-11 09:18:54,227 - INFO - > Epoch 3: took 75.4s (avg 76.8s) | Best so far: epoch 3	train_loss: 1.3364 train_accuracy-SBM: 0.4724	val_loss: 1.5730 val_accuracy-SBM: 0.3536	test_loss: 1.5520 test_accuracy-SBM: 0.3674
2025-07-11 09:20:06,829 - INFO - train: {'epoch': 4, 'time_epoch': 72.34802, 'eta': 6637.05823, 'eta_hours': 1.84363, 'loss': 1.26716265, 'lr': 0.0008, 'params': 425270, 'time_iter': 0.11576, 'accuracy': 0.495, 'f1': 0.48952, 'accuracy-SBM': 0.49494, 'auc': 0.83034}
2025-07-11 09:20:10,596 - INFO - val: {'epoch': 4, 'time_epoch': 3.71956, 'loss': 1.68315368, 'lr': 0, 'params': 425270, 'time_iter': 0.05904, 'accuracy': 0.34078, 'f1': 0.34332, 'accuracy-SBM': 0.33978, 'auc': 0.69149}
2025-07-11 09:20:14,167 - INFO - test: {'epoch': 4, 'time_epoch': 3.53523, 'loss': 1.65319126, 'lr': 0, 'params': 425270, 'time_iter': 0.05611, 'accuracy': 0.353, 'f1': 0.35625, 'accuracy-SBM': 0.35269, 'auc': 0.69958}
2025-07-11 09:20:14,170 - INFO - > Epoch 4: took 79.9s (avg 77.4s) | Best so far: epoch 3	train_loss: 1.3364 train_accuracy-SBM: 0.4724	val_loss: 1.5730 val_accuracy-SBM: 0.3536	test_loss: 1.5520 test_accuracy-SBM: 0.3674
2025-07-11 09:21:23,306 - INFO - train: {'epoch': 5, 'time_epoch': 68.88019, 'eta': 6551.78499, 'eta_hours': 1.81994, 'loss': 1.17606439, 'lr': 0.001, 'params': 425270, 'time_iter': 0.11021, 'accuracy': 0.54949, 'f1': 0.54606, 'accuracy-SBM': 0.54945, 'auc': 0.85528}
2025-07-11 09:21:26,814 - INFO - val: {'epoch': 5, 'time_epoch': 3.46423, 'loss': 1.35122275, 'lr': 0, 'params': 425270, 'time_iter': 0.05499, 'accuracy': 0.4824, 'f1': 0.47733, 'accuracy-SBM': 0.48127, 'auc': 0.80321}
2025-07-11 09:21:30,302 - INFO - test: {'epoch': 5, 'time_epoch': 3.45568, 'loss': 1.34430128, 'lr': 0, 'params': 425270, 'time_iter': 0.05485, 'accuracy': 0.48574, 'f1': 0.48228, 'accuracy-SBM': 0.48514, 'auc': 0.80487}
2025-07-11 09:21:30,303 - INFO - > Epoch 5: took 76.1s (avg 77.2s) | Best so far: epoch 5	train_loss: 1.1761 train_accuracy-SBM: 0.5494	val_loss: 1.3512 val_accuracy-SBM: 0.4813	test_loss: 1.3443 test_accuracy-SBM: 0.4851
2025-07-11 09:22:40,980 - INFO - train: {'epoch': 6, 'time_epoch': 70.4358, 'eta': 6491.86289, 'eta_hours': 1.8033, 'loss': 1.0621079, 'lr': 0.00099973, 'params': 425270, 'time_iter': 0.1127, 'accuracy': 0.60681, 'f1': 0.60563, 'accuracy-SBM': 0.60679, 'auc': 0.88428}
2025-07-11 09:22:44,525 - INFO - val: {'epoch': 6, 'time_epoch': 3.49976, 'loss': 1.21707891, 'lr': 0, 'params': 425270, 'time_iter': 0.05555, 'accuracy': 0.54882, 'f1': 0.54727, 'accuracy-SBM': 0.54958, 'auc': 0.84823}
2025-07-11 09:22:48,063 - INFO - test: {'epoch': 6, 'time_epoch': 3.49648, 'loss': 1.18991736, 'lr': 0, 'params': 425270, 'time_iter': 0.0555, 'accuracy': 0.56215, 'f1': 0.55954, 'accuracy-SBM': 0.56184, 'auc': 0.85548}
2025-07-11 09:22:48,065 - INFO - > Epoch 6: took 77.8s (avg 77.3s) | Best so far: epoch 6	train_loss: 1.0621 train_accuracy-SBM: 0.6068	val_loss: 1.2171 val_accuracy-SBM: 0.5496	test_loss: 1.1899 test_accuracy-SBM: 0.5618
2025-07-11 09:23:59,885 - INFO - train: {'epoch': 7, 'time_epoch': 71.56067, 'eta': 6442.24844, 'eta_hours': 1.78951, 'loss': 0.97426939, 'lr': 0.00099891, 'params': 425270, 'time_iter': 0.1145, 'accuracy': 0.64294, 'f1': 0.64271, 'accuracy-SBM': 0.64294, 'auc': 0.9043}
2025-07-11 09:24:03,428 - INFO - val: {'epoch': 7, 'time_epoch': 3.49144, 'loss': 0.97372229, 'lr': 0, 'params': 425270, 'time_iter': 0.05542, 'accuracy': 0.64388, 'f1': 0.64474, 'accuracy-SBM': 0.64364, 'auc': 0.90547}
2025-07-11 09:24:06,764 - INFO - test: {'epoch': 7, 'time_epoch': 3.29422, 'loss': 0.95647498, 'lr': 0, 'params': 425270, 'time_iter': 0.05229, 'accuracy': 0.64919, 'f1': 0.65006, 'accuracy-SBM': 0.6493, 'auc': 0.90932}
2025-07-11 09:24:06,768 - INFO - > Epoch 7: took 78.7s (avg 77.5s) | Best so far: epoch 7	train_loss: 0.9743 train_accuracy-SBM: 0.6429	val_loss: 0.9737 val_accuracy-SBM: 0.6436	test_loss: 0.9565 test_accuracy-SBM: 0.6493
2025-07-11 09:25:14,811 - INFO - train: {'epoch': 8, 'time_epoch': 67.79161, 'eta': 6349.64761, 'eta_hours': 1.76379, 'loss': 0.92182826, 'lr': 0.00099754, 'params': 425270, 'time_iter': 0.10847, 'accuracy': 0.66234, 'f1': 0.66226, 'accuracy-SBM': 0.66234, 'auc': 0.91461}
2025-07-11 09:25:17,940 - INFO - val: {'epoch': 8, 'time_epoch': 3.08483, 'loss': 1.00447045, 'lr': 0, 'params': 425270, 'time_iter': 0.04897, 'accuracy': 0.63075, 'f1': 0.62251, 'accuracy-SBM': 0.63074, 'auc': 0.90379}
2025-07-11 09:25:21,037 - INFO - test: {'epoch': 8, 'time_epoch': 3.06593, 'loss': 0.98237971, 'lr': 0, 'params': 425270, 'time_iter': 0.04867, 'accuracy': 0.63916, 'f1': 0.63069, 'accuracy-SBM': 0.6392, 'auc': 0.90834}
2025-07-11 09:25:21,040 - INFO - > Epoch 8: took 74.3s (avg 77.1s) | Best so far: epoch 7	train_loss: 0.9743 train_accuracy-SBM: 0.6429	val_loss: 0.9737 val_accuracy-SBM: 0.6436	test_loss: 0.9565 test_accuracy-SBM: 0.6493
2025-07-11 09:26:26,768 - INFO - train: {'epoch': 9, 'time_epoch': 65.48423, 'eta': 6241.24225, 'eta_hours': 1.73368, 'loss': 0.89468313, 'lr': 0.00099563, 'params': 425270, 'time_iter': 0.10477, 'accuracy': 0.67194, 'f1': 0.6719, 'accuracy-SBM': 0.67193, 'auc': 0.91961}
2025-07-11 09:26:29,979 - INFO - val: {'epoch': 9, 'time_epoch': 3.15838, 'loss': 0.95653225, 'lr': 0, 'params': 425270, 'time_iter': 0.05013, 'accuracy': 0.65516, 'f1': 0.65563, 'accuracy-SBM': 0.65546, 'auc': 0.91019}
2025-07-11 09:26:33,024 - INFO - test: {'epoch': 9, 'time_epoch': 3.0124, 'loss': 0.93539564, 'lr': 0, 'params': 425270, 'time_iter': 0.04782, 'accuracy': 0.65691, 'f1': 0.65693, 'accuracy-SBM': 0.65693, 'auc': 0.91426}
2025-07-11 09:26:33,026 - INFO - > Epoch 9: took 72.0s (avg 76.6s) | Best so far: epoch 9	train_loss: 0.8947 train_accuracy-SBM: 0.6719	val_loss: 0.9565 val_accuracy-SBM: 0.6555	test_loss: 0.9354 test_accuracy-SBM: 0.6569
2025-07-11 09:27:40,555 - INFO - train: {'epoch': 10, 'time_epoch': 67.20187, 'eta': 6154.53795, 'eta_hours': 1.70959, 'loss': 0.87665139, 'lr': 0.00099318, 'params': 425270, 'time_iter': 0.10752, 'accuracy': 0.67938, 'f1': 0.67934, 'accuracy-SBM': 0.67938, 'auc': 0.9228}
2025-07-11 09:27:43,593 - INFO - val: {'epoch': 10, 'time_epoch': 2.98776, 'loss': 0.87090178, 'lr': 0, 'params': 425270, 'time_iter': 0.04742, 'accuracy': 0.68525, 'f1': 0.68568, 'accuracy-SBM': 0.68528, 'auc': 0.92438}
2025-07-11 09:27:46,608 - INFO - test: {'epoch': 10, 'time_epoch': 2.98332, 'loss': 0.8375592, 'lr': 0, 'params': 425270, 'time_iter': 0.04735, 'accuracy': 0.69268, 'f1': 0.69319, 'accuracy-SBM': 0.6929, 'auc': 0.93068}
2025-07-11 09:27:46,610 - INFO - > Epoch 10: took 73.6s (avg 76.3s) | Best so far: epoch 10	train_loss: 0.8767 train_accuracy-SBM: 0.6794	val_loss: 0.8709 val_accuracy-SBM: 0.6853	test_loss: 0.8376 test_accuracy-SBM: 0.6929
2025-07-11 09:28:51,384 - INFO - train: {'epoch': 11, 'time_epoch': 64.54327, 'eta': 6051.58767, 'eta_hours': 1.681, 'loss': 0.86044184, 'lr': 0.00099019, 'params': 425270, 'time_iter': 0.10327, 'accuracy': 0.68555, 'f1': 0.68555, 'accuracy-SBM': 0.68555, 'auc': 0.92568}
2025-07-11 09:28:54,466 - INFO - val: {'epoch': 11, 'time_epoch': 3.02973, 'loss': 0.86396542, 'lr': 0, 'params': 425270, 'time_iter': 0.04809, 'accuracy': 0.68589, 'f1': 0.68547, 'accuracy-SBM': 0.68579, 'auc': 0.92669}
2025-07-11 09:28:57,546 - INFO - test: {'epoch': 11, 'time_epoch': 3.0454, 'loss': 0.8539459, 'lr': 0, 'params': 425270, 'time_iter': 0.04834, 'accuracy': 0.68773, 'f1': 0.68756, 'accuracy-SBM': 0.68763, 'auc': 0.92861}
2025-07-11 09:28:57,549 - INFO - > Epoch 11: took 70.9s (avg 75.9s) | Best so far: epoch 11	train_loss: 0.8604 train_accuracy-SBM: 0.6855	val_loss: 0.8640 val_accuracy-SBM: 0.6858	test_loss: 0.8539 test_accuracy-SBM: 0.6876
2025-07-11 09:30:00,233 - INFO - train: {'epoch': 12, 'time_epoch': 62.44609, 'eta': 5940.51115, 'eta_hours': 1.65014, 'loss': 0.8447899, 'lr': 0.00098666, 'params': 425270, 'time_iter': 0.09991, 'accuracy': 0.69063, 'f1': 0.69063, 'accuracy-SBM': 0.69063, 'auc': 0.92839}
2025-07-11 09:30:03,324 - INFO - val: {'epoch': 12, 'time_epoch': 3.04638, 'loss': 0.83356179, 'lr': 0, 'params': 425270, 'time_iter': 0.04836, 'accuracy': 0.69479, 'f1': 0.69346, 'accuracy-SBM': 0.69452, 'auc': 0.93313}
2025-07-11 09:30:06,324 - INFO - test: {'epoch': 12, 'time_epoch': 2.96859, 'loss': 0.81556429, 'lr': 0, 'params': 425270, 'time_iter': 0.04712, 'accuracy': 0.70187, 'f1': 0.70048, 'accuracy-SBM': 0.70154, 'auc': 0.93638}
2025-07-11 09:30:06,326 - INFO - > Epoch 12: took 68.8s (avg 75.3s) | Best so far: epoch 12	train_loss: 0.8448 train_accuracy-SBM: 0.6906	val_loss: 0.8336 val_accuracy-SBM: 0.6945	test_loss: 0.8156 test_accuracy-SBM: 0.7015
2025-07-11 09:31:08,484 - INFO - train: {'epoch': 13, 'time_epoch': 61.93122, 'eta': 5833.21909, 'eta_hours': 1.62034, 'loss': 0.83765987, 'lr': 0.0009826, 'params': 425270, 'time_iter': 0.09909, 'accuracy': 0.69379, 'f1': 0.69379, 'accuracy-SBM': 0.69379, 'auc': 0.92956}
2025-07-11 09:31:11,479 - INFO - val: {'epoch': 13, 'time_epoch': 2.95283, 'loss': 0.8797451, 'lr': 0, 'params': 425270, 'time_iter': 0.04687, 'accuracy': 0.68194, 'f1': 0.68101, 'accuracy-SBM': 0.68177, 'auc': 0.92392}
2025-07-11 09:31:14,433 - INFO - test: {'epoch': 13, 'time_epoch': 2.92301, 'loss': 0.84810674, 'lr': 0, 'params': 425270, 'time_iter': 0.0464, 'accuracy': 0.69228, 'f1': 0.69159, 'accuracy-SBM': 0.69189, 'auc': 0.92905}
2025-07-11 09:31:14,435 - INFO - > Epoch 13: took 68.1s (avg 74.8s) | Best so far: epoch 12	train_loss: 0.8448 train_accuracy-SBM: 0.6906	val_loss: 0.8336 val_accuracy-SBM: 0.6945	test_loss: 0.8156 test_accuracy-SBM: 0.7015
2025-07-11 09:32:16,242 - INFO - train: {'epoch': 14, 'time_epoch': 61.57962, 'eta': 5729.98276, 'eta_hours': 1.59166, 'loss': 0.82760201, 'lr': 0.00097802, 'params': 425270, 'time_iter': 0.09853, 'accuracy': 0.69708, 'f1': 0.69708, 'accuracy-SBM': 0.69708, 'auc': 0.93127}
2025-07-11 09:32:19,231 - INFO - val: {'epoch': 14, 'time_epoch': 2.94579, 'loss': 0.84423606, 'lr': 0, 'params': 425270, 'time_iter': 0.04676, 'accuracy': 0.69505, 'f1': 0.6952, 'accuracy-SBM': 0.69479, 'auc': 0.92928}
2025-07-11 09:32:22,199 - INFO - test: {'epoch': 14, 'time_epoch': 2.9382, 'loss': 0.82532415, 'lr': 0, 'params': 425270, 'time_iter': 0.04664, 'accuracy': 0.69793, 'f1': 0.69835, 'accuracy-SBM': 0.69791, 'auc': 0.93305}
2025-07-11 09:32:22,201 - INFO - > Epoch 14: took 67.8s (avg 74.3s) | Best so far: epoch 14	train_loss: 0.8276 train_accuracy-SBM: 0.6971	val_loss: 0.8442 val_accuracy-SBM: 0.6948	test_loss: 0.8253 test_accuracy-SBM: 0.6979
2025-07-11 09:33:23,604 - INFO - train: {'epoch': 15, 'time_epoch': 61.17255, 'eta': 5629.81637, 'eta_hours': 1.56384, 'loss': 0.81674493, 'lr': 0.00097291, 'params': 425270, 'time_iter': 0.09788, 'accuracy': 0.7011, 'f1': 0.7011, 'accuracy-SBM': 0.70109, 'auc': 0.93304}
2025-07-11 09:33:26,600 - INFO - val: {'epoch': 15, 'time_epoch': 2.95447, 'loss': 0.99361942, 'lr': 0, 'params': 425270, 'time_iter': 0.0469, 'accuracy': 0.6514, 'f1': 0.65282, 'accuracy-SBM': 0.65064, 'auc': 0.90845}
2025-07-11 09:33:29,570 - INFO - test: {'epoch': 15, 'time_epoch': 2.93855, 'loss': 0.93377429, 'lr': 0, 'params': 425270, 'time_iter': 0.04664, 'accuracy': 0.66545, 'f1': 0.66669, 'accuracy-SBM': 0.66512, 'auc': 0.91876}
2025-07-11 09:33:29,572 - INFO - > Epoch 15: took 67.4s (avg 73.9s) | Best so far: epoch 14	train_loss: 0.8276 train_accuracy-SBM: 0.6971	val_loss: 0.8442 val_accuracy-SBM: 0.6948	test_loss: 0.8253 test_accuracy-SBM: 0.6979
2025-07-11 09:34:31,294 - INFO - train: {'epoch': 16, 'time_epoch': 61.49153, 'eta': 5535.7949, 'eta_hours': 1.53772, 'loss': 0.81239396, 'lr': 0.00096728, 'params': 425270, 'time_iter': 0.09839, 'accuracy': 0.70336, 'f1': 0.70337, 'accuracy-SBM': 0.70336, 'auc': 0.93379}
2025-07-11 09:34:34,327 - INFO - val: {'epoch': 16, 'time_epoch': 2.98947, 'loss': 0.81824893, 'lr': 0, 'params': 425270, 'time_iter': 0.04745, 'accuracy': 0.70276, 'f1': 0.70244, 'accuracy-SBM': 0.7025, 'auc': 0.93356}
2025-07-11 09:34:37,336 - INFO - test: {'epoch': 16, 'time_epoch': 2.97889, 'loss': 0.82058762, 'lr': 0, 'params': 425270, 'time_iter': 0.04728, 'accuracy': 0.70455, 'f1': 0.70437, 'accuracy-SBM': 0.70434, 'auc': 0.9329}
2025-07-11 09:34:37,338 - INFO - > Epoch 16: took 67.8s (avg 73.5s) | Best so far: epoch 16	train_loss: 0.8124 train_accuracy-SBM: 0.7034	val_loss: 0.8182 val_accuracy-SBM: 0.7025	test_loss: 0.8206 test_accuracy-SBM: 0.7043
2025-07-11 09:35:38,972 - INFO - train: {'epoch': 17, 'time_epoch': 61.31471, 'eta': 5444.58233, 'eta_hours': 1.51238, 'loss': 0.80817158, 'lr': 0.00096114, 'params': 425270, 'time_iter': 0.0981, 'accuracy': 0.70469, 'f1': 0.70471, 'accuracy-SBM': 0.70469, 'auc': 0.93444}
2025-07-11 09:35:41,958 - INFO - val: {'epoch': 17, 'time_epoch': 2.94346, 'loss': 0.84173199, 'lr': 0, 'params': 425270, 'time_iter': 0.04672, 'accuracy': 0.69715, 'f1': 0.69592, 'accuracy-SBM': 0.69636, 'auc': 0.93137}
2025-07-11 09:35:44,903 - INFO - test: {'epoch': 17, 'time_epoch': 2.91417, 'loss': 0.82083513, 'lr': 0, 'params': 425270, 'time_iter': 0.04626, 'accuracy': 0.70188, 'f1': 0.70112, 'accuracy-SBM': 0.70136, 'auc': 0.93446}
2025-07-11 09:35:44,906 - INFO - > Epoch 17: took 67.6s (avg 73.2s) | Best so far: epoch 16	train_loss: 0.8124 train_accuracy-SBM: 0.7034	val_loss: 0.8182 val_accuracy-SBM: 0.7025	test_loss: 0.8206 test_accuracy-SBM: 0.7043
2025-07-11 09:36:45,491 - INFO - train: {'epoch': 18, 'time_epoch': 60.36222, 'eta': 5352.45628, 'eta_hours': 1.48679, 'loss': 0.80170859, 'lr': 0.0009545, 'params': 425270, 'time_iter': 0.09658, 'accuracy': 0.70722, 'f1': 0.70723, 'accuracy-SBM': 0.70722, 'auc': 0.93549}
2025-07-11 09:36:48,450 - INFO - val: {'epoch': 18, 'time_epoch': 2.91723, 'loss': 0.91850533, 'lr': 0, 'params': 425270, 'time_iter': 0.04631, 'accuracy': 0.66449, 'f1': 0.66456, 'accuracy-SBM': 0.66513, 'auc': 0.91633}
2025-07-11 09:36:51,405 - INFO - test: {'epoch': 18, 'time_epoch': 2.92441, 'loss': 0.9191033, 'lr': 0, 'params': 425270, 'time_iter': 0.04642, 'accuracy': 0.66681, 'f1': 0.66655, 'accuracy-SBM': 0.66714, 'auc': 0.91604}
2025-07-11 09:36:51,407 - INFO - > Epoch 18: took 66.5s (avg 72.9s) | Best so far: epoch 16	train_loss: 0.8124 train_accuracy-SBM: 0.7034	val_loss: 0.8182 val_accuracy-SBM: 0.7025	test_loss: 0.8206 test_accuracy-SBM: 0.7043
2025-07-11 09:37:54,415 - INFO - train: {'epoch': 19, 'time_epoch': 62.68338, 'eta': 5272.79126, 'eta_hours': 1.46466, 'loss': 0.79776895, 'lr': 0.00094736, 'params': 425270, 'time_iter': 0.10029, 'accuracy': 0.70833, 'f1': 0.70834, 'accuracy-SBM': 0.70833, 'auc': 0.93612}
2025-07-11 09:37:57,548 - INFO - val: {'epoch': 19, 'time_epoch': 3.09105, 'loss': 0.86096305, 'lr': 0, 'params': 425270, 'time_iter': 0.04906, 'accuracy': 0.69328, 'f1': 0.69364, 'accuracy-SBM': 0.69383, 'auc': 0.92614}
2025-07-11 09:38:00,538 - INFO - test: {'epoch': 19, 'time_epoch': 2.95895, 'loss': 0.8402778, 'lr': 0, 'params': 425270, 'time_iter': 0.04697, 'accuracy': 0.69714, 'f1': 0.69688, 'accuracy-SBM': 0.69748, 'auc': 0.92974}
2025-07-11 09:38:00,540 - INFO - > Epoch 19: took 69.1s (avg 72.7s) | Best so far: epoch 16	train_loss: 0.8124 train_accuracy-SBM: 0.7034	val_loss: 0.8182 val_accuracy-SBM: 0.7025	test_loss: 0.8206 test_accuracy-SBM: 0.7043
2025-07-11 09:39:01,250 - INFO - train: {'epoch': 20, 'time_epoch': 60.48508, 'eta': 5186.47374, 'eta_hours': 1.44069, 'loss': 0.78986965, 'lr': 0.00093974, 'params': 425270, 'time_iter': 0.09678, 'accuracy': 0.71156, 'f1': 0.71157, 'accuracy-SBM': 0.71156, 'auc': 0.93741}
2025-07-11 09:39:04,221 - INFO - val: {'epoch': 20, 'time_epoch': 2.91939, 'loss': 0.94473264, 'lr': 0, 'params': 425270, 'time_iter': 0.04634, 'accuracy': 0.67385, 'f1': 0.67375, 'accuracy-SBM': 0.67395, 'auc': 0.91262}
2025-07-11 09:39:07,133 - INFO - test: {'epoch': 20, 'time_epoch': 2.8815, 'loss': 0.9004564, 'lr': 0, 'params': 425270, 'time_iter': 0.04574, 'accuracy': 0.68325, 'f1': 0.68303, 'accuracy-SBM': 0.68299, 'auc': 0.92001}
2025-07-11 09:39:07,135 - INFO - > Epoch 20: took 66.6s (avg 72.4s) | Best so far: epoch 16	train_loss: 0.8124 train_accuracy-SBM: 0.7034	val_loss: 0.8182 val_accuracy-SBM: 0.7025	test_loss: 0.8206 test_accuracy-SBM: 0.7043
2025-07-11 09:40:07,647 - INFO - train: {'epoch': 21, 'time_epoch': 60.28764, 'eta': 5101.80461, 'eta_hours': 1.41717, 'loss': 0.78282903, 'lr': 0.00093163, 'params': 425270, 'time_iter': 0.09646, 'accuracy': 0.71431, 'f1': 0.71432, 'accuracy-SBM': 0.71431, 'auc': 0.93852}
2025-07-11 09:40:10,609 - INFO - val: {'epoch': 21, 'time_epoch': 2.91988, 'loss': 0.76649981, 'lr': 0, 'params': 425270, 'time_iter': 0.04635, 'accuracy': 0.72014, 'f1': 0.71976, 'accuracy-SBM': 0.71992, 'auc': 0.94151}
2025-07-11 09:40:13,549 - INFO - test: {'epoch': 21, 'time_epoch': 2.90876, 'loss': 0.75199723, 'lr': 0, 'params': 425270, 'time_iter': 0.04617, 'accuracy': 0.72624, 'f1': 0.72593, 'accuracy-SBM': 0.72603, 'auc': 0.94392}
2025-07-11 09:40:13,551 - INFO - > Epoch 21: took 66.4s (avg 72.1s) | Best so far: epoch 21	train_loss: 0.7828 train_accuracy-SBM: 0.7143	val_loss: 0.7665 val_accuracy-SBM: 0.7199	test_loss: 0.7520 test_accuracy-SBM: 0.7260
2025-07-11 09:41:14,159 - INFO - train: {'epoch': 22, 'time_epoch': 60.3836, 'eta': 5019.57687, 'eta_hours': 1.39433, 'loss': 0.77853722, 'lr': 0.00092305, 'params': 425270, 'time_iter': 0.09661, 'accuracy': 0.71539, 'f1': 0.7154, 'accuracy-SBM': 0.71539, 'auc': 0.93921}
2025-07-11 09:41:17,105 - INFO - val: {'epoch': 22, 'time_epoch': 2.90378, 'loss': 0.78586528, 'lr': 0, 'params': 425270, 'time_iter': 0.04609, 'accuracy': 0.71461, 'f1': 0.7149, 'accuracy-SBM': 0.7145, 'auc': 0.93938}
2025-07-11 09:41:20,024 - INFO - test: {'epoch': 22, 'time_epoch': 2.8884, 'loss': 0.77657916, 'lr': 0, 'params': 425270, 'time_iter': 0.04585, 'accuracy': 0.71679, 'f1': 0.71695, 'accuracy-SBM': 0.71698, 'auc': 0.94124}
2025-07-11 09:41:20,026 - INFO - > Epoch 22: took 66.5s (avg 71.9s) | Best so far: epoch 21	train_loss: 0.7828 train_accuracy-SBM: 0.7143	val_loss: 0.7665 val_accuracy-SBM: 0.7199	test_loss: 0.7520 test_accuracy-SBM: 0.7260
2025-07-11 09:42:21,116 - INFO - train: {'epoch': 23, 'time_epoch': 60.76946, 'eta': 4940.39136, 'eta_hours': 1.37233, 'loss': 0.77466085, 'lr': 0.000914, 'params': 425270, 'time_iter': 0.09723, 'accuracy': 0.71699, 'f1': 0.71701, 'accuracy-SBM': 0.71699, 'auc': 0.9398}
2025-07-11 09:42:24,103 - INFO - val: {'epoch': 23, 'time_epoch': 2.9447, 'loss': 0.77049662, 'lr': 0, 'params': 425270, 'time_iter': 0.04674, 'accuracy': 0.72179, 'f1': 0.72179, 'accuracy-SBM': 0.72162, 'auc': 0.94081}
2025-07-11 09:42:27,066 - INFO - test: {'epoch': 23, 'time_epoch': 2.93224, 'loss': 0.75466823, 'lr': 0, 'params': 425270, 'time_iter': 0.04654, 'accuracy': 0.72256, 'f1': 0.72275, 'accuracy-SBM': 0.72262, 'auc': 0.94339}
2025-07-11 09:42:27,068 - INFO - > Epoch 23: took 67.0s (avg 71.7s) | Best so far: epoch 23	train_loss: 0.7747 train_accuracy-SBM: 0.7170	val_loss: 0.7705 val_accuracy-SBM: 0.7216	test_loss: 0.7547 test_accuracy-SBM: 0.7226
2025-07-11 09:43:27,512 - INFO - train: {'epoch': 24, 'time_epoch': 60.21686, 'eta': 4861.02135, 'eta_hours': 1.35028, 'loss': 0.76937164, 'lr': 0.00090451, 'params': 425270, 'time_iter': 0.09635, 'accuracy': 0.71858, 'f1': 0.71858, 'accuracy-SBM': 0.71858, 'auc': 0.94064}
2025-07-11 09:43:30,599 - INFO - val: {'epoch': 24, 'time_epoch': 3.04481, 'loss': 0.81186343, 'lr': 0, 'params': 425270, 'time_iter': 0.04833, 'accuracy': 0.70914, 'f1': 0.70915, 'accuracy-SBM': 0.70905, 'auc': 0.93494}
2025-07-11 09:43:33,520 - INFO - test: {'epoch': 24, 'time_epoch': 2.89044, 'loss': 0.79622255, 'lr': 0, 'params': 425270, 'time_iter': 0.04588, 'accuracy': 0.71237, 'f1': 0.71218, 'accuracy-SBM': 0.71261, 'auc': 0.93763}
2025-07-11 09:43:33,522 - INFO - > Epoch 24: took 66.5s (avg 71.5s) | Best so far: epoch 23	train_loss: 0.7747 train_accuracy-SBM: 0.7170	val_loss: 0.7705 val_accuracy-SBM: 0.7216	test_loss: 0.7547 test_accuracy-SBM: 0.7226
2025-07-11 09:44:34,820 - INFO - train: {'epoch': 25, 'time_epoch': 61.06683, 'eta': 4785.54379, 'eta_hours': 1.32932, 'loss': 0.76659192, 'lr': 0.00089457, 'params': 425270, 'time_iter': 0.09771, 'accuracy': 0.71954, 'f1': 0.71955, 'accuracy-SBM': 0.71954, 'auc': 0.94108}
2025-07-11 09:44:37,800 - INFO - val: {'epoch': 25, 'time_epoch': 2.93716, 'loss': 0.74057503, 'lr': 0, 'params': 425270, 'time_iter': 0.04662, 'accuracy': 0.72875, 'f1': 0.72906, 'accuracy-SBM': 0.72889, 'auc': 0.94607}
2025-07-11 09:44:40,869 - INFO - test: {'epoch': 25, 'time_epoch': 3.03729, 'loss': 0.7346472, 'lr': 0, 'params': 425270, 'time_iter': 0.04821, 'accuracy': 0.7307, 'f1': 0.73088, 'accuracy-SBM': 0.73085, 'auc': 0.94726}
2025-07-11 09:44:40,871 - INFO - > Epoch 25: took 67.3s (avg 71.3s) | Best so far: epoch 25	train_loss: 0.7666 train_accuracy-SBM: 0.7195	val_loss: 0.7406 val_accuracy-SBM: 0.7289	test_loss: 0.7346 test_accuracy-SBM: 0.7308
2025-07-11 09:45:42,812 - INFO - train: {'epoch': 26, 'time_epoch': 61.7137, 'eta': 4712.88266, 'eta_hours': 1.30913, 'loss': 0.7628896, 'lr': 0.0008842, 'params': 425270, 'time_iter': 0.09874, 'accuracy': 0.72144, 'f1': 0.72145, 'accuracy-SBM': 0.72144, 'auc': 0.94164}
2025-07-11 09:45:45,805 - INFO - val: {'epoch': 26, 'time_epoch': 2.95053, 'loss': 0.75309946, 'lr': 0, 'params': 425270, 'time_iter': 0.04683, 'accuracy': 0.72583, 'f1': 0.72541, 'accuracy-SBM': 0.72596, 'auc': 0.94379}
2025-07-11 09:45:48,766 - INFO - test: {'epoch': 26, 'time_epoch': 2.93081, 'loss': 0.74336957, 'lr': 0, 'params': 425270, 'time_iter': 0.04652, 'accuracy': 0.7266, 'f1': 0.72604, 'accuracy-SBM': 0.72637, 'auc': 0.94527}
2025-07-11 09:45:48,768 - INFO - > Epoch 26: took 67.9s (avg 71.2s) | Best so far: epoch 25	train_loss: 0.7666 train_accuracy-SBM: 0.7195	val_loss: 0.7406 val_accuracy-SBM: 0.7289	test_loss: 0.7346 test_accuracy-SBM: 0.7308
2025-07-11 09:46:49,299 - INFO - train: {'epoch': 27, 'time_epoch': 60.29931, 'eta': 4637.36646, 'eta_hours': 1.28816, 'loss': 0.75962678, 'lr': 0.00087341, 'params': 425270, 'time_iter': 0.09648, 'accuracy': 0.72274, 'f1': 0.72275, 'accuracy-SBM': 0.72274, 'auc': 0.94212}
2025-07-11 09:46:52,274 - INFO - val: {'epoch': 27, 'time_epoch': 2.93315, 'loss': 0.75928725, 'lr': 0, 'params': 425270, 'time_iter': 0.04656, 'accuracy': 0.72555, 'f1': 0.72541, 'accuracy-SBM': 0.72574, 'auc': 0.94296}
2025-07-11 09:46:55,246 - INFO - test: {'epoch': 27, 'time_epoch': 2.93355, 'loss': 0.74575064, 'lr': 0, 'params': 425270, 'time_iter': 0.04656, 'accuracy': 0.72744, 'f1': 0.72729, 'accuracy-SBM': 0.72741, 'auc': 0.94523}
2025-07-11 09:46:55,248 - INFO - > Epoch 27: took 66.5s (avg 71.0s) | Best so far: epoch 25	train_loss: 0.7666 train_accuracy-SBM: 0.7195	val_loss: 0.7406 val_accuracy-SBM: 0.7289	test_loss: 0.7346 test_accuracy-SBM: 0.7308
2025-07-11 09:47:56,313 - INFO - train: {'epoch': 28, 'time_epoch': 60.74914, 'eta': 4564.00101, 'eta_hours': 1.26778, 'loss': 0.75836974, 'lr': 0.00086221, 'params': 425270, 'time_iter': 0.0972, 'accuracy': 0.72422, 'f1': 0.72423, 'accuracy-SBM': 0.72422, 'auc': 0.9423}
2025-07-11 09:47:59,278 - INFO - val: {'epoch': 28, 'time_epoch': 2.92292, 'loss': 0.73737547, 'lr': 0, 'params': 425270, 'time_iter': 0.0464, 'accuracy': 0.72892, 'f1': 0.72897, 'accuracy-SBM': 0.7292, 'auc': 0.94644}
2025-07-11 09:48:02,259 - INFO - test: {'epoch': 28, 'time_epoch': 2.93744, 'loss': 0.72138009, 'lr': 0, 'params': 425270, 'time_iter': 0.04663, 'accuracy': 0.73539, 'f1': 0.73529, 'accuracy-SBM': 0.7355, 'auc': 0.94902}
2025-07-11 09:48:02,262 - INFO - > Epoch 28: took 67.0s (avg 70.9s) | Best so far: epoch 28	train_loss: 0.7584 train_accuracy-SBM: 0.7242	val_loss: 0.7374 val_accuracy-SBM: 0.7292	test_loss: 0.7214 test_accuracy-SBM: 0.7355
2025-07-11 09:49:03,338 - INFO - train: {'epoch': 29, 'time_epoch': 60.7626, 'eta': 4491.50806, 'eta_hours': 1.24764, 'loss': 0.75201027, 'lr': 0.00085062, 'params': 425270, 'time_iter': 0.09722, 'accuracy': 0.7255, 'f1': 0.72551, 'accuracy-SBM': 0.7255, 'auc': 0.94329}
2025-07-11 09:49:06,178 - INFO - val: {'epoch': 29, 'time_epoch': 2.79952, 'loss': 0.75182634, 'lr': 0, 'params': 425270, 'time_iter': 0.04444, 'accuracy': 0.72859, 'f1': 0.72874, 'accuracy-SBM': 0.72858, 'auc': 0.94394}
2025-07-11 09:49:08,978 - INFO - test: {'epoch': 29, 'time_epoch': 2.77036, 'loss': 0.74242667, 'lr': 0, 'params': 425270, 'time_iter': 0.04397, 'accuracy': 0.72866, 'f1': 0.72857, 'accuracy-SBM': 0.72875, 'auc': 0.94548}
2025-07-11 09:49:08,980 - INFO - > Epoch 29: took 66.7s (avg 70.7s) | Best so far: epoch 28	train_loss: 0.7584 train_accuracy-SBM: 0.7242	val_loss: 0.7374 val_accuracy-SBM: 0.7292	test_loss: 0.7214 test_accuracy-SBM: 0.7355
2025-07-11 09:50:08,622 - INFO - train: {'epoch': 30, 'time_epoch': 59.41774, 'eta': 4416.77852, 'eta_hours': 1.22688, 'loss': 0.74825462, 'lr': 0.00083864, 'params': 425270, 'time_iter': 0.09507, 'accuracy': 0.72684, 'f1': 0.72686, 'accuracy-SBM': 0.72684, 'auc': 0.94388}
2025-07-11 09:50:11,591 - INFO - val: {'epoch': 30, 'time_epoch': 2.92678, 'loss': 0.89413841, 'lr': 0, 'params': 425270, 'time_iter': 0.04646, 'accuracy': 0.71097, 'f1': 0.71128, 'accuracy-SBM': 0.71102, 'auc': 0.9218}
2025-07-11 09:50:14,530 - INFO - test: {'epoch': 30, 'time_epoch': 2.90091, 'loss': 0.83032353, 'lr': 0, 'params': 425270, 'time_iter': 0.04605, 'accuracy': 0.71963, 'f1': 0.71967, 'accuracy-SBM': 0.71949, 'auc': 0.93163}
2025-07-11 09:50:14,532 - INFO - > Epoch 30: took 65.6s (avg 70.6s) | Best so far: epoch 28	train_loss: 0.7584 train_accuracy-SBM: 0.7242	val_loss: 0.7374 val_accuracy-SBM: 0.7292	test_loss: 0.7214 test_accuracy-SBM: 0.7355
2025-07-11 09:51:15,322 - INFO - train: {'epoch': 31, 'time_epoch': 60.56992, 'eta': 4345.45434, 'eta_hours': 1.20707, 'loss': 0.74385729, 'lr': 0.00082629, 'params': 425270, 'time_iter': 0.09691, 'accuracy': 0.72829, 'f1': 0.7283, 'accuracy-SBM': 0.72829, 'auc': 0.94453}
2025-07-11 09:51:18,282 - INFO - val: {'epoch': 31, 'time_epoch': 2.91835, 'loss': 0.79627996, 'lr': 0, 'params': 425270, 'time_iter': 0.04632, 'accuracy': 0.72013, 'f1': 0.72035, 'accuracy-SBM': 0.72013, 'auc': 0.93645}
2025-07-11 09:51:21,220 - INFO - test: {'epoch': 31, 'time_epoch': 2.901, 'loss': 0.74676492, 'lr': 0, 'params': 425270, 'time_iter': 0.04605, 'accuracy': 0.73293, 'f1': 0.73302, 'accuracy-SBM': 0.73297, 'auc': 0.94435}
2025-07-11 09:51:21,222 - INFO - > Epoch 31: took 66.7s (avg 70.4s) | Best so far: epoch 28	train_loss: 0.7584 train_accuracy-SBM: 0.7242	val_loss: 0.7374 val_accuracy-SBM: 0.7292	test_loss: 0.7214 test_accuracy-SBM: 0.7355
2025-07-11 09:52:21,885 - INFO - train: {'epoch': 32, 'time_epoch': 60.44407, 'eta': 4274.52641, 'eta_hours': 1.18737, 'loss': 0.741509, 'lr': 0.00081359, 'params': 425270, 'time_iter': 0.09671, 'accuracy': 0.72959, 'f1': 0.7296, 'accuracy-SBM': 0.72959, 'auc': 0.94487}
2025-07-11 09:52:24,879 - INFO - val: {'epoch': 32, 'time_epoch': 2.94495, 'loss': 0.73524903, 'lr': 0, 'params': 425270, 'time_iter': 0.04675, 'accuracy': 0.73275, 'f1': 0.73272, 'accuracy-SBM': 0.73267, 'auc': 0.94625}
2025-07-11 09:52:27,805 - INFO - test: {'epoch': 32, 'time_epoch': 2.89514, 'loss': 0.7252448, 'lr': 0, 'params': 425270, 'time_iter': 0.04595, 'accuracy': 0.73676, 'f1': 0.73676, 'accuracy-SBM': 0.73677, 'auc': 0.94786}
2025-07-11 09:52:27,807 - INFO - > Epoch 32: took 66.6s (avg 70.3s) | Best so far: epoch 32	train_loss: 0.7415 train_accuracy-SBM: 0.7296	val_loss: 0.7352 val_accuracy-SBM: 0.7327	test_loss: 0.7252 test_accuracy-SBM: 0.7368
2025-07-11 09:53:31,095 - INFO - train: {'epoch': 33, 'time_epoch': 63.06564, 'eta': 4209.30411, 'eta_hours': 1.16925, 'loss': 0.73726249, 'lr': 0.00080054, 'params': 425270, 'time_iter': 0.10091, 'accuracy': 0.73122, 'f1': 0.73123, 'accuracy-SBM': 0.73122, 'auc': 0.94549}
2025-07-11 09:53:34,060 - INFO - val: {'epoch': 33, 'time_epoch': 2.92219, 'loss': 0.8058867, 'lr': 0, 'params': 425270, 'time_iter': 0.04638, 'accuracy': 0.7239, 'f1': 0.72409, 'accuracy-SBM': 0.72376, 'auc': 0.93532}
2025-07-11 09:53:36,995 - INFO - test: {'epoch': 33, 'time_epoch': 2.90475, 'loss': 0.7653224, 'lr': 0, 'params': 425270, 'time_iter': 0.04611, 'accuracy': 0.72981, 'f1': 0.72982, 'accuracy-SBM': 0.72976, 'auc': 0.94156}
2025-07-11 09:53:36,997 - INFO - > Epoch 33: took 69.2s (avg 70.3s) | Best so far: epoch 32	train_loss: 0.7415 train_accuracy-SBM: 0.7296	val_loss: 0.7352 val_accuracy-SBM: 0.7327	test_loss: 0.7252 test_accuracy-SBM: 0.7368
2025-07-11 09:54:38,559 - INFO - train: {'epoch': 34, 'time_epoch': 61.33271, 'eta': 4140.98675, 'eta_hours': 1.15027, 'loss': 0.73553948, 'lr': 0.00078716, 'params': 425270, 'time_iter': 0.09813, 'accuracy': 0.73183, 'f1': 0.73184, 'accuracy-SBM': 0.73183, 'auc': 0.94576}
2025-07-11 09:54:41,703 - INFO - val: {'epoch': 34, 'time_epoch': 3.10129, 'loss': 0.72279242, 'lr': 0, 'params': 425270, 'time_iter': 0.04923, 'accuracy': 0.73902, 'f1': 0.73901, 'accuracy-SBM': 0.73899, 'auc': 0.94792}
2025-07-11 09:54:44,819 - INFO - test: {'epoch': 34, 'time_epoch': 3.07623, 'loss': 0.71162521, 'lr': 0, 'params': 425270, 'time_iter': 0.04883, 'accuracy': 0.74002, 'f1': 0.73998, 'accuracy-SBM': 0.74004, 'auc': 0.94968}
2025-07-11 09:54:44,821 - INFO - > Epoch 34: took 67.8s (avg 70.2s) | Best so far: epoch 34	train_loss: 0.7355 train_accuracy-SBM: 0.7318	val_loss: 0.7228 val_accuracy-SBM: 0.7390	test_loss: 0.7116 test_accuracy-SBM: 0.7400
2025-07-11 09:55:45,781 - INFO - train: {'epoch': 35, 'time_epoch': 60.73767, 'eta': 4071.99958, 'eta_hours': 1.13111, 'loss': 0.7288309, 'lr': 0.00077347, 'params': 425270, 'time_iter': 0.09718, 'accuracy': 0.7334, 'f1': 0.73341, 'accuracy-SBM': 0.7334, 'auc': 0.94675}
2025-07-11 09:55:48,771 - INFO - val: {'epoch': 35, 'time_epoch': 2.94729, 'loss': 0.75584308, 'lr': 0, 'params': 425270, 'time_iter': 0.04678, 'accuracy': 0.73195, 'f1': 0.73188, 'accuracy-SBM': 0.7319, 'auc': 0.94294}
2025-07-11 09:55:51,779 - INFO - test: {'epoch': 35, 'time_epoch': 2.97644, 'loss': 0.73634316, 'lr': 0, 'params': 425270, 'time_iter': 0.04725, 'accuracy': 0.73627, 'f1': 0.73619, 'accuracy-SBM': 0.7362, 'auc': 0.94598}
2025-07-11 09:55:51,782 - INFO - > Epoch 35: took 67.0s (avg 70.1s) | Best so far: epoch 34	train_loss: 0.7355 train_accuracy-SBM: 0.7318	val_loss: 0.7228 val_accuracy-SBM: 0.7390	test_loss: 0.7116 test_accuracy-SBM: 0.7400
2025-07-11 09:56:52,646 - INFO - train: {'epoch': 36, 'time_epoch': 60.64167, 'eta': 4003.29487, 'eta_hours': 1.11203, 'loss': 0.72780703, 'lr': 0.00075948, 'params': 425270, 'time_iter': 0.09703, 'accuracy': 0.73434, 'f1': 0.73435, 'accuracy-SBM': 0.73434, 'auc': 0.9469}
2025-07-11 09:56:55,637 - INFO - val: {'epoch': 36, 'time_epoch': 2.94858, 'loss': 0.73339396, 'lr': 0, 'params': 425270, 'time_iter': 0.0468, 'accuracy': 0.73834, 'f1': 0.73832, 'accuracy-SBM': 0.73825, 'auc': 0.9462}
2025-07-11 09:56:58,564 - INFO - test: {'epoch': 36, 'time_epoch': 2.89672, 'loss': 0.7081953, 'lr': 0, 'params': 425270, 'time_iter': 0.04598, 'accuracy': 0.74342, 'f1': 0.74346, 'accuracy-SBM': 0.74341, 'auc': 0.95005}
2025-07-11 09:56:58,566 - INFO - > Epoch 36: took 66.8s (avg 70.0s) | Best so far: epoch 34	train_loss: 0.7355 train_accuracy-SBM: 0.7318	val_loss: 0.7228 val_accuracy-SBM: 0.7390	test_loss: 0.7116 test_accuracy-SBM: 0.7400
2025-07-11 09:57:59,383 - INFO - train: {'epoch': 37, 'time_epoch': 60.50037, 'eta': 3934.78399, 'eta_hours': 1.093, 'loss': 0.72448477, 'lr': 0.00074521, 'params': 425270, 'time_iter': 0.0968, 'accuracy': 0.73587, 'f1': 0.73588, 'accuracy-SBM': 0.73587, 'auc': 0.94737}
2025-07-11 09:58:02,376 - INFO - val: {'epoch': 37, 'time_epoch': 2.94997, 'loss': 0.70979785, 'lr': 0, 'params': 425270, 'time_iter': 0.04682, 'accuracy': 0.7412, 'f1': 0.74115, 'accuracy-SBM': 0.74146, 'auc': 0.95026}
2025-07-11 09:58:05,398 - INFO - test: {'epoch': 37, 'time_epoch': 2.99146, 'loss': 0.71081561, 'lr': 0, 'params': 425270, 'time_iter': 0.04748, 'accuracy': 0.7405, 'f1': 0.74034, 'accuracy-SBM': 0.74051, 'auc': 0.95026}
2025-07-11 09:58:05,400 - INFO - > Epoch 37: took 66.8s (avg 70.0s) | Best so far: epoch 37	train_loss: 0.7245 train_accuracy-SBM: 0.7359	val_loss: 0.7098 val_accuracy-SBM: 0.7415	test_loss: 0.7108 test_accuracy-SBM: 0.7405
2025-07-11 09:59:06,901 - INFO - train: {'epoch': 38, 'time_epoch': 61.28447, 'eta': 3867.91032, 'eta_hours': 1.07442, 'loss': 0.72234929, 'lr': 0.00073067, 'params': 425270, 'time_iter': 0.09806, 'accuracy': 0.73685, 'f1': 0.73687, 'accuracy-SBM': 0.73685, 'auc': 0.94769}
2025-07-11 09:59:09,877 - INFO - val: {'epoch': 38, 'time_epoch': 2.93365, 'loss': 0.71703995, 'lr': 0, 'params': 425270, 'time_iter': 0.04657, 'accuracy': 0.7439, 'f1': 0.74402, 'accuracy-SBM': 0.74388, 'auc': 0.94853}
2025-07-11 09:59:12,850 - INFO - test: {'epoch': 38, 'time_epoch': 2.94147, 'loss': 0.69822779, 'lr': 0, 'params': 425270, 'time_iter': 0.04669, 'accuracy': 0.7443, 'f1': 0.74444, 'accuracy-SBM': 0.74442, 'auc': 0.95159}
2025-07-11 09:59:12,852 - INFO - > Epoch 38: took 67.5s (avg 69.9s) | Best so far: epoch 38	train_loss: 0.7223 train_accuracy-SBM: 0.7369	val_loss: 0.7170 val_accuracy-SBM: 0.7439	test_loss: 0.6982 test_accuracy-SBM: 0.7444
2025-07-11 10:00:14,709 - INFO - train: {'epoch': 39, 'time_epoch': 61.53518, 'eta': 3801.69218, 'eta_hours': 1.05603, 'loss': 0.71826244, 'lr': 0.00071588, 'params': 425270, 'time_iter': 0.09846, 'accuracy': 0.738, 'f1': 0.73801, 'accuracy-SBM': 0.738, 'auc': 0.94829}
2025-07-11 10:00:17,704 - INFO - val: {'epoch': 39, 'time_epoch': 2.95236, 'loss': 0.71343036, 'lr': 0, 'params': 425270, 'time_iter': 0.04686, 'accuracy': 0.7446, 'f1': 0.74472, 'accuracy-SBM': 0.74458, 'auc': 0.94947}
2025-07-11 10:00:20,678 - INFO - test: {'epoch': 39, 'time_epoch': 2.94304, 'loss': 0.69277675, 'lr': 0, 'params': 425270, 'time_iter': 0.04671, 'accuracy': 0.74961, 'f1': 0.74966, 'accuracy-SBM': 0.74964, 'auc': 0.95259}
2025-07-11 10:00:20,680 - INFO - > Epoch 39: took 67.8s (avg 69.8s) | Best so far: epoch 39	train_loss: 0.7183 train_accuracy-SBM: 0.7380	val_loss: 0.7134 val_accuracy-SBM: 0.7446	test_loss: 0.6928 test_accuracy-SBM: 0.7496
2025-07-11 10:01:21,633 - INFO - train: {'epoch': 40, 'time_epoch': 60.72635, 'eta': 3734.53855, 'eta_hours': 1.03737, 'loss': 0.71748025, 'lr': 0.00070085, 'params': 425270, 'time_iter': 0.09716, 'accuracy': 0.73784, 'f1': 0.73784, 'accuracy-SBM': 0.73783, 'auc': 0.94842}
2025-07-11 10:01:24,673 - INFO - val: {'epoch': 40, 'time_epoch': 2.9962, 'loss': 0.71183768, 'lr': 0, 'params': 425270, 'time_iter': 0.04756, 'accuracy': 0.74051, 'f1': 0.74062, 'accuracy-SBM': 0.74049, 'auc': 0.94972}
2025-07-11 10:01:27,655 - INFO - test: {'epoch': 40, 'time_epoch': 2.95102, 'loss': 0.69736771, 'lr': 0, 'params': 425270, 'time_iter': 0.04684, 'accuracy': 0.74417, 'f1': 0.74414, 'accuracy-SBM': 0.74424, 'auc': 0.95192}
2025-07-11 10:01:27,657 - INFO - > Epoch 40: took 67.0s (avg 69.8s) | Best so far: epoch 39	train_loss: 0.7183 train_accuracy-SBM: 0.7380	val_loss: 0.7134 val_accuracy-SBM: 0.7446	test_loss: 0.6928 test_accuracy-SBM: 0.7496
2025-07-11 10:02:29,024 - INFO - train: {'epoch': 41, 'time_epoch': 60.944, 'eta': 3667.99154, 'eta_hours': 1.01889, 'loss': 0.7130735, 'lr': 0.0006856, 'params': 425270, 'time_iter': 0.09751, 'accuracy': 0.74033, 'f1': 0.74034, 'accuracy-SBM': 0.74033, 'auc': 0.94904}
2025-07-11 10:02:32,065 - INFO - val: {'epoch': 41, 'time_epoch': 2.9993, 'loss': 0.6984064, 'lr': 0, 'params': 425270, 'time_iter': 0.04761, 'accuracy': 0.74904, 'f1': 0.74905, 'accuracy-SBM': 0.74889, 'auc': 0.95126}
2025-07-11 10:02:35,059 - INFO - test: {'epoch': 41, 'time_epoch': 2.96302, 'loss': 0.69015931, 'lr': 0, 'params': 425270, 'time_iter': 0.04703, 'accuracy': 0.74733, 'f1': 0.74736, 'accuracy-SBM': 0.74737, 'auc': 0.9527}
2025-07-11 10:02:35,061 - INFO - > Epoch 41: took 67.4s (avg 69.7s) | Best so far: epoch 41	train_loss: 0.7131 train_accuracy-SBM: 0.7403	val_loss: 0.6984 val_accuracy-SBM: 0.7489	test_loss: 0.6902 test_accuracy-SBM: 0.7474
2025-07-11 10:03:36,529 - INFO - train: {'epoch': 42, 'time_epoch': 61.24316, 'eta': 3602.10171, 'eta_hours': 1.00058, 'loss': 0.70765343, 'lr': 0.00067015, 'params': 425270, 'time_iter': 0.09799, 'accuracy': 0.74225, 'f1': 0.74226, 'accuracy-SBM': 0.74225, 'auc': 0.94982}
2025-07-11 10:03:39,511 - INFO - val: {'epoch': 42, 'time_epoch': 2.94039, 'loss': 0.70590652, 'lr': 0, 'params': 425270, 'time_iter': 0.04667, 'accuracy': 0.74626, 'f1': 0.74619, 'accuracy-SBM': 0.74613, 'auc': 0.95041}
2025-07-11 10:03:42,449 - INFO - test: {'epoch': 42, 'time_epoch': 2.90728, 'loss': 0.68826709, 'lr': 0, 'params': 425270, 'time_iter': 0.04615, 'accuracy': 0.74889, 'f1': 0.74888, 'accuracy-SBM': 0.74885, 'auc': 0.95312}
2025-07-11 10:03:42,451 - INFO - > Epoch 42: took 67.4s (avg 69.7s) | Best so far: epoch 41	train_loss: 0.7131 train_accuracy-SBM: 0.7403	val_loss: 0.6984 val_accuracy-SBM: 0.7489	test_loss: 0.6902 test_accuracy-SBM: 0.7474
2025-07-11 10:04:43,517 - INFO - train: {'epoch': 43, 'time_epoch': 60.74508, 'eta': 3535.78916, 'eta_hours': 0.98216, 'loss': 0.70788543, 'lr': 0.00065451, 'params': 425270, 'time_iter': 0.09719, 'accuracy': 0.74164, 'f1': 0.74165, 'accuracy-SBM': 0.74164, 'auc': 0.94981}
2025-07-11 10:04:46,528 - INFO - val: {'epoch': 43, 'time_epoch': 2.9677, 'loss': 0.71197497, 'lr': 0, 'params': 425270, 'time_iter': 0.04711, 'accuracy': 0.74645, 'f1': 0.74635, 'accuracy-SBM': 0.74633, 'auc': 0.94945}
2025-07-11 10:04:49,502 - INFO - test: {'epoch': 43, 'time_epoch': 2.94342, 'loss': 0.68568977, 'lr': 0, 'params': 425270, 'time_iter': 0.04672, 'accuracy': 0.75106, 'f1': 0.75099, 'accuracy-SBM': 0.75099, 'auc': 0.95328}
2025-07-11 10:04:49,504 - INFO - > Epoch 43: took 67.1s (avg 69.6s) | Best so far: epoch 41	train_loss: 0.7131 train_accuracy-SBM: 0.7403	val_loss: 0.6984 val_accuracy-SBM: 0.7489	test_loss: 0.6902 test_accuracy-SBM: 0.7474
2025-07-11 10:05:50,929 - INFO - train: {'epoch': 44, 'time_epoch': 61.1999, 'eta': 3470.27994, 'eta_hours': 0.96397, 'loss': 0.70366513, 'lr': 0.0006387, 'params': 425270, 'time_iter': 0.09792, 'accuracy': 0.74309, 'f1': 0.7431, 'accuracy-SBM': 0.74309, 'auc': 0.9504}
2025-07-11 10:05:53,933 - INFO - val: {'epoch': 44, 'time_epoch': 2.96105, 'loss': 0.80355886, 'lr': 0, 'params': 425270, 'time_iter': 0.047, 'accuracy': 0.72251, 'f1': 0.72225, 'accuracy-SBM': 0.7225, 'auc': 0.93615}
2025-07-11 10:05:56,981 - INFO - test: {'epoch': 44, 'time_epoch': 3.00887, 'loss': 0.76751853, 'lr': 0, 'params': 425270, 'time_iter': 0.04776, 'accuracy': 0.72963, 'f1': 0.72937, 'accuracy-SBM': 0.72948, 'auc': 0.94163}
2025-07-11 10:05:57,151 - INFO - > Epoch 44: took 67.6s (avg 69.6s) | Best so far: epoch 41	train_loss: 0.7131 train_accuracy-SBM: 0.7403	val_loss: 0.6984 val_accuracy-SBM: 0.7489	test_loss: 0.6902 test_accuracy-SBM: 0.7474
2025-07-11 10:06:58,536 - INFO - train: {'epoch': 45, 'time_epoch': 61.15988, 'eta': 3404.91111, 'eta_hours': 0.94581, 'loss': 0.70127477, 'lr': 0.00062274, 'params': 425270, 'time_iter': 0.09786, 'accuracy': 0.74452, 'f1': 0.74453, 'accuracy-SBM': 0.74452, 'auc': 0.95071}
2025-07-11 10:07:01,532 - INFO - val: {'epoch': 45, 'time_epoch': 2.95304, 'loss': 0.73501685, 'lr': 0, 'params': 425270, 'time_iter': 0.04687, 'accuracy': 0.74089, 'f1': 0.74075, 'accuracy-SBM': 0.7409, 'auc': 0.94617}
2025-07-11 10:07:04,496 - INFO - test: {'epoch': 45, 'time_epoch': 2.91645, 'loss': 0.68976183, 'lr': 0, 'params': 425270, 'time_iter': 0.04629, 'accuracy': 0.75014, 'f1': 0.75001, 'accuracy-SBM': 0.75004, 'auc': 0.95266}
2025-07-11 10:07:04,499 - INFO - > Epoch 45: took 67.3s (avg 69.5s) | Best so far: epoch 41	train_loss: 0.7131 train_accuracy-SBM: 0.7403	val_loss: 0.6984 val_accuracy-SBM: 0.7489	test_loss: 0.6902 test_accuracy-SBM: 0.7474
2025-07-11 10:08:07,425 - INFO - train: {'epoch': 46, 'time_epoch': 62.59899, 'eta': 3341.34422, 'eta_hours': 0.92815, 'loss': 0.69938087, 'lr': 0.00060665, 'params': 425270, 'time_iter': 0.10016, 'accuracy': 0.74472, 'f1': 0.74473, 'accuracy-SBM': 0.74472, 'auc': 0.951}
2025-07-11 10:08:10,488 - INFO - val: {'epoch': 46, 'time_epoch': 3.02019, 'loss': 0.6951956, 'lr': 0, 'params': 425270, 'time_iter': 0.04794, 'accuracy': 0.74924, 'f1': 0.74919, 'accuracy-SBM': 0.74907, 'auc': 0.95185}
2025-07-11 10:08:13,523 - INFO - test: {'epoch': 46, 'time_epoch': 3.0024, 'loss': 0.67489937, 'lr': 0, 'params': 425270, 'time_iter': 0.04766, 'accuracy': 0.75349, 'f1': 0.75345, 'accuracy-SBM': 0.75347, 'auc': 0.95486}
2025-07-11 10:08:13,525 - INFO - > Epoch 46: took 69.0s (avg 69.5s) | Best so far: epoch 46	train_loss: 0.6994 train_accuracy-SBM: 0.7447	val_loss: 0.6952 val_accuracy-SBM: 0.7491	test_loss: 0.6749 test_accuracy-SBM: 0.7535
2025-07-11 10:09:15,013 - INFO - train: {'epoch': 47, 'time_epoch': 61.156, 'eta': 3276.2544, 'eta_hours': 0.91007, 'loss': 0.69739435, 'lr': 0.00059044, 'params': 425270, 'time_iter': 0.09785, 'accuracy': 0.74561, 'f1': 0.74562, 'accuracy-SBM': 0.7456, 'auc': 0.95127}
2025-07-11 10:09:17,976 - INFO - val: {'epoch': 47, 'time_epoch': 2.91986, 'loss': 0.68328037, 'lr': 0, 'params': 425270, 'time_iter': 0.04635, 'accuracy': 0.75281, 'f1': 0.75279, 'accuracy-SBM': 0.75252, 'auc': 0.95359}
2025-07-11 10:09:20,902 - INFO - test: {'epoch': 47, 'time_epoch': 2.89571, 'loss': 0.68249426, 'lr': 0, 'params': 425270, 'time_iter': 0.04596, 'accuracy': 0.75299, 'f1': 0.75301, 'accuracy-SBM': 0.75295, 'auc': 0.95371}
2025-07-11 10:09:20,904 - INFO - > Epoch 47: took 67.4s (avg 69.5s) | Best so far: epoch 47	train_loss: 0.6974 train_accuracy-SBM: 0.7456	val_loss: 0.6833 val_accuracy-SBM: 0.7525	test_loss: 0.6825 test_accuracy-SBM: 0.7530
2025-07-11 10:10:23,120 - INFO - train: {'epoch': 48, 'time_epoch': 61.98823, 'eta': 3212.19135, 'eta_hours': 0.89228, 'loss': 0.69572505, 'lr': 0.00057413, 'params': 425270, 'time_iter': 0.09918, 'accuracy': 0.74577, 'f1': 0.74578, 'accuracy-SBM': 0.74577, 'auc': 0.9515}
2025-07-11 10:10:26,140 - INFO - val: {'epoch': 48, 'time_epoch': 2.97749, 'loss': 0.6868221, 'lr': 0, 'params': 425270, 'time_iter': 0.04726, 'accuracy': 0.75255, 'f1': 0.75253, 'accuracy-SBM': 0.75263, 'auc': 0.95286}
2025-07-11 10:10:29,114 - INFO - test: {'epoch': 48, 'time_epoch': 2.94338, 'loss': 0.67805356, 'lr': 0, 'params': 425270, 'time_iter': 0.04672, 'accuracy': 0.7531, 'f1': 0.75303, 'accuracy-SBM': 0.75308, 'auc': 0.95421}
2025-07-11 10:10:29,116 - INFO - > Epoch 48: took 68.2s (avg 69.4s) | Best so far: epoch 48	train_loss: 0.6957 train_accuracy-SBM: 0.7458	val_loss: 0.6868 val_accuracy-SBM: 0.7526	test_loss: 0.6781 test_accuracy-SBM: 0.7531
2025-07-11 10:11:32,010 - INFO - train: {'epoch': 49, 'time_epoch': 62.66676, 'eta': 3148.88982, 'eta_hours': 0.87469, 'loss': 0.69121929, 'lr': 0.00055774, 'params': 425270, 'time_iter': 0.10027, 'accuracy': 0.74776, 'f1': 0.74777, 'accuracy-SBM': 0.74776, 'auc': 0.95215}
2025-07-11 10:11:35,083 - INFO - val: {'epoch': 49, 'time_epoch': 3.0289, 'loss': 0.70955777, 'lr': 0, 'params': 425270, 'time_iter': 0.04808, 'accuracy': 0.74373, 'f1': 0.74361, 'accuracy-SBM': 0.74369, 'auc': 0.95}
2025-07-11 10:11:38,121 - INFO - test: {'epoch': 49, 'time_epoch': 3.00609, 'loss': 0.68549544, 'lr': 0, 'params': 425270, 'time_iter': 0.04772, 'accuracy': 0.75076, 'f1': 0.75054, 'accuracy-SBM': 0.75078, 'auc': 0.95339}
2025-07-11 10:11:38,123 - INFO - > Epoch 49: took 69.0s (avg 69.4s) | Best so far: epoch 48	train_loss: 0.6957 train_accuracy-SBM: 0.7458	val_loss: 0.6868 val_accuracy-SBM: 0.7526	test_loss: 0.6781 test_accuracy-SBM: 0.7531
2025-07-11 10:12:39,270 - INFO - train: {'epoch': 50, 'time_epoch': 60.92354, 'eta': 3083.93833, 'eta_hours': 0.85665, 'loss': 0.69117061, 'lr': 0.00054129, 'params': 425270, 'time_iter': 0.09748, 'accuracy': 0.74829, 'f1': 0.7483, 'accuracy-SBM': 0.7483, 'auc': 0.95212}
2025-07-11 10:12:42,222 - INFO - val: {'epoch': 50, 'time_epoch': 2.90791, 'loss': 0.70169271, 'lr': 0, 'params': 425270, 'time_iter': 0.04616, 'accuracy': 0.74847, 'f1': 0.74843, 'accuracy-SBM': 0.7485, 'auc': 0.95071}
2025-07-11 10:12:45,142 - INFO - test: {'epoch': 50, 'time_epoch': 2.89006, 'loss': 0.70528127, 'lr': 0, 'params': 425270, 'time_iter': 0.04587, 'accuracy': 0.75013, 'f1': 0.75011, 'accuracy-SBM': 0.75016, 'auc': 0.95019}
2025-07-11 10:12:45,144 - INFO - > Epoch 50: took 67.0s (avg 69.4s) | Best so far: epoch 48	train_loss: 0.6957 train_accuracy-SBM: 0.7458	val_loss: 0.6868 val_accuracy-SBM: 0.7526	test_loss: 0.6781 test_accuracy-SBM: 0.7531
2025-07-11 10:13:46,007 - INFO - train: {'epoch': 51, 'time_epoch': 60.54169, 'eta': 3018.78928, 'eta_hours': 0.83855, 'loss': 0.6861978, 'lr': 0.00052479, 'params': 425270, 'time_iter': 0.09687, 'accuracy': 0.74951, 'f1': 0.74952, 'accuracy-SBM': 0.74951, 'auc': 0.95283}
2025-07-11 10:13:48,980 - INFO - val: {'epoch': 51, 'time_epoch': 2.93177, 'loss': 0.68087284, 'lr': 0, 'params': 425270, 'time_iter': 0.04654, 'accuracy': 0.75268, 'f1': 0.75264, 'accuracy-SBM': 0.75255, 'auc': 0.95368}
2025-07-11 10:13:51,937 - INFO - test: {'epoch': 51, 'time_epoch': 2.92606, 'loss': 0.67349179, 'lr': 0, 'params': 425270, 'time_iter': 0.04645, 'accuracy': 0.75478, 'f1': 0.75481, 'accuracy-SBM': 0.75487, 'auc': 0.95485}
2025-07-11 10:13:51,939 - INFO - > Epoch 51: took 66.8s (avg 69.3s) | Best so far: epoch 48	train_loss: 0.6957 train_accuracy-SBM: 0.7458	val_loss: 0.6868 val_accuracy-SBM: 0.7526	test_loss: 0.6781 test_accuracy-SBM: 0.7531
2025-07-11 10:14:52,986 - INFO - train: {'epoch': 52, 'time_epoch': 60.71682, 'eta': 2953.9694, 'eta_hours': 0.82055, 'loss': 0.68406916, 'lr': 0.00050827, 'params': 425270, 'time_iter': 0.09715, 'accuracy': 0.75078, 'f1': 0.75078, 'accuracy-SBM': 0.75078, 'auc': 0.95313}
2025-07-11 10:14:56,056 - INFO - val: {'epoch': 52, 'time_epoch': 3.01188, 'loss': 0.69238103, 'lr': 0, 'params': 425270, 'time_iter': 0.04781, 'accuracy': 0.74866, 'f1': 0.74865, 'accuracy-SBM': 0.74872, 'auc': 0.95212}
2025-07-11 10:14:59,028 - INFO - test: {'epoch': 52, 'time_epoch': 2.94124, 'loss': 0.68375252, 'lr': 0, 'params': 425270, 'time_iter': 0.04669, 'accuracy': 0.7516, 'f1': 0.75162, 'accuracy-SBM': 0.75172, 'auc': 0.95337}
2025-07-11 10:14:59,030 - INFO - > Epoch 52: took 67.1s (avg 69.3s) | Best so far: epoch 48	train_loss: 0.6957 train_accuracy-SBM: 0.7458	val_loss: 0.6868 val_accuracy-SBM: 0.7526	test_loss: 0.6781 test_accuracy-SBM: 0.7531
2025-07-11 10:16:00,377 - INFO - train: {'epoch': 53, 'time_epoch': 61.12423, 'eta': 2889.64853, 'eta_hours': 0.80268, 'loss': 0.67951231, 'lr': 0.00049173, 'params': 425270, 'time_iter': 0.0978, 'accuracy': 0.75255, 'f1': 0.75255, 'accuracy-SBM': 0.75255, 'auc': 0.95374}
2025-07-11 10:16:03,397 - INFO - val: {'epoch': 53, 'time_epoch': 2.97774, 'loss': 0.7559136, 'lr': 0, 'params': 425270, 'time_iter': 0.04727, 'accuracy': 0.74349, 'f1': 0.74354, 'accuracy-SBM': 0.74357, 'auc': 0.94288}
2025-07-11 10:16:06,367 - INFO - test: {'epoch': 53, 'time_epoch': 2.93874, 'loss': 0.72437565, 'lr': 0, 'params': 425270, 'time_iter': 0.04665, 'accuracy': 0.74607, 'f1': 0.74606, 'accuracy-SBM': 0.74612, 'auc': 0.94754}
2025-07-11 10:16:06,369 - INFO - > Epoch 53: took 67.3s (avg 69.2s) | Best so far: epoch 48	train_loss: 0.6957 train_accuracy-SBM: 0.7458	val_loss: 0.6868 val_accuracy-SBM: 0.7526	test_loss: 0.6781 test_accuracy-SBM: 0.7531
2025-07-11 10:17:07,392 - INFO - train: {'epoch': 54, 'time_epoch': 60.79742, 'eta': 2825.17652, 'eta_hours': 0.78477, 'loss': 0.6791976, 'lr': 0.00047521, 'params': 425270, 'time_iter': 0.09728, 'accuracy': 0.75213, 'f1': 0.75213, 'accuracy-SBM': 0.75212, 'auc': 0.95379}
2025-07-11 10:17:10,387 - INFO - val: {'epoch': 54, 'time_epoch': 2.95285, 'loss': 0.68360782, 'lr': 0, 'params': 425270, 'time_iter': 0.04687, 'accuracy': 0.75352, 'f1': 0.75348, 'accuracy-SBM': 0.75331, 'auc': 0.9535}
2025-07-11 10:17:13,354 - INFO - test: {'epoch': 54, 'time_epoch': 2.93642, 'loss': 0.67094141, 'lr': 0, 'params': 425270, 'time_iter': 0.04661, 'accuracy': 0.75665, 'f1': 0.75675, 'accuracy-SBM': 0.75673, 'auc': 0.95548}
2025-07-11 10:17:13,357 - INFO - > Epoch 54: took 67.0s (avg 69.2s) | Best so far: epoch 54	train_loss: 0.6792 train_accuracy-SBM: 0.7521	val_loss: 0.6836 val_accuracy-SBM: 0.7533	test_loss: 0.6709 test_accuracy-SBM: 0.7567
2025-07-11 10:18:14,881 - INFO - train: {'epoch': 55, 'time_epoch': 61.29411, 'eta': 2761.226, 'eta_hours': 0.76701, 'loss': 0.67688263, 'lr': 0.00045871, 'params': 425270, 'time_iter': 0.09807, 'accuracy': 0.75345, 'f1': 0.75345, 'accuracy-SBM': 0.75345, 'auc': 0.95412}
2025-07-11 10:18:17,886 - INFO - val: {'epoch': 55, 'time_epoch': 2.96305, 'loss': 0.66363154, 'lr': 0, 'params': 425270, 'time_iter': 0.04703, 'accuracy': 0.75778, 'f1': 0.75765, 'accuracy-SBM': 0.75753, 'auc': 0.95606}
2025-07-11 10:18:20,882 - INFO - test: {'epoch': 55, 'time_epoch': 2.96436, 'loss': 0.65499588, 'lr': 0, 'params': 425270, 'time_iter': 0.04705, 'accuracy': 0.76199, 'f1': 0.76201, 'accuracy-SBM': 0.76198, 'auc': 0.95729}
2025-07-11 10:18:20,884 - INFO - > Epoch 55: took 67.5s (avg 69.2s) | Best so far: epoch 55	train_loss: 0.6769 train_accuracy-SBM: 0.7534	val_loss: 0.6636 val_accuracy-SBM: 0.7575	test_loss: 0.6550 test_accuracy-SBM: 0.7620
2025-07-11 10:19:22,061 - INFO - train: {'epoch': 56, 'time_epoch': 60.95537, 'eta': 2697.11314, 'eta_hours': 0.7492, 'loss': 0.67256619, 'lr': 0.00044226, 'params': 425270, 'time_iter': 0.09753, 'accuracy': 0.75436, 'f1': 0.75436, 'accuracy-SBM': 0.75436, 'auc': 0.95472}
2025-07-11 10:19:25,091 - INFO - val: {'epoch': 56, 'time_epoch': 2.9764, 'loss': 0.67672366, 'lr': 0, 'params': 425270, 'time_iter': 0.04724, 'accuracy': 0.75647, 'f1': 0.75628, 'accuracy-SBM': 0.75637, 'auc': 0.9543}
2025-07-11 10:19:28,047 - INFO - test: {'epoch': 56, 'time_epoch': 2.92409, 'loss': 0.66455781, 'lr': 0, 'params': 425270, 'time_iter': 0.04641, 'accuracy': 0.75815, 'f1': 0.75806, 'accuracy-SBM': 0.75807, 'auc': 0.956}
2025-07-11 10:19:28,049 - INFO - > Epoch 56: took 67.2s (avg 69.1s) | Best so far: epoch 55	train_loss: 0.6769 train_accuracy-SBM: 0.7534	val_loss: 0.6636 val_accuracy-SBM: 0.7575	test_loss: 0.6550 test_accuracy-SBM: 0.7620
2025-07-11 10:20:29,762 - INFO - train: {'epoch': 57, 'time_epoch': 61.48466, 'eta': 2633.49244, 'eta_hours': 0.73153, 'loss': 0.67132855, 'lr': 0.00042587, 'params': 425270, 'time_iter': 0.09838, 'accuracy': 0.75452, 'f1': 0.75453, 'accuracy-SBM': 0.75452, 'auc': 0.95488}
2025-07-11 10:20:32,886 - INFO - val: {'epoch': 57, 'time_epoch': 3.08021, 'loss': 0.72268845, 'lr': 0, 'params': 425270, 'time_iter': 0.04889, 'accuracy': 0.7483, 'f1': 0.74838, 'accuracy-SBM': 0.74842, 'auc': 0.94778}
2025-07-11 10:20:35,994 - INFO - test: {'epoch': 57, 'time_epoch': 3.07672, 'loss': 0.7102265, 'lr': 0, 'params': 425270, 'time_iter': 0.04884, 'accuracy': 0.75081, 'f1': 0.7508, 'accuracy-SBM': 0.75094, 'auc': 0.94962}
2025-07-11 10:20:35,997 - INFO - > Epoch 57: took 67.9s (avg 69.1s) | Best so far: epoch 55	train_loss: 0.6769 train_accuracy-SBM: 0.7534	val_loss: 0.6636 val_accuracy-SBM: 0.7575	test_loss: 0.6550 test_accuracy-SBM: 0.7620
2025-07-11 10:21:39,245 - INFO - train: {'epoch': 58, 'time_epoch': 63.0155, 'eta': 2571.00796, 'eta_hours': 0.71417, 'loss': 0.66957678, 'lr': 0.00040956, 'params': 425270, 'time_iter': 0.10082, 'accuracy': 0.75579, 'f1': 0.7558, 'accuracy-SBM': 0.75579, 'auc': 0.9551}
2025-07-11 10:21:42,395 - INFO - val: {'epoch': 58, 'time_epoch': 3.10687, 'loss': 0.67514793, 'lr': 0, 'params': 425270, 'time_iter': 0.04932, 'accuracy': 0.75543, 'f1': 0.75547, 'accuracy-SBM': 0.75545, 'auc': 0.95441}
2025-07-11 10:21:45,531 - INFO - test: {'epoch': 58, 'time_epoch': 3.10485, 'loss': 0.66784122, 'lr': 0, 'params': 425270, 'time_iter': 0.04928, 'accuracy': 0.7568, 'f1': 0.75681, 'accuracy-SBM': 0.7569, 'auc': 0.95553}
2025-07-11 10:21:45,533 - INFO - > Epoch 58: took 69.5s (avg 69.1s) | Best so far: epoch 55	train_loss: 0.6769 train_accuracy-SBM: 0.7534	val_loss: 0.6636 val_accuracy-SBM: 0.7575	test_loss: 0.6550 test_accuracy-SBM: 0.7620
2025-07-11 10:22:48,650 - INFO - train: {'epoch': 59, 'time_epoch': 62.68197, 'eta': 2508.28342, 'eta_hours': 0.69675, 'loss': 0.6675761, 'lr': 0.00039335, 'params': 425270, 'time_iter': 0.10029, 'accuracy': 0.75629, 'f1': 0.7563, 'accuracy-SBM': 0.75629, 'auc': 0.95539}
2025-07-11 10:22:51,818 - INFO - val: {'epoch': 59, 'time_epoch': 3.12385, 'loss': 0.68613637, 'lr': 0, 'params': 425270, 'time_iter': 0.04958, 'accuracy': 0.75549, 'f1': 0.75524, 'accuracy-SBM': 0.75501, 'auc': 0.9533}
2025-07-11 10:22:54,970 - INFO - test: {'epoch': 59, 'time_epoch': 3.12129, 'loss': 0.66162467, 'lr': 0, 'params': 425270, 'time_iter': 0.04954, 'accuracy': 0.76024, 'f1': 0.76024, 'accuracy-SBM': 0.76005, 'auc': 0.95669}
2025-07-11 10:22:54,972 - INFO - > Epoch 59: took 69.4s (avg 69.1s) | Best so far: epoch 55	train_loss: 0.6769 train_accuracy-SBM: 0.7534	val_loss: 0.6636 val_accuracy-SBM: 0.7575	test_loss: 0.6550 test_accuracy-SBM: 0.7620
2025-07-11 10:23:56,291 - INFO - train: {'epoch': 60, 'time_epoch': 61.10092, 'eta': 2444.54944, 'eta_hours': 0.67904, 'loss': 0.66316609, 'lr': 0.00037726, 'params': 425270, 'time_iter': 0.09776, 'accuracy': 0.75788, 'f1': 0.75789, 'accuracy-SBM': 0.75788, 'auc': 0.95596}
2025-07-11 10:23:59,391 - INFO - val: {'epoch': 60, 'time_epoch': 3.05728, 'loss': 0.66983125, 'lr': 0, 'params': 425270, 'time_iter': 0.04853, 'accuracy': 0.75911, 'f1': 0.759, 'accuracy-SBM': 0.75895, 'auc': 0.9551}
2025-07-11 10:24:02,471 - INFO - test: {'epoch': 60, 'time_epoch': 3.04877, 'loss': 0.65696091, 'lr': 0, 'params': 425270, 'time_iter': 0.04839, 'accuracy': 0.76104, 'f1': 0.76101, 'accuracy-SBM': 0.76098, 'auc': 0.95693}
2025-07-11 10:24:02,473 - INFO - > Epoch 60: took 67.5s (avg 69.1s) | Best so far: epoch 60	train_loss: 0.6632 train_accuracy-SBM: 0.7579	val_loss: 0.6698 val_accuracy-SBM: 0.7590	test_loss: 0.6570 test_accuracy-SBM: 0.7610
2025-07-11 10:25:05,441 - INFO - train: {'epoch': 61, 'time_epoch': 62.73432, 'eta': 2381.90152, 'eta_hours': 0.66164, 'loss': 0.66063546, 'lr': 0.0003613, 'params': 425270, 'time_iter': 0.10037, 'accuracy': 0.759, 'f1': 0.75901, 'accuracy-SBM': 0.759, 'auc': 0.95631}
2025-07-11 10:25:08,458 - INFO - val: {'epoch': 61, 'time_epoch': 2.97455, 'loss': 0.67612217, 'lr': 0, 'params': 425270, 'time_iter': 0.04722, 'accuracy': 0.75407, 'f1': 0.75411, 'accuracy-SBM': 0.75398, 'auc': 0.95443}
2025-07-11 10:25:11,484 - INFO - test: {'epoch': 61, 'time_epoch': 2.99493, 'loss': 0.67444112, 'lr': 0, 'params': 425270, 'time_iter': 0.04754, 'accuracy': 0.75671, 'f1': 0.75676, 'accuracy-SBM': 0.75686, 'auc': 0.95469}
2025-07-11 10:25:11,486 - INFO - > Epoch 61: took 69.0s (avg 69.1s) | Best so far: epoch 60	train_loss: 0.6632 train_accuracy-SBM: 0.7579	val_loss: 0.6698 val_accuracy-SBM: 0.7590	test_loss: 0.6570 test_accuracy-SBM: 0.7610
2025-07-11 10:26:13,709 - INFO - train: {'epoch': 62, 'time_epoch': 61.9752, 'eta': 2318.80502, 'eta_hours': 0.64411, 'loss': 0.6601931, 'lr': 0.00034549, 'params': 425270, 'time_iter': 0.09916, 'accuracy': 0.75891, 'f1': 0.75891, 'accuracy-SBM': 0.75891, 'auc': 0.95637}
2025-07-11 10:26:16,675 - INFO - val: {'epoch': 62, 'time_epoch': 2.92237, 'loss': 0.67149109, 'lr': 0, 'params': 425270, 'time_iter': 0.04639, 'accuracy': 0.75897, 'f1': 0.75888, 'accuracy-SBM': 0.75885, 'auc': 0.95491}
2025-07-11 10:26:19,640 - INFO - test: {'epoch': 62, 'time_epoch': 2.89821, 'loss': 0.656641, 'lr': 0, 'params': 425270, 'time_iter': 0.046, 'accuracy': 0.76132, 'f1': 0.7613, 'accuracy-SBM': 0.76133, 'auc': 0.95702}
2025-07-11 10:26:19,688 - INFO - > Epoch 62: took 68.2s (avg 69.1s) | Best so far: epoch 60	train_loss: 0.6632 train_accuracy-SBM: 0.7579	val_loss: 0.6698 val_accuracy-SBM: 0.7590	test_loss: 0.6570 test_accuracy-SBM: 0.7610
2025-07-11 10:27:20,341 - INFO - train: {'epoch': 63, 'time_epoch': 60.42755, 'eta': 2254.87301, 'eta_hours': 0.62635, 'loss': 0.65474364, 'lr': 0.00032985, 'params': 425270, 'time_iter': 0.09668, 'accuracy': 0.76081, 'f1': 0.76082, 'accuracy-SBM': 0.76081, 'auc': 0.95708}
2025-07-11 10:27:23,283 - INFO - val: {'epoch': 63, 'time_epoch': 2.90054, 'loss': 0.65453874, 'lr': 0, 'params': 425270, 'time_iter': 0.04604, 'accuracy': 0.76157, 'f1': 0.76144, 'accuracy-SBM': 0.76153, 'auc': 0.95724}
2025-07-11 10:27:26,239 - INFO - test: {'epoch': 63, 'time_epoch': 2.9159, 'loss': 0.65172956, 'lr': 0, 'params': 425270, 'time_iter': 0.04628, 'accuracy': 0.76376, 'f1': 0.76376, 'accuracy-SBM': 0.76376, 'auc': 0.95772}
2025-07-11 10:27:26,241 - INFO - > Epoch 63: took 66.6s (avg 69.0s) | Best so far: epoch 63	train_loss: 0.6547 train_accuracy-SBM: 0.7608	val_loss: 0.6545 val_accuracy-SBM: 0.7615	test_loss: 0.6517 test_accuracy-SBM: 0.7638
2025-07-11 10:28:27,469 - INFO - train: {'epoch': 64, 'time_epoch': 61.00147, 'eta': 2191.35786, 'eta_hours': 0.60871, 'loss': 0.65541495, 'lr': 0.0003144, 'params': 425270, 'time_iter': 0.0976, 'accuracy': 0.761, 'f1': 0.761, 'accuracy-SBM': 0.76099, 'auc': 0.95699}
2025-07-11 10:28:30,478 - INFO - val: {'epoch': 64, 'time_epoch': 2.96688, 'loss': 0.67917256, 'lr': 0, 'params': 425270, 'time_iter': 0.04709, 'accuracy': 0.75662, 'f1': 0.75653, 'accuracy-SBM': 0.7567, 'auc': 0.95404}
2025-07-11 10:28:33,505 - INFO - test: {'epoch': 64, 'time_epoch': 2.98828, 'loss': 0.65375369, 'lr': 0, 'params': 425270, 'time_iter': 0.04743, 'accuracy': 0.76457, 'f1': 0.76452, 'accuracy-SBM': 0.76454, 'auc': 0.95741}
2025-07-11 10:28:33,508 - INFO - > Epoch 64: took 67.3s (avg 69.0s) | Best so far: epoch 63	train_loss: 0.6547 train_accuracy-SBM: 0.7608	val_loss: 0.6545 val_accuracy-SBM: 0.7615	test_loss: 0.6517 test_accuracy-SBM: 0.7638
2025-07-11 10:29:34,812 - INFO - train: {'epoch': 65, 'time_epoch': 60.82966, 'eta': 2127.83037, 'eta_hours': 0.59106, 'loss': 0.65306384, 'lr': 0.00029915, 'params': 425270, 'time_iter': 0.09733, 'accuracy': 0.76204, 'f1': 0.76205, 'accuracy-SBM': 0.76204, 'auc': 0.9573}
2025-07-11 10:29:37,772 - INFO - val: {'epoch': 65, 'time_epoch': 2.919, 'loss': 0.65780376, 'lr': 0, 'params': 425270, 'time_iter': 0.04633, 'accuracy': 0.76116, 'f1': 0.76118, 'accuracy-SBM': 0.76131, 'auc': 0.95687}
2025-07-11 10:29:40,812 - INFO - test: {'epoch': 65, 'time_epoch': 3.00778, 'loss': 0.65493672, 'lr': 0, 'params': 425270, 'time_iter': 0.04774, 'accuracy': 0.7607, 'f1': 0.76064, 'accuracy-SBM': 0.76079, 'auc': 0.95734}
2025-07-11 10:29:40,814 - INFO - > Epoch 65: took 67.3s (avg 69.0s) | Best so far: epoch 63	train_loss: 0.6547 train_accuracy-SBM: 0.7608	val_loss: 0.6545 val_accuracy-SBM: 0.7615	test_loss: 0.6517 test_accuracy-SBM: 0.7638
2025-07-11 10:30:45,735 - INFO - train: {'epoch': 66, 'time_epoch': 64.68906, 'eta': 2066.28432, 'eta_hours': 0.57397, 'loss': 0.65032868, 'lr': 0.00028412, 'params': 425270, 'time_iter': 0.1035, 'accuracy': 0.76291, 'f1': 0.76291, 'accuracy-SBM': 0.76291, 'auc': 0.95766}
2025-07-11 10:30:49,132 - INFO - val: {'epoch': 66, 'time_epoch': 3.35342, 'loss': 0.69239185, 'lr': 0, 'params': 425270, 'time_iter': 0.05323, 'accuracy': 0.75742, 'f1': 0.75745, 'accuracy-SBM': 0.75752, 'auc': 0.95209}
2025-07-11 10:30:52,520 - INFO - test: {'epoch': 66, 'time_epoch': 3.35621, 'loss': 0.6677253, 'lr': 0, 'params': 425270, 'time_iter': 0.05327, 'accuracy': 0.76176, 'f1': 0.7617, 'accuracy-SBM': 0.76177, 'auc': 0.95545}
2025-07-11 10:30:52,523 - INFO - > Epoch 66: took 71.7s (avg 69.0s) | Best so far: epoch 63	train_loss: 0.6547 train_accuracy-SBM: 0.7608	val_loss: 0.6545 val_accuracy-SBM: 0.7615	test_loss: 0.6517 test_accuracy-SBM: 0.7638
2025-07-11 10:31:58,656 - INFO - train: {'epoch': 67, 'time_epoch': 65.89883, 'eta': 2005.21513, 'eta_hours': 0.557, 'loss': 0.64644344, 'lr': 0.00026933, 'params': 425270, 'time_iter': 0.10544, 'accuracy': 0.76386, 'f1': 0.76386, 'accuracy-SBM': 0.76386, 'auc': 0.95818}
2025-07-11 10:32:02,058 - INFO - val: {'epoch': 67, 'time_epoch': 3.35839, 'loss': 0.66772266, 'lr': 0, 'params': 425270, 'time_iter': 0.05331, 'accuracy': 0.76139, 'f1': 0.76126, 'accuracy-SBM': 0.76132, 'auc': 0.9553}
2025-07-11 10:32:05,448 - INFO - test: {'epoch': 67, 'time_epoch': 3.358, 'loss': 0.6504988, 'lr': 0, 'params': 425270, 'time_iter': 0.0533, 'accuracy': 0.76322, 'f1': 0.76319, 'accuracy-SBM': 0.76317, 'auc': 0.95775}
2025-07-11 10:32:05,451 - INFO - > Epoch 67: took 72.9s (avg 69.1s) | Best so far: epoch 63	train_loss: 0.6547 train_accuracy-SBM: 0.7608	val_loss: 0.6545 val_accuracy-SBM: 0.7615	test_loss: 0.6517 test_accuracy-SBM: 0.7638
2025-07-11 10:33:11,797 - INFO - train: {'epoch': 68, 'time_epoch': 66.11534, 'eta': 1944.10322, 'eta_hours': 0.54003, 'loss': 0.6452361, 'lr': 0.00025479, 'params': 425270, 'time_iter': 0.10578, 'accuracy': 0.7648, 'f1': 0.7648, 'accuracy-SBM': 0.7648, 'auc': 0.95833}
2025-07-11 10:33:15,199 - INFO - val: {'epoch': 68, 'time_epoch': 3.35866, 'loss': 0.65951698, 'lr': 0, 'params': 425270, 'time_iter': 0.05331, 'accuracy': 0.76184, 'f1': 0.76182, 'accuracy-SBM': 0.76176, 'auc': 0.9565}
2025-07-11 10:33:18,581 - INFO - test: {'epoch': 68, 'time_epoch': 3.35035, 'loss': 0.65254473, 'lr': 0, 'params': 425270, 'time_iter': 0.05318, 'accuracy': 0.76299, 'f1': 0.76301, 'accuracy-SBM': 0.76304, 'auc': 0.95748}
2025-07-11 10:33:18,583 - INFO - > Epoch 68: took 73.1s (avg 69.2s) | Best so far: epoch 68	train_loss: 0.6452 train_accuracy-SBM: 0.7648	val_loss: 0.6595 val_accuracy-SBM: 0.7618	test_loss: 0.6525 test_accuracy-SBM: 0.7630
2025-07-11 10:34:24,988 - INFO - train: {'epoch': 69, 'time_epoch': 66.17211, 'eta': 1882.87268, 'eta_hours': 0.52302, 'loss': 0.64360191, 'lr': 0.00024052, 'params': 425270, 'time_iter': 0.10588, 'accuracy': 0.76522, 'f1': 0.76523, 'accuracy-SBM': 0.76522, 'auc': 0.95854}
2025-07-11 10:34:28,389 - INFO - val: {'epoch': 69, 'time_epoch': 3.35629, 'loss': 0.65588544, 'lr': 0, 'params': 425270, 'time_iter': 0.05327, 'accuracy': 0.76401, 'f1': 0.76393, 'accuracy-SBM': 0.76392, 'auc': 0.95691}
2025-07-11 10:34:31,786 - INFO - test: {'epoch': 69, 'time_epoch': 3.36541, 'loss': 0.64187309, 'lr': 0, 'params': 425270, 'time_iter': 0.05342, 'accuracy': 0.76795, 'f1': 0.76797, 'accuracy-SBM': 0.76797, 'auc': 0.95883}
2025-07-11 10:34:31,788 - INFO - > Epoch 69: took 73.2s (avg 69.2s) | Best so far: epoch 69	train_loss: 0.6436 train_accuracy-SBM: 0.7652	val_loss: 0.6559 val_accuracy-SBM: 0.7639	test_loss: 0.6419 test_accuracy-SBM: 0.7680
2025-07-11 10:35:37,883 - INFO - train: {'epoch': 70, 'time_epoch': 65.8503, 'eta': 1821.3715, 'eta_hours': 0.50594, 'loss': 0.6426632, 'lr': 0.00022653, 'params': 425270, 'time_iter': 0.10536, 'accuracy': 0.76546, 'f1': 0.76546, 'accuracy-SBM': 0.76546, 'auc': 0.95869}
2025-07-11 10:35:41,293 - INFO - val: {'epoch': 70, 'time_epoch': 3.35916, 'loss': 0.6774206, 'lr': 0, 'params': 425270, 'time_iter': 0.05332, 'accuracy': 0.75943, 'f1': 0.75942, 'accuracy-SBM': 0.75947, 'auc': 0.95425}
2025-07-11 10:35:44,682 - INFO - test: {'epoch': 70, 'time_epoch': 3.35789, 'loss': 0.65456818, 'lr': 0, 'params': 425270, 'time_iter': 0.0533, 'accuracy': 0.76352, 'f1': 0.7635, 'accuracy-SBM': 0.76352, 'auc': 0.95726}
2025-07-11 10:35:44,684 - INFO - > Epoch 70: took 72.9s (avg 69.3s) | Best so far: epoch 69	train_loss: 0.6436 train_accuracy-SBM: 0.7652	val_loss: 0.6559 val_accuracy-SBM: 0.7639	test_loss: 0.6419 test_accuracy-SBM: 0.7680
2025-07-11 10:36:50,766 - INFO - train: {'epoch': 71, 'time_epoch': 65.84974, 'eta': 1759.7493, 'eta_hours': 0.48882, 'loss': 0.63914641, 'lr': 0.00021284, 'params': 425270, 'time_iter': 0.10536, 'accuracy': 0.76721, 'f1': 0.76721, 'accuracy-SBM': 0.76721, 'auc': 0.95911}
2025-07-11 10:36:53,953 - INFO - val: {'epoch': 71, 'time_epoch': 3.14131, 'loss': 0.66803575, 'lr': 0, 'params': 425270, 'time_iter': 0.04986, 'accuracy': 0.76416, 'f1': 0.76409, 'accuracy-SBM': 0.7641, 'auc': 0.95524}
2025-07-11 10:36:57,063 - INFO - test: {'epoch': 71, 'time_epoch': 3.07888, 'loss': 0.64630384, 'lr': 0, 'params': 425270, 'time_iter': 0.04887, 'accuracy': 0.76746, 'f1': 0.76747, 'accuracy-SBM': 0.76748, 'auc': 0.95817}
2025-07-11 10:36:57,067 - INFO - > Epoch 71: took 72.4s (avg 69.3s) | Best so far: epoch 71	train_loss: 0.6391 train_accuracy-SBM: 0.7672	val_loss: 0.6680 val_accuracy-SBM: 0.7641	test_loss: 0.6463 test_accuracy-SBM: 0.7675
2025-07-11 10:38:03,857 - INFO - train: {'epoch': 72, 'time_epoch': 66.54689, 'eta': 1698.26912, 'eta_hours': 0.47174, 'loss': 0.63562584, 'lr': 0.00019946, 'params': 425270, 'time_iter': 0.10648, 'accuracy': 0.76839, 'f1': 0.76839, 'accuracy-SBM': 0.76839, 'auc': 0.95957}
2025-07-11 10:38:07,317 - INFO - val: {'epoch': 72, 'time_epoch': 3.41657, 'loss': 0.65025144, 'lr': 0, 'params': 425270, 'time_iter': 0.05423, 'accuracy': 0.76437, 'f1': 0.76428, 'accuracy-SBM': 0.76427, 'auc': 0.95769}
2025-07-11 10:38:10,744 - INFO - test: {'epoch': 72, 'time_epoch': 3.39321, 'loss': 0.64483576, 'lr': 0, 'params': 425270, 'time_iter': 0.05386, 'accuracy': 0.76457, 'f1': 0.76459, 'accuracy-SBM': 0.76463, 'auc': 0.95846}
2025-07-11 10:38:10,746 - INFO - > Epoch 72: took 73.7s (avg 69.4s) | Best so far: epoch 72	train_loss: 0.6356 train_accuracy-SBM: 0.7684	val_loss: 0.6503 val_accuracy-SBM: 0.7643	test_loss: 0.6448 test_accuracy-SBM: 0.7646
2025-07-11 10:39:18,605 - INFO - train: {'epoch': 73, 'time_epoch': 67.61988, 'eta': 1637.029, 'eta_hours': 0.45473, 'loss': 0.63592167, 'lr': 0.00018641, 'params': 425270, 'time_iter': 0.10819, 'accuracy': 0.76816, 'f1': 0.76816, 'accuracy-SBM': 0.76816, 'auc': 0.95953}
2025-07-11 10:39:22,065 - INFO - val: {'epoch': 73, 'time_epoch': 3.4157, 'loss': 0.65303269, 'lr': 0, 'params': 425270, 'time_iter': 0.05422, 'accuracy': 0.76453, 'f1': 0.76451, 'accuracy-SBM': 0.76452, 'auc': 0.95738}
2025-07-11 10:39:25,529 - INFO - test: {'epoch': 73, 'time_epoch': 3.42995, 'loss': 0.63956085, 'lr': 0, 'params': 425270, 'time_iter': 0.05444, 'accuracy': 0.76758, 'f1': 0.76759, 'accuracy-SBM': 0.76762, 'auc': 0.95916}
2025-07-11 10:39:25,532 - INFO - > Epoch 73: took 74.8s (avg 69.4s) | Best so far: epoch 73	train_loss: 0.6359 train_accuracy-SBM: 0.7682	val_loss: 0.6530 val_accuracy-SBM: 0.7645	test_loss: 0.6396 test_accuracy-SBM: 0.7676
2025-07-11 10:40:33,238 - INFO - train: {'epoch': 74, 'time_epoch': 67.46572, 'eta': 1575.56737, 'eta_hours': 0.43766, 'loss': 0.63255557, 'lr': 0.00017371, 'params': 425270, 'time_iter': 0.10795, 'accuracy': 0.76875, 'f1': 0.76876, 'accuracy-SBM': 0.76875, 'auc': 0.95998}
2025-07-11 10:40:36,705 - INFO - val: {'epoch': 74, 'time_epoch': 3.42281, 'loss': 0.64425908, 'lr': 0, 'params': 425270, 'time_iter': 0.05433, 'accuracy': 0.76783, 'f1': 0.76772, 'accuracy-SBM': 0.76773, 'auc': 0.95848}
2025-07-11 10:40:40,153 - INFO - test: {'epoch': 74, 'time_epoch': 3.4149, 'loss': 0.63443829, 'lr': 0, 'params': 425270, 'time_iter': 0.0542, 'accuracy': 0.7709, 'f1': 0.77089, 'accuracy-SBM': 0.77091, 'auc': 0.95973}
2025-07-11 10:40:40,155 - INFO - > Epoch 74: took 74.6s (avg 69.5s) | Best so far: epoch 74	train_loss: 0.6326 train_accuracy-SBM: 0.7688	val_loss: 0.6443 val_accuracy-SBM: 0.7677	test_loss: 0.6344 test_accuracy-SBM: 0.7709
2025-07-11 10:41:47,687 - INFO - train: {'epoch': 75, 'time_epoch': 67.30167, 'eta': 1513.89593, 'eta_hours': 0.42053, 'loss': 0.63154859, 'lr': 0.00016136, 'params': 425270, 'time_iter': 0.10768, 'accuracy': 0.76952, 'f1': 0.76953, 'accuracy-SBM': 0.76952, 'auc': 0.9601}
2025-07-11 10:41:51,147 - INFO - val: {'epoch': 75, 'time_epoch': 3.40841, 'loss': 0.64461431, 'lr': 0, 'params': 425270, 'time_iter': 0.0541, 'accuracy': 0.76556, 'f1': 0.76548, 'accuracy-SBM': 0.76555, 'auc': 0.95851}
2025-07-11 10:41:54,592 - INFO - test: {'epoch': 75, 'time_epoch': 3.41135, 'loss': 0.64056538, 'lr': 0, 'params': 425270, 'time_iter': 0.05415, 'accuracy': 0.76635, 'f1': 0.76637, 'accuracy-SBM': 0.76641, 'auc': 0.95905}
2025-07-11 10:41:54,594 - INFO - > Epoch 75: took 74.4s (avg 69.6s) | Best so far: epoch 74	train_loss: 0.6326 train_accuracy-SBM: 0.7688	val_loss: 0.6443 val_accuracy-SBM: 0.7677	test_loss: 0.6344 test_accuracy-SBM: 0.7709
2025-07-11 10:43:01,748 - INFO - train: {'epoch': 76, 'time_epoch': 66.9213, 'eta': 1451.96464, 'eta_hours': 0.40332, 'loss': 0.62904378, 'lr': 0.00014938, 'params': 425270, 'time_iter': 0.10707, 'accuracy': 0.77099, 'f1': 0.77099, 'accuracy-SBM': 0.77099, 'auc': 0.96039}
2025-07-11 10:43:05,158 - INFO - val: {'epoch': 76, 'time_epoch': 3.36728, 'loss': 0.6472128, 'lr': 0, 'params': 425270, 'time_iter': 0.05345, 'accuracy': 0.76535, 'f1': 0.76537, 'accuracy-SBM': 0.76536, 'auc': 0.95813}
2025-07-11 10:43:08,555 - INFO - test: {'epoch': 76, 'time_epoch': 3.36418, 'loss': 0.63797853, 'lr': 0, 'params': 425270, 'time_iter': 0.0534, 'accuracy': 0.76764, 'f1': 0.7677, 'accuracy-SBM': 0.76773, 'auc': 0.9594}
2025-07-11 10:43:08,557 - INFO - > Epoch 76: took 74.0s (avg 69.6s) | Best so far: epoch 74	train_loss: 0.6326 train_accuracy-SBM: 0.7688	val_loss: 0.6443 val_accuracy-SBM: 0.7677	test_loss: 0.6344 test_accuracy-SBM: 0.7709
2025-07-11 10:44:15,000 - INFO - train: {'epoch': 77, 'time_epoch': 66.20796, 'eta': 1389.70419, 'eta_hours': 0.38603, 'loss': 0.62840704, 'lr': 0.00013779, 'params': 425270, 'time_iter': 0.10593, 'accuracy': 0.77066, 'f1': 0.77066, 'accuracy-SBM': 0.77066, 'auc': 0.96048}
2025-07-11 10:44:18,402 - INFO - val: {'epoch': 77, 'time_epoch': 3.35834, 'loss': 0.64433184, 'lr': 0, 'params': 425270, 'time_iter': 0.05331, 'accuracy': 0.76761, 'f1': 0.76749, 'accuracy-SBM': 0.76745, 'auc': 0.9586}
2025-07-11 10:44:21,794 - INFO - test: {'epoch': 77, 'time_epoch': 3.36056, 'loss': 0.63722082, 'lr': 0, 'params': 425270, 'time_iter': 0.05334, 'accuracy': 0.76937, 'f1': 0.76939, 'accuracy-SBM': 0.76938, 'auc': 0.9595}
2025-07-11 10:44:21,796 - INFO - > Epoch 77: took 73.2s (avg 69.7s) | Best so far: epoch 74	train_loss: 0.6326 train_accuracy-SBM: 0.7688	val_loss: 0.6443 val_accuracy-SBM: 0.7677	test_loss: 0.6344 test_accuracy-SBM: 0.7709
2025-07-11 10:45:28,567 - INFO - train: {'epoch': 78, 'time_epoch': 66.43659, 'eta': 1327.40459, 'eta_hours': 0.36872, 'loss': 0.62613826, 'lr': 0.00012659, 'params': 425270, 'time_iter': 0.1063, 'accuracy': 0.77157, 'f1': 0.77158, 'accuracy-SBM': 0.77157, 'auc': 0.96078}
2025-07-11 10:45:31,986 - INFO - val: {'epoch': 78, 'time_epoch': 3.37412, 'loss': 0.65451388, 'lr': 0, 'params': 425270, 'time_iter': 0.05356, 'accuracy': 0.7661, 'f1': 0.76601, 'accuracy-SBM': 0.76605, 'auc': 0.95708}
2025-07-11 10:45:35,382 - INFO - test: {'epoch': 78, 'time_epoch': 3.36412, 'loss': 0.6379225, 'lr': 0, 'params': 425270, 'time_iter': 0.0534, 'accuracy': 0.76819, 'f1': 0.76817, 'accuracy-SBM': 0.76818, 'auc': 0.95931}
2025-07-11 10:45:35,384 - INFO - > Epoch 78: took 73.6s (avg 69.7s) | Best so far: epoch 74	train_loss: 0.6326 train_accuracy-SBM: 0.7688	val_loss: 0.6443 val_accuracy-SBM: 0.7677	test_loss: 0.6344 test_accuracy-SBM: 0.7709
2025-07-11 10:46:41,824 - INFO - train: {'epoch': 79, 'time_epoch': 66.11476, 'eta': 1264.9211, 'eta_hours': 0.35137, 'loss': 0.62491377, 'lr': 0.0001158, 'params': 425270, 'time_iter': 0.10578, 'accuracy': 0.77229, 'f1': 0.77229, 'accuracy-SBM': 0.77229, 'auc': 0.96093}
2025-07-11 10:46:45,225 - INFO - val: {'epoch': 79, 'time_epoch': 3.35613, 'loss': 0.65374002, 'lr': 0, 'params': 425270, 'time_iter': 0.05327, 'accuracy': 0.76558, 'f1': 0.7655, 'accuracy-SBM': 0.76554, 'auc': 0.9573}
2025-07-11 10:46:48,614 - INFO - test: {'epoch': 79, 'time_epoch': 3.35718, 'loss': 0.63772365, 'lr': 0, 'params': 425270, 'time_iter': 0.05329, 'accuracy': 0.76931, 'f1': 0.7693, 'accuracy-SBM': 0.76931, 'auc': 0.95938}
2025-07-11 10:46:48,616 - INFO - > Epoch 79: took 73.2s (avg 69.8s) | Best so far: epoch 74	train_loss: 0.6326 train_accuracy-SBM: 0.7688	val_loss: 0.6443 val_accuracy-SBM: 0.7677	test_loss: 0.6344 test_accuracy-SBM: 0.7709
2025-07-11 10:47:54,954 - INFO - train: {'epoch': 80, 'time_epoch': 66.10513, 'eta': 1202.34569, 'eta_hours': 0.33398, 'loss': 0.62040503, 'lr': 0.00010543, 'params': 425270, 'time_iter': 0.10577, 'accuracy': 0.77357, 'f1': 0.77358, 'accuracy-SBM': 0.77358, 'auc': 0.9615}
2025-07-11 10:47:58,352 - INFO - val: {'epoch': 80, 'time_epoch': 3.35443, 'loss': 0.64269363, 'lr': 0, 'params': 425270, 'time_iter': 0.05324, 'accuracy': 0.76825, 'f1': 0.76814, 'accuracy-SBM': 0.76813, 'auc': 0.95875}
2025-07-11 10:48:01,740 - INFO - test: {'epoch': 80, 'time_epoch': 3.35644, 'loss': 0.63466158, 'lr': 0, 'params': 425270, 'time_iter': 0.05328, 'accuracy': 0.76975, 'f1': 0.76977, 'accuracy-SBM': 0.76976, 'auc': 0.95978}
2025-07-11 10:48:01,742 - INFO - > Epoch 80: took 73.1s (avg 69.8s) | Best so far: epoch 80	train_loss: 0.6204 train_accuracy-SBM: 0.7736	val_loss: 0.6427 val_accuracy-SBM: 0.7681	test_loss: 0.6347 test_accuracy-SBM: 0.7698
2025-07-11 10:49:08,704 - INFO - train: {'epoch': 81, 'time_epoch': 66.72537, 'eta': 1139.82034, 'eta_hours': 0.31662, 'loss': 0.62161919, 'lr': 9.549e-05, 'params': 425270, 'time_iter': 0.10676, 'accuracy': 0.77279, 'f1': 0.77279, 'accuracy-SBM': 0.77279, 'auc': 0.96135}
2025-07-11 10:49:12,106 - INFO - val: {'epoch': 81, 'time_epoch': 3.35866, 'loss': 0.65684869, 'lr': 0, 'params': 425270, 'time_iter': 0.05331, 'accuracy': 0.76567, 'f1': 0.76559, 'accuracy-SBM': 0.76559, 'auc': 0.95706}
2025-07-11 10:49:15,489 - INFO - test: {'epoch': 81, 'time_epoch': 3.35189, 'loss': 0.64201333, 'lr': 0, 'params': 425270, 'time_iter': 0.0532, 'accuracy': 0.76918, 'f1': 0.76919, 'accuracy-SBM': 0.7692, 'auc': 0.95894}
2025-07-11 10:49:15,491 - INFO - > Epoch 81: took 73.7s (avg 69.9s) | Best so far: epoch 80	train_loss: 0.6204 train_accuracy-SBM: 0.7736	val_loss: 0.6427 val_accuracy-SBM: 0.7681	test_loss: 0.6347 test_accuracy-SBM: 0.7698
2025-07-11 10:50:22,227 - INFO - train: {'epoch': 82, 'time_epoch': 66.50872, 'eta': 1077.14942, 'eta_hours': 0.29921, 'loss': 0.62009338, 'lr': 8.6e-05, 'params': 425270, 'time_iter': 0.10641, 'accuracy': 0.77355, 'f1': 0.77355, 'accuracy-SBM': 0.77355, 'auc': 0.96153}
2025-07-11 10:50:25,643 - INFO - val: {'epoch': 82, 'time_epoch': 3.36537, 'loss': 0.64458169, 'lr': 0, 'params': 425270, 'time_iter': 0.05342, 'accuracy': 0.76855, 'f1': 0.76851, 'accuracy-SBM': 0.76853, 'auc': 0.95849}
2025-07-11 10:50:29,023 - INFO - test: {'epoch': 82, 'time_epoch': 3.34828, 'loss': 0.63493624, 'lr': 0, 'params': 425270, 'time_iter': 0.05315, 'accuracy': 0.77012, 'f1': 0.77013, 'accuracy-SBM': 0.77016, 'auc': 0.95974}
2025-07-11 10:50:29,025 - INFO - > Epoch 82: took 73.5s (avg 69.9s) | Best so far: epoch 82	train_loss: 0.6201 train_accuracy-SBM: 0.7735	val_loss: 0.6446 val_accuracy-SBM: 0.7685	test_loss: 0.6349 test_accuracy-SBM: 0.7702
2025-07-11 10:51:35,890 - INFO - train: {'epoch': 83, 'time_epoch': 66.63815, 'eta': 1014.41177, 'eta_hours': 0.28178, 'loss': 0.61715128, 'lr': 7.695e-05, 'params': 425270, 'time_iter': 0.10662, 'accuracy': 0.77448, 'f1': 0.77449, 'accuracy-SBM': 0.77449, 'auc': 0.9619}
2025-07-11 10:51:39,292 - INFO - val: {'epoch': 83, 'time_epoch': 3.35767, 'loss': 0.65013442, 'lr': 0, 'params': 425270, 'time_iter': 0.0533, 'accuracy': 0.7668, 'f1': 0.76672, 'accuracy-SBM': 0.76676, 'auc': 0.95779}
2025-07-11 10:51:42,673 - INFO - test: {'epoch': 83, 'time_epoch': 3.34937, 'loss': 0.6386544, 'lr': 0, 'params': 425270, 'time_iter': 0.05316, 'accuracy': 0.76931, 'f1': 0.76931, 'accuracy-SBM': 0.76931, 'auc': 0.95924}
2025-07-11 10:51:42,675 - INFO - > Epoch 83: took 73.6s (avg 69.9s) | Best so far: epoch 82	train_loss: 0.6201 train_accuracy-SBM: 0.7735	val_loss: 0.6446 val_accuracy-SBM: 0.7685	test_loss: 0.6349 test_accuracy-SBM: 0.7702
2025-07-11 10:52:48,574 - INFO - train: {'epoch': 84, 'time_epoch': 65.67526, 'eta': 951.41242, 'eta_hours': 0.26428, 'loss': 0.61761557, 'lr': 6.837e-05, 'params': 425270, 'time_iter': 0.10508, 'accuracy': 0.77499, 'f1': 0.77499, 'accuracy-SBM': 0.77499, 'auc': 0.96184}
2025-07-11 10:52:51,659 - INFO - val: {'epoch': 84, 'time_epoch': 3.04217, 'loss': 0.64367054, 'lr': 0, 'params': 425270, 'time_iter': 0.04829, 'accuracy': 0.76763, 'f1': 0.76757, 'accuracy-SBM': 0.76755, 'auc': 0.95853}
2025-07-11 10:52:54,732 - INFO - test: {'epoch': 84, 'time_epoch': 3.04066, 'loss': 0.63431959, 'lr': 0, 'params': 425270, 'time_iter': 0.04826, 'accuracy': 0.77024, 'f1': 0.77023, 'accuracy-SBM': 0.77026, 'auc': 0.95976}
2025-07-11 10:52:54,734 - INFO - > Epoch 84: took 72.1s (avg 70.0s) | Best so far: epoch 82	train_loss: 0.6201 train_accuracy-SBM: 0.7735	val_loss: 0.6446 val_accuracy-SBM: 0.7685	test_loss: 0.6349 test_accuracy-SBM: 0.7702
2025-07-11 10:53:58,460 - INFO - train: {'epoch': 85, 'time_epoch': 63.41012, 'eta': 887.9821, 'eta_hours': 0.24666, 'loss': 0.61514281, 'lr': 6.026e-05, 'params': 425270, 'time_iter': 0.10146, 'accuracy': 0.77532, 'f1': 0.77532, 'accuracy-SBM': 0.77532, 'auc': 0.96215}
2025-07-11 10:54:01,529 - INFO - val: {'epoch': 85, 'time_epoch': 3.02791, 'loss': 0.65392121, 'lr': 0, 'params': 425270, 'time_iter': 0.04806, 'accuracy': 0.76651, 'f1': 0.76648, 'accuracy-SBM': 0.76653, 'auc': 0.95727}
2025-07-11 10:54:04,552 - INFO - test: {'epoch': 85, 'time_epoch': 2.99188, 'loss': 0.63377278, 'lr': 0, 'params': 425270, 'time_iter': 0.04749, 'accuracy': 0.77021, 'f1': 0.7702, 'accuracy-SBM': 0.77025, 'auc': 0.95989}
2025-07-11 10:54:04,554 - INFO - > Epoch 85: took 69.8s (avg 70.0s) | Best so far: epoch 82	train_loss: 0.6201 train_accuracy-SBM: 0.7735	val_loss: 0.6446 val_accuracy-SBM: 0.7685	test_loss: 0.6349 test_accuracy-SBM: 0.7702
2025-07-11 10:55:08,392 - INFO - train: {'epoch': 86, 'time_epoch': 63.50949, 'eta': 824.56709, 'eta_hours': 0.22905, 'loss': 0.61531012, 'lr': 5.264e-05, 'params': 425270, 'time_iter': 0.10162, 'accuracy': 0.77573, 'f1': 0.77573, 'accuracy-SBM': 0.77573, 'auc': 0.96213}
2025-07-11 10:55:11,558 - INFO - val: {'epoch': 86, 'time_epoch': 3.12275, 'loss': 0.6391129, 'lr': 0, 'params': 425270, 'time_iter': 0.04957, 'accuracy': 0.76883, 'f1': 0.76875, 'accuracy-SBM': 0.76877, 'auc': 0.9592}
2025-07-11 10:55:14,662 - INFO - test: {'epoch': 86, 'time_epoch': 3.07346, 'loss': 0.63642834, 'lr': 0, 'params': 425270, 'time_iter': 0.04879, 'accuracy': 0.77068, 'f1': 0.77068, 'accuracy-SBM': 0.7707, 'auc': 0.95954}
2025-07-11 10:55:14,664 - INFO - > Epoch 86: took 70.1s (avg 70.0s) | Best so far: epoch 86	train_loss: 0.6153 train_accuracy-SBM: 0.7757	val_loss: 0.6391 val_accuracy-SBM: 0.7688	test_loss: 0.6364 test_accuracy-SBM: 0.7707
2025-07-11 10:56:19,182 - INFO - train: {'epoch': 87, 'time_epoch': 64.28317, 'eta': 761.25543, 'eta_hours': 0.21146, 'loss': 0.61406208, 'lr': 4.55e-05, 'params': 425270, 'time_iter': 0.10285, 'accuracy': 0.7761, 'f1': 0.77611, 'accuracy-SBM': 0.7761, 'auc': 0.96227}
2025-07-11 10:56:22,323 - INFO - val: {'epoch': 87, 'time_epoch': 3.09655, 'loss': 0.64191822, 'lr': 0, 'params': 425270, 'time_iter': 0.04915, 'accuracy': 0.76823, 'f1': 0.76815, 'accuracy-SBM': 0.7682, 'auc': 0.95888}
2025-07-11 10:56:25,453 - INFO - test: {'epoch': 87, 'time_epoch': 3.09934, 'loss': 0.63316136, 'lr': 0, 'params': 425270, 'time_iter': 0.0492, 'accuracy': 0.77114, 'f1': 0.77114, 'accuracy-SBM': 0.77115, 'auc': 0.95997}
2025-07-11 10:56:25,455 - INFO - > Epoch 87: took 70.8s (avg 70.0s) | Best so far: epoch 86	train_loss: 0.6153 train_accuracy-SBM: 0.7757	val_loss: 0.6391 val_accuracy-SBM: 0.7688	test_loss: 0.6364 test_accuracy-SBM: 0.7707
2025-07-11 10:57:29,911 - INFO - train: {'epoch': 88, 'time_epoch': 64.22014, 'eta': 697.91415, 'eta_hours': 0.19387, 'loss': 0.61328924, 'lr': 3.886e-05, 'params': 425270, 'time_iter': 0.10275, 'accuracy': 0.77638, 'f1': 0.77638, 'accuracy-SBM': 0.77638, 'auc': 0.96238}
2025-07-11 10:57:33,313 - INFO - val: {'epoch': 88, 'time_epoch': 3.35904, 'loss': 0.6490452, 'lr': 0, 'params': 425270, 'time_iter': 0.05332, 'accuracy': 0.76692, 'f1': 0.76688, 'accuracy-SBM': 0.76687, 'auc': 0.95782}
2025-07-11 10:57:36,725 - INFO - test: {'epoch': 88, 'time_epoch': 3.38063, 'loss': 0.63552933, 'lr': 0, 'params': 425270, 'time_iter': 0.05366, 'accuracy': 0.76998, 'f1': 0.76998, 'accuracy-SBM': 0.77004, 'auc': 0.95962}
2025-07-11 10:57:36,727 - INFO - > Epoch 88: took 71.3s (avg 70.0s) | Best so far: epoch 86	train_loss: 0.6153 train_accuracy-SBM: 0.7757	val_loss: 0.6391 val_accuracy-SBM: 0.7688	test_loss: 0.6364 test_accuracy-SBM: 0.7707
2025-07-11 10:58:41,667 - INFO - train: {'epoch': 89, 'time_epoch': 64.70671, 'eta': 634.60741, 'eta_hours': 0.17628, 'loss': 0.61207052, 'lr': 3.272e-05, 'params': 425270, 'time_iter': 0.10353, 'accuracy': 0.7772, 'f1': 0.7772, 'accuracy-SBM': 0.7772, 'auc': 0.96252}
2025-07-11 10:58:44,753 - INFO - val: {'epoch': 89, 'time_epoch': 3.04248, 'loss': 0.64472881, 'lr': 0, 'params': 425270, 'time_iter': 0.04829, 'accuracy': 0.76725, 'f1': 0.76715, 'accuracy-SBM': 0.76717, 'auc': 0.95853}
2025-07-11 10:58:47,807 - INFO - test: {'epoch': 89, 'time_epoch': 3.02183, 'loss': 0.63456243, 'lr': 0, 'params': 425270, 'time_iter': 0.04797, 'accuracy': 0.7708, 'f1': 0.77081, 'accuracy-SBM': 0.7708, 'auc': 0.9598}
2025-07-11 10:58:47,809 - INFO - > Epoch 89: took 71.1s (avg 70.0s) | Best so far: epoch 86	train_loss: 0.6153 train_accuracy-SBM: 0.7757	val_loss: 0.6391 val_accuracy-SBM: 0.7688	test_loss: 0.6364 test_accuracy-SBM: 0.7707
2025-07-11 10:59:51,097 - INFO - train: {'epoch': 90, 'time_epoch': 62.96819, 'eta': 571.09795, 'eta_hours': 0.15864, 'loss': 0.61150412, 'lr': 2.709e-05, 'params': 425270, 'time_iter': 0.10075, 'accuracy': 0.77711, 'f1': 0.77711, 'accuracy-SBM': 0.77711, 'auc': 0.96259}
2025-07-11 10:59:54,144 - INFO - val: {'epoch': 90, 'time_epoch': 3.0047, 'loss': 0.64820029, 'lr': 0, 'params': 425270, 'time_iter': 0.04769, 'accuracy': 0.76833, 'f1': 0.76826, 'accuracy-SBM': 0.7683, 'auc': 0.95805}
2025-07-11 10:59:57,200 - INFO - test: {'epoch': 90, 'time_epoch': 3.02511, 'loss': 0.63559264, 'lr': 0, 'params': 425270, 'time_iter': 0.04802, 'accuracy': 0.77087, 'f1': 0.77087, 'accuracy-SBM': 0.77088, 'auc': 0.95968}
2025-07-11 10:59:57,203 - INFO - > Epoch 90: took 69.4s (avg 70.0s) | Best so far: epoch 86	train_loss: 0.6153 train_accuracy-SBM: 0.7757	val_loss: 0.6391 val_accuracy-SBM: 0.7688	test_loss: 0.6364 test_accuracy-SBM: 0.7707
2025-07-11 11:01:01,258 - INFO - train: {'epoch': 91, 'time_epoch': 63.82669, 'eta': 507.67492, 'eta_hours': 0.14102, 'loss': 0.60992254, 'lr': 2.198e-05, 'params': 425270, 'time_iter': 0.10212, 'accuracy': 0.77764, 'f1': 0.77765, 'accuracy-SBM': 0.77764, 'auc': 0.96278}
2025-07-11 11:01:04,378 - INFO - val: {'epoch': 91, 'time_epoch': 3.07698, 'loss': 0.65722777, 'lr': 0, 'params': 425270, 'time_iter': 0.04884, 'accuracy': 0.76742, 'f1': 0.76735, 'accuracy-SBM': 0.76737, 'auc': 0.95691}
2025-07-11 11:01:07,511 - INFO - test: {'epoch': 91, 'time_epoch': 3.10078, 'loss': 0.63584979, 'lr': 0, 'params': 425270, 'time_iter': 0.04922, 'accuracy': 0.7705, 'f1': 0.77049, 'accuracy-SBM': 0.77049, 'auc': 0.95967}
2025-07-11 11:01:07,513 - INFO - > Epoch 91: took 70.3s (avg 70.0s) | Best so far: epoch 86	train_loss: 0.6153 train_accuracy-SBM: 0.7757	val_loss: 0.6391 val_accuracy-SBM: 0.7688	test_loss: 0.6364 test_accuracy-SBM: 0.7707
2025-07-11 11:02:11,508 - INFO - train: {'epoch': 92, 'time_epoch': 63.76274, 'eta': 444.23839, 'eta_hours': 0.1234, 'loss': 0.60984563, 'lr': 1.74e-05, 'params': 425270, 'time_iter': 0.10202, 'accuracy': 0.77729, 'f1': 0.77729, 'accuracy-SBM': 0.77729, 'auc': 0.96281}
2025-07-11 11:02:14,645 - INFO - val: {'epoch': 92, 'time_epoch': 3.09273, 'loss': 0.64879231, 'lr': 0, 'params': 425270, 'time_iter': 0.04909, 'accuracy': 0.76795, 'f1': 0.76788, 'accuracy-SBM': 0.76795, 'auc': 0.958}
2025-07-11 11:02:17,740 - INFO - test: {'epoch': 92, 'time_epoch': 3.05603, 'loss': 0.63664356, 'lr': 0, 'params': 425270, 'time_iter': 0.04851, 'accuracy': 0.77048, 'f1': 0.77048, 'accuracy-SBM': 0.77048, 'auc': 0.95958}
2025-07-11 11:02:17,742 - INFO - > Epoch 92: took 70.2s (avg 70.0s) | Best so far: epoch 86	train_loss: 0.6153 train_accuracy-SBM: 0.7757	val_loss: 0.6391 val_accuracy-SBM: 0.7688	test_loss: 0.6364 test_accuracy-SBM: 0.7707
2025-07-11 11:03:20,855 - INFO - train: {'epoch': 93, 'time_epoch': 62.88655, 'eta': 380.73899, 'eta_hours': 0.10576, 'loss': 0.60767427, 'lr': 1.334e-05, 'params': 425270, 'time_iter': 0.10062, 'accuracy': 0.77847, 'f1': 0.77847, 'accuracy-SBM': 0.77847, 'auc': 0.96306}
2025-07-11 11:03:23,965 - INFO - val: {'epoch': 93, 'time_epoch': 3.06678, 'loss': 0.63803586, 'lr': 0, 'params': 425270, 'time_iter': 0.04868, 'accuracy': 0.76981, 'f1': 0.76975, 'accuracy-SBM': 0.76975, 'auc': 0.95929}
2025-07-11 11:03:27,077 - INFO - test: {'epoch': 93, 'time_epoch': 3.07295, 'loss': 0.63226189, 'lr': 0, 'params': 425270, 'time_iter': 0.04878, 'accuracy': 0.77071, 'f1': 0.7707, 'accuracy-SBM': 0.77074, 'auc': 0.96007}
2025-07-11 11:03:27,079 - INFO - > Epoch 93: took 69.3s (avg 70.0s) | Best so far: epoch 93	train_loss: 0.6077 train_accuracy-SBM: 0.7785	val_loss: 0.6380 val_accuracy-SBM: 0.7698	test_loss: 0.6323 test_accuracy-SBM: 0.7707
2025-07-11 11:04:30,728 - INFO - train: {'epoch': 94, 'time_epoch': 63.41791, 'eta': 317.28046, 'eta_hours': 0.08813, 'loss': 0.60779515, 'lr': 9.81e-06, 'params': 425270, 'time_iter': 0.10147, 'accuracy': 0.77863, 'f1': 0.77863, 'accuracy-SBM': 0.77863, 'auc': 0.96305}
2025-07-11 11:04:33,800 - INFO - val: {'epoch': 94, 'time_epoch': 3.03034, 'loss': 0.64231909, 'lr': 0, 'params': 425270, 'time_iter': 0.0481, 'accuracy': 0.76873, 'f1': 0.76865, 'accuracy-SBM': 0.76867, 'auc': 0.95887}
2025-07-11 11:04:36,883 - INFO - test: {'epoch': 94, 'time_epoch': 3.05006, 'loss': 0.63472946, 'lr': 0, 'params': 425270, 'time_iter': 0.04841, 'accuracy': 0.77107, 'f1': 0.77106, 'accuracy-SBM': 0.77106, 'auc': 0.95981}
2025-07-11 11:04:36,885 - INFO - > Epoch 94: took 69.8s (avg 70.0s) | Best so far: epoch 93	train_loss: 0.6077 train_accuracy-SBM: 0.7785	val_loss: 0.6380 val_accuracy-SBM: 0.7698	test_loss: 0.6323 test_accuracy-SBM: 0.7707
2025-07-11 11:05:42,991 - INFO - train: {'epoch': 95, 'time_epoch': 65.87442, 'eta': 253.92513, 'eta_hours': 0.07053, 'loss': 0.60915183, 'lr': 6.82e-06, 'params': 425270, 'time_iter': 0.1054, 'accuracy': 0.7778, 'f1': 0.7778, 'accuracy-SBM': 0.7778, 'auc': 0.96288}
2025-07-11 11:05:46,388 - INFO - val: {'epoch': 95, 'time_epoch': 3.35354, 'loss': 0.63737377, 'lr': 0, 'params': 425270, 'time_iter': 0.05323, 'accuracy': 0.77032, 'f1': 0.77024, 'accuracy-SBM': 0.77025, 'auc': 0.95936}
2025-07-11 11:05:49,771 - INFO - test: {'epoch': 95, 'time_epoch': 3.35106, 'loss': 0.63229134, 'lr': 0, 'params': 425270, 'time_iter': 0.05319, 'accuracy': 0.77126, 'f1': 0.77127, 'accuracy-SBM': 0.77128, 'auc': 0.96003}
2025-07-11 11:05:49,773 - INFO - > Epoch 95: took 72.9s (avg 70.0s) | Best so far: epoch 95	train_loss: 0.6092 train_accuracy-SBM: 0.7778	val_loss: 0.6374 val_accuracy-SBM: 0.7702	test_loss: 0.6323 test_accuracy-SBM: 0.7713
2025-07-11 11:06:56,624 - INFO - train: {'epoch': 96, 'time_epoch': 66.52095, 'eta': 190.53786, 'eta_hours': 0.05293, 'loss': 0.60896438, 'lr': 4.37e-06, 'params': 425270, 'time_iter': 0.10643, 'accuracy': 0.77732, 'f1': 0.77732, 'accuracy-SBM': 0.77732, 'auc': 0.96292}
2025-07-11 11:07:00,102 - INFO - val: {'epoch': 96, 'time_epoch': 3.43402, 'loss': 0.63871692, 'lr': 0, 'params': 425270, 'time_iter': 0.05451, 'accuracy': 0.76968, 'f1': 0.76963, 'accuracy-SBM': 0.76967, 'auc': 0.95923}
2025-07-11 11:07:03,522 - INFO - test: {'epoch': 96, 'time_epoch': 3.38755, 'loss': 0.6339243, 'lr': 0, 'params': 425270, 'time_iter': 0.05377, 'accuracy': 0.77066, 'f1': 0.77067, 'accuracy-SBM': 0.77071, 'auc': 0.95986}
2025-07-11 11:07:03,524 - INFO - > Epoch 96: took 73.8s (avg 70.1s) | Best so far: epoch 95	train_loss: 0.6092 train_accuracy-SBM: 0.7778	val_loss: 0.6374 val_accuracy-SBM: 0.7702	test_loss: 0.6323 test_accuracy-SBM: 0.7713
2025-07-11 11:08:11,133 - INFO - train: {'epoch': 97, 'time_epoch': 67.17118, 'eta': 127.0999, 'eta_hours': 0.03531, 'loss': 0.60668797, 'lr': 2.46e-06, 'params': 425270, 'time_iter': 0.10747, 'accuracy': 0.77906, 'f1': 0.77906, 'accuracy-SBM': 0.77906, 'auc': 0.96318}
2025-07-11 11:08:14,551 - INFO - val: {'epoch': 97, 'time_epoch': 3.37287, 'loss': 0.63946391, 'lr': 0, 'params': 425270, 'time_iter': 0.05354, 'accuracy': 0.77034, 'f1': 0.77027, 'accuracy-SBM': 0.77031, 'auc': 0.95917}
2025-07-11 11:08:17,957 - INFO - test: {'epoch': 97, 'time_epoch': 3.37323, 'loss': 0.63477184, 'lr': 0, 'params': 425270, 'time_iter': 0.05354, 'accuracy': 0.77075, 'f1': 0.77074, 'accuracy-SBM': 0.77076, 'auc': 0.95977}
2025-07-11 11:08:17,960 - INFO - > Epoch 97: took 74.4s (avg 70.1s) | Best so far: epoch 97	train_loss: 0.6067 train_accuracy-SBM: 0.7791	val_loss: 0.6395 val_accuracy-SBM: 0.7703	test_loss: 0.6348 test_accuracy-SBM: 0.7708
2025-07-11 11:09:23,433 - INFO - train: {'epoch': 98, 'time_epoch': 65.23346, 'eta': 63.56696, 'eta_hours': 0.01766, 'loss': 0.60725773, 'lr': 1.09e-06, 'params': 425270, 'time_iter': 0.10437, 'accuracy': 0.77806, 'f1': 0.77806, 'accuracy-SBM': 0.77806, 'auc': 0.96313}
2025-07-11 11:09:26,541 - INFO - val: {'epoch': 98, 'time_epoch': 3.06497, 'loss': 0.64919778, 'lr': 0, 'params': 425270, 'time_iter': 0.04865, 'accuracy': 0.76838, 'f1': 0.76831, 'accuracy-SBM': 0.76836, 'auc': 0.95793}
2025-07-11 11:09:29,580 - INFO - test: {'epoch': 98, 'time_epoch': 3.00427, 'loss': 0.6356377, 'lr': 0, 'params': 425270, 'time_iter': 0.04769, 'accuracy': 0.77032, 'f1': 0.7703, 'accuracy-SBM': 0.77032, 'auc': 0.9597}
2025-07-11 11:09:29,582 - INFO - > Epoch 98: took 71.6s (avg 70.1s) | Best so far: epoch 97	train_loss: 0.6067 train_accuracy-SBM: 0.7791	val_loss: 0.6395 val_accuracy-SBM: 0.7703	test_loss: 0.6348 test_accuracy-SBM: 0.7708
2025-07-11 11:10:33,316 - INFO - train: {'epoch': 99, 'time_epoch': 63.40597, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.60718975, 'lr': 2.7e-07, 'params': 425270, 'time_iter': 0.10145, 'accuracy': 0.77824, 'f1': 0.77825, 'accuracy-SBM': 0.77825, 'auc': 0.96313}
2025-07-11 11:10:36,417 - INFO - val: {'epoch': 99, 'time_epoch': 3.05554, 'loss': 0.64040492, 'lr': 0, 'params': 425270, 'time_iter': 0.0485, 'accuracy': 0.76982, 'f1': 0.76976, 'accuracy-SBM': 0.76979, 'auc': 0.9591}
2025-07-11 11:10:39,487 - INFO - test: {'epoch': 99, 'time_epoch': 3.03917, 'loss': 0.63491586, 'lr': 0, 'params': 425270, 'time_iter': 0.04824, 'accuracy': 0.77106, 'f1': 0.77106, 'accuracy-SBM': 0.77109, 'auc': 0.95979}
2025-07-11 11:10:39,885 - INFO - > Epoch 99: took 69.9s (avg 70.1s) | Best so far: epoch 97	train_loss: 0.6067 train_accuracy-SBM: 0.7791	val_loss: 0.6395 val_accuracy-SBM: 0.7703	test_loss: 0.6348 test_accuracy-SBM: 0.7708
2025-07-11 11:10:39,885 - INFO - Avg time per epoch: 70.12s
2025-07-11 11:10:39,885 - INFO - Total train loop time: 1.95h
2025-07-11 11:10:39,887 - INFO - Task done, results saved in results/Cluster/Cluster-GINE-45
2025-07-11 11:10:39,887 - INFO - Total time: 7079.82s (1.97h)
2025-07-11 11:10:39,888 - INFO - Results aggregated across runs saved in results/Cluster/Cluster-GINE-45/agg
2025-07-11 11:10:39,888 - INFO - === OPTIMIZED TRAINING COMPLETED SUCCESSFULLY! ===
2025-07-11 11:10:39,888 - INFO - Results saved in: results/Cluster/Cluster-GINE-45
2025-07-11 11:10:39,888 - INFO - Test results JSON files saved in: results/Cluster/Cluster-GINE-45/test_results/
Completed seed 45. Results saved in results/Cluster/Cluster-GINE-45
----------------------------------------
Submitting next job for seed 47
Submitted batch job 5348793
