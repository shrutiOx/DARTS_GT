Lmod has detected the following error: The following module(s) are unknown:
"cuda/12.0"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore-cache load "cuda/12.0"

Also make sure that all modulefiles written in TCL start with the string
#%Module



              total        used        free      shared  buff/cache   available
Mem:          251Gi        10Gi       217Gi       1.1Gi        22Gi       237Gi
Swap:         1.9Gi        19Mi       1.8Gi
Fri Jul 11 11:11:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA TITAN RTX               On  |   00000000:1E:00.0 Off |                  N/A |
| 41%   45C    P8             20W /  280W |       1MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Running experiment with seed: 47
Starting training for seed 47...
Changed working directory to: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GINE
Loading config from: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GINE/confignas.yaml
Using device: cuda
2025-07-11 11:12:47,129 - INFO - GPU Mem: 25.2GB
2025-07-11 11:12:47,130 - INFO - Run directory: results/Cluster/Cluster-GINE-47
2025-07-11 11:12:47,130 - INFO - Seed: 47
2025-07-11 11:12:47,130 - INFO - === OPTIMIZED MOE CONFIGURATION ===
2025-07-11 11:12:47,130 - INFO - Routing mode: none
2025-07-11 11:12:47,130 - INFO - Expert types: ['GINE', 'CustomGatedGCN', 'GATV2']
2025-07-11 11:12:47,130 - INFO - Number of layers: 16
2025-07-11 11:12:47,130 - INFO - Uncertainty enabled: False
2025-07-11 11:12:47,130 - INFO - Training mode: custom
2025-07-11 11:12:47,130 - INFO - Optimization: Uncertainty only for last layer + test/val
2025-07-11 11:12:47,130 - INFO - Additional features: Router weights logging + JSON export
2025-07-11 11:13:00,219 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 11:13:00,221 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 11:13:00,244 - INFO -   undirected: True
2025-07-11 11:13:00,244 - INFO -   num graphs: 12000
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 11:13:00,244 - INFO -   avg num_nodes/graph: 117
2025-07-11 11:13:00,245 - INFO -   num node features: 7
2025-07-11 11:13:00,245 - INFO -   num edge features: 0
2025-07-11 11:13:00,251 - INFO -   num classes: 6
2025-07-11 11:13:00,251 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 11:13:00,251 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 11:13:00,380 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s]  0%|          | 0/12000 [00:16<?, ?it/s]  1%|          | 81/12000 [00:16<40:16,  4.93it/s]  1%|          | 132/12000 [00:26<39:46,  4.97it/s]  2%|▏         | 184/12000 [00:36<39:07,  5.03it/s]  2%|▏         | 234/12000 [00:46<39:23,  4.98it/s]  2%|▏         | 285/12000 [00:57<38:56,  5.01it/s]  3%|▎         | 334/12000 [01:07<39:10,  4.96it/s]  3%|▎         | 387/12000 [01:17<38:23,  5.04it/s]  4%|▎         | 437/12000 [01:27<38:19,  5.03it/s]  4%|▍         | 485/12000 [01:37<38:52,  4.94it/s]  4%|▍         | 537/12000 [01:47<38:11,  5.00it/s]  5%|▍         | 588/12000 [01:57<37:52,  5.02it/s]  5%|▌         | 633/12000 [02:07<39:00,  4.86it/s]  6%|▌         | 680/12000 [02:17<39:15,  4.81it/s]  6%|▌         | 730/12000 [02:27<38:46,  4.84it/s]  6%|▋         | 779/12000 [02:37<38:39,  4.84it/s]  7%|▋         | 828/12000 [02:48<38:33,  4.83it/s]  7%|▋         | 876/12000 [02:58<38:32,  4.81it/s]  8%|▊         | 927/12000 [03:08<37:46,  4.89it/s]  8%|▊         | 976/12000 [03:18<37:41,  4.87it/s]  9%|▊         | 1023/12000 [03:28<37:57,  4.82it/s]  9%|▉         | 1069/12000 [03:38<38:31,  4.73it/s]  9%|▉         | 1120/12000 [03:48<37:29,  4.84it/s] 10%|▉         | 1169/12000 [03:58<37:17,  4.84it/s] 10%|█         | 1218/12000 [04:08<37:12,  4.83it/s] 11%|█         | 1266/12000 [04:18<37:09,  4.81it/s] 11%|█         | 1313/12000 [04:29<37:22,  4.77it/s] 11%|█▏        | 1360/12000 [04:39<37:29,  4.73it/s] 12%|█▏        | 1408/12000 [04:49<37:13,  4.74it/s] 12%|█▏        | 1460/12000 [04:59<36:01,  4.88it/s] 13%|█▎        | 1509/12000 [05:09<35:57,  4.86it/s] 13%|█▎        | 1558/12000 [05:19<35:50,  4.86it/s] 13%|█▎        | 1606/12000 [05:29<35:57,  4.82it/s] 14%|█▍        | 1653/12000 [05:39<36:08,  4.77it/s] 14%|█▍        | 1701/12000 [05:49<35:56,  4.78it/s] 15%|█▍        | 1749/12000 [05:59<35:55,  4.76it/s] 15%|█▌        | 1800/12000 [06:10<35:10,  4.83it/s] 15%|█▌        | 1848/12000 [06:20<35:05,  4.82it/s] 16%|█▌        | 1897/12000 [06:30<34:46,  4.84it/s] 16%|█▌        | 1946/12000 [06:40<34:31,  4.85it/s] 17%|█▋        | 1994/12000 [06:50<34:37,  4.82it/s] 17%|█▋        | 2042/12000 [07:00<34:41,  4.78it/s] 17%|█▋        | 2090/12000 [07:10<34:30,  4.79it/s] 18%|█▊        | 2136/12000 [07:20<34:47,  4.72it/s] 18%|█▊        | 2183/12000 [07:30<34:42,  4.71it/s] 19%|█▊        | 2230/12000 [07:40<34:36,  4.70it/s] 19%|█▉        | 2279/12000 [07:50<34:07,  4.75it/s] 19%|█▉        | 2325/12000 [08:00<34:18,  4.70it/s] 20%|█▉        | 2375/12000 [08:10<33:36,  4.77it/s] 20%|██        | 2423/12000 [08:21<33:35,  4.75it/s] 21%|██        | 2471/12000 [08:31<33:25,  4.75it/s] 21%|██        | 2520/12000 [08:41<33:26,  4.72it/s] 21%|██▏       | 2569/12000 [08:51<33:03,  4.76it/s] 22%|██▏       | 2616/12000 [09:02<33:09,  4.72it/s] 22%|██▏       | 2665/12000 [09:12<32:45,  4.75it/s] 23%|██▎       | 2711/12000 [09:22<32:56,  4.70it/s] 23%|██▎       | 2758/12000 [09:32<32:48,  4.69it/s] 23%|██▎       | 2809/12000 [09:42<31:53,  4.80it/s] 24%|██▍       | 2858/12000 [09:52<31:33,  4.83it/s] 24%|██▍       | 2905/12000 [10:02<31:52,  4.76it/s] 25%|██▍       | 2954/12000 [10:12<31:36,  4.77it/s] 25%|██▌       | 3002/12000 [10:22<31:23,  4.78it/s] 25%|██▌       | 3050/12000 [10:32<31:16,  4.77it/s] 26%|██▌       | 3097/12000 [10:43<31:25,  4.72it/s] 26%|██▌       | 3143/12000 [10:53<31:39,  4.66it/s] 27%|██▋       | 3191/12000 [11:03<31:17,  4.69it/s] 27%|██▋       | 3241/12000 [11:13<30:40,  4.76it/s] 27%|██▋       | 3284/12000 [11:23<31:35,  4.60it/s] 28%|██▊       | 3325/12000 [11:33<32:39,  4.43it/s] 28%|██▊       | 3364/12000 [11:44<33:48,  4.26it/s] 28%|██▊       | 3410/12000 [11:54<32:59,  4.34it/s] 29%|██▉       | 3456/12000 [12:04<32:27,  4.39it/s] 29%|██▉       | 3504/12000 [12:14<31:33,  4.49it/s] 30%|██▉       | 3551/12000 [12:24<31:08,  4.52it/s] 30%|███       | 3601/12000 [12:34<30:04,  4.66it/s] 30%|███       | 3651/12000 [12:45<29:24,  4.73it/s] 31%|███       | 3700/12000 [12:55<29:05,  4.75it/s] 31%|███       | 3749/12000 [13:05<28:40,  4.80it/s] 32%|███▏      | 3796/12000 [13:15<28:45,  4.76it/s] 32%|███▏      | 3844/12000 [13:25<28:38,  4.75it/s] 32%|███▏      | 3892/12000 [13:35<28:29,  4.74it/s] 33%|███▎      | 3940/12000 [13:45<28:16,  4.75it/s] 33%|███▎      | 3991/12000 [13:55<27:40,  4.82it/s] 34%|███▎      | 4039/12000 [14:06<27:42,  4.79it/s] 34%|███▍      | 4090/12000 [14:16<27:05,  4.87it/s] 34%|███▍      | 4139/12000 [14:26<26:52,  4.87it/s] 35%|███▍      | 4184/12000 [14:36<27:26,  4.75it/s] 35%|███▌      | 4234/12000 [14:46<26:56,  4.81it/s] 36%|███▌      | 4288/12000 [14:56<25:51,  4.97it/s] 36%|███▌      | 4335/12000 [15:06<26:11,  4.88it/s] 37%|███▋      | 4383/12000 [15:16<26:12,  4.84it/s] 37%|███▋      | 4430/12000 [15:26<26:22,  4.78it/s] 37%|███▋      | 4478/12000 [15:36<26:11,  4.79it/s] 38%|███▊      | 4529/12000 [15:46<25:38,  4.86it/s] 38%|███▊      | 4581/12000 [15:57<25:02,  4.94it/s] 39%|███▊      | 4627/12000 [16:07<25:33,  4.81it/s] 39%|███▉      | 4675/12000 [16:17<25:29,  4.79it/s] 39%|███▉      | 4724/12000 [16:27<25:18,  4.79it/s] 40%|███▉      | 4772/12000 [16:37<25:12,  4.78it/s] 40%|████      | 4819/12000 [16:47<25:15,  4.74it/s] 41%|████      | 4868/12000 [16:57<24:52,  4.78it/s] 41%|████      | 4916/12000 [17:08<24:43,  4.78it/s] 41%|████▏     | 4965/12000 [17:18<24:26,  4.80it/s] 42%|████▏     | 5019/12000 [17:28<23:29,  4.95it/s] 42%|████▏     | 5065/12000 [17:38<23:50,  4.85it/s] 43%|████▎     | 5114/12000 [17:48<23:41,  4.84it/s] 43%|████▎     | 5162/12000 [17:58<23:39,  4.82it/s] 43%|████▎     | 5209/12000 [18:08<23:46,  4.76it/s] 44%|████▍     | 5256/12000 [18:18<23:45,  4.73it/s] 44%|████▍     | 5305/12000 [18:28<23:28,  4.75it/s] 45%|████▍     | 5354/12000 [18:39<23:08,  4.79it/s] 45%|████▌     | 5402/12000 [18:49<22:59,  4.78it/s] 45%|████▌     | 5453/12000 [18:59<22:30,  4.85it/s] 46%|████▌     | 5502/12000 [19:09<22:18,  4.85it/s] 46%|████▌     | 5545/12000 [19:19<22:58,  4.68it/s] 47%|████▋     | 5593/12000 [19:29<22:42,  4.70it/s] 47%|████▋     | 5642/12000 [19:39<22:20,  4.74it/s] 47%|████▋     | 5688/12000 [19:49<22:29,  4.68it/s] 48%|████▊     | 5734/12000 [19:59<22:33,  4.63it/s] 48%|████▊     | 5782/12000 [20:10<22:15,  4.66it/s] 49%|████▊     | 5831/12000 [20:20<21:48,  4.71it/s] 49%|████▉     | 5882/12000 [20:30<21:14,  4.80it/s] 49%|████▉     | 5929/12000 [20:40<21:16,  4.76it/s] 50%|████▉     | 5977/12000 [20:50<21:08,  4.75it/s] 50%|█████     | 6025/12000 [21:00<21:01,  4.74it/s] 51%|█████     | 6073/12000 [21:11<20:53,  4.73it/s] 51%|█████     | 6120/12000 [21:21<20:50,  4.70it/s] 51%|█████▏    | 6170/12000 [21:31<20:23,  4.76it/s] 52%|█████▏    | 6218/12000 [21:41<20:13,  4.77it/s] 52%|█████▏    | 6269/12000 [21:51<19:45,  4.83it/s] 53%|█████▎    | 6318/12000 [22:01<19:31,  4.85it/s] 53%|█████▎    | 6365/12000 [22:11<19:34,  4.80it/s] 53%|█████▎    | 6415/12000 [22:21<19:09,  4.86it/s] 54%|█████▍    | 6465/12000 [22:31<18:52,  4.89it/s] 54%|█████▍    | 6512/12000 [22:42<19:00,  4.81it/s] 55%|█████▍    | 6560/12000 [22:52<18:51,  4.81it/s] 55%|█████▌    | 6608/12000 [23:02<18:48,  4.78it/s] 55%|█████▌    | 6656/12000 [23:12<18:42,  4.76it/s] 56%|█████▌    | 6705/12000 [23:22<18:23,  4.80it/s] 56%|█████▋    | 6754/12000 [23:32<18:13,  4.80it/s] 57%|█████▋    | 6802/12000 [23:42<18:09,  4.77it/s] 57%|█████▋    | 6850/12000 [23:52<18:02,  4.76it/s] 58%|█████▊    | 6902/12000 [24:02<17:23,  4.89it/s] 58%|█████▊    | 6951/12000 [24:13<17:16,  4.87it/s] 58%|█████▊    | 6998/12000 [24:23<17:19,  4.81it/s] 59%|█████▊    | 7047/12000 [24:33<17:06,  4.82it/s] 59%|█████▉    | 7094/12000 [24:43<17:07,  4.77it/s] 60%|█████▉    | 7143/12000 [24:53<16:50,  4.81it/s] 60%|█████▉    | 7195/12000 [25:03<16:20,  4.90it/s] 60%|██████    | 7243/12000 [25:13<16:19,  4.86it/s] 61%|██████    | 7290/12000 [25:23<16:23,  4.79it/s] 61%|██████    | 7337/12000 [25:33<16:24,  4.74it/s] 62%|██████▏   | 7386/12000 [25:44<16:06,  4.77it/s] 62%|██████▏   | 7432/12000 [25:54<16:10,  4.70it/s] 62%|██████▏   | 7478/12000 [26:04<16:08,  4.67it/s] 63%|██████▎   | 7526/12000 [26:14<15:56,  4.68it/s] 63%|██████▎   | 7569/12000 [26:24<16:15,  4.54it/s] 63%|██████▎   | 7610/12000 [26:34<16:36,  4.41it/s] 64%|██████▍   | 7651/12000 [26:44<16:54,  4.29it/s] 64%|██████▍   | 7699/12000 [26:54<16:12,  4.42it/s] 65%|██████▍   | 7744/12000 [27:05<15:59,  4.43it/s] 65%|██████▍   | 7793/12000 [27:15<15:25,  4.55it/s] 65%|██████▌   | 7840/12000 [27:25<15:07,  4.58it/s] 66%|██████▌   | 7888/12000 [27:35<14:44,  4.65it/s] 66%|██████▌   | 7936/12000 [27:45<14:29,  4.67it/s] 67%|██████▋   | 7986/12000 [27:55<14:06,  4.74it/s] 67%|██████▋   | 8034/12000 [28:05<13:56,  4.74it/s] 67%|██████▋   | 8084/12000 [28:15<13:32,  4.82it/s] 68%|██████▊   | 8134/12000 [28:25<13:15,  4.86it/s] 68%|██████▊   | 8183/12000 [28:35<13:04,  4.87it/s] 69%|██████▊   | 8232/12000 [28:46<12:56,  4.85it/s] 69%|██████▉   | 8279/12000 [28:56<12:59,  4.78it/s] 69%|██████▉   | 8326/12000 [29:06<12:52,  4.75it/s] 70%|██████▉   | 8376/12000 [29:16<12:34,  4.80it/s] 70%|███████   | 8426/12000 [29:26<12:18,  4.84it/s] 71%|███████   | 8471/12000 [29:36<12:25,  4.73it/s] 71%|███████   | 8523/12000 [29:46<11:56,  4.85it/s] 71%|███████▏  | 8571/12000 [29:56<11:50,  4.83it/s] 72%|███████▏  | 8619/12000 [30:06<11:42,  4.81it/s] 72%|███████▏  | 8667/12000 [30:16<11:33,  4.80it/s] 73%|███████▎  | 8714/12000 [30:27<11:32,  4.75it/s] 73%|███████▎  | 8760/12000 [30:37<11:31,  4.68it/s] 73%|███████▎  | 8807/12000 [30:47<11:21,  4.68it/s] 74%|███████▍  | 8855/12000 [30:57<11:09,  4.70it/s] 74%|███████▍  | 8904/12000 [31:07<10:52,  4.74it/s] 75%|███████▍  | 8955/12000 [31:17<10:32,  4.81it/s] 75%|███████▌  | 9005/12000 [31:27<10:17,  4.85it/s] 75%|███████▌  | 9055/12000 [31:38<10:03,  4.88it/s] 76%|███████▌  | 9105/12000 [31:48<09:50,  4.90it/s] 76%|███████▋  | 9154/12000 [31:58<09:42,  4.89it/s] 77%|███████▋  | 9201/12000 [32:08<09:41,  4.81it/s] 77%|███████▋  | 9246/12000 [32:18<09:46,  4.70it/s] 77%|███████▋  | 9295/12000 [32:28<09:28,  4.76it/s] 78%|███████▊  | 9341/12000 [32:38<09:25,  4.70it/s] 78%|███████▊  | 9392/12000 [32:48<09:04,  4.79it/s] 79%|███████▊  | 9439/12000 [32:58<08:58,  4.75it/s] 79%|███████▉  | 9487/12000 [33:09<08:50,  4.74it/s] 79%|███████▉  | 9535/12000 [33:19<08:40,  4.74it/s] 80%|███████▉  | 9584/12000 [33:29<08:24,  4.79it/s] 80%|████████  | 9631/12000 [33:39<08:19,  4.74it/s] 81%|████████  | 9679/12000 [33:49<08:09,  4.74it/s] 81%|████████  | 9727/12000 [33:59<07:58,  4.75it/s] 81%|████████▏ | 9775/12000 [34:09<07:49,  4.74it/s] 82%|████████▏ | 9826/12000 [34:19<07:28,  4.84it/s] 82%|████████▏ | 9876/12000 [34:29<07:16,  4.87it/s] 83%|████████▎ | 9926/12000 [34:39<07:02,  4.91it/s] 83%|████████▎ | 9975/12000 [34:50<06:54,  4.89it/s] 84%|████████▎ | 10023/12000 [35:00<06:48,  4.84it/s] 84%|████████▍ | 10073/12000 [35:10<06:34,  4.88it/s] 84%|████████▍ | 10122/12000 [35:20<06:26,  4.86it/s] 85%|████████▍ | 10172/12000 [35:30<06:14,  4.89it/s] 85%|████████▌ | 10218/12000 [35:40<06:11,  4.79it/s] 86%|████████▌ | 10265/12000 [35:50<06:04,  4.76it/s] 86%|████████▌ | 10316/12000 [36:00<05:48,  4.83it/s] 86%|████████▋ | 10365/12000 [36:10<05:38,  4.83it/s] 87%|████████▋ | 10411/12000 [36:20<05:33,  4.76it/s] 87%|████████▋ | 10459/12000 [36:31<05:24,  4.76it/s] 88%|████████▊ | 10509/12000 [36:41<05:08,  4.83it/s] 88%|████████▊ | 10558/12000 [36:51<04:57,  4.84it/s] 88%|████████▊ | 10607/12000 [37:01<04:48,  4.83it/s] 89%|████████▉ | 10654/12000 [37:11<04:42,  4.77it/s] 89%|████████▉ | 10703/12000 [37:21<04:31,  4.78it/s] 90%|████████▉ | 10753/12000 [37:31<04:17,  4.84it/s] 90%|█████████ | 10801/12000 [37:41<04:08,  4.82it/s] 90%|█████████ | 10850/12000 [37:51<03:58,  4.82it/s] 91%|█████████ | 10899/12000 [38:01<03:47,  4.84it/s] 91%|█████████▏| 10951/12000 [38:12<03:33,  4.92it/s] 92%|█████████▏| 11001/12000 [38:22<03:23,  4.92it/s] 92%|█████████▏| 11047/12000 [38:32<03:17,  4.81it/s] 92%|█████████▏| 11094/12000 [38:42<03:09,  4.78it/s] 93%|█████████▎| 11142/12000 [38:52<02:59,  4.77it/s] 93%|█████████▎| 11194/12000 [39:02<02:45,  4.88it/s] 94%|█████████▎| 11245/12000 [39:12<02:33,  4.93it/s] 94%|█████████▍| 11295/12000 [39:22<02:22,  4.95it/s] 95%|█████████▍| 11342/12000 [39:32<02:15,  4.85it/s] 95%|█████████▍| 11389/12000 [39:43<02:07,  4.78it/s] 95%|█████████▌| 11438/12000 [39:53<01:57,  4.79it/s] 96%|█████████▌| 11488/12000 [40:03<01:45,  4.85it/s] 96%|█████████▌| 11536/12000 [40:13<01:36,  4.82it/s] 97%|█████████▋| 11585/12000 [40:23<01:25,  4.83it/s] 97%|█████████▋| 11635/12000 [40:33<01:14,  4.87it/s] 97%|█████████▋| 11682/12000 [40:43<01:06,  4.81it/s] 98%|█████████▊| 11729/12000 [40:53<00:56,  4.77it/s] 98%|█████████▊| 11779/12000 [41:03<00:45,  4.82it/s] 99%|█████████▊| 11824/12000 [41:13<00:37,  4.71it/s] 99%|█████████▉| 11866/12000 [41:24<00:29,  4.54it/s] 99%|█████████▉| 11908/12000 [41:34<00:20,  4.42it/s]100%|█████████▉| 11955/12000 [41:44<00:10,  4.49it/s]100%|██████████| 12000/12000 [41:53<00:00,  4.77it/s]
2025-07-11 11:54:54,847 - INFO - Done! Took 00:41:54.60
2025-07-11 11:54:54,867 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 11:54:55,182 - INFO - Created model type: <class 'torch_geometric.graphgym.model_builder.GraphGymModule'>
2025-07-11 11:54:55,182 - INFO - Inner model type: <class 'graphgps.network.uniform_gt_model_edge.UNIFORM_GTModelEdge'>
2025-07-11 11:54:55,182 - INFO - Inner model has get_darts_model: False
2025-07-11 11:54:55,186 - INFO - GraphGymModule(
  (model): UNIFORM_GTModelEdge(
    (encoder): FeatureEncoder(
      (node_encoder): LapPENodeEncoder(
        (linear_x): Linear(in_features=7, out_features=32, bias=True)
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (edge_encoder): DummyEdgeEncoder(
        (encoder): Embedding(1, 48)
      )
    )
    (layers): Sequential(
      (0): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (1): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (2): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (3): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (4): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (5): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (6): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (7): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (8): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (9): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (10): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (11): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (12): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (13): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (14): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (15): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (post_mp): GNNInductiveNodeHead(
      (layer_post_mp): MLP(
        (model): Sequential(
          (0): GeneralMultiLayer(
            (Layer_0): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
            (Layer_1): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
          )
          (1): Linear(
            (model): Linear(48, 6, bias=True)
          )
        )
      )
    )
  )
)
2025-07-11 11:54:55,191 - INFO - Number of parameters: 425,270
2025-07-11 11:54:55,191 - INFO - Starting optimized training: 2025-07-11 11:54:55.191134
2025-07-11 11:55:00,699 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
2025-07-11 11:55:00,700 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 11:55:00,719 - INFO -   undirected: True
2025-07-11 11:55:00,719 - INFO -   num graphs: 12000
2025-07-11 11:55:00,720 - INFO -   avg num_nodes/graph: 117
2025-07-11 11:55:00,720 - INFO -   num node features: 7
2025-07-11 11:55:00,720 - INFO -   num edge features: 0
2025-07-11 11:55:00,730 - INFO -   num classes: 6
2025-07-11 11:55:00,730 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 11:55:00,730 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 11:55:00,927 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s]  0%|          | 0/12000 [00:16<?, ?it/s]  1%|          | 75/12000 [00:16<43:16,  4.59it/s]  1%|          | 127/12000 [00:26<40:48,  4.85it/s]  1%|▏         | 174/12000 [00:36<41:15,  4.78it/s]  2%|▏         | 222/12000 [00:46<41:06,  4.78it/s]  2%|▏         | 272/12000 [00:56<40:31,  4.82it/s]  3%|▎         | 321/12000 [01:06<40:19,  4.83it/s]  3%|▎         | 369/12000 [01:16<40:17,  4.81it/s]  3%|▎         | 418/12000 [01:27<40:07,  4.81it/s]  4%|▍         | 468/12000 [01:37<39:33,  4.86it/s]  4%|▍         | 520/12000 [01:47<38:43,  4.94it/s]  5%|▍         | 567/12000 [01:57<39:10,  4.86it/s]  5%|▌         | 616/12000 [02:07<38:58,  4.87it/s]  6%|▌         | 664/12000 [02:17<39:02,  4.84it/s]  6%|▌         | 714/12000 [02:27<38:44,  4.86it/s]  6%|▋         | 761/12000 [02:37<39:00,  4.80it/s]  7%|▋         | 807/12000 [02:47<39:26,  4.73it/s]  7%|▋         | 858/12000 [02:57<38:30,  4.82it/s]  8%|▊         | 912/12000 [03:07<37:02,  4.99it/s]  8%|▊         | 959/12000 [03:18<37:44,  4.88it/s]  8%|▊         | 1008/12000 [03:28<37:36,  4.87it/s]  9%|▉         | 1055/12000 [03:38<37:56,  4.81it/s]  9%|▉         | 1105/12000 [03:48<37:21,  4.86it/s] 10%|▉         | 1156/12000 [03:58<36:44,  4.92it/s] 10%|█         | 1205/12000 [04:08<36:50,  4.88it/s] 10%|█         | 1253/12000 [04:18<37:03,  4.83it/s] 11%|█         | 1301/12000 [04:29<37:11,  4.79it/s] 11%|█▏        | 1353/12000 [04:39<36:09,  4.91it/s] 12%|█▏        | 1400/12000 [04:49<36:37,  4.82it/s] 12%|█▏        | 1450/12000 [04:59<36:11,  4.86it/s] 12%|█▏        | 1495/12000 [05:09<37:02,  4.73it/s] 13%|█▎        | 1542/12000 [05:19<37:09,  4.69it/s] 13%|█▎        | 1592/12000 [05:29<36:25,  4.76it/s] 14%|█▎        | 1642/12000 [05:39<35:50,  4.82it/s] 14%|█▍        | 1691/12000 [05:50<35:37,  4.82it/s] 14%|█▍        | 1739/12000 [06:00<35:32,  4.81it/s] 15%|█▍        | 1789/12000 [06:10<34:58,  4.87it/s] 15%|█▌        | 1837/12000 [06:20<35:06,  4.83it/s] 16%|█▌        | 1885/12000 [06:30<35:04,  4.81it/s] 16%|█▌        | 1935/12000 [06:40<34:39,  4.84it/s] 17%|█▋        | 1981/12000 [06:50<35:05,  4.76it/s] 17%|█▋        | 2030/12000 [07:00<34:39,  4.79it/s] 17%|█▋        | 2079/12000 [07:10<34:16,  4.82it/s] 18%|█▊        | 2126/12000 [07:20<34:30,  4.77it/s] 18%|█▊        | 2175/12000 [07:30<34:07,  4.80it/s] 19%|█▊        | 2223/12000 [07:40<34:03,  4.79it/s] 19%|█▉        | 2282/12000 [07:50<31:39,  5.12it/s] 20%|█▉        | 2348/12000 [08:01<29:01,  5.54it/s] 20%|██        | 2412/12000 [08:11<27:34,  5.80it/s] 21%|██        | 2475/12000 [08:21<26:42,  5.95it/s] 21%|██        | 2540/12000 [08:31<25:53,  6.09it/s] 22%|██▏       | 2608/12000 [08:41<24:50,  6.30it/s] 22%|██▏       | 2675/12000 [08:51<24:13,  6.42it/s] 23%|██▎       | 2746/12000 [09:01<23:20,  6.61it/s] 23%|██▎       | 2813/12000 [09:11<23:09,  6.61it/s] 24%|██▍       | 2894/12000 [09:21<21:35,  7.03it/s] 25%|██▍       | 2973/12000 [09:31<20:43,  7.26it/s] 25%|██▌       | 3031/12000 [09:41<21:56,  6.81it/s] 26%|██▌       | 3089/12000 [09:51<22:51,  6.50it/s] 26%|██▋       | 3162/12000 [10:01<21:53,  6.73it/s] 27%|██▋       | 3250/12000 [10:12<19:57,  7.31it/s] 28%|██▊       | 3325/12000 [10:22<19:38,  7.36it/s] 28%|██▊       | 3389/12000 [10:32<20:18,  7.07it/s] 29%|██▊       | 3449/12000 [10:42<21:12,  6.72it/s] 29%|██▉       | 3524/12000 [10:52<20:19,  6.95it/s] 30%|██▉       | 3596/12000 [11:02<19:58,  7.01it/s] 31%|███       | 3666/12000 [11:12<19:52,  6.99it/s] 31%|███       | 3728/12000 [11:22<20:26,  6.74it/s] 32%|███▏      | 3797/12000 [11:32<20:08,  6.79it/s] 32%|███▏      | 3860/12000 [11:42<20:28,  6.63it/s] 33%|███▎      | 3927/12000 [11:52<20:18,  6.63it/s] 33%|███▎      | 3991/12000 [12:02<20:22,  6.55it/s] 34%|███▍      | 4053/12000 [12:12<20:37,  6.42it/s] 34%|███▍      | 4118/12000 [12:22<20:28,  6.41it/s] 35%|███▍      | 4180/12000 [12:33<20:32,  6.34it/s] 35%|███▌      | 4239/12000 [12:43<20:49,  6.21it/s] 36%|███▌      | 4307/12000 [12:53<20:06,  6.38it/s] 36%|███▋      | 4366/12000 [13:03<20:26,  6.22it/s] 37%|███▋      | 4427/12000 [13:13<20:24,  6.18it/s] 37%|███▋      | 4493/12000 [13:23<19:53,  6.29it/s] 38%|███▊      | 4553/12000 [13:33<20:08,  6.16it/s] 38%|███▊      | 4602/12000 [13:43<21:24,  5.76it/s] 39%|███▉      | 4653/12000 [13:53<22:03,  5.55it/s] 39%|███▉      | 4700/12000 [14:03<23:00,  5.29it/s] 40%|███▉      | 4744/12000 [14:13<24:12,  5.00it/s] 40%|███▉      | 4785/12000 [14:23<25:27,  4.72it/s] 40%|████      | 4828/12000 [14:34<26:00,  4.59it/s] 41%|████      | 4873/12000 [14:44<26:08,  4.54it/s] 41%|████      | 4916/12000 [14:54<26:26,  4.47it/s] 41%|████▏     | 4965/12000 [15:04<25:38,  4.57it/s] 42%|████▏     | 5014/12000 [15:14<25:02,  4.65it/s] 42%|████▏     | 5062/12000 [15:24<24:42,  4.68it/s] 43%|████▎     | 5112/12000 [15:34<24:03,  4.77it/s] 43%|████▎     | 5160/12000 [15:44<23:52,  4.77it/s] 43%|████▎     | 5208/12000 [15:54<23:44,  4.77it/s] 44%|████▍     | 5255/12000 [16:04<23:42,  4.74it/s] 44%|████▍     | 5313/12000 [16:14<22:04,  5.05it/s] 45%|████▍     | 5364/12000 [16:25<21:54,  5.05it/s] 45%|████▌     | 5413/12000 [16:35<21:58,  5.00it/s] 46%|████▌     | 5462/12000 [16:45<22:02,  4.95it/s] 46%|████▌     | 5511/12000 [16:55<22:00,  4.91it/s] 46%|████▋     | 5559/12000 [17:05<22:01,  4.87it/s] 47%|████▋     | 5610/12000 [17:15<21:38,  4.92it/s] 47%|████▋     | 5659/12000 [17:25<21:31,  4.91it/s] 48%|████▊     | 5711/12000 [17:35<21:06,  4.97it/s] 48%|████▊     | 5759/12000 [17:45<21:15,  4.89it/s] 48%|████▊     | 5810/12000 [17:56<20:52,  4.94it/s] 49%|████▉     | 5859/12000 [18:06<20:53,  4.90it/s] 49%|████▉     | 5908/12000 [18:16<20:47,  4.88it/s] 50%|████▉     | 5958/12000 [18:26<20:34,  4.89it/s] 50%|█████     | 6006/12000 [18:36<20:37,  4.85it/s] 50%|█████     | 6055/12000 [18:46<20:28,  4.84it/s] 51%|█████     | 6104/12000 [18:56<20:14,  4.85it/s] 51%|█████▏    | 6155/12000 [19:06<19:49,  4.91it/s] 52%|█████▏    | 6205/12000 [19:17<19:39,  4.91it/s] 52%|█████▏    | 6253/12000 [19:27<19:43,  4.86it/s] 53%|█████▎    | 6303/12000 [19:37<19:24,  4.89it/s] 53%|█████▎    | 6352/12000 [19:47<19:19,  4.87it/s] 53%|█████▎    | 6400/12000 [19:57<19:17,  4.84it/s] 54%|█████▎    | 6447/12000 [20:07<19:20,  4.78it/s] 54%|█████▍    | 6494/12000 [20:17<19:20,  4.74it/s] 55%|█████▍    | 6543/12000 [20:27<18:59,  4.79it/s] 55%|█████▍    | 6594/12000 [20:37<18:31,  4.86it/s] 55%|█████▌    | 6642/12000 [20:47<18:26,  4.84it/s] 56%|█████▌    | 6689/12000 [20:57<18:28,  4.79it/s] 56%|█████▌    | 6739/12000 [21:08<18:05,  4.85it/s] 57%|█████▋    | 6788/12000 [21:18<17:54,  4.85it/s] 57%|█████▋    | 6835/12000 [21:28<17:58,  4.79it/s] 57%|█████▋    | 6882/12000 [21:38<17:58,  4.75it/s] 58%|█████▊    | 6930/12000 [21:48<17:44,  4.76it/s] 58%|█████▊    | 6978/12000 [21:58<17:36,  4.75it/s] 59%|█████▊    | 7028/12000 [22:08<17:10,  4.83it/s] 59%|█████▉    | 7078/12000 [22:18<16:52,  4.86it/s] 59%|█████▉    | 7126/12000 [22:28<16:46,  4.84it/s] 60%|█████▉    | 7173/12000 [22:38<16:47,  4.79it/s] 60%|██████    | 7220/12000 [22:48<16:45,  4.75it/s] 61%|██████    | 7269/12000 [22:58<16:28,  4.79it/s] 61%|██████    | 7317/12000 [23:08<16:19,  4.78it/s] 61%|██████▏   | 7366/12000 [23:18<16:04,  4.80it/s] 62%|██████▏   | 7415/12000 [23:29<15:54,  4.81it/s] 62%|██████▏   | 7461/12000 [23:39<16:00,  4.73it/s] 63%|██████▎   | 7508/12000 [23:49<15:54,  4.71it/s] 63%|██████▎   | 7555/12000 [23:59<15:47,  4.69it/s] 63%|██████▎   | 7603/12000 [24:09<15:33,  4.71it/s] 64%|██████▍   | 7652/12000 [24:19<15:15,  4.75it/s] 64%|██████▍   | 7700/12000 [24:29<15:04,  4.75it/s] 65%|██████▍   | 7748/12000 [24:39<14:57,  4.74it/s] 65%|██████▍   | 7796/12000 [24:49<14:44,  4.75it/s] 65%|██████▌   | 7845/12000 [25:00<14:28,  4.79it/s] 66%|██████▌   | 7892/12000 [25:10<14:24,  4.75it/s] 66%|██████▌   | 7941/12000 [25:20<14:07,  4.79it/s] 67%|██████▋   | 7991/12000 [25:30<13:48,  4.84it/s] 67%|██████▋   | 8038/12000 [25:40<13:45,  4.80it/s] 67%|██████▋   | 8086/12000 [25:50<13:39,  4.77it/s] 68%|██████▊   | 8135/12000 [26:00<13:28,  4.78it/s] 68%|██████▊   | 8185/12000 [26:10<13:09,  4.83it/s] 69%|██████▊   | 8233/12000 [26:20<13:03,  4.81it/s] 69%|██████▉   | 8281/12000 [26:30<12:55,  4.80it/s] 69%|██████▉   | 8331/12000 [26:41<12:38,  4.84it/s] 70%|██████▉   | 8378/12000 [26:51<12:36,  4.79it/s] 70%|███████   | 8426/12000 [27:01<12:27,  4.78it/s] 71%|███████   | 8474/12000 [27:11<12:17,  4.78it/s] 71%|███████   | 8525/12000 [27:21<11:55,  4.85it/s] 71%|███████▏  | 8573/12000 [27:31<11:48,  4.84it/s] 72%|███████▏  | 8621/12000 [27:41<11:40,  4.82it/s] 72%|███████▏  | 8671/12000 [27:51<11:22,  4.88it/s] 73%|███████▎  | 8719/12000 [28:01<11:16,  4.85it/s] 73%|███████▎  | 8768/12000 [28:11<11:07,  4.84it/s] 73%|███████▎  | 8816/12000 [28:21<11:00,  4.82it/s] 74%|███████▍  | 8866/12000 [28:31<10:43,  4.87it/s] 74%|███████▍  | 8913/12000 [28:41<10:40,  4.82it/s] 75%|███████▍  | 8967/12000 [28:51<10:07,  4.99it/s] 75%|███████▌  | 9012/12000 [29:01<10:18,  4.83it/s] 75%|███████▌  | 9055/12000 [29:11<10:30,  4.67it/s] 76%|███████▌  | 9097/12000 [29:21<10:42,  4.52it/s] 76%|███████▌  | 9142/12000 [29:32<10:36,  4.49it/s] 77%|███████▋  | 9192/12000 [29:42<10:08,  4.61it/s] 77%|███████▋  | 9244/12000 [29:52<09:37,  4.78it/s] 77%|███████▋  | 9293/12000 [30:02<09:23,  4.81it/s] 78%|███████▊  | 9339/12000 [30:12<09:23,  4.72it/s] 78%|███████▊  | 9389/12000 [30:22<09:06,  4.78it/s] 79%|███████▊  | 9437/12000 [30:32<08:56,  4.78it/s] 79%|███████▉  | 9485/12000 [30:42<08:48,  4.76it/s] 79%|███████▉  | 9531/12000 [30:52<08:43,  4.71it/s] 80%|███████▉  | 9578/12000 [31:02<08:35,  4.70it/s] 80%|████████  | 9627/12000 [31:13<08:19,  4.75it/s] 81%|████████  | 9676/12000 [31:23<08:06,  4.78it/s] 81%|████████  | 9726/12000 [31:33<07:51,  4.83it/s] 81%|████████▏ | 9773/12000 [31:43<07:46,  4.78it/s] 82%|████████▏ | 9822/12000 [31:53<07:32,  4.81it/s] 82%|████████▏ | 9869/12000 [32:03<07:27,  4.76it/s] 83%|████████▎ | 9918/12000 [32:13<07:14,  4.79it/s] 83%|████████▎ | 9965/12000 [32:23<07:07,  4.76it/s] 83%|████████▎ | 10015/12000 [32:33<06:52,  4.81it/s] 84%|████████▍ | 10062/12000 [32:43<06:46,  4.77it/s] 84%|████████▍ | 10109/12000 [32:53<06:39,  4.73it/s] 85%|████████▍ | 10157/12000 [33:04<06:28,  4.74it/s] 85%|████████▌ | 10205/12000 [33:14<06:18,  4.74it/s] 85%|████████▌ | 10253/12000 [33:24<06:08,  4.74it/s] 86%|████████▌ | 10298/12000 [33:34<06:04,  4.67it/s] 86%|████████▌ | 10347/12000 [33:44<05:50,  4.72it/s] 87%|████████▋ | 10396/12000 [33:54<05:36,  4.77it/s] 87%|████████▋ | 10444/12000 [34:04<05:25,  4.78it/s] 87%|████████▋ | 10492/12000 [34:14<05:16,  4.76it/s] 88%|████████▊ | 10539/12000 [34:24<05:09,  4.73it/s] 88%|████████▊ | 10588/12000 [34:34<04:56,  4.76it/s] 89%|████████▊ | 10634/12000 [34:44<04:50,  4.70it/s] 89%|████████▉ | 10685/12000 [34:55<04:33,  4.81it/s] 89%|████████▉ | 10735/12000 [35:05<04:21,  4.83it/s] 90%|████████▉ | 10781/12000 [35:15<04:16,  4.76it/s] 90%|█████████ | 10828/12000 [35:25<04:07,  4.73it/s] 91%|█████████ | 10876/12000 [35:35<03:56,  4.74it/s] 91%|█████████ | 10924/12000 [35:45<03:46,  4.74it/s] 91%|█████████▏| 10972/12000 [35:55<03:36,  4.75it/s] 92%|█████████▏| 11022/12000 [36:05<03:23,  4.82it/s] 92%|█████████▏| 11070/12000 [36:15<03:13,  4.80it/s] 93%|█████████▎| 11120/12000 [36:25<03:01,  4.84it/s] 93%|█████████▎| 11167/12000 [36:35<02:53,  4.79it/s] 93%|█████████▎| 11217/12000 [36:46<02:41,  4.84it/s] 94%|█████████▍| 11264/12000 [36:56<02:33,  4.79it/s] 94%|█████████▍| 11314/12000 [37:06<02:22,  4.82it/s] 95%|█████████▍| 11363/12000 [37:16<02:11,  4.83it/s] 95%|█████████▌| 11412/12000 [37:26<02:01,  4.84it/s] 96%|█████████▌| 11460/12000 [37:36<01:52,  4.80it/s] 96%|█████████▌| 11508/12000 [37:46<01:42,  4.79it/s] 96%|█████████▋| 11558/12000 [37:56<01:31,  4.85it/s] 97%|█████████▋| 11607/12000 [38:06<01:21,  4.85it/s] 97%|█████████▋| 11656/12000 [38:16<01:10,  4.85it/s] 98%|█████████▊| 11705/12000 [38:27<01:00,  4.86it/s] 98%|█████████▊| 11755/12000 [38:37<00:50,  4.87it/s] 98%|█████████▊| 11807/12000 [38:47<00:39,  4.95it/s] 99%|█████████▉| 11858/12000 [38:57<00:28,  4.98it/s] 99%|█████████▉| 11908/12000 [39:07<00:18,  4.98it/s]100%|█████████▉| 11958/12000 [39:17<00:08,  4.97it/s]100%|██████████| 12000/12000 [39:25<00:00,  5.07it/s]
2025-07-11 12:34:27,704 - INFO - Done! Took 00:39:26.97
2025-07-11 12:34:27,728 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 12:34:27,745 - INFO - === STARTING OPTIMIZED TRAINING ===
2025-07-11 12:34:27,745 - INFO - Start from epoch 0
2025-07-11 12:35:43,850 - INFO - train: {'epoch': 0, 'time_epoch': 75.62286, 'eta': 7486.66354, 'eta_hours': 2.07963, 'loss': 1.79304057, 'lr': 0.0, 'params': 425270, 'time_iter': 0.121, 'accuracy': 0.16588, 'f1': 0.1198, 'accuracy-SBM': 0.16582, 'auc': 0.5005}
2025-07-11 12:35:43,858 - INFO - ...computing epoch stats took: 0.47s
2025-07-11 12:35:48,008 - INFO - val: {'epoch': 0, 'time_epoch': 4.09531, 'loss': 1.79337019, 'lr': 0, 'params': 425270, 'time_iter': 0.065, 'accuracy': 0.16596, 'f1': 0.10303, 'accuracy-SBM': 0.16706, 'auc': 0.50053}
2025-07-11 12:35:48,011 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 12:35:52,145 - INFO - test: {'epoch': 0, 'time_epoch': 4.07931, 'loss': 1.79337871, 'lr': 0, 'params': 425270, 'time_iter': 0.06475, 'accuracy': 0.1643, 'f1': 0.10036, 'accuracy-SBM': 0.16476, 'auc': 0.49999}
2025-07-11 12:35:52,148 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 12:35:52,148 - INFO - > Epoch 0: took 84.4s (avg 84.4s) | Best so far: epoch 0	train_loss: 1.7930 train_accuracy-SBM: 0.1658	val_loss: 1.7934 val_accuracy-SBM: 0.1671	test_loss: 1.7934 test_accuracy-SBM: 0.1648
2025-07-11 12:37:06,606 - INFO - train: {'epoch': 1, 'time_epoch': 74.21794, 'eta': 7342.19942, 'eta_hours': 2.0395, 'loss': 1.76623599, 'lr': 0.0002, 'params': 425270, 'time_iter': 0.11875, 'accuracy': 0.21354, 'f1': 0.21114, 'accuracy-SBM': 0.21358, 'auc': 0.56609}
2025-07-11 12:37:06,612 - INFO - ...computing epoch stats took: 0.23s
2025-07-11 12:37:10,674 - INFO - val: {'epoch': 1, 'time_epoch': 4.01633, 'loss': 1.83429346, 'lr': 0, 'params': 425270, 'time_iter': 0.06375, 'accuracy': 0.17385, 'f1': 0.1225, 'accuracy-SBM': 0.17312, 'auc': 0.5182}
2025-07-11 12:37:10,676 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 12:37:14,760 - INFO - test: {'epoch': 1, 'time_epoch': 4.03182, 'loss': 1.83669581, 'lr': 0, 'params': 425270, 'time_iter': 0.064, 'accuracy': 0.17124, 'f1': 0.11851, 'accuracy-SBM': 0.17222, 'auc': 0.51964}
2025-07-11 12:37:14,763 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 12:37:14,763 - INFO - > Epoch 1: took 82.6s (avg 83.5s) | Best so far: epoch 1	train_loss: 1.7662 train_accuracy-SBM: 0.2136	val_loss: 1.8343 val_accuracy-SBM: 0.1731	test_loss: 1.8367 test_accuracy-SBM: 0.1722
2025-07-11 12:38:29,422 - INFO - train: {'epoch': 2, 'time_epoch': 74.43649, 'eta': 7251.63267, 'eta_hours': 2.01434, 'loss': 1.55710309, 'lr': 0.0004, 'params': 425270, 'time_iter': 0.1191, 'accuracy': 0.35399, 'f1': 0.34412, 'accuracy-SBM': 0.35411, 'auc': 0.7293}
2025-07-11 12:38:29,428 - INFO - ...computing epoch stats took: 0.21s
2025-07-11 12:38:33,449 - INFO - val: {'epoch': 2, 'time_epoch': 3.96623, 'loss': 1.66999404, 'lr': 0, 'params': 425270, 'time_iter': 0.06296, 'accuracy': 0.29832, 'f1': 0.282, 'accuracy-SBM': 0.29902, 'auc': 0.68101}
2025-07-11 12:38:33,451 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 12:38:37,580 - INFO - test: {'epoch': 2, 'time_epoch': 4.07733, 'loss': 1.6477003, 'lr': 0, 'params': 425270, 'time_iter': 0.06472, 'accuracy': 0.30827, 'f1': 0.29106, 'accuracy-SBM': 0.30815, 'auc': 0.69035}
2025-07-11 12:38:37,582 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 12:38:37,583 - INFO - > Epoch 2: took 82.8s (avg 83.3s) | Best so far: epoch 2	train_loss: 1.5571 train_accuracy-SBM: 0.3541	val_loss: 1.6700 val_accuracy-SBM: 0.2990	test_loss: 1.6477 test_accuracy-SBM: 0.3081
2025-07-11 12:39:53,086 - INFO - train: {'epoch': 3, 'time_epoch': 75.25095, 'eta': 7188.67787, 'eta_hours': 1.99685, 'loss': 1.37379525, 'lr': 0.0006, 'params': 425270, 'time_iter': 0.1204, 'accuracy': 0.44307, 'f1': 0.43437, 'accuracy-SBM': 0.44315, 'auc': 0.80097}
2025-07-11 12:39:57,214 - INFO - val: {'epoch': 3, 'time_epoch': 4.07255, 'loss': 1.87713458, 'lr': 0, 'params': 425270, 'time_iter': 0.06464, 'accuracy': 0.2494, 'f1': 0.22098, 'accuracy-SBM': 0.24886, 'auc': 0.60364}
2025-07-11 12:40:01,397 - INFO - test: {'epoch': 3, 'time_epoch': 4.13862, 'loss': 1.85963462, 'lr': 0, 'params': 425270, 'time_iter': 0.06569, 'accuracy': 0.25586, 'f1': 0.23114, 'accuracy-SBM': 0.25624, 'auc': 0.61144}
2025-07-11 12:40:01,400 - INFO - > Epoch 3: took 83.8s (avg 83.4s) | Best so far: epoch 2	train_loss: 1.5571 train_accuracy-SBM: 0.3541	val_loss: 1.6700 val_accuracy-SBM: 0.2990	test_loss: 1.6477 test_accuracy-SBM: 0.3081
2025-07-11 12:41:17,495 - INFO - train: {'epoch': 4, 'time_epoch': 75.87145, 'eta': 7132.59428, 'eta_hours': 1.98128, 'loss': 1.28374414, 'lr': 0.0008, 'params': 425270, 'time_iter': 0.12139, 'accuracy': 0.47502, 'f1': 0.46658, 'accuracy-SBM': 0.47507, 'auc': 0.82526}
2025-07-11 12:41:21,683 - INFO - val: {'epoch': 4, 'time_epoch': 4.13681, 'loss': 1.36826014, 'lr': 0, 'params': 425270, 'time_iter': 0.06566, 'accuracy': 0.44738, 'f1': 0.44836, 'accuracy-SBM': 0.44716, 'auc': 0.80505}
2025-07-11 12:41:25,748 - INFO - test: {'epoch': 4, 'time_epoch': 4.02345, 'loss': 1.35116324, 'lr': 0, 'params': 425270, 'time_iter': 0.06386, 'accuracy': 0.45947, 'f1': 0.46002, 'accuracy-SBM': 0.45982, 'auc': 0.81078}
2025-07-11 12:41:25,750 - INFO - > Epoch 4: took 84.3s (avg 83.6s) | Best so far: epoch 4	train_loss: 1.2837 train_accuracy-SBM: 0.4751	val_loss: 1.3683 val_accuracy-SBM: 0.4472	test_loss: 1.3512 test_accuracy-SBM: 0.4598
2025-07-11 12:42:41,200 - INFO - train: {'epoch': 5, 'time_epoch': 75.2259, 'eta': 7059.80102, 'eta_hours': 1.96106, 'loss': 1.19080104, 'lr': 0.001, 'params': 425270, 'time_iter': 0.12036, 'accuracy': 0.51854, 'f1': 0.50983, 'accuracy-SBM': 0.51854, 'auc': 0.84993}
2025-07-11 12:42:45,329 - INFO - val: {'epoch': 5, 'time_epoch': 4.05942, 'loss': 1.34536482, 'lr': 0, 'params': 425270, 'time_iter': 0.06444, 'accuracy': 0.46928, 'f1': 0.45865, 'accuracy-SBM': 0.46867, 'auc': 0.81317}
2025-07-11 12:42:49,364 - INFO - test: {'epoch': 5, 'time_epoch': 3.99267, 'loss': 1.32475855, 'lr': 0, 'params': 425270, 'time_iter': 0.06338, 'accuracy': 0.47704, 'f1': 0.46697, 'accuracy-SBM': 0.47807, 'auc': 0.81996}
2025-07-11 12:42:49,367 - INFO - > Epoch 5: took 83.6s (avg 83.6s) | Best so far: epoch 5	train_loss: 1.1908 train_accuracy-SBM: 0.5185	val_loss: 1.3454 val_accuracy-SBM: 0.4687	test_loss: 1.3248 test_accuracy-SBM: 0.4781
2025-07-11 12:44:04,332 - INFO - train: {'epoch': 6, 'time_epoch': 74.73422, 'eta': 6979.78048, 'eta_hours': 1.93883, 'loss': 1.12079725, 'lr': 0.00099973, 'params': 425270, 'time_iter': 0.11957, 'accuracy': 0.54556, 'f1': 0.5355, 'accuracy-SBM': 0.54554, 'auc': 0.86658}
2025-07-11 12:44:08,350 - INFO - val: {'epoch': 6, 'time_epoch': 3.96439, 'loss': 1.37220057, 'lr': 0, 'params': 425270, 'time_iter': 0.06293, 'accuracy': 0.45527, 'f1': 0.4362, 'accuracy-SBM': 0.45655, 'auc': 0.84481}
2025-07-11 12:44:12,395 - INFO - test: {'epoch': 6, 'time_epoch': 4.00377, 'loss': 1.33830618, 'lr': 0, 'params': 425270, 'time_iter': 0.06355, 'accuracy': 0.46726, 'f1': 0.44847, 'accuracy-SBM': 0.46831, 'auc': 0.8522}
2025-07-11 12:44:12,398 - INFO - > Epoch 6: took 83.0s (avg 83.5s) | Best so far: epoch 5	train_loss: 1.1908 train_accuracy-SBM: 0.5185	val_loss: 1.3454 val_accuracy-SBM: 0.4687	test_loss: 1.3248 test_accuracy-SBM: 0.4781
2025-07-11 12:45:27,810 - INFO - train: {'epoch': 7, 'time_epoch': 75.1889, 'eta': 6906.31034, 'eta_hours': 1.91842, 'loss': 1.07867385, 'lr': 0.00099891, 'params': 425270, 'time_iter': 0.1203, 'accuracy': 0.56016, 'f1': 0.5495, 'accuracy-SBM': 0.56013, 'auc': 0.87562}
2025-07-11 12:45:31,858 - INFO - val: {'epoch': 7, 'time_epoch': 3.99395, 'loss': 1.20756773, 'lr': 0, 'params': 425270, 'time_iter': 0.0634, 'accuracy': 0.52025, 'f1': 0.51147, 'accuracy-SBM': 0.5198, 'auc': 0.84728}
2025-07-11 12:45:35,834 - INFO - test: {'epoch': 7, 'time_epoch': 3.93182, 'loss': 1.19112699, 'lr': 0, 'params': 425270, 'time_iter': 0.06241, 'accuracy': 0.5285, 'f1': 0.52014, 'accuracy-SBM': 0.5292, 'auc': 0.85222}
2025-07-11 12:45:35,836 - INFO - > Epoch 7: took 83.4s (avg 83.5s) | Best so far: epoch 7	train_loss: 1.0787 train_accuracy-SBM: 0.5601	val_loss: 1.2076 val_accuracy-SBM: 0.5198	test_loss: 1.1911 test_accuracy-SBM: 0.5292
2025-07-11 12:46:50,563 - INFO - train: {'epoch': 8, 'time_epoch': 74.42073, 'eta': 6824.69116, 'eta_hours': 1.89575, 'loss': 1.05512701, 'lr': 0.00099754, 'params': 425270, 'time_iter': 0.11907, 'accuracy': 0.56918, 'f1': 0.55549, 'accuracy-SBM': 0.56915, 'auc': 0.88039}
2025-07-11 12:46:54,591 - INFO - val: {'epoch': 8, 'time_epoch': 3.97541, 'loss': 1.11609262, 'lr': 0, 'params': 425270, 'time_iter': 0.0631, 'accuracy': 0.55681, 'f1': 0.5468, 'accuracy-SBM': 0.55626, 'auc': 0.87013}
2025-07-11 12:46:58,703 - INFO - test: {'epoch': 8, 'time_epoch': 4.07064, 'loss': 1.08466412, 'lr': 0, 'params': 425270, 'time_iter': 0.06461, 'accuracy': 0.56418, 'f1': 0.55497, 'accuracy-SBM': 0.56532, 'auc': 0.87678}
2025-07-11 12:46:58,705 - INFO - > Epoch 8: took 82.9s (avg 83.4s) | Best so far: epoch 8	train_loss: 1.0551 train_accuracy-SBM: 0.5692	val_loss: 1.1161 val_accuracy-SBM: 0.5563	test_loss: 1.0847 test_accuracy-SBM: 0.5653
2025-07-11 12:48:13,904 - INFO - train: {'epoch': 9, 'time_epoch': 74.87422, 'eta': 6748.59305, 'eta_hours': 1.87461, 'loss': 1.03922364, 'lr': 0.00099563, 'params': 425270, 'time_iter': 0.1198, 'accuracy': 0.57444, 'f1': 0.56395, 'accuracy-SBM': 0.5744, 'auc': 0.88354}
2025-07-11 12:48:17,998 - INFO - val: {'epoch': 9, 'time_epoch': 4.0391, 'loss': 1.19760778, 'lr': 0, 'params': 425270, 'time_iter': 0.06411, 'accuracy': 0.52267, 'f1': 0.47645, 'accuracy-SBM': 0.52186, 'auc': 0.85708}
2025-07-11 12:48:22,095 - INFO - test: {'epoch': 9, 'time_epoch': 4.0542, 'loss': 1.17896385, 'lr': 0, 'params': 425270, 'time_iter': 0.06435, 'accuracy': 0.52506, 'f1': 0.47799, 'accuracy-SBM': 0.52656, 'auc': 0.86219}
2025-07-11 12:48:22,098 - INFO - > Epoch 9: took 83.4s (avg 83.4s) | Best so far: epoch 8	train_loss: 1.0551 train_accuracy-SBM: 0.5692	val_loss: 1.1161 val_accuracy-SBM: 0.5563	test_loss: 1.0847 test_accuracy-SBM: 0.5653
2025-07-11 12:49:36,851 - INFO - train: {'epoch': 10, 'time_epoch': 74.4343, 'eta': 6669.15815, 'eta_hours': 1.85254, 'loss': 1.02826825, 'lr': 0.00099318, 'params': 425270, 'time_iter': 0.11909, 'accuracy': 0.57778, 'f1': 0.5668, 'accuracy-SBM': 0.57775, 'auc': 0.88565}
2025-07-11 12:49:40,854 - INFO - val: {'epoch': 10, 'time_epoch': 3.95005, 'loss': 1.2200601, 'lr': 0, 'params': 425270, 'time_iter': 0.0627, 'accuracy': 0.54182, 'f1': 0.53156, 'accuracy-SBM': 0.54163, 'auc': 0.85144}
2025-07-11 12:49:44,825 - INFO - test: {'epoch': 10, 'time_epoch': 3.93151, 'loss': 1.18981885, 'lr': 0, 'params': 425270, 'time_iter': 0.0624, 'accuracy': 0.54558, 'f1': 0.53527, 'accuracy-SBM': 0.54681, 'auc': 0.85774}
2025-07-11 12:49:44,828 - INFO - > Epoch 10: took 82.7s (avg 83.4s) | Best so far: epoch 8	train_loss: 1.0551 train_accuracy-SBM: 0.5692	val_loss: 1.1161 val_accuracy-SBM: 0.5563	test_loss: 1.0847 test_accuracy-SBM: 0.5653
2025-07-11 12:50:58,643 - INFO - train: {'epoch': 11, 'time_epoch': 73.57899, 'eta': 6584.2844, 'eta_hours': 1.82897, 'loss': 1.01682246, 'lr': 0.00099019, 'params': 425270, 'time_iter': 0.11773, 'accuracy': 0.58229, 'f1': 0.57027, 'accuracy-SBM': 0.58225, 'auc': 0.88792}
2025-07-11 12:51:02,627 - INFO - val: {'epoch': 11, 'time_epoch': 3.93321, 'loss': 1.15193844, 'lr': 0, 'params': 425270, 'time_iter': 0.06243, 'accuracy': 0.56101, 'f1': 0.54539, 'accuracy-SBM': 0.55946, 'auc': 0.86501}
2025-07-11 12:51:06,614 - INFO - test: {'epoch': 11, 'time_epoch': 3.95158, 'loss': 1.07400328, 'lr': 0, 'params': 425270, 'time_iter': 0.06272, 'accuracy': 0.57437, 'f1': 0.55944, 'accuracy-SBM': 0.57502, 'auc': 0.88021}
2025-07-11 12:51:06,617 - INFO - > Epoch 11: took 81.8s (avg 83.2s) | Best so far: epoch 11	train_loss: 1.0168 train_accuracy-SBM: 0.5823	val_loss: 1.1519 val_accuracy-SBM: 0.5595	test_loss: 1.0740 test_accuracy-SBM: 0.5750
2025-07-11 12:52:20,089 - INFO - train: {'epoch': 12, 'time_epoch': 73.23514, 'eta': 6498.84716, 'eta_hours': 1.80524, 'loss': 1.00859306, 'lr': 0.00098666, 'params': 425270, 'time_iter': 0.11718, 'accuracy': 0.58468, 'f1': 0.57287, 'accuracy-SBM': 0.58464, 'auc': 0.88946}
2025-07-11 12:52:24,003 - INFO - val: {'epoch': 12, 'time_epoch': 3.86248, 'loss': 1.05965393, 'lr': 0, 'params': 425270, 'time_iter': 0.06131, 'accuracy': 0.57219, 'f1': 0.55258, 'accuracy-SBM': 0.5712, 'auc': 0.88183}
2025-07-11 12:52:27,891 - INFO - test: {'epoch': 12, 'time_epoch': 3.84823, 'loss': 1.06214392, 'lr': 0, 'params': 425270, 'time_iter': 0.06108, 'accuracy': 0.57155, 'f1': 0.55264, 'accuracy-SBM': 0.57269, 'auc': 0.88117}
2025-07-11 12:52:27,894 - INFO - > Epoch 12: took 81.3s (avg 83.1s) | Best so far: epoch 12	train_loss: 1.0086 train_accuracy-SBM: 0.5846	val_loss: 1.0597 val_accuracy-SBM: 0.5712	test_loss: 1.0621 test_accuracy-SBM: 0.5727
2025-07-11 12:53:40,973 - INFO - train: {'epoch': 13, 'time_epoch': 72.86304, 'eta': 6412.86732, 'eta_hours': 1.78135, 'loss': 1.00309054, 'lr': 0.0009826, 'params': 425270, 'time_iter': 0.11658, 'accuracy': 0.58738, 'f1': 0.57512, 'accuracy-SBM': 0.58735, 'auc': 0.8906}
2025-07-11 12:53:45,001 - INFO - val: {'epoch': 13, 'time_epoch': 3.96493, 'loss': 1.333605, 'lr': 0, 'params': 425270, 'time_iter': 0.06294, 'accuracy': 0.52568, 'f1': 0.48837, 'accuracy-SBM': 0.52676, 'auc': 0.83165}
2025-07-11 12:53:48,925 - INFO - test: {'epoch': 13, 'time_epoch': 3.88232, 'loss': 1.28697309, 'lr': 0, 'params': 425270, 'time_iter': 0.06162, 'accuracy': 0.53422, 'f1': 0.49623, 'accuracy-SBM': 0.53595, 'auc': 0.83935}
2025-07-11 12:53:48,928 - INFO - > Epoch 13: took 81.0s (avg 82.9s) | Best so far: epoch 12	train_loss: 1.0086 train_accuracy-SBM: 0.5846	val_loss: 1.0597 val_accuracy-SBM: 0.5712	test_loss: 1.0621 test_accuracy-SBM: 0.5727
2025-07-11 12:55:03,901 - INFO - train: {'epoch': 14, 'time_epoch': 74.75106, 'eta': 6339.33515, 'eta_hours': 1.76093, 'loss': 0.99741806, 'lr': 0.00097802, 'params': 425270, 'time_iter': 0.1196, 'accuracy': 0.58967, 'f1': 0.57699, 'accuracy-SBM': 0.58964, 'auc': 0.89163}
2025-07-11 12:55:08,006 - INFO - val: {'epoch': 14, 'time_epoch': 4.05511, 'loss': 1.06274221, 'lr': 0, 'params': 425270, 'time_iter': 0.06437, 'accuracy': 0.57331, 'f1': 0.54345, 'accuracy-SBM': 0.57347, 'auc': 0.88216}
2025-07-11 12:55:12,017 - INFO - test: {'epoch': 14, 'time_epoch': 3.97366, 'loss': 1.02656611, 'lr': 0, 'params': 425270, 'time_iter': 0.06307, 'accuracy': 0.58251, 'f1': 0.55209, 'accuracy-SBM': 0.58411, 'auc': 0.88913}
2025-07-11 12:55:12,020 - INFO - > Epoch 14: took 83.1s (avg 83.0s) | Best so far: epoch 14	train_loss: 0.9974 train_accuracy-SBM: 0.5896	val_loss: 1.0627 val_accuracy-SBM: 0.5735	test_loss: 1.0266 test_accuracy-SBM: 0.5841
2025-07-11 12:56:25,100 - INFO - train: {'epoch': 15, 'time_epoch': 72.85551, 'eta': 6255.69899, 'eta_hours': 1.73769, 'loss': 0.9888011, 'lr': 0.00097291, 'params': 425270, 'time_iter': 0.11657, 'accuracy': 0.59241, 'f1': 0.57983, 'accuracy-SBM': 0.59237, 'auc': 0.89334}
2025-07-11 12:56:29,059 - INFO - val: {'epoch': 15, 'time_epoch': 3.90822, 'loss': 1.02562548, 'lr': 0, 'params': 425270, 'time_iter': 0.06204, 'accuracy': 0.58352, 'f1': 0.57576, 'accuracy-SBM': 0.58316, 'auc': 0.88931}
2025-07-11 12:56:32,997 - INFO - test: {'epoch': 15, 'time_epoch': 3.89232, 'loss': 1.01543076, 'lr': 0, 'params': 425270, 'time_iter': 0.06178, 'accuracy': 0.58822, 'f1': 0.5816, 'accuracy-SBM': 0.58923, 'auc': 0.89115}
2025-07-11 12:56:33,000 - INFO - > Epoch 15: took 81.0s (avg 82.8s) | Best so far: epoch 15	train_loss: 0.9888 train_accuracy-SBM: 0.5924	val_loss: 1.0256 val_accuracy-SBM: 0.5832	test_loss: 1.0154 test_accuracy-SBM: 0.5892
2025-07-11 12:57:46,208 - INFO - train: {'epoch': 16, 'time_epoch': 72.98739, 'eta': 6173.97503, 'eta_hours': 1.71499, 'loss': 0.98056665, 'lr': 0.00096728, 'params': 425270, 'time_iter': 0.11678, 'accuracy': 0.59483, 'f1': 0.58278, 'accuracy-SBM': 0.59479, 'auc': 0.89486}
2025-07-11 12:57:50,120 - INFO - val: {'epoch': 16, 'time_epoch': 3.85638, 'loss': 1.00709418, 'lr': 0, 'params': 425270, 'time_iter': 0.06121, 'accuracy': 0.58737, 'f1': 0.55917, 'accuracy-SBM': 0.58551, 'auc': 0.89304}
2025-07-11 12:57:54,090 - INFO - test: {'epoch': 16, 'time_epoch': 3.91694, 'loss': 0.99745561, 'lr': 0, 'params': 425270, 'time_iter': 0.06217, 'accuracy': 0.58853, 'f1': 0.56043, 'accuracy-SBM': 0.58848, 'auc': 0.89466}
2025-07-11 12:57:54,093 - INFO - > Epoch 16: took 81.1s (avg 82.7s) | Best so far: epoch 16	train_loss: 0.9806 train_accuracy-SBM: 0.5948	val_loss: 1.0071 val_accuracy-SBM: 0.5855	test_loss: 0.9975 test_accuracy-SBM: 0.5885
2025-07-11 12:59:07,440 - INFO - train: {'epoch': 17, 'time_epoch': 73.12956, 'eta': 6093.86946, 'eta_hours': 1.69274, 'loss': 0.97708732, 'lr': 0.00096114, 'params': 425270, 'time_iter': 0.11701, 'accuracy': 0.59651, 'f1': 0.58413, 'accuracy-SBM': 0.59647, 'auc': 0.89548}
2025-07-11 12:59:11,416 - INFO - val: {'epoch': 17, 'time_epoch': 3.929, 'loss': 1.01582444, 'lr': 0, 'params': 425270, 'time_iter': 0.06237, 'accuracy': 0.589, 'f1': 0.54243, 'accuracy-SBM': 0.58644, 'auc': 0.88918}
2025-07-11 12:59:15,472 - INFO - test: {'epoch': 17, 'time_epoch': 4.00482, 'loss': 0.99092228, 'lr': 0, 'params': 425270, 'time_iter': 0.06357, 'accuracy': 0.59345, 'f1': 0.54757, 'accuracy-SBM': 0.59338, 'auc': 0.89409}
2025-07-11 12:59:15,475 - INFO - > Epoch 17: took 81.4s (avg 82.7s) | Best so far: epoch 17	train_loss: 0.9771 train_accuracy-SBM: 0.5965	val_loss: 1.0158 val_accuracy-SBM: 0.5864	test_loss: 0.9909 test_accuracy-SBM: 0.5934
2025-07-11 13:00:28,489 - INFO - train: {'epoch': 18, 'time_epoch': 72.79913, 'eta': 6013.08955, 'eta_hours': 1.6703, 'loss': 0.97348535, 'lr': 0.0009545, 'params': 425270, 'time_iter': 0.11648, 'accuracy': 0.59731, 'f1': 0.58581, 'accuracy-SBM': 0.59727, 'auc': 0.89618}
2025-07-11 13:00:32,508 - INFO - val: {'epoch': 18, 'time_epoch': 3.94956, 'loss': 1.92919053, 'lr': 0, 'params': 425270, 'time_iter': 0.06269, 'accuracy': 0.48743, 'f1': 0.47178, 'accuracy-SBM': 0.48706, 'auc': 0.75326}
2025-07-11 13:00:36,447 - INFO - test: {'epoch': 18, 'time_epoch': 3.88511, 'loss': 1.77836457, 'lr': 0, 'params': 425270, 'time_iter': 0.06167, 'accuracy': 0.50415, 'f1': 0.48836, 'accuracy-SBM': 0.50525, 'auc': 0.77452}
2025-07-11 13:00:36,449 - INFO - > Epoch 18: took 81.0s (avg 82.6s) | Best so far: epoch 17	train_loss: 0.9771 train_accuracy-SBM: 0.5965	val_loss: 1.0158 val_accuracy-SBM: 0.5864	test_loss: 0.9909 test_accuracy-SBM: 0.5934
2025-07-11 13:01:49,685 - INFO - train: {'epoch': 19, 'time_epoch': 72.83559, 'eta': 5933.25353, 'eta_hours': 1.64813, 'loss': 0.9675885, 'lr': 0.00094736, 'params': 425270, 'time_iter': 0.11654, 'accuracy': 0.59894, 'f1': 0.58704, 'accuracy-SBM': 0.5989, 'auc': 0.89729}
2025-07-11 13:01:53,659 - INFO - val: {'epoch': 19, 'time_epoch': 3.9195, 'loss': 1.04371261, 'lr': 0, 'params': 425270, 'time_iter': 0.06221, 'accuracy': 0.58938, 'f1': 0.54895, 'accuracy-SBM': 0.58988, 'auc': 0.88521}
2025-07-11 13:01:57,601 - INFO - test: {'epoch': 19, 'time_epoch': 3.90166, 'loss': 1.01214685, 'lr': 0, 'params': 425270, 'time_iter': 0.06193, 'accuracy': 0.59491, 'f1': 0.55296, 'accuracy-SBM': 0.59677, 'auc': 0.89119}
2025-07-11 13:01:57,603 - INFO - > Epoch 19: took 81.2s (avg 82.5s) | Best so far: epoch 19	train_loss: 0.9676 train_accuracy-SBM: 0.5989	val_loss: 1.0437 val_accuracy-SBM: 0.5899	test_loss: 1.0121 test_accuracy-SBM: 0.5968
2025-07-11 13:03:10,360 - INFO - train: {'epoch': 20, 'time_epoch': 72.43843, 'eta': 5852.59017, 'eta_hours': 1.62572, 'loss': 0.96269767, 'lr': 0.00093974, 'params': 425270, 'time_iter': 0.1159, 'accuracy': 0.6, 'f1': 0.58878, 'accuracy-SBM': 0.59996, 'auc': 0.89827}
2025-07-11 13:03:14,327 - INFO - val: {'epoch': 20, 'time_epoch': 3.91423, 'loss': 0.98027807, 'lr': 0, 'params': 425270, 'time_iter': 0.06213, 'accuracy': 0.59967, 'f1': 0.5804, 'accuracy-SBM': 0.59825, 'auc': 0.89639}
2025-07-11 13:03:18,355 - INFO - test: {'epoch': 20, 'time_epoch': 3.98831, 'loss': 0.98723772, 'lr': 0, 'params': 425270, 'time_iter': 0.06331, 'accuracy': 0.5912, 'f1': 0.57157, 'accuracy-SBM': 0.59194, 'auc': 0.89555}
2025-07-11 13:03:18,358 - INFO - > Epoch 20: took 80.8s (avg 82.4s) | Best so far: epoch 20	train_loss: 0.9627 train_accuracy-SBM: 0.6000	val_loss: 0.9803 val_accuracy-SBM: 0.5982	test_loss: 0.9872 test_accuracy-SBM: 0.5919
2025-07-11 13:04:31,790 - INFO - train: {'epoch': 21, 'time_epoch': 73.18875, 'eta': 5775.33473, 'eta_hours': 1.60426, 'loss': 0.95540381, 'lr': 0.00093163, 'params': 425270, 'time_iter': 0.1171, 'accuracy': 0.60253, 'f1': 0.59122, 'accuracy-SBM': 0.60249, 'auc': 0.8996}
2025-07-11 13:04:35,751 - INFO - val: {'epoch': 21, 'time_epoch': 3.91009, 'loss': 1.93240008, 'lr': 0, 'params': 425270, 'time_iter': 0.06206, 'accuracy': 0.47314, 'f1': 0.43711, 'accuracy-SBM': 0.47025, 'auc': 0.74549}
2025-07-11 13:04:39,698 - INFO - test: {'epoch': 21, 'time_epoch': 3.90554, 'loss': 1.91085431, 'lr': 0, 'params': 425270, 'time_iter': 0.06199, 'accuracy': 0.47383, 'f1': 0.4376, 'accuracy-SBM': 0.4736, 'auc': 0.74978}
2025-07-11 13:04:39,700 - INFO - > Epoch 21: took 81.3s (avg 82.4s) | Best so far: epoch 20	train_loss: 0.9627 train_accuracy-SBM: 0.6000	val_loss: 0.9803 val_accuracy-SBM: 0.5982	test_loss: 0.9872 test_accuracy-SBM: 0.5919
2025-07-11 13:05:52,701 - INFO - train: {'epoch': 22, 'time_epoch': 72.74683, 'eta': 5696.95346, 'eta_hours': 1.58249, 'loss': 0.95342022, 'lr': 0.00092305, 'params': 425270, 'time_iter': 0.11639, 'accuracy': 0.60388, 'f1': 0.59234, 'accuracy-SBM': 0.60383, 'auc': 0.89989}
2025-07-11 13:05:56,653 - INFO - val: {'epoch': 22, 'time_epoch': 3.89992, 'loss': 0.94208455, 'lr': 0, 'params': 425270, 'time_iter': 0.0619, 'accuracy': 0.60799, 'f1': 0.59836, 'accuracy-SBM': 0.60726, 'auc': 0.9035}
2025-07-11 13:06:00,600 - INFO - test: {'epoch': 22, 'time_epoch': 3.89386, 'loss': 0.93459333, 'lr': 0, 'params': 425270, 'time_iter': 0.06181, 'accuracy': 0.60282, 'f1': 0.5928, 'accuracy-SBM': 0.60393, 'auc': 0.90485}
2025-07-11 13:06:00,602 - INFO - > Epoch 22: took 80.9s (avg 82.3s) | Best so far: epoch 22	train_loss: 0.9534 train_accuracy-SBM: 0.6038	val_loss: 0.9421 val_accuracy-SBM: 0.6073	test_loss: 0.9346 test_accuracy-SBM: 0.6039
2025-07-11 13:07:13,887 - INFO - train: {'epoch': 23, 'time_epoch': 72.96214, 'eta': 5619.72354, 'eta_hours': 1.56103, 'loss': 0.94794668, 'lr': 0.000914, 'params': 425270, 'time_iter': 0.11674, 'accuracy': 0.60552, 'f1': 0.59493, 'accuracy-SBM': 0.60547, 'auc': 0.90094}
2025-07-11 13:07:17,808 - INFO - val: {'epoch': 23, 'time_epoch': 3.87072, 'loss': 0.9217812, 'lr': 0, 'params': 425270, 'time_iter': 0.06144, 'accuracy': 0.61572, 'f1': 0.58104, 'accuracy-SBM': 0.61615, 'auc': 0.9065}
2025-07-11 13:07:21,854 - INFO - test: {'epoch': 23, 'time_epoch': 4.00229, 'loss': 0.91699385, 'lr': 0, 'params': 425270, 'time_iter': 0.06353, 'accuracy': 0.61449, 'f1': 0.57938, 'accuracy-SBM': 0.61641, 'auc': 0.90688}
2025-07-11 13:07:21,856 - INFO - > Epoch 23: took 81.3s (avg 82.3s) | Best so far: epoch 23	train_loss: 0.9479 train_accuracy-SBM: 0.6055	val_loss: 0.9218 val_accuracy-SBM: 0.6161	test_loss: 0.9170 test_accuracy-SBM: 0.6164
2025-07-11 13:08:35,594 - INFO - train: {'epoch': 24, 'time_epoch': 73.50343, 'eta': 5544.45892, 'eta_hours': 1.54013, 'loss': 0.94712079, 'lr': 0.00090451, 'params': 425270, 'time_iter': 0.11761, 'accuracy': 0.60605, 'f1': 0.59475, 'accuracy-SBM': 0.60601, 'auc': 0.90107}
2025-07-11 13:08:39,666 - INFO - val: {'epoch': 24, 'time_epoch': 4.02123, 'loss': 1.09546087, 'lr': 0, 'params': 425270, 'time_iter': 0.06383, 'accuracy': 0.5851, 'f1': 0.56534, 'accuracy-SBM': 0.58479, 'auc': 0.87809}
2025-07-11 13:08:43,657 - INFO - test: {'epoch': 24, 'time_epoch': 3.95166, 'loss': 1.03647009, 'lr': 0, 'params': 425270, 'time_iter': 0.06272, 'accuracy': 0.59176, 'f1': 0.571, 'accuracy-SBM': 0.59328, 'auc': 0.88804}
2025-07-11 13:08:43,659 - INFO - > Epoch 24: took 81.8s (avg 82.2s) | Best so far: epoch 23	train_loss: 0.9479 train_accuracy-SBM: 0.6055	val_loss: 0.9218 val_accuracy-SBM: 0.6161	test_loss: 0.9170 test_accuracy-SBM: 0.6164
2025-07-11 13:09:57,825 - INFO - train: {'epoch': 25, 'time_epoch': 73.92993, 'eta': 5470.54365, 'eta_hours': 1.5196, 'loss': 0.94358458, 'lr': 0.00089457, 'params': 425270, 'time_iter': 0.11829, 'accuracy': 0.60681, 'f1': 0.59378, 'accuracy-SBM': 0.60677, 'auc': 0.90176}
2025-07-11 13:10:02,100 - INFO - val: {'epoch': 25, 'time_epoch': 4.21646, 'loss': 0.92377172, 'lr': 0, 'params': 425270, 'time_iter': 0.06693, 'accuracy': 0.61468, 'f1': 0.59302, 'accuracy-SBM': 0.61452, 'auc': 0.90622}
2025-07-11 13:10:06,103 - INFO - test: {'epoch': 25, 'time_epoch': 3.95903, 'loss': 0.9130435, 'lr': 0, 'params': 425270, 'time_iter': 0.06284, 'accuracy': 0.61316, 'f1': 0.5918, 'accuracy-SBM': 0.61481, 'auc': 0.90824}
2025-07-11 13:10:06,106 - INFO - > Epoch 25: took 82.4s (avg 82.2s) | Best so far: epoch 23	train_loss: 0.9479 train_accuracy-SBM: 0.6055	val_loss: 0.9218 val_accuracy-SBM: 0.6161	test_loss: 0.9170 test_accuracy-SBM: 0.6164
2025-07-11 13:11:19,905 - INFO - train: {'epoch': 26, 'time_epoch': 73.57415, 'eta': 5395.66538, 'eta_hours': 1.4988, 'loss': 0.93949983, 'lr': 0.0008842, 'params': 425270, 'time_iter': 0.11772, 'accuracy': 0.60837, 'f1': 0.59551, 'accuracy-SBM': 0.60833, 'auc': 0.90249}
2025-07-11 13:11:23,858 - INFO - val: {'epoch': 26, 'time_epoch': 3.8991, 'loss': 0.94164038, 'lr': 0, 'params': 425270, 'time_iter': 0.06189, 'accuracy': 0.60834, 'f1': 0.58325, 'accuracy-SBM': 0.60841, 'auc': 0.90367}
2025-07-11 13:11:27,831 - INFO - test: {'epoch': 26, 'time_epoch': 3.92896, 'loss': 0.93486953, 'lr': 0, 'params': 425270, 'time_iter': 0.06236, 'accuracy': 0.60553, 'f1': 0.58056, 'accuracy-SBM': 0.60719, 'auc': 0.90506}
2025-07-11 13:11:27,833 - INFO - > Epoch 26: took 81.7s (avg 82.2s) | Best so far: epoch 23	train_loss: 0.9479 train_accuracy-SBM: 0.6055	val_loss: 0.9218 val_accuracy-SBM: 0.6161	test_loss: 0.9170 test_accuracy-SBM: 0.6164
2025-07-11 13:12:40,573 - INFO - train: {'epoch': 27, 'time_epoch': 72.51725, 'eta': 5318.16251, 'eta_hours': 1.47727, 'loss': 0.93682144, 'lr': 0.00087341, 'params': 425270, 'time_iter': 0.11603, 'accuracy': 0.60899, 'f1': 0.59657, 'accuracy-SBM': 0.60895, 'auc': 0.90296}
2025-07-11 13:12:44,588 - INFO - val: {'epoch': 27, 'time_epoch': 3.96482, 'loss': 0.94426255, 'lr': 0, 'params': 425270, 'time_iter': 0.06293, 'accuracy': 0.60996, 'f1': 0.59856, 'accuracy-SBM': 0.60923, 'auc': 0.90235}
2025-07-11 13:12:48,530 - INFO - test: {'epoch': 27, 'time_epoch': 3.89429, 'loss': 0.93068548, 'lr': 0, 'params': 425270, 'time_iter': 0.06181, 'accuracy': 0.60483, 'f1': 0.59307, 'accuracy-SBM': 0.60605, 'auc': 0.90457}
2025-07-11 13:12:48,532 - INFO - > Epoch 27: took 80.7s (avg 82.2s) | Best so far: epoch 23	train_loss: 0.9479 train_accuracy-SBM: 0.6055	val_loss: 0.9218 val_accuracy-SBM: 0.6161	test_loss: 0.9170 test_accuracy-SBM: 0.6164
2025-07-11 13:14:01,118 - INFO - train: {'epoch': 28, 'time_epoch': 72.37524, 'eta': 5240.65581, 'eta_hours': 1.45574, 'loss': 0.93492129, 'lr': 0.00086221, 'params': 425270, 'time_iter': 0.1158, 'accuracy': 0.61145, 'f1': 0.59621, 'accuracy-SBM': 0.61141, 'auc': 0.90336}
2025-07-11 13:14:05,011 - INFO - val: {'epoch': 28, 'time_epoch': 3.8376, 'loss': 0.93584413, 'lr': 0, 'params': 425270, 'time_iter': 0.06091, 'accuracy': 0.61198, 'f1': 0.59295, 'accuracy-SBM': 0.61203, 'auc': 0.90482}
2025-07-11 13:14:08,967 - INFO - test: {'epoch': 28, 'time_epoch': 3.91702, 'loss': 0.93508874, 'lr': 0, 'params': 425270, 'time_iter': 0.06217, 'accuracy': 0.6115, 'f1': 0.59234, 'accuracy-SBM': 0.61306, 'auc': 0.90501}
2025-07-11 13:14:08,969 - INFO - > Epoch 28: took 80.4s (avg 82.1s) | Best so far: epoch 23	train_loss: 0.9479 train_accuracy-SBM: 0.6055	val_loss: 0.9218 val_accuracy-SBM: 0.6161	test_loss: 0.9170 test_accuracy-SBM: 0.6164
2025-07-11 13:15:22,093 - INFO - train: {'epoch': 29, 'time_epoch': 72.81047, 'eta': 5164.50673, 'eta_hours': 1.43459, 'loss': 0.92869279, 'lr': 0.00085062, 'params': 425270, 'time_iter': 0.1165, 'accuracy': 0.61247, 'f1': 0.59848, 'accuracy-SBM': 0.61243, 'auc': 0.90452}
2025-07-11 13:15:25,986 - INFO - val: {'epoch': 29, 'time_epoch': 3.84311, 'loss': 0.96213695, 'lr': 0, 'params': 425270, 'time_iter': 0.061, 'accuracy': 0.60737, 'f1': 0.58393, 'accuracy-SBM': 0.60727, 'auc': 0.90119}
2025-07-11 13:15:29,963 - INFO - test: {'epoch': 29, 'time_epoch': 3.93109, 'loss': 0.92349217, 'lr': 0, 'params': 425270, 'time_iter': 0.0624, 'accuracy': 0.61285, 'f1': 0.5912, 'accuracy-SBM': 0.61434, 'auc': 0.90695}
2025-07-11 13:15:29,966 - INFO - > Epoch 29: took 81.0s (avg 82.1s) | Best so far: epoch 23	train_loss: 0.9479 train_accuracy-SBM: 0.6055	val_loss: 0.9218 val_accuracy-SBM: 0.6161	test_loss: 0.9170 test_accuracy-SBM: 0.6164
2025-07-11 13:16:42,741 - INFO - train: {'epoch': 30, 'time_epoch': 72.55091, 'eta': 5087.99531, 'eta_hours': 1.41333, 'loss': 0.92826682, 'lr': 0.00083864, 'params': 425270, 'time_iter': 0.11608, 'accuracy': 0.61272, 'f1': 0.59863, 'accuracy-SBM': 0.61268, 'auc': 0.90455}
2025-07-11 13:16:46,709 - INFO - val: {'epoch': 30, 'time_epoch': 3.92176, 'loss': 1.0223874, 'lr': 0, 'params': 425270, 'time_iter': 0.06225, 'accuracy': 0.60311, 'f1': 0.58395, 'accuracy-SBM': 0.60323, 'auc': 0.89119}
2025-07-11 13:16:50,575 - INFO - test: {'epoch': 30, 'time_epoch': 3.82682, 'loss': 0.96742256, 'lr': 0, 'params': 425270, 'time_iter': 0.06074, 'accuracy': 0.60864, 'f1': 0.59019, 'accuracy-SBM': 0.61024, 'auc': 0.89992}
2025-07-11 13:16:50,578 - INFO - > Epoch 30: took 80.6s (avg 82.0s) | Best so far: epoch 23	train_loss: 0.9479 train_accuracy-SBM: 0.6055	val_loss: 0.9218 val_accuracy-SBM: 0.6161	test_loss: 0.9170 test_accuracy-SBM: 0.6164
2025-07-11 13:18:03,766 - INFO - train: {'epoch': 31, 'time_epoch': 72.86835, 'eta': 5012.40598, 'eta_hours': 1.39233, 'loss': 0.92402881, 'lr': 0.00082629, 'params': 425270, 'time_iter': 0.11659, 'accuracy': 0.61402, 'f1': 0.60036, 'accuracy-SBM': 0.61398, 'auc': 0.90532}
2025-07-11 13:18:07,747 - INFO - val: {'epoch': 31, 'time_epoch': 3.9288, 'loss': 0.94108673, 'lr': 0, 'params': 425270, 'time_iter': 0.06236, 'accuracy': 0.61155, 'f1': 0.59569, 'accuracy-SBM': 0.61123, 'auc': 0.90409}
2025-07-11 13:18:11,750 - INFO - test: {'epoch': 31, 'time_epoch': 3.95745, 'loss': 0.91770837, 'lr': 0, 'params': 425270, 'time_iter': 0.06282, 'accuracy': 0.61341, 'f1': 0.59684, 'accuracy-SBM': 0.6148, 'auc': 0.90792}
2025-07-11 13:18:11,752 - INFO - > Epoch 31: took 81.2s (avg 82.0s) | Best so far: epoch 23	train_loss: 0.9479 train_accuracy-SBM: 0.6055	val_loss: 0.9218 val_accuracy-SBM: 0.6161	test_loss: 0.9170 test_accuracy-SBM: 0.6164
2025-07-11 13:19:25,001 - INFO - train: {'epoch': 32, 'time_epoch': 72.8381, 'eta': 4936.92015, 'eta_hours': 1.37137, 'loss': 0.92120057, 'lr': 0.00081359, 'params': 425270, 'time_iter': 0.11654, 'accuracy': 0.61581, 'f1': 0.6033, 'accuracy-SBM': 0.61577, 'auc': 0.90582}
2025-07-11 13:19:28,911 - INFO - val: {'epoch': 32, 'time_epoch': 3.859, 'loss': 1.0599303, 'lr': 0, 'params': 425270, 'time_iter': 0.06125, 'accuracy': 0.59114, 'f1': 0.5673, 'accuracy-SBM': 0.59118, 'auc': 0.8829}
2025-07-11 13:19:32,919 - INFO - test: {'epoch': 32, 'time_epoch': 3.96922, 'loss': 0.99916608, 'lr': 0, 'params': 425270, 'time_iter': 0.063, 'accuracy': 0.60096, 'f1': 0.57749, 'accuracy-SBM': 0.60259, 'auc': 0.8932}
2025-07-11 13:19:32,922 - INFO - > Epoch 32: took 81.2s (avg 82.0s) | Best so far: epoch 23	train_loss: 0.9479 train_accuracy-SBM: 0.6055	val_loss: 0.9218 val_accuracy-SBM: 0.6161	test_loss: 0.9170 test_accuracy-SBM: 0.6164
2025-07-11 13:20:47,382 - INFO - train: {'epoch': 33, 'time_epoch': 74.23126, 'eta': 4864.29443, 'eta_hours': 1.35119, 'loss': 0.9187205, 'lr': 0.00080054, 'params': 425270, 'time_iter': 0.11877, 'accuracy': 0.61569, 'f1': 0.60207, 'accuracy-SBM': 0.61565, 'auc': 0.90633}
2025-07-11 13:20:51,450 - INFO - val: {'epoch': 33, 'time_epoch': 4.00851, 'loss': 0.90698449, 'lr': 0, 'params': 425270, 'time_iter': 0.06363, 'accuracy': 0.62166, 'f1': 0.60618, 'accuracy-SBM': 0.62132, 'auc': 0.90942}
2025-07-11 13:20:55,441 - INFO - test: {'epoch': 33, 'time_epoch': 3.95095, 'loss': 0.89356105, 'lr': 0, 'params': 425270, 'time_iter': 0.06271, 'accuracy': 0.62046, 'f1': 0.60495, 'accuracy-SBM': 0.62185, 'auc': 0.91148}
2025-07-11 13:20:55,443 - INFO - > Epoch 33: took 82.5s (avg 82.0s) | Best so far: epoch 33	train_loss: 0.9187 train_accuracy-SBM: 0.6157	val_loss: 0.9070 val_accuracy-SBM: 0.6213	test_loss: 0.8936 test_accuracy-SBM: 0.6219
2025-07-11 13:22:09,170 - INFO - train: {'epoch': 34, 'time_epoch': 73.43216, 'eta': 4790.09291, 'eta_hours': 1.33058, 'loss': 0.91375786, 'lr': 0.00078716, 'params': 425270, 'time_iter': 0.11749, 'accuracy': 0.61835, 'f1': 0.60497, 'accuracy-SBM': 0.61831, 'auc': 0.90721}
2025-07-11 13:22:13,190 - INFO - val: {'epoch': 34, 'time_epoch': 3.96823, 'loss': 0.95824159, 'lr': 0, 'params': 425270, 'time_iter': 0.06299, 'accuracy': 0.61712, 'f1': 0.60372, 'accuracy-SBM': 0.61693, 'auc': 0.90111}
2025-07-11 13:22:17,192 - INFO - test: {'epoch': 34, 'time_epoch': 3.95611, 'loss': 0.9151289, 'lr': 0, 'params': 425270, 'time_iter': 0.0628, 'accuracy': 0.61787, 'f1': 0.60527, 'accuracy-SBM': 0.61922, 'auc': 0.90777}
2025-07-11 13:22:17,194 - INFO - > Epoch 34: took 81.8s (avg 82.0s) | Best so far: epoch 33	train_loss: 0.9187 train_accuracy-SBM: 0.6157	val_loss: 0.9070 val_accuracy-SBM: 0.6213	test_loss: 0.8936 test_accuracy-SBM: 0.6219
2025-07-11 13:23:31,019 - INFO - train: {'epoch': 35, 'time_epoch': 73.60276, 'eta': 4716.23743, 'eta_hours': 1.31007, 'loss': 0.9119518, 'lr': 0.00077347, 'params': 425270, 'time_iter': 0.11776, 'accuracy': 0.61963, 'f1': 0.60699, 'accuracy-SBM': 0.61959, 'auc': 0.90759}
2025-07-11 13:23:35,008 - INFO - val: {'epoch': 35, 'time_epoch': 3.92299, 'loss': 0.94529403, 'lr': 0, 'params': 425270, 'time_iter': 0.06227, 'accuracy': 0.61424, 'f1': 0.60369, 'accuracy-SBM': 0.61358, 'auc': 0.90346}
2025-07-11 13:23:39,069 - INFO - test: {'epoch': 35, 'time_epoch': 4.01493, 'loss': 0.94431368, 'lr': 0, 'params': 425270, 'time_iter': 0.06373, 'accuracy': 0.60991, 'f1': 0.59996, 'accuracy-SBM': 0.611, 'auc': 0.90349}
2025-07-11 13:23:39,071 - INFO - > Epoch 35: took 81.9s (avg 82.0s) | Best so far: epoch 33	train_loss: 0.9187 train_accuracy-SBM: 0.6157	val_loss: 0.9070 val_accuracy-SBM: 0.6213	test_loss: 0.8936 test_accuracy-SBM: 0.6219
2025-07-11 13:24:54,285 - INFO - train: {'epoch': 36, 'time_epoch': 74.9853, 'eta': 4644.74967, 'eta_hours': 1.29021, 'loss': 0.90648509, 'lr': 0.00075948, 'params': 425270, 'time_iter': 0.11998, 'accuracy': 0.62184, 'f1': 0.60996, 'accuracy-SBM': 0.62179, 'auc': 0.90857}
2025-07-11 13:24:58,444 - INFO - val: {'epoch': 36, 'time_epoch': 4.11003, 'loss': 0.90984397, 'lr': 0, 'params': 425270, 'time_iter': 0.06524, 'accuracy': 0.62584, 'f1': 0.61447, 'accuracy-SBM': 0.62557, 'auc': 0.90862}
2025-07-11 13:25:02,422 - INFO - test: {'epoch': 36, 'time_epoch': 3.93999, 'loss': 0.890783, 'lr': 0, 'params': 425270, 'time_iter': 0.06254, 'accuracy': 0.62641, 'f1': 0.61506, 'accuracy-SBM': 0.62774, 'auc': 0.91154}
2025-07-11 13:25:02,424 - INFO - > Epoch 36: took 83.4s (avg 82.0s) | Best so far: epoch 36	train_loss: 0.9065 train_accuracy-SBM: 0.6218	val_loss: 0.9098 val_accuracy-SBM: 0.6256	test_loss: 0.8908 test_accuracy-SBM: 0.6277
2025-07-11 13:26:16,276 - INFO - train: {'epoch': 37, 'time_epoch': 73.62592, 'eta': 4570.8599, 'eta_hours': 1.26968, 'loss': 0.90290655, 'lr': 0.00074521, 'params': 425270, 'time_iter': 0.1178, 'accuracy': 0.6243, 'f1': 0.61149, 'accuracy-SBM': 0.62426, 'auc': 0.90928}
2025-07-11 13:26:20,258 - INFO - val: {'epoch': 37, 'time_epoch': 3.93211, 'loss': 0.92252483, 'lr': 0, 'params': 425270, 'time_iter': 0.06241, 'accuracy': 0.62407, 'f1': 0.60561, 'accuracy-SBM': 0.62389, 'auc': 0.90726}
2025-07-11 13:26:24,285 - INFO - test: {'epoch': 37, 'time_epoch': 3.98426, 'loss': 0.89588279, 'lr': 0, 'params': 425270, 'time_iter': 0.06324, 'accuracy': 0.6301, 'f1': 0.61248, 'accuracy-SBM': 0.6315, 'auc': 0.91152}
2025-07-11 13:26:24,288 - INFO - > Epoch 37: took 81.9s (avg 82.0s) | Best so far: epoch 36	train_loss: 0.9065 train_accuracy-SBM: 0.6218	val_loss: 0.9098 val_accuracy-SBM: 0.6256	test_loss: 0.8908 test_accuracy-SBM: 0.6277
2025-07-11 13:27:37,841 - INFO - train: {'epoch': 38, 'time_epoch': 73.15807, 'eta': 4496.25188, 'eta_hours': 1.24896, 'loss': 0.90031005, 'lr': 0.00073067, 'params': 425270, 'time_iter': 0.11705, 'accuracy': 0.63252, 'f1': 0.62102, 'accuracy-SBM': 0.63248, 'auc': 0.9103}
2025-07-11 13:27:41,797 - INFO - val: {'epoch': 38, 'time_epoch': 3.90683, 'loss': 1.12268106, 'lr': 0, 'params': 425270, 'time_iter': 0.06201, 'accuracy': 0.60561, 'f1': 0.59455, 'accuracy-SBM': 0.60531, 'auc': 0.87745}
2025-07-11 13:27:45,778 - INFO - test: {'epoch': 38, 'time_epoch': 3.93478, 'loss': 1.09421387, 'lr': 0, 'params': 425270, 'time_iter': 0.06246, 'accuracy': 0.61495, 'f1': 0.60415, 'accuracy-SBM': 0.6159, 'auc': 0.88227}
2025-07-11 13:27:45,780 - INFO - > Epoch 38: took 81.5s (avg 82.0s) | Best so far: epoch 36	train_loss: 0.9065 train_accuracy-SBM: 0.6218	val_loss: 0.9098 val_accuracy-SBM: 0.6256	test_loss: 0.8908 test_accuracy-SBM: 0.6277
2025-07-11 13:28:59,550 - INFO - train: {'epoch': 39, 'time_epoch': 73.54631, 'eta': 4422.29873, 'eta_hours': 1.22842, 'loss': 0.81838723, 'lr': 0.00071588, 'params': 425270, 'time_iter': 0.11767, 'accuracy': 0.70534, 'f1': 0.70496, 'accuracy-SBM': 0.70533, 'auc': 0.93192}
2025-07-11 13:29:03,574 - INFO - val: {'epoch': 39, 'time_epoch': 3.97452, 'loss': 0.75605839, 'lr': 0, 'params': 425270, 'time_iter': 0.06309, 'accuracy': 0.729, 'f1': 0.72902, 'accuracy-SBM': 0.72921, 'auc': 0.94336}
2025-07-11 13:29:07,520 - INFO - test: {'epoch': 39, 'time_epoch': 3.90734, 'loss': 0.75645417, 'lr': 0, 'params': 425270, 'time_iter': 0.06202, 'accuracy': 0.72817, 'f1': 0.72817, 'accuracy-SBM': 0.72838, 'auc': 0.94351}
2025-07-11 13:29:07,522 - INFO - > Epoch 39: took 81.7s (avg 82.0s) | Best so far: epoch 39	train_loss: 0.8184 train_accuracy-SBM: 0.7053	val_loss: 0.7561 val_accuracy-SBM: 0.7292	test_loss: 0.7565 test_accuracy-SBM: 0.7284
2025-07-11 13:30:21,464 - INFO - train: {'epoch': 40, 'time_epoch': 73.68964, 'eta': 4348.57168, 'eta_hours': 1.20794, 'loss': 0.76837408, 'lr': 0.00070085, 'params': 425270, 'time_iter': 0.1179, 'accuracy': 0.72202, 'f1': 0.72202, 'accuracy-SBM': 0.72202, 'auc': 0.9408}
2025-07-11 13:30:25,515 - INFO - val: {'epoch': 40, 'time_epoch': 3.99995, 'loss': 0.83797958, 'lr': 0, 'params': 425270, 'time_iter': 0.06349, 'accuracy': 0.71808, 'f1': 0.71838, 'accuracy-SBM': 0.71794, 'auc': 0.93006}
2025-07-11 13:30:29,499 - INFO - test: {'epoch': 40, 'time_epoch': 3.94459, 'loss': 0.82387187, 'lr': 0, 'params': 425270, 'time_iter': 0.06261, 'accuracy': 0.72154, 'f1': 0.7219, 'accuracy-SBM': 0.7215, 'auc': 0.93228}
2025-07-11 13:30:29,501 - INFO - > Epoch 40: took 82.0s (avg 82.0s) | Best so far: epoch 39	train_loss: 0.8184 train_accuracy-SBM: 0.7053	val_loss: 0.7561 val_accuracy-SBM: 0.7292	test_loss: 0.7565 test_accuracy-SBM: 0.7284
2025-07-11 13:31:43,182 - INFO - train: {'epoch': 41, 'time_epoch': 73.45564, 'eta': 4274.52326, 'eta_hours': 1.18737, 'loss': 0.75385449, 'lr': 0.0006856, 'params': 425270, 'time_iter': 0.11753, 'accuracy': 0.72705, 'f1': 0.72708, 'accuracy-SBM': 0.72706, 'auc': 0.94303}
2025-07-11 13:31:47,193 - INFO - val: {'epoch': 41, 'time_epoch': 3.96175, 'loss': 0.73999635, 'lr': 0, 'params': 425270, 'time_iter': 0.06288, 'accuracy': 0.73482, 'f1': 0.73465, 'accuracy-SBM': 0.73459, 'auc': 0.94622}
2025-07-11 13:31:51,179 - INFO - test: {'epoch': 41, 'time_epoch': 3.94504, 'loss': 0.74097372, 'lr': 0, 'params': 425270, 'time_iter': 0.06262, 'accuracy': 0.73185, 'f1': 0.73187, 'accuracy-SBM': 0.73193, 'auc': 0.94621}
2025-07-11 13:31:51,181 - INFO - > Epoch 41: took 81.7s (avg 82.0s) | Best so far: epoch 41	train_loss: 0.7539 train_accuracy-SBM: 0.7271	val_loss: 0.7400 val_accuracy-SBM: 0.7346	test_loss: 0.7410 test_accuracy-SBM: 0.7319
2025-07-11 13:33:04,870 - INFO - train: {'epoch': 42, 'time_epoch': 73.46323, 'eta': 4200.51248, 'eta_hours': 1.16681, 'loss': 0.74582733, 'lr': 0.00067015, 'params': 425270, 'time_iter': 0.11754, 'accuracy': 0.72901, 'f1': 0.72902, 'accuracy-SBM': 0.72901, 'auc': 0.94426}
2025-07-11 13:33:08,818 - INFO - val: {'epoch': 42, 'time_epoch': 3.89763, 'loss': 0.72736337, 'lr': 0, 'params': 425270, 'time_iter': 0.06187, 'accuracy': 0.73726, 'f1': 0.73722, 'accuracy-SBM': 0.73688, 'auc': 0.94806}
2025-07-11 13:33:12,790 - INFO - test: {'epoch': 42, 'time_epoch': 3.93237, 'loss': 0.72128011, 'lr': 0, 'params': 425270, 'time_iter': 0.06242, 'accuracy': 0.73732, 'f1': 0.73749, 'accuracy-SBM': 0.73737, 'auc': 0.9491}
2025-07-11 13:33:12,792 - INFO - > Epoch 42: took 81.6s (avg 82.0s) | Best so far: epoch 42	train_loss: 0.7458 train_accuracy-SBM: 0.7290	val_loss: 0.7274 val_accuracy-SBM: 0.7369	test_loss: 0.7213 test_accuracy-SBM: 0.7374
2025-07-11 13:34:26,327 - INFO - train: {'epoch': 43, 'time_epoch': 73.20722, 'eta': 4126.20075, 'eta_hours': 1.14617, 'loss': 0.73850554, 'lr': 0.00065451, 'params': 425270, 'time_iter': 0.11713, 'accuracy': 0.73138, 'f1': 0.7314, 'accuracy-SBM': 0.73138, 'auc': 0.94535}
2025-07-11 13:34:30,296 - INFO - val: {'epoch': 43, 'time_epoch': 3.9179, 'loss': 0.80802342, 'lr': 0, 'params': 425270, 'time_iter': 0.06219, 'accuracy': 0.71987, 'f1': 0.72015, 'accuracy-SBM': 0.71965, 'auc': 0.93466}
2025-07-11 13:34:34,251 - INFO - test: {'epoch': 43, 'time_epoch': 3.89968, 'loss': 0.76238141, 'lr': 0, 'params': 425270, 'time_iter': 0.0619, 'accuracy': 0.73147, 'f1': 0.73172, 'accuracy-SBM': 0.73143, 'auc': 0.94205}
2025-07-11 13:34:34,253 - INFO - > Epoch 43: took 81.5s (avg 82.0s) | Best so far: epoch 42	train_loss: 0.7458 train_accuracy-SBM: 0.7290	val_loss: 0.7274 val_accuracy-SBM: 0.7369	test_loss: 0.7213 test_accuracy-SBM: 0.7374
2025-07-11 13:35:47,957 - INFO - train: {'epoch': 44, 'time_epoch': 73.36964, 'eta': 4052.13664, 'eta_hours': 1.12559, 'loss': 0.73259056, 'lr': 0.0006387, 'params': 425270, 'time_iter': 0.11739, 'accuracy': 0.7333, 'f1': 0.73332, 'accuracy-SBM': 0.7333, 'auc': 0.94625}
2025-07-11 13:35:51,893 - INFO - val: {'epoch': 44, 'time_epoch': 3.87781, 'loss': 0.75449115, 'lr': 0, 'params': 425270, 'time_iter': 0.06155, 'accuracy': 0.72906, 'f1': 0.72916, 'accuracy-SBM': 0.72887, 'auc': 0.94346}
2025-07-11 13:35:55,938 - INFO - test: {'epoch': 44, 'time_epoch': 4.00514, 'loss': 0.73894857, 'lr': 0, 'params': 425270, 'time_iter': 0.06357, 'accuracy': 0.73253, 'f1': 0.73276, 'accuracy-SBM': 0.73262, 'auc': 0.94576}
2025-07-11 13:35:55,940 - INFO - > Epoch 44: took 81.7s (avg 82.0s) | Best so far: epoch 42	train_loss: 0.7458 train_accuracy-SBM: 0.7290	val_loss: 0.7274 val_accuracy-SBM: 0.7369	test_loss: 0.7213 test_accuracy-SBM: 0.7374
2025-07-11 13:37:09,693 - INFO - train: {'epoch': 45, 'time_epoch': 73.49348, 'eta': 3978.24809, 'eta_hours': 1.10507, 'loss': 0.73011753, 'lr': 0.00062274, 'params': 425270, 'time_iter': 0.11759, 'accuracy': 0.73447, 'f1': 0.73448, 'accuracy-SBM': 0.73447, 'auc': 0.94658}
2025-07-11 13:37:13,670 - INFO - val: {'epoch': 45, 'time_epoch': 3.92759, 'loss': 0.7170418, 'lr': 0, 'params': 425270, 'time_iter': 0.06234, 'accuracy': 0.73985, 'f1': 0.73997, 'accuracy-SBM': 0.73991, 'auc': 0.94905}
2025-07-11 13:37:17,618 - INFO - test: {'epoch': 45, 'time_epoch': 3.89753, 'loss': 0.70498778, 'lr': 0, 'params': 425270, 'time_iter': 0.06187, 'accuracy': 0.74417, 'f1': 0.7442, 'accuracy-SBM': 0.7443, 'auc': 0.95087}
2025-07-11 13:37:17,637 - INFO - > Epoch 45: took 81.7s (avg 82.0s) | Best so far: epoch 45	train_loss: 0.7301 train_accuracy-SBM: 0.7345	val_loss: 0.7170 val_accuracy-SBM: 0.7399	test_loss: 0.7050 test_accuracy-SBM: 0.7443
2025-07-11 13:38:31,299 - INFO - train: {'epoch': 46, 'time_epoch': 73.43892, 'eta': 3904.31482, 'eta_hours': 1.08453, 'loss': 0.72587613, 'lr': 0.00060665, 'params': 425270, 'time_iter': 0.1175, 'accuracy': 0.73594, 'f1': 0.73596, 'accuracy-SBM': 0.73595, 'auc': 0.94721}
2025-07-11 13:38:35,225 - INFO - val: {'epoch': 46, 'time_epoch': 3.87647, 'loss': 0.71756764, 'lr': 0, 'params': 425270, 'time_iter': 0.06153, 'accuracy': 0.74038, 'f1': 0.74029, 'accuracy-SBM': 0.74009, 'auc': 0.94902}
2025-07-11 13:38:39,182 - INFO - test: {'epoch': 46, 'time_epoch': 3.91143, 'loss': 0.70194968, 'lr': 0, 'params': 425270, 'time_iter': 0.06209, 'accuracy': 0.74535, 'f1': 0.74538, 'accuracy-SBM': 0.74522, 'auc': 0.95139}
2025-07-11 13:38:39,185 - INFO - > Epoch 46: took 81.5s (avg 81.9s) | Best so far: epoch 46	train_loss: 0.7259 train_accuracy-SBM: 0.7359	val_loss: 0.7176 val_accuracy-SBM: 0.7401	test_loss: 0.7019 test_accuracy-SBM: 0.7452
2025-07-11 13:39:53,681 - INFO - train: {'epoch': 47, 'time_epoch': 74.28106, 'eta': 3831.31448, 'eta_hours': 1.06425, 'loss': 0.72111483, 'lr': 0.00059044, 'params': 425270, 'time_iter': 0.11885, 'accuracy': 0.73801, 'f1': 0.73802, 'accuracy-SBM': 0.73801, 'auc': 0.94788}
2025-07-11 13:39:57,699 - INFO - val: {'epoch': 47, 'time_epoch': 3.97052, 'loss': 0.72195577, 'lr': 0, 'params': 425270, 'time_iter': 0.06302, 'accuracy': 0.74112, 'f1': 0.74117, 'accuracy-SBM': 0.74116, 'auc': 0.94769}
2025-07-11 13:40:01,686 - INFO - test: {'epoch': 47, 'time_epoch': 3.94767, 'loss': 0.69948738, 'lr': 0, 'params': 425270, 'time_iter': 0.06266, 'accuracy': 0.74567, 'f1': 0.74572, 'accuracy-SBM': 0.74576, 'auc': 0.95113}
2025-07-11 13:40:01,689 - INFO - > Epoch 47: took 82.5s (avg 82.0s) | Best so far: epoch 47	train_loss: 0.7211 train_accuracy-SBM: 0.7380	val_loss: 0.7220 val_accuracy-SBM: 0.7412	test_loss: 0.6995 test_accuracy-SBM: 0.7458
2025-07-11 13:41:15,132 - INFO - train: {'epoch': 48, 'time_epoch': 73.22215, 'eta': 3757.15972, 'eta_hours': 1.04366, 'loss': 0.71555159, 'lr': 0.00057413, 'params': 425270, 'time_iter': 0.11716, 'accuracy': 0.73931, 'f1': 0.73933, 'accuracy-SBM': 0.73931, 'auc': 0.9487}
2025-07-11 13:41:19,098 - INFO - val: {'epoch': 48, 'time_epoch': 3.92037, 'loss': 0.73980716, 'lr': 0, 'params': 425270, 'time_iter': 0.06223, 'accuracy': 0.73592, 'f1': 0.73597, 'accuracy-SBM': 0.73591, 'auc': 0.94513}
2025-07-11 13:41:23,011 - INFO - test: {'epoch': 48, 'time_epoch': 3.87792, 'loss': 0.73963739, 'lr': 0, 'params': 425270, 'time_iter': 0.06155, 'accuracy': 0.73776, 'f1': 0.73785, 'accuracy-SBM': 0.7378, 'auc': 0.94524}
2025-07-11 13:41:23,013 - INFO - > Epoch 48: took 81.3s (avg 81.9s) | Best so far: epoch 47	train_loss: 0.7211 train_accuracy-SBM: 0.7380	val_loss: 0.7220 val_accuracy-SBM: 0.7412	test_loss: 0.6995 test_accuracy-SBM: 0.7458
2025-07-11 13:42:35,864 - INFO - train: {'epoch': 49, 'time_epoch': 72.62812, 'eta': 3682.44825, 'eta_hours': 1.0229, 'loss': 0.71133977, 'lr': 0.00055774, 'params': 425270, 'time_iter': 0.1162, 'accuracy': 0.74134, 'f1': 0.74136, 'accuracy-SBM': 0.74134, 'auc': 0.94928}
2025-07-11 13:42:39,926 - INFO - val: {'epoch': 49, 'time_epoch': 4.01149, 'loss': 0.72901179, 'lr': 0, 'params': 425270, 'time_iter': 0.06367, 'accuracy': 0.73976, 'f1': 0.73977, 'accuracy-SBM': 0.73956, 'auc': 0.94678}
2025-07-11 13:42:43,932 - INFO - test: {'epoch': 49, 'time_epoch': 3.96619, 'loss': 0.70877926, 'lr': 0, 'params': 425270, 'time_iter': 0.06296, 'accuracy': 0.74309, 'f1': 0.74324, 'accuracy-SBM': 0.74311, 'auc': 0.94996}
2025-07-11 13:42:43,934 - INFO - > Epoch 49: took 80.9s (avg 81.9s) | Best so far: epoch 47	train_loss: 0.7211 train_accuracy-SBM: 0.7380	val_loss: 0.7220 val_accuracy-SBM: 0.7412	test_loss: 0.6995 test_accuracy-SBM: 0.7458
2025-07-11 13:43:57,089 - INFO - train: {'epoch': 50, 'time_epoch': 72.74589, 'eta': 3607.93162, 'eta_hours': 1.0022, 'loss': 0.70843624, 'lr': 0.00054129, 'params': 425270, 'time_iter': 0.11639, 'accuracy': 0.74191, 'f1': 0.74193, 'accuracy-SBM': 0.74192, 'auc': 0.94971}
2025-07-11 13:44:00,998 - INFO - val: {'epoch': 50, 'time_epoch': 3.85838, 'loss': 0.84044246, 'lr': 0, 'params': 425270, 'time_iter': 0.06124, 'accuracy': 0.71592, 'f1': 0.71622, 'accuracy-SBM': 0.71586, 'auc': 0.93007}
2025-07-11 13:44:04,961 - INFO - test: {'epoch': 50, 'time_epoch': 3.92466, 'loss': 0.7777895, 'lr': 0, 'params': 425270, 'time_iter': 0.0623, 'accuracy': 0.73139, 'f1': 0.73155, 'accuracy-SBM': 0.73144, 'auc': 0.93974}
2025-07-11 13:44:04,963 - INFO - > Epoch 50: took 81.0s (avg 81.9s) | Best so far: epoch 47	train_loss: 0.7211 train_accuracy-SBM: 0.7380	val_loss: 0.7220 val_accuracy-SBM: 0.7412	test_loss: 0.6995 test_accuracy-SBM: 0.7458
2025-07-11 13:45:18,542 - INFO - train: {'epoch': 51, 'time_epoch': 73.25944, 'eta': 3533.95715, 'eta_hours': 0.98165, 'loss': 0.70475079, 'lr': 0.00052479, 'params': 425270, 'time_iter': 0.11722, 'accuracy': 0.74354, 'f1': 0.74356, 'accuracy-SBM': 0.74354, 'auc': 0.95023}
2025-07-11 13:45:22,526 - INFO - val: {'epoch': 51, 'time_epoch': 3.9288, 'loss': 0.73650283, 'lr': 0, 'params': 425270, 'time_iter': 0.06236, 'accuracy': 0.73858, 'f1': 0.7386, 'accuracy-SBM': 0.73853, 'auc': 0.94564}
2025-07-11 13:45:26,555 - INFO - test: {'epoch': 51, 'time_epoch': 3.99403, 'loss': 0.71667034, 'lr': 0, 'params': 425270, 'time_iter': 0.0634, 'accuracy': 0.74391, 'f1': 0.74397, 'accuracy-SBM': 0.74395, 'auc': 0.9487}
2025-07-11 13:45:26,558 - INFO - > Epoch 51: took 81.6s (avg 81.9s) | Best so far: epoch 47	train_loss: 0.7211 train_accuracy-SBM: 0.7380	val_loss: 0.7220 val_accuracy-SBM: 0.7412	test_loss: 0.6995 test_accuracy-SBM: 0.7458
2025-07-11 13:46:39,395 - INFO - train: {'epoch': 52, 'time_epoch': 72.61934, 'eta': 3459.44202, 'eta_hours': 0.96096, 'loss': 0.70328598, 'lr': 0.00050827, 'params': 425270, 'time_iter': 0.11619, 'accuracy': 0.74482, 'f1': 0.74484, 'accuracy-SBM': 0.74482, 'auc': 0.95043}
2025-07-11 13:46:43,358 - INFO - val: {'epoch': 52, 'time_epoch': 3.90031, 'loss': 0.97915946, 'lr': 0, 'params': 425270, 'time_iter': 0.06191, 'accuracy': 0.70294, 'f1': 0.70438, 'accuracy-SBM': 0.70278, 'auc': 0.91068}
2025-07-11 13:46:47,276 - INFO - test: {'epoch': 52, 'time_epoch': 3.87853, 'loss': 0.98856924, 'lr': 0, 'params': 425270, 'time_iter': 0.06156, 'accuracy': 0.70334, 'f1': 0.70475, 'accuracy-SBM': 0.70339, 'auc': 0.9091}
2025-07-11 13:46:47,278 - INFO - > Epoch 52: took 80.7s (avg 81.9s) | Best so far: epoch 47	train_loss: 0.7211 train_accuracy-SBM: 0.7380	val_loss: 0.7220 val_accuracy-SBM: 0.7412	test_loss: 0.6995 test_accuracy-SBM: 0.7458
2025-07-11 13:48:00,968 - INFO - train: {'epoch': 53, 'time_epoch': 73.35987, 'eta': 3385.62793, 'eta_hours': 0.94045, 'loss': 0.70178005, 'lr': 0.00049173, 'params': 425270, 'time_iter': 0.11738, 'accuracy': 0.74451, 'f1': 0.74453, 'accuracy-SBM': 0.74451, 'auc': 0.95067}
2025-07-11 13:48:04,863 - INFO - val: {'epoch': 53, 'time_epoch': 3.8359, 'loss': 0.70656529, 'lr': 0, 'params': 425270, 'time_iter': 0.06089, 'accuracy': 0.74672, 'f1': 0.74676, 'accuracy-SBM': 0.74645, 'auc': 0.95016}
2025-07-11 13:48:08,799 - INFO - test: {'epoch': 53, 'time_epoch': 3.89551, 'loss': 0.698619, 'lr': 0, 'params': 425270, 'time_iter': 0.06183, 'accuracy': 0.74752, 'f1': 0.74768, 'accuracy-SBM': 0.74748, 'auc': 0.95133}
2025-07-11 13:48:08,801 - INFO - > Epoch 53: took 81.5s (avg 81.9s) | Best so far: epoch 53	train_loss: 0.7018 train_accuracy-SBM: 0.7445	val_loss: 0.7066 val_accuracy-SBM: 0.7464	test_loss: 0.6986 test_accuracy-SBM: 0.7475
2025-07-11 13:49:22,434 - INFO - train: {'epoch': 54, 'time_epoch': 73.41117, 'eta': 3311.87233, 'eta_hours': 0.91996, 'loss': 0.69578665, 'lr': 0.00047521, 'params': 425270, 'time_iter': 0.11746, 'accuracy': 0.74661, 'f1': 0.74662, 'accuracy-SBM': 0.74661, 'auc': 0.95152}
2025-07-11 13:49:26,453 - INFO - val: {'epoch': 54, 'time_epoch': 3.96176, 'loss': 0.77172269, 'lr': 0, 'params': 425270, 'time_iter': 0.06289, 'accuracy': 0.73632, 'f1': 0.73646, 'accuracy-SBM': 0.73621, 'auc': 0.94047}
2025-07-11 13:49:30,433 - INFO - test: {'epoch': 54, 'time_epoch': 3.94169, 'loss': 0.72147223, 'lr': 0, 'params': 425270, 'time_iter': 0.06257, 'accuracy': 0.74367, 'f1': 0.74377, 'accuracy-SBM': 0.74372, 'auc': 0.94815}
2025-07-11 13:49:30,435 - INFO - > Epoch 54: took 81.6s (avg 81.9s) | Best so far: epoch 53	train_loss: 0.7018 train_accuracy-SBM: 0.7445	val_loss: 0.7066 val_accuracy-SBM: 0.7464	test_loss: 0.6986 test_accuracy-SBM: 0.7475
2025-07-11 13:50:43,567 - INFO - train: {'epoch': 55, 'time_epoch': 72.91146, 'eta': 3237.7364, 'eta_hours': 0.89937, 'loss': 0.69432507, 'lr': 0.00045871, 'params': 425270, 'time_iter': 0.11666, 'accuracy': 0.74684, 'f1': 0.74685, 'accuracy-SBM': 0.74684, 'auc': 0.95172}
2025-07-11 13:50:47,491 - INFO - val: {'epoch': 55, 'time_epoch': 3.8732, 'loss': 0.69203883, 'lr': 0, 'params': 425270, 'time_iter': 0.06148, 'accuracy': 0.74825, 'f1': 0.74823, 'accuracy-SBM': 0.74809, 'auc': 0.95201}
2025-07-11 13:50:51,381 - INFO - test: {'epoch': 55, 'time_epoch': 3.85117, 'loss': 0.69106744, 'lr': 0, 'params': 425270, 'time_iter': 0.06113, 'accuracy': 0.74919, 'f1': 0.74923, 'accuracy-SBM': 0.74921, 'auc': 0.95227}
2025-07-11 13:50:51,383 - INFO - > Epoch 55: took 80.9s (avg 81.9s) | Best so far: epoch 55	train_loss: 0.6943 train_accuracy-SBM: 0.7468	val_loss: 0.6920 val_accuracy-SBM: 0.7481	test_loss: 0.6911 test_accuracy-SBM: 0.7492
2025-07-11 13:52:04,895 - INFO - train: {'epoch': 56, 'time_epoch': 73.12982, 'eta': 3163.80817, 'eta_hours': 0.87884, 'loss': 0.68962735, 'lr': 0.00044226, 'params': 425270, 'time_iter': 0.11701, 'accuracy': 0.74876, 'f1': 0.74878, 'accuracy-SBM': 0.74876, 'auc': 0.95236}
2025-07-11 13:52:08,984 - INFO - val: {'epoch': 56, 'time_epoch': 4.03543, 'loss': 0.70826995, 'lr': 0, 'params': 425270, 'time_iter': 0.06405, 'accuracy': 0.74737, 'f1': 0.74737, 'accuracy-SBM': 0.74725, 'auc': 0.94968}
2025-07-11 13:52:12,968 - INFO - test: {'epoch': 56, 'time_epoch': 3.9438, 'loss': 0.68571436, 'lr': 0, 'params': 425270, 'time_iter': 0.0626, 'accuracy': 0.752, 'f1': 0.752, 'accuracy-SBM': 0.75202, 'auc': 0.95296}
2025-07-11 13:52:12,970 - INFO - > Epoch 56: took 81.6s (avg 81.8s) | Best so far: epoch 55	train_loss: 0.6943 train_accuracy-SBM: 0.7468	val_loss: 0.6920 val_accuracy-SBM: 0.7481	test_loss: 0.6911 test_accuracy-SBM: 0.7492
2025-07-11 13:53:26,374 - INFO - train: {'epoch': 57, 'time_epoch': 73.18219, 'eta': 3089.94538, 'eta_hours': 0.85832, 'loss': 0.6866852, 'lr': 0.00042587, 'params': 425270, 'time_iter': 0.11709, 'accuracy': 0.75013, 'f1': 0.75015, 'accuracy-SBM': 0.75013, 'auc': 0.95276}
2025-07-11 13:53:30,299 - INFO - val: {'epoch': 57, 'time_epoch': 3.87478, 'loss': 0.69604378, 'lr': 0, 'params': 425270, 'time_iter': 0.0615, 'accuracy': 0.74853, 'f1': 0.74845, 'accuracy-SBM': 0.74826, 'auc': 0.95157}
2025-07-11 13:53:34,257 - INFO - test: {'epoch': 57, 'time_epoch': 3.92038, 'loss': 0.69250325, 'lr': 0, 'params': 425270, 'time_iter': 0.06223, 'accuracy': 0.7497, 'f1': 0.74973, 'accuracy-SBM': 0.74973, 'auc': 0.95213}
2025-07-11 13:53:34,259 - INFO - > Epoch 57: took 81.3s (avg 81.8s) | Best so far: epoch 57	train_loss: 0.6867 train_accuracy-SBM: 0.7501	val_loss: 0.6960 val_accuracy-SBM: 0.7483	test_loss: 0.6925 test_accuracy-SBM: 0.7497
2025-07-11 13:54:49,092 - INFO - train: {'epoch': 58, 'time_epoch': 74.61117, 'eta': 3017.09869, 'eta_hours': 0.83808, 'loss': 0.68457838, 'lr': 0.00040956, 'params': 425270, 'time_iter': 0.11938, 'accuracy': 0.75113, 'f1': 0.75114, 'accuracy-SBM': 0.75113, 'auc': 0.95307}
2025-07-11 13:54:53,050 - INFO - val: {'epoch': 58, 'time_epoch': 3.90746, 'loss': 0.68119084, 'lr': 0, 'params': 425270, 'time_iter': 0.06202, 'accuracy': 0.75353, 'f1': 0.75346, 'accuracy-SBM': 0.75345, 'auc': 0.95365}
2025-07-11 13:54:57,013 - INFO - test: {'epoch': 58, 'time_epoch': 3.92921, 'loss': 0.67264002, 'lr': 0, 'params': 425270, 'time_iter': 0.06237, 'accuracy': 0.75513, 'f1': 0.75509, 'accuracy-SBM': 0.75523, 'auc': 0.95494}
2025-07-11 13:54:57,016 - INFO - > Epoch 58: took 82.8s (avg 81.9s) | Best so far: epoch 58	train_loss: 0.6846 train_accuracy-SBM: 0.7511	val_loss: 0.6812 val_accuracy-SBM: 0.7534	test_loss: 0.6726 test_accuracy-SBM: 0.7552
2025-07-11 13:56:10,655 - INFO - train: {'epoch': 59, 'time_epoch': 73.42468, 'eta': 2943.40219, 'eta_hours': 0.81761, 'loss': 0.68052303, 'lr': 0.00039335, 'params': 425270, 'time_iter': 0.11748, 'accuracy': 0.75227, 'f1': 0.75228, 'accuracy-SBM': 0.75227, 'auc': 0.95361}
2025-07-11 13:56:14,573 - INFO - val: {'epoch': 59, 'time_epoch': 3.87118, 'loss': 0.68482901, 'lr': 0, 'params': 425270, 'time_iter': 0.06145, 'accuracy': 0.75343, 'f1': 0.75335, 'accuracy-SBM': 0.7532, 'auc': 0.95337}
2025-07-11 13:56:18,530 - INFO - test: {'epoch': 59, 'time_epoch': 3.91269, 'loss': 0.66803988, 'lr': 0, 'params': 425270, 'time_iter': 0.06211, 'accuracy': 0.75748, 'f1': 0.7575, 'accuracy-SBM': 0.75737, 'auc': 0.95562}
2025-07-11 13:56:18,532 - INFO - > Epoch 59: took 81.5s (avg 81.8s) | Best so far: epoch 58	train_loss: 0.6846 train_accuracy-SBM: 0.7511	val_loss: 0.6812 val_accuracy-SBM: 0.7534	test_loss: 0.6726 test_accuracy-SBM: 0.7552
2025-07-11 13:57:31,885 - INFO - train: {'epoch': 60, 'time_epoch': 73.12169, 'eta': 2869.52089, 'eta_hours': 0.79709, 'loss': 0.68152833, 'lr': 0.00037726, 'params': 425270, 'time_iter': 0.11699, 'accuracy': 0.7516, 'f1': 0.75162, 'accuracy-SBM': 0.7516, 'auc': 0.95347}
2025-07-11 13:57:35,772 - INFO - val: {'epoch': 60, 'time_epoch': 3.83783, 'loss': 0.69904223, 'lr': 0, 'params': 425270, 'time_iter': 0.06092, 'accuracy': 0.75044, 'f1': 0.75042, 'accuracy-SBM': 0.75023, 'auc': 0.95124}
2025-07-11 13:57:39,728 - INFO - test: {'epoch': 60, 'time_epoch': 3.91965, 'loss': 0.68247726, 'lr': 0, 'params': 425270, 'time_iter': 0.06222, 'accuracy': 0.75339, 'f1': 0.75342, 'accuracy-SBM': 0.75336, 'auc': 0.95358}
2025-07-11 13:57:39,731 - INFO - > Epoch 60: took 81.2s (avg 81.8s) | Best so far: epoch 58	train_loss: 0.6846 train_accuracy-SBM: 0.7511	val_loss: 0.6812 val_accuracy-SBM: 0.7534	test_loss: 0.6726 test_accuracy-SBM: 0.7552
2025-07-11 13:58:53,494 - INFO - train: {'epoch': 61, 'time_epoch': 73.53044, 'eta': 2795.91461, 'eta_hours': 0.77664, 'loss': 0.67617673, 'lr': 0.0003613, 'params': 425270, 'time_iter': 0.11765, 'accuracy': 0.75299, 'f1': 0.75301, 'accuracy-SBM': 0.75299, 'auc': 0.95421}
2025-07-11 13:58:57,422 - INFO - val: {'epoch': 61, 'time_epoch': 3.87967, 'loss': 0.68436822, 'lr': 0, 'params': 425270, 'time_iter': 0.06158, 'accuracy': 0.75374, 'f1': 0.75383, 'accuracy-SBM': 0.7537, 'auc': 0.95343}
2025-07-11 13:59:01,384 - INFO - test: {'epoch': 61, 'time_epoch': 3.92029, 'loss': 0.67353239, 'lr': 0, 'params': 425270, 'time_iter': 0.06223, 'accuracy': 0.75572, 'f1': 0.75584, 'accuracy-SBM': 0.75579, 'auc': 0.95509}
2025-07-11 13:59:01,387 - INFO - > Epoch 61: took 81.7s (avg 81.8s) | Best so far: epoch 61	train_loss: 0.6762 train_accuracy-SBM: 0.7530	val_loss: 0.6844 val_accuracy-SBM: 0.7537	test_loss: 0.6735 test_accuracy-SBM: 0.7558
2025-07-11 14:00:14,522 - INFO - train: {'epoch': 62, 'time_epoch': 72.81141, 'eta': 2721.88846, 'eta_hours': 0.75608, 'loss': 0.67378442, 'lr': 0.00034549, 'params': 425270, 'time_iter': 0.1165, 'accuracy': 0.75454, 'f1': 0.75456, 'accuracy-SBM': 0.75454, 'auc': 0.95454}
2025-07-11 14:00:18,443 - INFO - val: {'epoch': 62, 'time_epoch': 3.85947, 'loss': 0.69191282, 'lr': 0, 'params': 425270, 'time_iter': 0.06126, 'accuracy': 0.75273, 'f1': 0.75274, 'accuracy-SBM': 0.75277, 'auc': 0.9521}
2025-07-11 14:00:22,367 - INFO - test: {'epoch': 62, 'time_epoch': 3.88523, 'loss': 0.67471997, 'lr': 0, 'params': 425270, 'time_iter': 0.06167, 'accuracy': 0.75725, 'f1': 0.75717, 'accuracy-SBM': 0.75731, 'auc': 0.95451}
2025-07-11 14:00:22,369 - INFO - > Epoch 62: took 81.0s (avg 81.8s) | Best so far: epoch 61	train_loss: 0.6762 train_accuracy-SBM: 0.7530	val_loss: 0.6844 val_accuracy-SBM: 0.7537	test_loss: 0.6735 test_accuracy-SBM: 0.7558
2025-07-11 14:01:36,015 - INFO - train: {'epoch': 63, 'time_epoch': 73.23107, 'eta': 2648.13632, 'eta_hours': 0.73559, 'loss': 0.67149938, 'lr': 0.00032985, 'params': 425270, 'time_iter': 0.11717, 'accuracy': 0.75555, 'f1': 0.75556, 'accuracy-SBM': 0.75555, 'auc': 0.95484}
2025-07-11 14:01:39,981 - INFO - val: {'epoch': 63, 'time_epoch': 3.91481, 'loss': 0.67411384, 'lr': 0, 'params': 425270, 'time_iter': 0.06214, 'accuracy': 0.7562, 'f1': 0.75613, 'accuracy-SBM': 0.75608, 'auc': 0.95454}
2025-07-11 14:01:43,979 - INFO - test: {'epoch': 63, 'time_epoch': 3.95994, 'loss': 0.65588759, 'lr': 0, 'params': 425270, 'time_iter': 0.06286, 'accuracy': 0.76273, 'f1': 0.76275, 'accuracy-SBM': 0.76265, 'auc': 0.95705}
2025-07-11 14:01:43,981 - INFO - > Epoch 63: took 81.6s (avg 81.8s) | Best so far: epoch 63	train_loss: 0.6715 train_accuracy-SBM: 0.7556	val_loss: 0.6741 val_accuracy-SBM: 0.7561	test_loss: 0.6559 test_accuracy-SBM: 0.7627
2025-07-11 14:02:56,787 - INFO - train: {'epoch': 64, 'time_epoch': 72.58063, 'eta': 2574.04998, 'eta_hours': 0.71501, 'loss': 0.67048535, 'lr': 0.0003144, 'params': 425270, 'time_iter': 0.11613, 'accuracy': 0.75631, 'f1': 0.75633, 'accuracy-SBM': 0.75631, 'auc': 0.95498}
2025-07-11 14:03:00,857 - INFO - val: {'epoch': 64, 'time_epoch': 4.01798, 'loss': 0.66827507, 'lr': 0, 'params': 425270, 'time_iter': 0.06378, 'accuracy': 0.75794, 'f1': 0.75786, 'accuracy-SBM': 0.75778, 'auc': 0.95537}
2025-07-11 14:03:04,857 - INFO - test: {'epoch': 64, 'time_epoch': 3.96168, 'loss': 0.66551745, 'lr': 0, 'params': 425270, 'time_iter': 0.06288, 'accuracy': 0.75902, 'f1': 0.75909, 'accuracy-SBM': 0.75901, 'auc': 0.95583}
2025-07-11 14:03:04,860 - INFO - > Epoch 64: took 80.9s (avg 81.8s) | Best so far: epoch 64	train_loss: 0.6705 train_accuracy-SBM: 0.7563	val_loss: 0.6683 val_accuracy-SBM: 0.7578	test_loss: 0.6655 test_accuracy-SBM: 0.7590
2025-07-11 14:04:18,117 - INFO - train: {'epoch': 65, 'time_epoch': 73.02437, 'eta': 2500.23786, 'eta_hours': 0.69451, 'loss': 0.66663788, 'lr': 0.00029915, 'params': 425270, 'time_iter': 0.11684, 'accuracy': 0.75722, 'f1': 0.75724, 'accuracy-SBM': 0.75722, 'auc': 0.9555}
2025-07-11 14:04:21,990 - INFO - val: {'epoch': 65, 'time_epoch': 3.82743, 'loss': 0.67351664, 'lr': 0, 'params': 425270, 'time_iter': 0.06075, 'accuracy': 0.75726, 'f1': 0.75727, 'accuracy-SBM': 0.75732, 'auc': 0.95457}
2025-07-11 14:04:26,019 - INFO - test: {'epoch': 65, 'time_epoch': 3.98912, 'loss': 0.6589235, 'lr': 0, 'params': 425270, 'time_iter': 0.06332, 'accuracy': 0.76034, 'f1': 0.76035, 'accuracy-SBM': 0.76043, 'auc': 0.95662}
2025-07-11 14:04:26,021 - INFO - > Epoch 65: took 81.2s (avg 81.8s) | Best so far: epoch 64	train_loss: 0.6705 train_accuracy-SBM: 0.7563	val_loss: 0.6683 val_accuracy-SBM: 0.7578	test_loss: 0.6655 test_accuracy-SBM: 0.7590
2025-07-11 14:05:38,779 - INFO - train: {'epoch': 66, 'time_epoch': 72.54182, 'eta': 2426.21158, 'eta_hours': 0.67395, 'loss': 0.66355287, 'lr': 0.00028412, 'params': 425270, 'time_iter': 0.11607, 'accuracy': 0.75812, 'f1': 0.75814, 'accuracy-SBM': 0.75812, 'auc': 0.95591}
2025-07-11 14:05:42,748 - INFO - val: {'epoch': 66, 'time_epoch': 3.91775, 'loss': 0.66173585, 'lr': 0, 'params': 425270, 'time_iter': 0.06219, 'accuracy': 0.76109, 'f1': 0.76102, 'accuracy-SBM': 0.761, 'auc': 0.95613}
2025-07-11 14:05:46,650 - INFO - test: {'epoch': 66, 'time_epoch': 3.86539, 'loss': 0.65414448, 'lr': 0, 'params': 425270, 'time_iter': 0.06136, 'accuracy': 0.76365, 'f1': 0.76365, 'accuracy-SBM': 0.76374, 'auc': 0.95728}
2025-07-11 14:05:46,653 - INFO - > Epoch 66: took 80.6s (avg 81.8s) | Best so far: epoch 66	train_loss: 0.6636 train_accuracy-SBM: 0.7581	val_loss: 0.6617 val_accuracy-SBM: 0.7610	test_loss: 0.6541 test_accuracy-SBM: 0.7637
2025-07-11 14:06:59,376 - INFO - train: {'epoch': 67, 'time_epoch': 72.49821, 'eta': 2352.20844, 'eta_hours': 0.65339, 'loss': 0.66148539, 'lr': 0.00026933, 'params': 425270, 'time_iter': 0.116, 'accuracy': 0.75852, 'f1': 0.75854, 'accuracy-SBM': 0.75852, 'auc': 0.95622}
2025-07-11 14:07:03,323 - INFO - val: {'epoch': 67, 'time_epoch': 3.89642, 'loss': 0.68115578, 'lr': 0, 'params': 425270, 'time_iter': 0.06185, 'accuracy': 0.75528, 'f1': 0.75531, 'accuracy-SBM': 0.75524, 'auc': 0.95362}
2025-07-11 14:07:07,287 - INFO - test: {'epoch': 67, 'time_epoch': 3.90071, 'loss': 0.65680618, 'lr': 0, 'params': 425270, 'time_iter': 0.06192, 'accuracy': 0.76178, 'f1': 0.76177, 'accuracy-SBM': 0.7618, 'auc': 0.95694}
2025-07-11 14:07:07,290 - INFO - > Epoch 67: took 80.6s (avg 81.8s) | Best so far: epoch 66	train_loss: 0.6636 train_accuracy-SBM: 0.7581	val_loss: 0.6617 val_accuracy-SBM: 0.7610	test_loss: 0.6541 test_accuracy-SBM: 0.7637
2025-07-11 14:08:20,224 - INFO - train: {'epoch': 68, 'time_epoch': 72.69604, 'eta': 2278.3378, 'eta_hours': 0.63287, 'loss': 0.66041171, 'lr': 0.00025479, 'params': 425270, 'time_iter': 0.11631, 'accuracy': 0.75944, 'f1': 0.75946, 'accuracy-SBM': 0.75944, 'auc': 0.95634}
2025-07-11 14:08:24,226 - INFO - val: {'epoch': 68, 'time_epoch': 3.9508, 'loss': 0.67110931, 'lr': 0, 'params': 425270, 'time_iter': 0.06271, 'accuracy': 0.75888, 'f1': 0.75881, 'accuracy-SBM': 0.75873, 'auc': 0.95489}
2025-07-11 14:08:28,118 - INFO - test: {'epoch': 68, 'time_epoch': 3.85301, 'loss': 0.65159681, 'lr': 0, 'params': 425270, 'time_iter': 0.06116, 'accuracy': 0.7631, 'f1': 0.76311, 'accuracy-SBM': 0.76311, 'auc': 0.95754}
2025-07-11 14:08:28,120 - INFO - > Epoch 68: took 80.8s (avg 81.7s) | Best so far: epoch 66	train_loss: 0.6636 train_accuracy-SBM: 0.7581	val_loss: 0.6617 val_accuracy-SBM: 0.7610	test_loss: 0.6541 test_accuracy-SBM: 0.7637
2025-07-11 14:09:42,768 - INFO - train: {'epoch': 69, 'time_epoch': 74.42539, 'eta': 2205.24187, 'eta_hours': 0.61257, 'loss': 0.65725014, 'lr': 0.00024052, 'params': 425270, 'time_iter': 0.11908, 'accuracy': 0.76074, 'f1': 0.76075, 'accuracy-SBM': 0.76074, 'auc': 0.95675}
2025-07-11 14:09:46,705 - INFO - val: {'epoch': 69, 'time_epoch': 3.88094, 'loss': 0.68234256, 'lr': 0, 'params': 425270, 'time_iter': 0.0616, 'accuracy': 0.75767, 'f1': 0.7576, 'accuracy-SBM': 0.75756, 'auc': 0.95344}
2025-07-11 14:09:50,670 - INFO - test: {'epoch': 69, 'time_epoch': 3.92796, 'loss': 0.658522, 'lr': 0, 'params': 425270, 'time_iter': 0.06235, 'accuracy': 0.76159, 'f1': 0.76162, 'accuracy-SBM': 0.76161, 'auc': 0.95674}
2025-07-11 14:09:50,672 - INFO - > Epoch 69: took 82.6s (avg 81.8s) | Best so far: epoch 66	train_loss: 0.6636 train_accuracy-SBM: 0.7581	val_loss: 0.6617 val_accuracy-SBM: 0.7610	test_loss: 0.6541 test_accuracy-SBM: 0.7637
2025-07-11 14:11:03,954 - INFO - train: {'epoch': 70, 'time_epoch': 72.96849, 'eta': 2131.51342, 'eta_hours': 0.59209, 'loss': 0.65721157, 'lr': 0.00022653, 'params': 425270, 'time_iter': 0.11675, 'accuracy': 0.76036, 'f1': 0.76038, 'accuracy-SBM': 0.76037, 'auc': 0.95676}
2025-07-11 14:11:07,906 - INFO - val: {'epoch': 70, 'time_epoch': 3.88286, 'loss': 0.71510419, 'lr': 0, 'params': 425270, 'time_iter': 0.06163, 'accuracy': 0.74973, 'f1': 0.74979, 'accuracy-SBM': 0.74961, 'auc': 0.94903}
2025-07-11 14:11:11,889 - INFO - test: {'epoch': 70, 'time_epoch': 3.94733, 'loss': 0.67125297, 'lr': 0, 'params': 425270, 'time_iter': 0.06266, 'accuracy': 0.76027, 'f1': 0.76029, 'accuracy-SBM': 0.76027, 'auc': 0.9549}
2025-07-11 14:11:11,892 - INFO - > Epoch 70: took 81.2s (avg 81.7s) | Best so far: epoch 66	train_loss: 0.6636 train_accuracy-SBM: 0.7581	val_loss: 0.6617 val_accuracy-SBM: 0.7610	test_loss: 0.6541 test_accuracy-SBM: 0.7637
2025-07-11 14:12:24,574 - INFO - train: {'epoch': 71, 'time_epoch': 72.45875, 'eta': 2057.60785, 'eta_hours': 0.57156, 'loss': 0.65421179, 'lr': 0.00021284, 'params': 425270, 'time_iter': 0.11593, 'accuracy': 0.76207, 'f1': 0.76209, 'accuracy-SBM': 0.76207, 'auc': 0.95716}
2025-07-11 14:12:28,573 - INFO - val: {'epoch': 71, 'time_epoch': 3.93005, 'loss': 0.65984017, 'lr': 0, 'params': 425270, 'time_iter': 0.06238, 'accuracy': 0.75997, 'f1': 0.75986, 'accuracy-SBM': 0.75972, 'auc': 0.95652}
2025-07-11 14:12:32,579 - INFO - test: {'epoch': 71, 'time_epoch': 3.96606, 'loss': 0.6542199, 'lr': 0, 'params': 425270, 'time_iter': 0.06295, 'accuracy': 0.76236, 'f1': 0.76237, 'accuracy-SBM': 0.76231, 'auc': 0.95733}
2025-07-11 14:12:32,581 - INFO - > Epoch 71: took 80.7s (avg 81.7s) | Best so far: epoch 66	train_loss: 0.6636 train_accuracy-SBM: 0.7581	val_loss: 0.6617 val_accuracy-SBM: 0.7610	test_loss: 0.6541 test_accuracy-SBM: 0.7637
2025-07-11 14:13:45,922 - INFO - train: {'epoch': 72, 'time_epoch': 73.09497, 'eta': 1983.97723, 'eta_hours': 0.5511, 'loss': 0.651753, 'lr': 0.00019946, 'params': 425270, 'time_iter': 0.11695, 'accuracy': 0.76294, 'f1': 0.76296, 'accuracy-SBM': 0.76294, 'auc': 0.95748}
2025-07-11 14:13:49,913 - INFO - val: {'epoch': 72, 'time_epoch': 3.9406, 'loss': 0.67398976, 'lr': 0, 'params': 425270, 'time_iter': 0.06255, 'accuracy': 0.75755, 'f1': 0.75744, 'accuracy-SBM': 0.75748, 'auc': 0.95453}
2025-07-11 14:13:53,879 - INFO - test: {'epoch': 72, 'time_epoch': 3.92078, 'loss': 0.66651678, 'lr': 0, 'params': 425270, 'time_iter': 0.06223, 'accuracy': 0.75806, 'f1': 0.75804, 'accuracy-SBM': 0.75804, 'auc': 0.95561}
2025-07-11 14:13:53,881 - INFO - > Epoch 72: took 81.3s (avg 81.7s) | Best so far: epoch 66	train_loss: 0.6636 train_accuracy-SBM: 0.7581	val_loss: 0.6617 val_accuracy-SBM: 0.7610	test_loss: 0.6541 test_accuracy-SBM: 0.7637
2025-07-11 14:15:07,112 - INFO - train: {'epoch': 73, 'time_epoch': 72.90079, 'eta': 1910.29286, 'eta_hours': 0.53064, 'loss': 0.65005383, 'lr': 0.00018641, 'params': 425270, 'time_iter': 0.11664, 'accuracy': 0.76292, 'f1': 0.76294, 'accuracy-SBM': 0.76293, 'auc': 0.95772}
2025-07-11 14:15:11,166 - INFO - val: {'epoch': 73, 'time_epoch': 3.99784, 'loss': 0.65779236, 'lr': 0, 'params': 425270, 'time_iter': 0.06346, 'accuracy': 0.76159, 'f1': 0.76145, 'accuracy-SBM': 0.76143, 'auc': 0.95673}
2025-07-11 14:15:15,156 - INFO - test: {'epoch': 73, 'time_epoch': 3.95378, 'loss': 0.65010491, 'lr': 0, 'params': 425270, 'time_iter': 0.06276, 'accuracy': 0.76334, 'f1': 0.76332, 'accuracy-SBM': 0.76334, 'auc': 0.9578}
2025-07-11 14:15:15,159 - INFO - > Epoch 73: took 81.3s (avg 81.7s) | Best so far: epoch 73	train_loss: 0.6501 train_accuracy-SBM: 0.7629	val_loss: 0.6578 val_accuracy-SBM: 0.7614	test_loss: 0.6501 test_accuracy-SBM: 0.7633
2025-07-11 14:16:28,047 - INFO - train: {'epoch': 74, 'time_epoch': 72.66071, 'eta': 1836.54936, 'eta_hours': 0.51015, 'loss': 0.64715475, 'lr': 0.00017371, 'params': 425270, 'time_iter': 0.11626, 'accuracy': 0.76395, 'f1': 0.76397, 'accuracy-SBM': 0.76395, 'auc': 0.9581}
2025-07-11 14:16:31,974 - INFO - val: {'epoch': 74, 'time_epoch': 3.87702, 'loss': 0.71111137, 'lr': 0, 'params': 425270, 'time_iter': 0.06154, 'accuracy': 0.751, 'f1': 0.75098, 'accuracy-SBM': 0.75083, 'auc': 0.9496}
2025-07-11 14:16:35,897 - INFO - test: {'epoch': 74, 'time_epoch': 3.88754, 'loss': 0.67408452, 'lr': 0, 'params': 425270, 'time_iter': 0.06171, 'accuracy': 0.75837, 'f1': 0.75838, 'accuracy-SBM': 0.75838, 'auc': 0.9547}
2025-07-11 14:16:35,900 - INFO - > Epoch 74: took 80.7s (avg 81.7s) | Best so far: epoch 73	train_loss: 0.6501 train_accuracy-SBM: 0.7629	val_loss: 0.6578 val_accuracy-SBM: 0.7614	test_loss: 0.6501 test_accuracy-SBM: 0.7633
2025-07-11 14:17:49,002 - INFO - train: {'epoch': 75, 'time_epoch': 72.88775, 'eta': 1762.90605, 'eta_hours': 0.4897, 'loss': 0.64355914, 'lr': 0.00016136, 'params': 425270, 'time_iter': 0.11662, 'accuracy': 0.76558, 'f1': 0.76559, 'accuracy-SBM': 0.76558, 'auc': 0.95855}
2025-07-11 14:17:52,886 - INFO - val: {'epoch': 75, 'time_epoch': 3.83324, 'loss': 0.67406409, 'lr': 0, 'params': 425270, 'time_iter': 0.06085, 'accuracy': 0.75974, 'f1': 0.75963, 'accuracy-SBM': 0.75953, 'auc': 0.95469}
2025-07-11 14:17:56,803 - INFO - test: {'epoch': 75, 'time_epoch': 3.87862, 'loss': 0.65752655, 'lr': 0, 'params': 425270, 'time_iter': 0.06157, 'accuracy': 0.76294, 'f1': 0.7629, 'accuracy-SBM': 0.7629, 'auc': 0.95689}
2025-07-11 14:17:56,806 - INFO - > Epoch 75: took 80.9s (avg 81.7s) | Best so far: epoch 73	train_loss: 0.6501 train_accuracy-SBM: 0.7629	val_loss: 0.6578 val_accuracy-SBM: 0.7614	test_loss: 0.6501 test_accuracy-SBM: 0.7633
2025-07-11 14:19:10,058 - INFO - train: {'epoch': 76, 'time_epoch': 72.86039, 'eta': 1689.27419, 'eta_hours': 0.46924, 'loss': 0.64258134, 'lr': 0.00014938, 'params': 425270, 'time_iter': 0.11658, 'accuracy': 0.76565, 'f1': 0.76567, 'accuracy-SBM': 0.76565, 'auc': 0.95868}
2025-07-11 14:19:14,043 - INFO - val: {'epoch': 76, 'time_epoch': 3.92722, 'loss': 0.68223101, 'lr': 0, 'params': 425270, 'time_iter': 0.06234, 'accuracy': 0.75846, 'f1': 0.7584, 'accuracy-SBM': 0.75837, 'auc': 0.95348}
2025-07-11 14:19:18,024 - INFO - test: {'epoch': 76, 'time_epoch': 3.94192, 'loss': 0.6543164, 'lr': 0, 'params': 425270, 'time_iter': 0.06257, 'accuracy': 0.76463, 'f1': 0.7646, 'accuracy-SBM': 0.76461, 'auc': 0.95725}
2025-07-11 14:19:18,027 - INFO - > Epoch 76: took 81.2s (avg 81.7s) | Best so far: epoch 73	train_loss: 0.6501 train_accuracy-SBM: 0.7629	val_loss: 0.6578 val_accuracy-SBM: 0.7614	test_loss: 0.6501 test_accuracy-SBM: 0.7633
2025-07-11 14:20:30,974 - INFO - train: {'epoch': 77, 'time_epoch': 72.73176, 'eta': 1615.62584, 'eta_hours': 0.44878, 'loss': 0.64046264, 'lr': 0.00013779, 'params': 425270, 'time_iter': 0.11637, 'accuracy': 0.76638, 'f1': 0.76639, 'accuracy-SBM': 0.76638, 'auc': 0.95895}
2025-07-11 14:20:35,020 - INFO - val: {'epoch': 77, 'time_epoch': 3.9906, 'loss': 0.67174529, 'lr': 0, 'params': 425270, 'time_iter': 0.06334, 'accuracy': 0.76006, 'f1': 0.76003, 'accuracy-SBM': 0.76001, 'auc': 0.95493}
2025-07-11 14:20:39,002 - INFO - test: {'epoch': 77, 'time_epoch': 3.94626, 'loss': 0.6471828, 'lr': 0, 'params': 425270, 'time_iter': 0.06264, 'accuracy': 0.76556, 'f1': 0.76555, 'accuracy-SBM': 0.76556, 'auc': 0.95816}
2025-07-11 14:20:39,005 - INFO - > Epoch 77: took 81.0s (avg 81.7s) | Best so far: epoch 73	train_loss: 0.6501 train_accuracy-SBM: 0.7629	val_loss: 0.6578 val_accuracy-SBM: 0.7614	test_loss: 0.6501 test_accuracy-SBM: 0.7633
2025-07-11 14:21:51,721 - INFO - train: {'epoch': 78, 'time_epoch': 72.49686, 'eta': 1541.93825, 'eta_hours': 0.42832, 'loss': 0.63896423, 'lr': 0.00012659, 'params': 425270, 'time_iter': 0.11599, 'accuracy': 0.76773, 'f1': 0.76775, 'accuracy-SBM': 0.76773, 'auc': 0.95914}
2025-07-11 14:21:55,641 - INFO - val: {'epoch': 78, 'time_epoch': 3.86124, 'loss': 0.66460745, 'lr': 0, 'params': 425270, 'time_iter': 0.06129, 'accuracy': 0.76176, 'f1': 0.76174, 'accuracy-SBM': 0.76164, 'auc': 0.95579}
2025-07-11 14:21:59,623 - INFO - test: {'epoch': 78, 'time_epoch': 3.94373, 'loss': 0.65396382, 'lr': 0, 'params': 425270, 'time_iter': 0.0626, 'accuracy': 0.76401, 'f1': 0.76404, 'accuracy-SBM': 0.76402, 'auc': 0.9572}
2025-07-11 14:21:59,626 - INFO - > Epoch 78: took 80.6s (avg 81.7s) | Best so far: epoch 78	train_loss: 0.6390 train_accuracy-SBM: 0.7677	val_loss: 0.6646 val_accuracy-SBM: 0.7616	test_loss: 0.6540 test_accuracy-SBM: 0.7640
2025-07-11 14:23:13,302 - INFO - train: {'epoch': 79, 'time_epoch': 73.44506, 'eta': 1468.51747, 'eta_hours': 0.40792, 'loss': 0.63866004, 'lr': 0.0001158, 'params': 425270, 'time_iter': 0.11751, 'accuracy': 0.76711, 'f1': 0.76712, 'accuracy-SBM': 0.76711, 'auc': 0.95919}
2025-07-11 14:23:17,314 - INFO - val: {'epoch': 79, 'time_epoch': 3.96248, 'loss': 0.65647403, 'lr': 0, 'params': 425270, 'time_iter': 0.0629, 'accuracy': 0.76308, 'f1': 0.76301, 'accuracy-SBM': 0.76294, 'auc': 0.95683}
2025-07-11 14:23:21,274 - INFO - test: {'epoch': 79, 'time_epoch': 3.92189, 'loss': 0.64528668, 'lr': 0, 'params': 425270, 'time_iter': 0.06225, 'accuracy': 0.76739, 'f1': 0.76737, 'accuracy-SBM': 0.76739, 'auc': 0.95831}
2025-07-11 14:23:21,276 - INFO - > Epoch 79: took 81.7s (avg 81.7s) | Best so far: epoch 79	train_loss: 0.6387 train_accuracy-SBM: 0.7671	val_loss: 0.6565 val_accuracy-SBM: 0.7629	test_loss: 0.6453 test_accuracy-SBM: 0.7674
2025-07-11 14:24:35,373 - INFO - train: {'epoch': 80, 'time_epoch': 73.76562, 'eta': 1395.17129, 'eta_hours': 0.38755, 'loss': 0.63609123, 'lr': 0.00010543, 'params': 425270, 'time_iter': 0.11802, 'accuracy': 0.76848, 'f1': 0.76849, 'accuracy-SBM': 0.76848, 'auc': 0.95953}
2025-07-11 14:24:39,353 - INFO - val: {'epoch': 80, 'time_epoch': 3.92085, 'loss': 0.65413554, 'lr': 0, 'params': 425270, 'time_iter': 0.06224, 'accuracy': 0.76367, 'f1': 0.76359, 'accuracy-SBM': 0.76352, 'auc': 0.95722}
2025-07-11 14:24:43,331 - INFO - test: {'epoch': 80, 'time_epoch': 3.93942, 'loss': 0.64667883, 'lr': 0, 'params': 425270, 'time_iter': 0.06253, 'accuracy': 0.76629, 'f1': 0.76626, 'accuracy-SBM': 0.76628, 'auc': 0.95818}
2025-07-11 14:24:43,333 - INFO - > Epoch 80: took 82.1s (avg 81.7s) | Best so far: epoch 80	train_loss: 0.6361 train_accuracy-SBM: 0.7685	val_loss: 0.6541 val_accuracy-SBM: 0.7635	test_loss: 0.6467 test_accuracy-SBM: 0.7663
2025-07-11 14:25:56,887 - INFO - train: {'epoch': 81, 'time_epoch': 73.1824, 'eta': 1321.68686, 'eta_hours': 0.36714, 'loss': 0.63552628, 'lr': 9.549e-05, 'params': 425270, 'time_iter': 0.11709, 'accuracy': 0.76841, 'f1': 0.76842, 'accuracy-SBM': 0.76841, 'auc': 0.9596}
2025-07-11 14:26:00,933 - INFO - val: {'epoch': 81, 'time_epoch': 3.98707, 'loss': 0.65191785, 'lr': 0, 'params': 425270, 'time_iter': 0.06329, 'accuracy': 0.7645, 'f1': 0.76437, 'accuracy-SBM': 0.76424, 'auc': 0.95751}
2025-07-11 14:26:04,864 - INFO - test: {'epoch': 81, 'time_epoch': 3.88493, 'loss': 0.64733033, 'lr': 0, 'params': 425270, 'time_iter': 0.06167, 'accuracy': 0.76583, 'f1': 0.76583, 'accuracy-SBM': 0.76577, 'auc': 0.95814}
2025-07-11 14:26:04,867 - INFO - > Epoch 81: took 81.5s (avg 81.7s) | Best so far: epoch 81	train_loss: 0.6355 train_accuracy-SBM: 0.7684	val_loss: 0.6519 val_accuracy-SBM: 0.7642	test_loss: 0.6473 test_accuracy-SBM: 0.7658
2025-07-11 14:27:17,706 - INFO - train: {'epoch': 82, 'time_epoch': 72.59799, 'eta': 1248.09, 'eta_hours': 0.34669, 'loss': 0.63411875, 'lr': 8.6e-05, 'params': 425270, 'time_iter': 0.11616, 'accuracy': 0.76946, 'f1': 0.76948, 'accuracy-SBM': 0.76947, 'auc': 0.95974}
2025-07-11 14:27:21,641 - INFO - val: {'epoch': 82, 'time_epoch': 3.8872, 'loss': 0.66459435, 'lr': 0, 'params': 425270, 'time_iter': 0.0617, 'accuracy': 0.76295, 'f1': 0.76285, 'accuracy-SBM': 0.76282, 'auc': 0.95571}
2025-07-11 14:27:25,613 - INFO - test: {'epoch': 82, 'time_epoch': 3.93221, 'loss': 0.65078956, 'lr': 0, 'params': 425270, 'time_iter': 0.06242, 'accuracy': 0.76485, 'f1': 0.76481, 'accuracy-SBM': 0.76482, 'auc': 0.95768}
2025-07-11 14:27:25,615 - INFO - > Epoch 82: took 80.7s (avg 81.7s) | Best so far: epoch 81	train_loss: 0.6355 train_accuracy-SBM: 0.7684	val_loss: 0.6519 val_accuracy-SBM: 0.7642	test_loss: 0.6473 test_accuracy-SBM: 0.7658
2025-07-11 14:28:38,071 - INFO - train: {'epoch': 83, 'time_epoch': 72.22596, 'eta': 1174.44607, 'eta_hours': 0.32624, 'loss': 0.63242044, 'lr': 7.695e-05, 'params': 425270, 'time_iter': 0.11556, 'accuracy': 0.76941, 'f1': 0.76943, 'accuracy-SBM': 0.76942, 'auc': 0.95999}
2025-07-11 14:28:42,094 - INFO - val: {'epoch': 83, 'time_epoch': 3.96246, 'loss': 0.65647483, 'lr': 0, 'params': 425270, 'time_iter': 0.0629, 'accuracy': 0.76322, 'f1': 0.76312, 'accuracy-SBM': 0.76302, 'auc': 0.95695}
2025-07-11 14:28:46,094 - INFO - test: {'epoch': 83, 'time_epoch': 3.96139, 'loss': 0.65635307, 'lr': 0, 'params': 425270, 'time_iter': 0.06288, 'accuracy': 0.76341, 'f1': 0.76344, 'accuracy-SBM': 0.7634, 'auc': 0.95697}
2025-07-11 14:28:46,096 - INFO - > Epoch 83: took 80.5s (avg 81.6s) | Best so far: epoch 81	train_loss: 0.6355 train_accuracy-SBM: 0.7684	val_loss: 0.6519 val_accuracy-SBM: 0.7642	test_loss: 0.6473 test_accuracy-SBM: 0.7658
2025-07-11 14:29:58,724 - INFO - train: {'epoch': 84, 'time_epoch': 72.40923, 'eta': 1100.86784, 'eta_hours': 0.3058, 'loss': 0.63062426, 'lr': 6.837e-05, 'params': 425270, 'time_iter': 0.11585, 'accuracy': 0.77023, 'f1': 0.77024, 'accuracy-SBM': 0.77023, 'auc': 0.96022}
2025-07-11 14:30:02,763 - INFO - val: {'epoch': 84, 'time_epoch': 3.98811, 'loss': 0.65510234, 'lr': 0, 'params': 425270, 'time_iter': 0.0633, 'accuracy': 0.76421, 'f1': 0.76414, 'accuracy-SBM': 0.76411, 'auc': 0.95708}
2025-07-11 14:30:06,815 - INFO - test: {'epoch': 84, 'time_epoch': 4.00388, 'loss': 0.65209275, 'lr': 0, 'params': 425270, 'time_iter': 0.06355, 'accuracy': 0.7651, 'f1': 0.7651, 'accuracy-SBM': 0.76512, 'auc': 0.95751}
2025-07-11 14:30:06,817 - INFO - > Epoch 84: took 80.7s (avg 81.6s) | Best so far: epoch 81	train_loss: 0.6355 train_accuracy-SBM: 0.7684	val_loss: 0.6519 val_accuracy-SBM: 0.7642	test_loss: 0.6473 test_accuracy-SBM: 0.7658
2025-07-11 14:31:20,099 - INFO - train: {'epoch': 85, 'time_epoch': 73.0474, 'eta': 1027.42068, 'eta_hours': 0.28539, 'loss': 0.62908793, 'lr': 6.026e-05, 'params': 425270, 'time_iter': 0.11688, 'accuracy': 0.7709, 'f1': 0.77092, 'accuracy-SBM': 0.7709, 'auc': 0.96041}
2025-07-11 14:31:24,110 - INFO - val: {'epoch': 85, 'time_epoch': 3.96142, 'loss': 0.65726502, 'lr': 0, 'params': 425270, 'time_iter': 0.06288, 'accuracy': 0.76283, 'f1': 0.76277, 'accuracy-SBM': 0.76272, 'auc': 0.95673}
2025-07-11 14:31:28,010 - INFO - test: {'epoch': 85, 'time_epoch': 3.86346, 'loss': 0.65406632, 'lr': 0, 'params': 425270, 'time_iter': 0.06132, 'accuracy': 0.76379, 'f1': 0.76378, 'accuracy-SBM': 0.76374, 'auc': 0.95718}
2025-07-11 14:31:28,012 - INFO - > Epoch 85: took 81.2s (avg 81.6s) | Best so far: epoch 81	train_loss: 0.6355 train_accuracy-SBM: 0.7684	val_loss: 0.6519 val_accuracy-SBM: 0.7642	test_loss: 0.6473 test_accuracy-SBM: 0.7658
2025-07-11 14:32:40,784 - INFO - train: {'epoch': 86, 'time_epoch': 72.53199, 'eta': 953.9057, 'eta_hours': 0.26497, 'loss': 0.62762449, 'lr': 5.264e-05, 'params': 425270, 'time_iter': 0.11605, 'accuracy': 0.77135, 'f1': 0.77137, 'accuracy-SBM': 0.77135, 'auc': 0.96058}
2025-07-11 14:32:44,714 - INFO - val: {'epoch': 86, 'time_epoch': 3.87235, 'loss': 0.65278227, 'lr': 0, 'params': 425270, 'time_iter': 0.06147, 'accuracy': 0.76467, 'f1': 0.7646, 'accuracy-SBM': 0.76451, 'auc': 0.95735}
2025-07-11 14:32:48,617 - INFO - test: {'epoch': 86, 'time_epoch': 3.86429, 'loss': 0.64980167, 'lr': 0, 'params': 425270, 'time_iter': 0.06134, 'accuracy': 0.76555, 'f1': 0.76555, 'accuracy-SBM': 0.76554, 'auc': 0.95778}
2025-07-11 14:32:48,619 - INFO - > Epoch 86: took 80.6s (avg 81.6s) | Best so far: epoch 86	train_loss: 0.6276 train_accuracy-SBM: 0.7713	val_loss: 0.6528 val_accuracy-SBM: 0.7645	test_loss: 0.6498 test_accuracy-SBM: 0.7655
2025-07-11 14:34:02,055 - INFO - train: {'epoch': 87, 'time_epoch': 73.20957, 'eta': 880.50546, 'eta_hours': 0.24458, 'loss': 0.62886697, 'lr': 4.55e-05, 'params': 425270, 'time_iter': 0.11714, 'accuracy': 0.77087, 'f1': 0.77088, 'accuracy-SBM': 0.77087, 'auc': 0.96044}
2025-07-11 14:34:05,996 - INFO - val: {'epoch': 87, 'time_epoch': 3.89173, 'loss': 0.67009083, 'lr': 0, 'params': 425270, 'time_iter': 0.06177, 'accuracy': 0.76221, 'f1': 0.76216, 'accuracy-SBM': 0.76207, 'auc': 0.95505}
2025-07-11 14:34:09,942 - INFO - test: {'epoch': 87, 'time_epoch': 3.91031, 'loss': 0.6469484, 'lr': 0, 'params': 425270, 'time_iter': 0.06207, 'accuracy': 0.76636, 'f1': 0.76636, 'accuracy-SBM': 0.76635, 'auc': 0.95816}
2025-07-11 14:34:09,944 - INFO - > Epoch 87: took 81.3s (avg 81.6s) | Best so far: epoch 86	train_loss: 0.6276 train_accuracy-SBM: 0.7713	val_loss: 0.6528 val_accuracy-SBM: 0.7645	test_loss: 0.6498 test_accuracy-SBM: 0.7655
2025-07-11 14:35:23,070 - INFO - train: {'epoch': 88, 'time_epoch': 72.89393, 'eta': 807.07049, 'eta_hours': 0.22419, 'loss': 0.62815108, 'lr': 3.886e-05, 'params': 425270, 'time_iter': 0.11663, 'accuracy': 0.77142, 'f1': 0.77144, 'accuracy-SBM': 0.77142, 'auc': 0.96052}
2025-07-11 14:35:27,030 - INFO - val: {'epoch': 88, 'time_epoch': 3.89319, 'loss': 0.65333361, 'lr': 0, 'params': 425270, 'time_iter': 0.0618, 'accuracy': 0.76526, 'f1': 0.76518, 'accuracy-SBM': 0.76509, 'auc': 0.95732}
2025-07-11 14:35:30,981 - INFO - test: {'epoch': 88, 'time_epoch': 3.91343, 'loss': 0.64409037, 'lr': 0, 'params': 425270, 'time_iter': 0.06212, 'accuracy': 0.76772, 'f1': 0.76769, 'accuracy-SBM': 0.76766, 'auc': 0.9585}
2025-07-11 14:35:30,983 - INFO - > Epoch 88: took 81.0s (avg 81.6s) | Best so far: epoch 88	train_loss: 0.6282 train_accuracy-SBM: 0.7714	val_loss: 0.6533 val_accuracy-SBM: 0.7651	test_loss: 0.6441 test_accuracy-SBM: 0.7677
2025-07-11 14:36:43,977 - INFO - train: {'epoch': 89, 'time_epoch': 72.75298, 'eta': 733.63188, 'eta_hours': 0.20379, 'loss': 0.62660911, 'lr': 3.272e-05, 'params': 425270, 'time_iter': 0.1164, 'accuracy': 0.77175, 'f1': 0.77176, 'accuracy-SBM': 0.77175, 'auc': 0.96072}
2025-07-11 14:36:47,967 - INFO - val: {'epoch': 89, 'time_epoch': 3.93992, 'loss': 0.65406051, 'lr': 0, 'params': 425270, 'time_iter': 0.06254, 'accuracy': 0.76576, 'f1': 0.76568, 'accuracy-SBM': 0.7656, 'auc': 0.9573}
2025-07-11 14:36:51,960 - INFO - test: {'epoch': 89, 'time_epoch': 3.92695, 'loss': 0.64402639, 'lr': 0, 'params': 425270, 'time_iter': 0.06233, 'accuracy': 0.76856, 'f1': 0.76855, 'accuracy-SBM': 0.76852, 'auc': 0.95858}
2025-07-11 14:36:51,962 - INFO - > Epoch 89: took 81.0s (avg 81.6s) | Best so far: epoch 89	train_loss: 0.6266 train_accuracy-SBM: 0.7718	val_loss: 0.6541 val_accuracy-SBM: 0.7656	test_loss: 0.6440 test_accuracy-SBM: 0.7685
2025-07-11 14:38:05,344 - INFO - train: {'epoch': 90, 'time_epoch': 73.16034, 'eta': 660.24863, 'eta_hours': 0.1834, 'loss': 0.62513074, 'lr': 2.709e-05, 'params': 425270, 'time_iter': 0.11706, 'accuracy': 0.77242, 'f1': 0.77244, 'accuracy-SBM': 0.77243, 'auc': 0.9609}
2025-07-11 14:38:09,299 - INFO - val: {'epoch': 90, 'time_epoch': 3.90498, 'loss': 0.65824958, 'lr': 0, 'params': 425270, 'time_iter': 0.06198, 'accuracy': 0.76379, 'f1': 0.76371, 'accuracy-SBM': 0.76362, 'auc': 0.95675}
2025-07-11 14:38:13,218 - INFO - test: {'epoch': 90, 'time_epoch': 3.86785, 'loss': 0.64325519, 'lr': 0, 'params': 425270, 'time_iter': 0.06139, 'accuracy': 0.7674, 'f1': 0.76736, 'accuracy-SBM': 0.76734, 'auc': 0.95864}
2025-07-11 14:38:13,221 - INFO - > Epoch 90: took 81.3s (avg 81.6s) | Best so far: epoch 89	train_loss: 0.6266 train_accuracy-SBM: 0.7718	val_loss: 0.6541 val_accuracy-SBM: 0.7656	test_loss: 0.6440 test_accuracy-SBM: 0.7685
2025-07-11 14:39:29,553 - INFO - train: {'epoch': 91, 'time_epoch': 76.0203, 'eta': 587.11892, 'eta_hours': 0.16309, 'loss': 0.62278037, 'lr': 2.198e-05, 'params': 425270, 'time_iter': 0.12163, 'accuracy': 0.77286, 'f1': 0.77287, 'accuracy-SBM': 0.77286, 'auc': 0.9612}
2025-07-11 14:39:33,466 - INFO - val: {'epoch': 91, 'time_epoch': 3.85472, 'loss': 0.65446704, 'lr': 0, 'params': 425270, 'time_iter': 0.06119, 'accuracy': 0.7649, 'f1': 0.7648, 'accuracy-SBM': 0.76475, 'auc': 0.95723}
2025-07-11 14:39:37,417 - INFO - test: {'epoch': 91, 'time_epoch': 3.91175, 'loss': 0.64843357, 'lr': 0, 'params': 425270, 'time_iter': 0.06209, 'accuracy': 0.76584, 'f1': 0.76581, 'accuracy-SBM': 0.7658, 'auc': 0.95803}
2025-07-11 14:39:37,419 - INFO - > Epoch 91: took 84.2s (avg 81.6s) | Best so far: epoch 89	train_loss: 0.6266 train_accuracy-SBM: 0.7718	val_loss: 0.6541 val_accuracy-SBM: 0.7656	test_loss: 0.6440 test_accuracy-SBM: 0.7685
2025-07-11 14:40:50,626 - INFO - train: {'epoch': 92, 'time_epoch': 72.98562, 'eta': 513.69863, 'eta_hours': 0.14269, 'loss': 0.62331214, 'lr': 1.74e-05, 'params': 425270, 'time_iter': 0.11678, 'accuracy': 0.77291, 'f1': 0.77292, 'accuracy-SBM': 0.77291, 'auc': 0.96114}
2025-07-11 14:40:54,519 - INFO - val: {'epoch': 92, 'time_epoch': 3.82942, 'loss': 0.65370234, 'lr': 0, 'params': 425270, 'time_iter': 0.06078, 'accuracy': 0.76618, 'f1': 0.76607, 'accuracy-SBM': 0.76603, 'auc': 0.95719}
2025-07-11 14:40:58,438 - INFO - test: {'epoch': 92, 'time_epoch': 3.88265, 'loss': 0.64378344, 'lr': 0, 'params': 425270, 'time_iter': 0.06163, 'accuracy': 0.76776, 'f1': 0.76775, 'accuracy-SBM': 0.76774, 'auc': 0.95854}
2025-07-11 14:40:58,441 - INFO - > Epoch 92: took 81.0s (avg 81.6s) | Best so far: epoch 92	train_loss: 0.6233 train_accuracy-SBM: 0.7729	val_loss: 0.6537 val_accuracy-SBM: 0.7660	test_loss: 0.6438 test_accuracy-SBM: 0.7677
2025-07-11 14:42:11,703 - INFO - train: {'epoch': 93, 'time_epoch': 73.03043, 'eta': 440.29045, 'eta_hours': 0.1223, 'loss': 0.62163746, 'lr': 1.334e-05, 'params': 425270, 'time_iter': 0.11685, 'accuracy': 0.77347, 'f1': 0.77349, 'accuracy-SBM': 0.77347, 'auc': 0.96135}
2025-07-11 14:42:15,713 - INFO - val: {'epoch': 93, 'time_epoch': 3.95804, 'loss': 0.65218935, 'lr': 0, 'params': 425270, 'time_iter': 0.06283, 'accuracy': 0.76636, 'f1': 0.76627, 'accuracy-SBM': 0.7662, 'auc': 0.95741}
2025-07-11 14:42:19,633 - INFO - test: {'epoch': 93, 'time_epoch': 3.88114, 'loss': 0.64461148, 'lr': 0, 'params': 425270, 'time_iter': 0.06161, 'accuracy': 0.76759, 'f1': 0.76758, 'accuracy-SBM': 0.76758, 'auc': 0.95842}
2025-07-11 14:42:19,635 - INFO - > Epoch 93: took 81.2s (avg 81.6s) | Best so far: epoch 93	train_loss: 0.6216 train_accuracy-SBM: 0.7735	val_loss: 0.6522 val_accuracy-SBM: 0.7662	test_loss: 0.6446 test_accuracy-SBM: 0.7676
2025-07-11 14:43:32,444 - INFO - train: {'epoch': 94, 'time_epoch': 72.58099, 'eta': 366.86656, 'eta_hours': 0.10191, 'loss': 0.62288618, 'lr': 9.81e-06, 'params': 425270, 'time_iter': 0.11613, 'accuracy': 0.77303, 'f1': 0.77304, 'accuracy-SBM': 0.77303, 'auc': 0.96119}
2025-07-11 14:43:36,388 - INFO - val: {'epoch': 94, 'time_epoch': 3.88526, 'loss': 0.65200709, 'lr': 0, 'params': 425270, 'time_iter': 0.06167, 'accuracy': 0.7664, 'f1': 0.76631, 'accuracy-SBM': 0.76624, 'auc': 0.95749}
2025-07-11 14:43:40,298 - INFO - test: {'epoch': 94, 'time_epoch': 3.87145, 'loss': 0.64505998, 'lr': 0, 'params': 425270, 'time_iter': 0.06145, 'accuracy': 0.76789, 'f1': 0.76787, 'accuracy-SBM': 0.76787, 'auc': 0.95841}
2025-07-11 14:43:40,300 - INFO - > Epoch 94: took 80.7s (avg 81.6s) | Best so far: epoch 94	train_loss: 0.6229 train_accuracy-SBM: 0.7730	val_loss: 0.6520 val_accuracy-SBM: 0.7662	test_loss: 0.6451 test_accuracy-SBM: 0.7679
2025-07-11 14:44:53,311 - INFO - train: {'epoch': 95, 'time_epoch': 72.7767, 'eta': 293.46839, 'eta_hours': 0.08152, 'loss': 0.6229639, 'lr': 6.82e-06, 'params': 425270, 'time_iter': 0.11644, 'accuracy': 0.77298, 'f1': 0.77299, 'accuracy-SBM': 0.77298, 'auc': 0.96119}
2025-07-11 14:44:57,247 - INFO - val: {'epoch': 95, 'time_epoch': 3.88584, 'loss': 0.6505849, 'lr': 0, 'params': 425270, 'time_iter': 0.06168, 'accuracy': 0.76692, 'f1': 0.76681, 'accuracy-SBM': 0.76674, 'auc': 0.95765}
2025-07-11 14:45:01,257 - INFO - test: {'epoch': 95, 'time_epoch': 3.97029, 'loss': 0.64398871, 'lr': 0, 'params': 425270, 'time_iter': 0.06302, 'accuracy': 0.76744, 'f1': 0.76741, 'accuracy-SBM': 0.7674, 'auc': 0.95851}
2025-07-11 14:45:01,259 - INFO - > Epoch 95: took 81.0s (avg 81.6s) | Best so far: epoch 95	train_loss: 0.6230 train_accuracy-SBM: 0.7730	val_loss: 0.6506 val_accuracy-SBM: 0.7667	test_loss: 0.6440 test_accuracy-SBM: 0.7674
2025-07-11 14:46:14,511 - INFO - train: {'epoch': 96, 'time_epoch': 73.01857, 'eta': 220.09051, 'eta_hours': 0.06114, 'loss': 0.62141346, 'lr': 4.37e-06, 'params': 425270, 'time_iter': 0.11683, 'accuracy': 0.7732, 'f1': 0.77322, 'accuracy-SBM': 0.77321, 'auc': 0.96139}
2025-07-11 14:46:18,538 - INFO - val: {'epoch': 96, 'time_epoch': 3.96598, 'loss': 0.65408649, 'lr': 0, 'params': 425270, 'time_iter': 0.06295, 'accuracy': 0.76634, 'f1': 0.76625, 'accuracy-SBM': 0.76618, 'auc': 0.95726}
2025-07-11 14:46:22,554 - INFO - test: {'epoch': 96, 'time_epoch': 3.97753, 'loss': 0.64778499, 'lr': 0, 'params': 425270, 'time_iter': 0.06314, 'accuracy': 0.76657, 'f1': 0.76654, 'accuracy-SBM': 0.76654, 'auc': 0.95809}
2025-07-11 14:46:22,556 - INFO - > Epoch 96: took 81.3s (avg 81.6s) | Best so far: epoch 95	train_loss: 0.6230 train_accuracy-SBM: 0.7730	val_loss: 0.6506 val_accuracy-SBM: 0.7667	test_loss: 0.6440 test_accuracy-SBM: 0.7674
2025-07-11 14:47:35,851 - INFO - train: {'epoch': 97, 'time_epoch': 73.06745, 'eta': 146.72097, 'eta_hours': 0.04076, 'loss': 0.62226272, 'lr': 2.46e-06, 'params': 425270, 'time_iter': 0.11691, 'accuracy': 0.77336, 'f1': 0.77338, 'accuracy-SBM': 0.77336, 'auc': 0.96127}
2025-07-11 14:47:39,845 - INFO - val: {'epoch': 97, 'time_epoch': 3.94372, 'loss': 0.6513851, 'lr': 0, 'params': 425270, 'time_iter': 0.0626, 'accuracy': 0.76585, 'f1': 0.76575, 'accuracy-SBM': 0.7657, 'auc': 0.95751}
2025-07-11 14:47:43,865 - INFO - test: {'epoch': 97, 'time_epoch': 3.98158, 'loss': 0.64212273, 'lr': 0, 'params': 425270, 'time_iter': 0.0632, 'accuracy': 0.76843, 'f1': 0.76841, 'accuracy-SBM': 0.76841, 'auc': 0.95873}
2025-07-11 14:47:43,868 - INFO - > Epoch 97: took 81.3s (avg 81.6s) | Best so far: epoch 95	train_loss: 0.6230 train_accuracy-SBM: 0.7730	val_loss: 0.6506 val_accuracy-SBM: 0.7667	test_loss: 0.6440 test_accuracy-SBM: 0.7674
2025-07-11 14:48:57,469 - INFO - train: {'epoch': 98, 'time_epoch': 73.37739, 'eta': 73.36065, 'eta_hours': 0.02038, 'loss': 0.62073611, 'lr': 1.09e-06, 'params': 425270, 'time_iter': 0.1174, 'accuracy': 0.77365, 'f1': 0.77367, 'accuracy-SBM': 0.77365, 'auc': 0.96146}
2025-07-11 14:49:01,554 - INFO - val: {'epoch': 98, 'time_epoch': 4.02852, 'loss': 0.65330949, 'lr': 0, 'params': 425270, 'time_iter': 0.06394, 'accuracy': 0.76634, 'f1': 0.76625, 'accuracy-SBM': 0.7662, 'auc': 0.95732}
2025-07-11 14:49:05,563 - INFO - test: {'epoch': 98, 'time_epoch': 3.96708, 'loss': 0.64633104, 'lr': 0, 'params': 425270, 'time_iter': 0.06297, 'accuracy': 0.76671, 'f1': 0.76668, 'accuracy-SBM': 0.76669, 'auc': 0.95826}
2025-07-11 14:49:05,565 - INFO - > Epoch 98: took 81.7s (avg 81.6s) | Best so far: epoch 95	train_loss: 0.6230 train_accuracy-SBM: 0.7730	val_loss: 0.6506 val_accuracy-SBM: 0.7667	test_loss: 0.6440 test_accuracy-SBM: 0.7674
2025-07-11 14:50:18,660 - INFO - train: {'epoch': 99, 'time_epoch': 72.87455, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.62065447, 'lr': 2.7e-07, 'params': 425270, 'time_iter': 0.1166, 'accuracy': 0.77362, 'f1': 0.77363, 'accuracy-SBM': 0.77362, 'auc': 0.96148}
2025-07-11 14:50:22,645 - INFO - val: {'epoch': 99, 'time_epoch': 3.93558, 'loss': 0.65258162, 'lr': 0, 'params': 425270, 'time_iter': 0.06247, 'accuracy': 0.76647, 'f1': 0.76637, 'accuracy-SBM': 0.7663, 'auc': 0.95742}
2025-07-11 14:50:26,574 - INFO - test: {'epoch': 99, 'time_epoch': 3.89062, 'loss': 0.64531459, 'lr': 0, 'params': 425270, 'time_iter': 0.06176, 'accuracy': 0.76727, 'f1': 0.76725, 'accuracy-SBM': 0.76725, 'auc': 0.95838}
2025-07-11 14:50:26,757 - INFO - > Epoch 99: took 81.0s (avg 81.6s) | Best so far: epoch 95	train_loss: 0.6230 train_accuracy-SBM: 0.7730	val_loss: 0.6506 val_accuracy-SBM: 0.7667	test_loss: 0.6440 test_accuracy-SBM: 0.7674
2025-07-11 14:50:26,758 - INFO - Avg time per epoch: 81.59s
2025-07-11 14:50:26,758 - INFO - Total train loop time: 2.27h
2025-07-11 14:50:26,768 - INFO - Task done, results saved in results/Cluster/Cluster-GINE-47
2025-07-11 14:50:26,768 - INFO - Total time: 10531.58s (2.93h)
2025-07-11 14:50:26,770 - INFO - Results aggregated across runs saved in results/Cluster/Cluster-GINE-47/agg
2025-07-11 14:50:26,770 - INFO - === OPTIMIZED TRAINING COMPLETED SUCCESSFULLY! ===
2025-07-11 14:50:26,770 - INFO - Results saved in: results/Cluster/Cluster-GINE-47
2025-07-11 14:50:26,770 - INFO - Test results JSON files saved in: results/Cluster/Cluster-GINE-47/test_results/
Completed seed 47. Results saved in results/Cluster/Cluster-GINE-47
----------------------------------------
All experiments completed!
