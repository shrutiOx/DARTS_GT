Lmod has detected the following error: The following module(s) are unknown:
"cuda/12.0"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore-cache load "cuda/12.0"

Also make sure that all modulefiles written in TCL start with the string
#%Module



              total        used        free      shared  buff/cache   available
Mem:          376Gi        18Gi       282Gi       2.8Gi        75Gi       352Gi
Swap:         1.9Gi       2.0Mi       1.9Gi
Fri Jul 11 06:39:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:1C:00.0 Off |                    0 |
| N/A   34C    P0             49W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Running experiment with seed: 41
Starting training for seed 41...
Changed working directory to: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GINE
Loading config from: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/UNI_GT/GINE/confignas.yaml
Using device: cuda
2025-07-11 06:40:08,083 - INFO - GPU Mem: 34.1GB
2025-07-11 06:40:08,083 - INFO - Run directory: results/Cluster/Cluster-GINE-41
2025-07-11 06:40:08,083 - INFO - Seed: 41
2025-07-11 06:40:08,083 - INFO - === OPTIMIZED MOE CONFIGURATION ===
2025-07-11 06:40:08,083 - INFO - Routing mode: none
2025-07-11 06:40:08,083 - INFO - Expert types: ['GINE', 'CustomGatedGCN', 'GATV2']
2025-07-11 06:40:08,083 - INFO - Number of layers: 16
2025-07-11 06:40:08,083 - INFO - Uncertainty enabled: False
2025-07-11 06:40:08,083 - INFO - Training mode: custom
2025-07-11 06:40:08,083 - INFO - Optimization: Uncertainty only for last layer + test/val
2025-07-11 06:40:08,083 - INFO - Additional features: Router weights logging + JSON export
2025-07-11 06:40:21,854 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 06:40:21,855 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 06:40:21,856 - INFO -   undirected: True
2025-07-11 06:40:21,856 - INFO -   num graphs: 12000
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-07-11 06:40:21,857 - INFO -   avg num_nodes/graph: 117
2025-07-11 06:40:21,857 - INFO -   num node features: 7
2025-07-11 06:40:21,857 - INFO -   num edge features: 0
2025-07-11 06:40:21,858 - INFO -   num classes: 6
2025-07-11 06:40:21,859 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 06:40:21,859 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 06:40:21,867 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s] 15%|█▌        | 1845/12000 [00:10<00:55, 184.34it/s] 32%|███▏      | 3811/12000 [00:20<00:42, 191.53it/s] 48%|████▊     | 5801/12000 [00:30<00:31, 194.91it/s] 65%|██████▍   | 7782/12000 [00:40<00:21, 196.16it/s] 81%|████████▏ | 9760/12000 [00:50<00:11, 196.73it/s] 98%|█████████▊| 11815/12000 [01:00<00:00, 199.71it/s]100%|██████████| 12000/12000 [01:00<00:00, 197.08it/s]
2025-07-11 06:41:23,506 - INFO - Done! Took 00:01:01.65
2025-07-11 06:41:23,527 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 06:41:23,726 - INFO - Created model type: <class 'torch_geometric.graphgym.model_builder.GraphGymModule'>
2025-07-11 06:41:23,726 - INFO - Inner model type: <class 'graphgps.network.uniform_gt_model_edge.UNIFORM_GTModelEdge'>
2025-07-11 06:41:23,727 - INFO - Inner model has get_darts_model: False
2025-07-11 06:41:23,730 - INFO - GraphGymModule(
  (model): UNIFORM_GTModelEdge(
    (encoder): FeatureEncoder(
      (node_encoder): LapPENodeEncoder(
        (linear_x): Linear(in_features=7, out_features=32, bias=True)
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (edge_encoder): DummyEdgeEncoder(
        (encoder): Embedding(1, 48)
      )
    )
    (layers): Sequential(
      (0): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (1): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (2): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (3): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (4): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (5): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (6): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (7): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (8): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (9): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (10): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (11): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (12): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (13): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (14): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (15): UNIFORMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (post_mp): GNNInductiveNodeHead(
      (layer_post_mp): MLP(
        (model): Sequential(
          (0): GeneralMultiLayer(
            (Layer_0): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
            (Layer_1): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
          )
          (1): Linear(
            (model): Linear(48, 6, bias=True)
          )
        )
      )
    )
  )
)
2025-07-11 06:41:23,735 - INFO - Number of parameters: 425,270
2025-07-11 06:41:23,735 - INFO - Starting optimized training: 2025-07-11 06:41:23.735976
2025-07-11 06:41:29,274 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
2025-07-11 06:41:29,274 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-07-11 06:41:29,275 - INFO -   undirected: True
2025-07-11 06:41:29,276 - INFO -   num graphs: 12000
2025-07-11 06:41:29,276 - INFO -   avg num_nodes/graph: 117
2025-07-11 06:41:29,276 - INFO -   num node features: 7
2025-07-11 06:41:29,276 - INFO -   num edge features: 0
2025-07-11 06:41:29,277 - INFO -   num classes: 6
2025-07-11 06:41:29,278 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-07-11 06:41:29,278 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-07-11 06:41:29,286 - INFO -   ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s] 17%|█▋        | 2046/12000 [00:10<00:48, 204.52it/s] 34%|███▎      | 4032/12000 [00:20<00:39, 201.04it/s] 50%|█████     | 6031/12000 [00:30<00:29, 200.51it/s] 67%|██████▋   | 8044/12000 [00:40<00:19, 200.81it/s] 83%|████████▎ | 9992/12000 [00:50<00:10, 198.63it/s]100%|██████████| 12000/12000 [00:59<00:00, 200.55it/s]
2025-07-11 06:42:29,832 - INFO - Done! Took 00:01:00.55
2025-07-11 06:42:29,855 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-07-11 06:42:29,892 - INFO - === STARTING OPTIMIZED TRAINING ===
2025-07-11 06:42:29,892 - INFO - Start from epoch 0
2025-07-11 06:43:44,980 - INFO - train: {'epoch': 0, 'time_epoch': 74.52181, 'eta': 7377.65891, 'eta_hours': 2.04935, 'loss': 1.79498213, 'lr': 0.0, 'params': 425270, 'time_iter': 0.11923, 'accuracy': 0.16684, 'f1': 0.07573, 'accuracy-SBM': 0.16689, 'auc': 0.50097}
2025-07-11 06:43:44,988 - INFO - ...computing epoch stats took: 0.56s
2025-07-11 06:43:48,969 - INFO - val: {'epoch': 0, 'time_epoch': 3.9148, 'loss': 1.79478421, 'lr': 0, 'params': 425270, 'time_iter': 0.06214, 'accuracy': 0.16323, 'f1': 0.06935, 'accuracy-SBM': 0.16622, 'auc': 0.50147}
2025-07-11 06:43:48,972 - INFO - ...computing epoch stats took: 0.07s
2025-07-11 06:43:53,140 - INFO - test: {'epoch': 0, 'time_epoch': 4.11845, 'loss': 1.79459032, 'lr': 0, 'params': 425270, 'time_iter': 0.06537, 'accuracy': 0.1663, 'f1': 0.07101, 'accuracy-SBM': 0.16654, 'auc': 0.50027}
2025-07-11 06:43:53,142 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 06:43:53,142 - INFO - > Epoch 0: took 83.2s (avg 83.2s) | Best so far: epoch 0	train_loss: 1.7950 train_accuracy-SBM: 0.1669	val_loss: 1.7948 val_accuracy-SBM: 0.1662	test_loss: 1.7946 test_accuracy-SBM: 0.1665
2025-07-11 06:45:04,523 - INFO - train: {'epoch': 1, 'time_epoch': 70.84601, 'eta': 7123.02311, 'eta_hours': 1.97862, 'loss': 1.75844635, 'lr': 0.0002, 'params': 425270, 'time_iter': 0.11335, 'accuracy': 0.22312, 'f1': 0.21474, 'accuracy-SBM': 0.22314, 'auc': 0.58262}
2025-07-11 06:45:04,529 - INFO - ...computing epoch stats took: 0.52s
2025-07-11 06:45:08,270 - INFO - val: {'epoch': 1, 'time_epoch': 3.70322, 'loss': 1.75909145, 'lr': 0, 'params': 425270, 'time_iter': 0.05878, 'accuracy': 0.23431, 'f1': 0.18639, 'accuracy-SBM': 0.23557, 'auc': 0.59404}
2025-07-11 06:45:08,272 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 06:45:11,881 - INFO - test: {'epoch': 1, 'time_epoch': 3.56537, 'loss': 1.75983387, 'lr': 0, 'params': 425270, 'time_iter': 0.05659, 'accuracy': 0.2323, 'f1': 0.18436, 'accuracy-SBM': 0.23242, 'auc': 0.59201}
2025-07-11 06:45:11,883 - INFO - ...computing epoch stats took: 0.04s
2025-07-11 06:45:11,883 - INFO - > Epoch 1: took 78.7s (avg 81.0s) | Best so far: epoch 1	train_loss: 1.7584 train_accuracy-SBM: 0.2231	val_loss: 1.7591 val_accuracy-SBM: 0.2356	test_loss: 1.7598 test_accuracy-SBM: 0.2324
2025-07-11 06:46:21,504 - INFO - train: {'epoch': 2, 'time_epoch': 69.34481, 'eta': 6942.375, 'eta_hours': 1.92844, 'loss': 1.55960621, 'lr': 0.0004, 'params': 425270, 'time_iter': 0.11095, 'accuracy': 0.35094, 'f1': 0.34093, 'accuracy-SBM': 0.35092, 'auc': 0.73521}
2025-07-11 06:46:21,511 - INFO - ...computing epoch stats took: 0.26s
2025-07-11 06:46:24,886 - INFO - val: {'epoch': 2, 'time_epoch': 3.30744, 'loss': 1.912965, 'lr': 0, 'params': 425270, 'time_iter': 0.0525, 'accuracy': 0.1718, 'f1': 0.09125, 'accuracy-SBM': 0.17225, 'auc': 0.51487}
2025-07-11 06:46:24,890 - INFO - ...computing epoch stats took: 0.07s
2025-07-11 06:46:28,409 - INFO - test: {'epoch': 2, 'time_epoch': 3.46293, 'loss': 1.91568048, 'lr': 0, 'params': 425270, 'time_iter': 0.05497, 'accuracy': 0.1742, 'f1': 0.09661, 'accuracy-SBM': 0.17568, 'auc': 0.51392}
2025-07-11 06:46:28,411 - INFO - ...computing epoch stats took: 0.05s
2025-07-11 06:46:28,412 - INFO - > Epoch 2: took 76.5s (avg 79.5s) | Best so far: epoch 1	train_loss: 1.7584 train_accuracy-SBM: 0.2231	val_loss: 1.7591 val_accuracy-SBM: 0.2356	test_loss: 1.7598 test_accuracy-SBM: 0.2324
2025-07-11 06:47:41,236 - INFO - train: {'epoch': 3, 'time_epoch': 72.57602, 'eta': 6894.92746, 'eta_hours': 1.91526, 'loss': 1.453742, 'lr': 0.0006, 'params': 425270, 'time_iter': 0.11612, 'accuracy': 0.38722, 'f1': 0.38547, 'accuracy-SBM': 0.3872, 'auc': 0.77522}
2025-07-11 06:47:44,757 - INFO - val: {'epoch': 3, 'time_epoch': 3.46019, 'loss': 1.75928892, 'lr': 0, 'params': 425270, 'time_iter': 0.05492, 'accuracy': 0.24075, 'f1': 0.18942, 'accuracy-SBM': 0.24147, 'auc': 0.64379}
2025-07-11 06:47:48,264 - INFO - test: {'epoch': 3, 'time_epoch': 3.4606, 'loss': 1.75785945, 'lr': 0, 'params': 425270, 'time_iter': 0.05493, 'accuracy': 0.24155, 'f1': 0.19114, 'accuracy-SBM': 0.24306, 'auc': 0.64393}
2025-07-11 06:47:48,265 - INFO - > Epoch 3: took 79.9s (avg 79.6s) | Best so far: epoch 3	train_loss: 1.4537 train_accuracy-SBM: 0.3872	val_loss: 1.7593 val_accuracy-SBM: 0.2415	test_loss: 1.7579 test_accuracy-SBM: 0.2431
2025-07-11 06:48:57,910 - INFO - train: {'epoch': 4, 'time_epoch': 69.40777, 'eta': 6777.23183, 'eta_hours': 1.88256, 'loss': 1.36964451, 'lr': 0.0008, 'params': 425270, 'time_iter': 0.11105, 'accuracy': 0.44004, 'f1': 0.43842, 'accuracy-SBM': 0.44003, 'auc': 0.80201}
2025-07-11 06:49:01,500 - INFO - val: {'epoch': 4, 'time_epoch': 3.54356, 'loss': 1.52160446, 'lr': 0, 'params': 425270, 'time_iter': 0.05625, 'accuracy': 0.43242, 'f1': 0.42795, 'accuracy-SBM': 0.4312, 'auc': 0.75461}
2025-07-11 06:49:05,066 - INFO - test: {'epoch': 4, 'time_epoch': 3.53264, 'loss': 1.5288641, 'lr': 0, 'params': 425270, 'time_iter': 0.05607, 'accuracy': 0.42489, 'f1': 0.42092, 'accuracy-SBM': 0.42495, 'auc': 0.75097}
2025-07-11 06:49:05,068 - INFO - > Epoch 4: took 76.8s (avg 79.0s) | Best so far: epoch 4	train_loss: 1.3696 train_accuracy-SBM: 0.4400	val_loss: 1.5216 val_accuracy-SBM: 0.4312	test_loss: 1.5289 test_accuracy-SBM: 0.4249
2025-07-11 06:50:16,681 - INFO - train: {'epoch': 5, 'time_epoch': 71.36339, 'eta': 6706.27016, 'eta_hours': 1.86285, 'loss': 1.18860438, 'lr': 0.001, 'params': 425270, 'time_iter': 0.11418, 'accuracy': 0.55144, 'f1': 0.54947, 'accuracy-SBM': 0.55144, 'auc': 0.85388}
2025-07-11 06:50:20,505 - INFO - val: {'epoch': 5, 'time_epoch': 3.77291, 'loss': 1.85440179, 'lr': 0, 'params': 425270, 'time_iter': 0.05989, 'accuracy': 0.39384, 'f1': 0.40068, 'accuracy-SBM': 0.39266, 'auc': 0.68926}
2025-07-11 06:50:24,309 - INFO - test: {'epoch': 5, 'time_epoch': 3.76695, 'loss': 1.79865069, 'lr': 0, 'params': 425270, 'time_iter': 0.05979, 'accuracy': 0.40524, 'f1': 0.41141, 'accuracy-SBM': 0.4047, 'auc': 0.70228}
2025-07-11 06:50:24,311 - INFO - > Epoch 5: took 79.2s (avg 79.1s) | Best so far: epoch 4	train_loss: 1.3696 train_accuracy-SBM: 0.4400	val_loss: 1.5216 val_accuracy-SBM: 0.4312	test_loss: 1.5289 test_accuracy-SBM: 0.4249
2025-07-11 06:51:36,483 - INFO - train: {'epoch': 6, 'time_epoch': 71.93304, 'eta': 6642.76203, 'eta_hours': 1.84521, 'loss': 1.0634473, 'lr': 0.00099973, 'params': 425270, 'time_iter': 0.11509, 'accuracy': 0.60764, 'f1': 0.60726, 'accuracy-SBM': 0.60764, 'auc': 0.88537}
2025-07-11 06:51:39,999 - INFO - val: {'epoch': 6, 'time_epoch': 3.47012, 'loss': 1.32962979, 'lr': 0, 'params': 425270, 'time_iter': 0.05508, 'accuracy': 0.52942, 'f1': 0.52939, 'accuracy-SBM': 0.52979, 'auc': 0.82201}
2025-07-11 06:51:43,485 - INFO - test: {'epoch': 6, 'time_epoch': 3.45387, 'loss': 1.33482033, 'lr': 0, 'params': 425270, 'time_iter': 0.05482, 'accuracy': 0.5259, 'f1': 0.52555, 'accuracy-SBM': 0.52585, 'auc': 0.82019}
2025-07-11 06:51:43,487 - INFO - > Epoch 6: took 79.2s (avg 79.1s) | Best so far: epoch 6	train_loss: 1.0634 train_accuracy-SBM: 0.6076	val_loss: 1.3296 val_accuracy-SBM: 0.5298	test_loss: 1.3348 test_accuracy-SBM: 0.5259
2025-07-11 06:52:53,095 - INFO - train: {'epoch': 7, 'time_epoch': 69.36995, 'eta': 6547.67204, 'eta_hours': 1.8188, 'loss': 0.99414465, 'lr': 0.00099891, 'params': 425270, 'time_iter': 0.11099, 'accuracy': 0.63369, 'f1': 0.6334, 'accuracy-SBM': 0.63368, 'auc': 0.90031}
2025-07-11 06:52:56,681 - INFO - val: {'epoch': 7, 'time_epoch': 3.54165, 'loss': 1.35003915, 'lr': 0, 'params': 425270, 'time_iter': 0.05622, 'accuracy': 0.52321, 'f1': 0.52506, 'accuracy-SBM': 0.52402, 'auc': 0.83194}
2025-07-11 06:53:00,261 - INFO - test: {'epoch': 7, 'time_epoch': 3.54793, 'loss': 1.34804592, 'lr': 0, 'params': 425270, 'time_iter': 0.05632, 'accuracy': 0.52569, 'f1': 0.52724, 'accuracy-SBM': 0.52581, 'auc': 0.8296}
2025-07-11 06:53:00,263 - INFO - > Epoch 7: took 76.8s (avg 78.8s) | Best so far: epoch 6	train_loss: 1.0634 train_accuracy-SBM: 0.6076	val_loss: 1.3296 val_accuracy-SBM: 0.5298	test_loss: 1.3348 test_accuracy-SBM: 0.5259
2025-07-11 06:54:13,174 - INFO - train: {'epoch': 8, 'time_epoch': 72.63696, 'eta': 6491.33081, 'eta_hours': 1.80315, 'loss': 0.96009367, 'lr': 0.00099754, 'params': 425270, 'time_iter': 0.11622, 'accuracy': 0.64576, 'f1': 0.64554, 'accuracy-SBM': 0.64575, 'auc': 0.90703}
2025-07-11 06:54:17,222 - INFO - val: {'epoch': 8, 'time_epoch': 3.99311, 'loss': 1.0108027, 'lr': 0, 'params': 425270, 'time_iter': 0.06338, 'accuracy': 0.62137, 'f1': 0.62052, 'accuracy-SBM': 0.62075, 'auc': 0.90217}
2025-07-11 06:54:21,143 - INFO - test: {'epoch': 8, 'time_epoch': 3.87541, 'loss': 0.99214241, 'lr': 0, 'params': 425270, 'time_iter': 0.06151, 'accuracy': 0.63001, 'f1': 0.62939, 'accuracy-SBM': 0.62976, 'auc': 0.9057}
2025-07-11 06:54:21,144 - INFO - > Epoch 8: took 80.9s (avg 79.0s) | Best so far: epoch 8	train_loss: 0.9601 train_accuracy-SBM: 0.6458	val_loss: 1.0108 val_accuracy-SBM: 0.6208	test_loss: 0.9921 test_accuracy-SBM: 0.6298
2025-07-11 06:55:30,875 - INFO - train: {'epoch': 9, 'time_epoch': 69.49301, 'eta': 6403.43485, 'eta_hours': 1.77873, 'loss': 0.93660854, 'lr': 0.00099563, 'params': 425270, 'time_iter': 0.11119, 'accuracy': 0.65451, 'f1': 0.65433, 'accuracy-SBM': 0.65451, 'auc': 0.91159}
2025-07-11 06:55:34,562 - INFO - val: {'epoch': 9, 'time_epoch': 3.64213, 'loss': 1.23445036, 'lr': 0, 'params': 425270, 'time_iter': 0.05781, 'accuracy': 0.55271, 'f1': 0.55478, 'accuracy-SBM': 0.55342, 'auc': 0.86118}
2025-07-11 06:55:38,085 - INFO - test: {'epoch': 9, 'time_epoch': 3.47671, 'loss': 1.18345707, 'lr': 0, 'params': 425270, 'time_iter': 0.05519, 'accuracy': 0.56338, 'f1': 0.56543, 'accuracy-SBM': 0.56328, 'auc': 0.87203}
2025-07-11 06:55:38,087 - INFO - > Epoch 9: took 76.9s (avg 78.8s) | Best so far: epoch 8	train_loss: 0.9601 train_accuracy-SBM: 0.6458	val_loss: 1.0108 val_accuracy-SBM: 0.6208	test_loss: 0.9921 test_accuracy-SBM: 0.6298
2025-07-11 06:56:48,964 - INFO - train: {'epoch': 10, 'time_epoch': 70.52549, 'eta': 6327.2386, 'eta_hours': 1.75757, 'loss': 0.91682213, 'lr': 0.00099318, 'params': 425270, 'time_iter': 0.11284, 'accuracy': 0.66209, 'f1': 0.66195, 'accuracy-SBM': 0.66209, 'auc': 0.91531}
2025-07-11 06:56:52,692 - INFO - val: {'epoch': 10, 'time_epoch': 3.68023, 'loss': 0.93719906, 'lr': 0, 'params': 425270, 'time_iter': 0.05842, 'accuracy': 0.66147, 'f1': 0.66119, 'accuracy-SBM': 0.66128, 'auc': 0.91232}
2025-07-11 06:56:56,511 - INFO - test: {'epoch': 10, 'time_epoch': 3.7746, 'loss': 0.91571354, 'lr': 0, 'params': 425270, 'time_iter': 0.05991, 'accuracy': 0.6648, 'f1': 0.66476, 'accuracy-SBM': 0.66499, 'auc': 0.91669}
2025-07-11 06:56:56,513 - INFO - > Epoch 10: took 78.4s (avg 78.8s) | Best so far: epoch 10	train_loss: 0.9168 train_accuracy-SBM: 0.6621	val_loss: 0.9372 val_accuracy-SBM: 0.6613	test_loss: 0.9157 test_accuracy-SBM: 0.6650
2025-07-11 06:58:08,012 - INFO - train: {'epoch': 11, 'time_epoch': 71.25214, 'eta': 6257.31624, 'eta_hours': 1.73814, 'loss': 0.90123999, 'lr': 0.00099019, 'params': 425270, 'time_iter': 0.114, 'accuracy': 0.66838, 'f1': 0.6682, 'accuracy-SBM': 0.66838, 'auc': 0.91812}
2025-07-11 06:58:11,580 - INFO - val: {'epoch': 11, 'time_epoch': 3.51105, 'loss': 0.93049558, 'lr': 0, 'params': 425270, 'time_iter': 0.05573, 'accuracy': 0.66463, 'f1': 0.66503, 'accuracy-SBM': 0.66442, 'auc': 0.91355}
2025-07-11 06:58:15,043 - INFO - test: {'epoch': 11, 'time_epoch': 3.4304, 'loss': 0.90570931, 'lr': 0, 'params': 425270, 'time_iter': 0.05445, 'accuracy': 0.67161, 'f1': 0.67218, 'accuracy-SBM': 0.67178, 'auc': 0.91873}
2025-07-11 06:58:15,045 - INFO - > Epoch 11: took 78.5s (avg 78.8s) | Best so far: epoch 11	train_loss: 0.9012 train_accuracy-SBM: 0.6684	val_loss: 0.9305 val_accuracy-SBM: 0.6644	test_loss: 0.9057 test_accuracy-SBM: 0.6718
2025-07-11 06:59:22,773 - INFO - train: {'epoch': 12, 'time_epoch': 67.49108, 'eta': 6162.01912, 'eta_hours': 1.71167, 'loss': 0.88655364, 'lr': 0.00098666, 'params': 425270, 'time_iter': 0.10799, 'accuracy': 0.67411, 'f1': 0.67398, 'accuracy-SBM': 0.67411, 'auc': 0.92085}
2025-07-11 06:59:26,260 - INFO - val: {'epoch': 12, 'time_epoch': 3.44201, 'loss': 1.36143665, 'lr': 0, 'params': 425270, 'time_iter': 0.05464, 'accuracy': 0.58318, 'f1': 0.5866, 'accuracy-SBM': 0.58301, 'auc': 0.8329}
2025-07-11 06:59:29,696 - INFO - test: {'epoch': 12, 'time_epoch': 3.40536, 'loss': 1.40099858, 'lr': 0, 'params': 425270, 'time_iter': 0.05405, 'accuracy': 0.5761, 'f1': 0.58003, 'accuracy-SBM': 0.57632, 'auc': 0.82601}
2025-07-11 06:59:29,699 - INFO - > Epoch 12: took 74.7s (avg 78.4s) | Best so far: epoch 11	train_loss: 0.9012 train_accuracy-SBM: 0.6684	val_loss: 0.9305 val_accuracy-SBM: 0.6644	test_loss: 0.9057 test_accuracy-SBM: 0.6718
2025-07-11 07:00:38,072 - INFO - train: {'epoch': 13, 'time_epoch': 68.1268, 'eta': 6074.59939, 'eta_hours': 1.68739, 'loss': 0.87790236, 'lr': 0.0009826, 'params': 425270, 'time_iter': 0.109, 'accuracy': 0.67667, 'f1': 0.67655, 'accuracy-SBM': 0.67667, 'auc': 0.92238}
2025-07-11 07:00:41,734 - INFO - val: {'epoch': 13, 'time_epoch': 3.61521, 'loss': 0.93829514, 'lr': 0, 'params': 425270, 'time_iter': 0.05738, 'accuracy': 0.66167, 'f1': 0.66116, 'accuracy-SBM': 0.66167, 'auc': 0.91252}
2025-07-11 07:00:45,352 - INFO - test: {'epoch': 13, 'time_epoch': 3.58519, 'loss': 0.90089798, 'lr': 0, 'params': 425270, 'time_iter': 0.05691, 'accuracy': 0.66784, 'f1': 0.66715, 'accuracy-SBM': 0.66828, 'auc': 0.92033}
2025-07-11 07:00:45,355 - INFO - > Epoch 13: took 75.7s (avg 78.2s) | Best so far: epoch 11	train_loss: 0.9012 train_accuracy-SBM: 0.6684	val_loss: 0.9305 val_accuracy-SBM: 0.6644	test_loss: 0.9057 test_accuracy-SBM: 0.6718
2025-07-11 07:01:54,292 - INFO - train: {'epoch': 14, 'time_epoch': 68.69905, 'eta': 5992.9948, 'eta_hours': 1.66472, 'loss': 0.86853993, 'lr': 0.00097802, 'params': 425270, 'time_iter': 0.10992, 'accuracy': 0.68034, 'f1': 0.68022, 'accuracy-SBM': 0.68034, 'auc': 0.92407}
2025-07-11 07:01:57,780 - INFO - val: {'epoch': 14, 'time_epoch': 3.44203, 'loss': 0.9179429, 'lr': 0, 'params': 425270, 'time_iter': 0.05464, 'accuracy': 0.67031, 'f1': 0.67015, 'accuracy-SBM': 0.67049, 'auc': 0.91589}
2025-07-11 07:02:01,247 - INFO - test: {'epoch': 14, 'time_epoch': 3.43401, 'loss': 0.89155958, 'lr': 0, 'params': 425270, 'time_iter': 0.05451, 'accuracy': 0.67894, 'f1': 0.67867, 'accuracy-SBM': 0.6792, 'auc': 0.92126}
2025-07-11 07:02:01,249 - INFO - > Epoch 14: took 75.9s (avg 78.1s) | Best so far: epoch 14	train_loss: 0.8685 train_accuracy-SBM: 0.6803	val_loss: 0.9179 val_accuracy-SBM: 0.6705	test_loss: 0.8916 test_accuracy-SBM: 0.6792
2025-07-11 07:03:08,985 - INFO - train: {'epoch': 15, 'time_epoch': 67.50331, 'eta': 5906.72578, 'eta_hours': 1.64076, 'loss': 0.85534458, 'lr': 0.00097291, 'params': 425270, 'time_iter': 0.10801, 'accuracy': 0.68565, 'f1': 0.68557, 'accuracy-SBM': 0.68565, 'auc': 0.92641}
2025-07-11 07:03:12,468 - INFO - val: {'epoch': 15, 'time_epoch': 3.43938, 'loss': 1.19186685, 'lr': 0, 'params': 425270, 'time_iter': 0.05459, 'accuracy': 0.59875, 'f1': 0.60302, 'accuracy-SBM': 0.59889, 'auc': 0.86138}
2025-07-11 07:03:15,936 - INFO - test: {'epoch': 15, 'time_epoch': 3.43439, 'loss': 1.14425586, 'lr': 0, 'params': 425270, 'time_iter': 0.05451, 'accuracy': 0.6103, 'f1': 0.6145, 'accuracy-SBM': 0.61033, 'auc': 0.87009}
2025-07-11 07:03:15,938 - INFO - > Epoch 15: took 74.7s (avg 77.9s) | Best so far: epoch 14	train_loss: 0.8685 train_accuracy-SBM: 0.6803	val_loss: 0.9179 val_accuracy-SBM: 0.6705	test_loss: 0.8916 test_accuracy-SBM: 0.6792
2025-07-11 07:04:25,218 - INFO - train: {'epoch': 16, 'time_epoch': 69.0356, 'eta': 5830.14569, 'eta_hours': 1.61948, 'loss': 0.84605453, 'lr': 0.00096728, 'params': 425270, 'time_iter': 0.11046, 'accuracy': 0.68933, 'f1': 0.68927, 'accuracy-SBM': 0.68933, 'auc': 0.92802}
2025-07-11 07:04:28,982 - INFO - val: {'epoch': 16, 'time_epoch': 3.71495, 'loss': 1.00517464, 'lr': 0, 'params': 425270, 'time_iter': 0.05897, 'accuracy': 0.64389, 'f1': 0.64296, 'accuracy-SBM': 0.64341, 'auc': 0.89893}
2025-07-11 07:04:32,738 - INFO - test: {'epoch': 16, 'time_epoch': 3.72101, 'loss': 0.96244372, 'lr': 0, 'params': 425270, 'time_iter': 0.05906, 'accuracy': 0.65544, 'f1': 0.65491, 'accuracy-SBM': 0.65561, 'auc': 0.90773}
2025-07-11 07:04:32,740 - INFO - > Epoch 16: took 76.8s (avg 77.8s) | Best so far: epoch 14	train_loss: 0.8685 train_accuracy-SBM: 0.6803	val_loss: 0.9179 val_accuracy-SBM: 0.6705	test_loss: 0.8916 test_accuracy-SBM: 0.6792
2025-07-11 07:05:43,403 - INFO - train: {'epoch': 17, 'time_epoch': 70.42793, 'eta': 5760.7467, 'eta_hours': 1.60021, 'loss': 0.84010693, 'lr': 0.00096114, 'params': 425270, 'time_iter': 0.11268, 'accuracy': 0.69219, 'f1': 0.69213, 'accuracy-SBM': 0.69219, 'auc': 0.92902}
2025-07-11 07:05:46,915 - INFO - val: {'epoch': 17, 'time_epoch': 3.46548, 'loss': 0.81882559, 'lr': 0, 'params': 425270, 'time_iter': 0.05501, 'accuracy': 0.70194, 'f1': 0.70165, 'accuracy-SBM': 0.70171, 'auc': 0.93362}
2025-07-11 07:05:50,412 - INFO - test: {'epoch': 17, 'time_epoch': 3.46299, 'loss': 0.81528478, 'lr': 0, 'params': 425270, 'time_iter': 0.05497, 'accuracy': 0.70178, 'f1': 0.70173, 'accuracy-SBM': 0.70192, 'auc': 0.93443}
2025-07-11 07:05:50,415 - INFO - > Epoch 17: took 77.7s (avg 77.8s) | Best so far: epoch 17	train_loss: 0.8401 train_accuracy-SBM: 0.6922	val_loss: 0.8188 val_accuracy-SBM: 0.7017	test_loss: 0.8153 test_accuracy-SBM: 0.7019
2025-07-11 07:06:59,241 - INFO - train: {'epoch': 18, 'time_epoch': 68.58793, 'eta': 5683.39518, 'eta_hours': 1.57872, 'loss': 0.83110783, 'lr': 0.0009545, 'params': 425270, 'time_iter': 0.10974, 'accuracy': 0.69473, 'f1': 0.69469, 'accuracy-SBM': 0.69473, 'auc': 0.9306}
2025-07-11 07:07:02,804 - INFO - val: {'epoch': 18, 'time_epoch': 3.51704, 'loss': 0.93509112, 'lr': 0, 'params': 425270, 'time_iter': 0.05583, 'accuracy': 0.6555, 'f1': 0.65806, 'accuracy-SBM': 0.65455, 'auc': 0.91852}
2025-07-11 07:07:06,340 - INFO - test: {'epoch': 18, 'time_epoch': 3.50225, 'loss': 0.91692728, 'lr': 0, 'params': 425270, 'time_iter': 0.05559, 'accuracy': 0.66023, 'f1': 0.66284, 'accuracy-SBM': 0.66018, 'auc': 0.92147}
2025-07-11 07:07:06,343 - INFO - > Epoch 18: took 75.9s (avg 77.7s) | Best so far: epoch 17	train_loss: 0.8401 train_accuracy-SBM: 0.6922	val_loss: 0.8188 val_accuracy-SBM: 0.7017	test_loss: 0.8153 test_accuracy-SBM: 0.7019
2025-07-11 07:08:18,136 - INFO - train: {'epoch': 19, 'time_epoch': 71.53935, 'eta': 5618.72573, 'eta_hours': 1.56076, 'loss': 0.82445353, 'lr': 0.00094736, 'params': 425270, 'time_iter': 0.11446, 'accuracy': 0.69763, 'f1': 0.69759, 'accuracy-SBM': 0.69763, 'auc': 0.93171}
2025-07-11 07:08:21,906 - INFO - val: {'epoch': 19, 'time_epoch': 3.72148, 'loss': 0.92791559, 'lr': 0, 'params': 425270, 'time_iter': 0.05907, 'accuracy': 0.668, 'f1': 0.6671, 'accuracy-SBM': 0.66737, 'auc': 0.91515}
2025-07-11 07:08:25,610 - INFO - test: {'epoch': 19, 'time_epoch': 3.6681, 'loss': 0.92605802, 'lr': 0, 'params': 425270, 'time_iter': 0.05822, 'accuracy': 0.66679, 'f1': 0.66613, 'accuracy-SBM': 0.66686, 'auc': 0.91539}
2025-07-11 07:08:25,612 - INFO - > Epoch 19: took 79.3s (avg 77.8s) | Best so far: epoch 17	train_loss: 0.8401 train_accuracy-SBM: 0.6922	val_loss: 0.8188 val_accuracy-SBM: 0.7017	test_loss: 0.8153 test_accuracy-SBM: 0.7019
2025-07-11 07:09:34,168 - INFO - train: {'epoch': 20, 'time_epoch': 68.19331, 'eta': 5540.81451, 'eta_hours': 1.53912, 'loss': 0.81584466, 'lr': 0.00093974, 'params': 425270, 'time_iter': 0.10911, 'accuracy': 0.70117, 'f1': 0.70113, 'accuracy-SBM': 0.70117, 'auc': 0.93315}
2025-07-11 07:09:37,607 - INFO - val: {'epoch': 20, 'time_epoch': 3.38681, 'loss': 0.92029981, 'lr': 0, 'params': 425270, 'time_iter': 0.05376, 'accuracy': 0.66379, 'f1': 0.6608, 'accuracy-SBM': 0.66332, 'auc': 0.91568}
2025-07-11 07:09:41,018 - INFO - test: {'epoch': 20, 'time_epoch': 3.37703, 'loss': 0.90639239, 'lr': 0, 'params': 425270, 'time_iter': 0.0536, 'accuracy': 0.66575, 'f1': 0.66299, 'accuracy-SBM': 0.66529, 'auc': 0.91835}
2025-07-11 07:09:41,020 - INFO - > Epoch 20: took 75.4s (avg 77.7s) | Best so far: epoch 17	train_loss: 0.8401 train_accuracy-SBM: 0.6922	val_loss: 0.8188 val_accuracy-SBM: 0.7017	test_loss: 0.8153 test_accuracy-SBM: 0.7019
2025-07-11 07:10:47,891 - INFO - train: {'epoch': 21, 'time_epoch': 66.63341, 'eta': 5458.25619, 'eta_hours': 1.51618, 'loss': 0.81288168, 'lr': 0.00093163, 'params': 425270, 'time_iter': 0.10661, 'accuracy': 0.70149, 'f1': 0.70145, 'accuracy-SBM': 0.70149, 'auc': 0.93363}
2025-07-11 07:10:51,310 - INFO - val: {'epoch': 21, 'time_epoch': 3.37385, 'loss': 0.80192874, 'lr': 0, 'params': 425270, 'time_iter': 0.05355, 'accuracy': 0.7053, 'f1': 0.70554, 'accuracy-SBM': 0.70532, 'auc': 0.93727}
2025-07-11 07:10:54,743 - INFO - test: {'epoch': 21, 'time_epoch': 3.39953, 'loss': 0.7986729, 'lr': 0, 'params': 425270, 'time_iter': 0.05396, 'accuracy': 0.70592, 'f1': 0.70622, 'accuracy-SBM': 0.7062, 'auc': 0.938}
2025-07-11 07:10:54,746 - INFO - > Epoch 21: took 73.7s (avg 77.5s) | Best so far: epoch 21	train_loss: 0.8129 train_accuracy-SBM: 0.7015	val_loss: 0.8019 val_accuracy-SBM: 0.7053	test_loss: 0.7987 test_accuracy-SBM: 0.7062
2025-07-11 07:12:04,892 - INFO - train: {'epoch': 22, 'time_epoch': 69.78965, 'eta': 5387.64918, 'eta_hours': 1.49657, 'loss': 0.80595304, 'lr': 0.00092305, 'params': 425270, 'time_iter': 0.11166, 'accuracy': 0.70456, 'f1': 0.70453, 'accuracy-SBM': 0.70456, 'auc': 0.9348}
2025-07-11 07:12:08,580 - INFO - val: {'epoch': 22, 'time_epoch': 3.63451, 'loss': 1.37999111, 'lr': 0, 'params': 425270, 'time_iter': 0.05769, 'accuracy': 0.57697, 'f1': 0.58207, 'accuracy-SBM': 0.57681, 'auc': 0.83445}
2025-07-11 07:12:12,144 - INFO - test: {'epoch': 22, 'time_epoch': 3.52893, 'loss': 1.36262491, 'lr': 0, 'params': 425270, 'time_iter': 0.05601, 'accuracy': 0.58272, 'f1': 0.58856, 'accuracy-SBM': 0.58221, 'auc': 0.83695}
2025-07-11 07:12:12,182 - INFO - > Epoch 22: took 77.4s (avg 77.5s) | Best so far: epoch 21	train_loss: 0.8129 train_accuracy-SBM: 0.7015	val_loss: 0.8019 val_accuracy-SBM: 0.7053	test_loss: 0.7987 test_accuracy-SBM: 0.7062
2025-07-11 07:13:19,117 - INFO - train: {'epoch': 23, 'time_epoch': 66.69624, 'eta': 5307.31448, 'eta_hours': 1.47425, 'loss': 0.79820542, 'lr': 0.000914, 'params': 425270, 'time_iter': 0.10671, 'accuracy': 0.70712, 'f1': 0.7071, 'accuracy-SBM': 0.70712, 'auc': 0.93604}
2025-07-11 07:13:22,508 - INFO - val: {'epoch': 23, 'time_epoch': 3.33776, 'loss': 0.90965129, 'lr': 0, 'params': 425270, 'time_iter': 0.05298, 'accuracy': 0.67255, 'f1': 0.67258, 'accuracy-SBM': 0.67226, 'auc': 0.9178}
2025-07-11 07:13:25,900 - INFO - test: {'epoch': 23, 'time_epoch': 3.35942, 'loss': 0.89909208, 'lr': 0, 'params': 425270, 'time_iter': 0.05332, 'accuracy': 0.67896, 'f1': 0.67909, 'accuracy-SBM': 0.67899, 'auc': 0.91985}
2025-07-11 07:13:25,901 - INFO - > Epoch 23: took 73.7s (avg 77.3s) | Best so far: epoch 21	train_loss: 0.8129 train_accuracy-SBM: 0.7015	val_loss: 0.8019 val_accuracy-SBM: 0.7053	test_loss: 0.7987 test_accuracy-SBM: 0.7062
2025-07-11 07:14:31,584 - INFO - train: {'epoch': 24, 'time_epoch': 65.45209, 'eta': 5224.33841, 'eta_hours': 1.45121, 'loss': 0.79452991, 'lr': 0.00090451, 'params': 425270, 'time_iter': 0.10472, 'accuracy': 0.70878, 'f1': 0.70876, 'accuracy-SBM': 0.70878, 'auc': 0.93663}
2025-07-11 07:14:34,972 - INFO - val: {'epoch': 24, 'time_epoch': 3.34332, 'loss': 0.88363503, 'lr': 0, 'params': 425270, 'time_iter': 0.05307, 'accuracy': 0.68124, 'f1': 0.68071, 'accuracy-SBM': 0.68043, 'auc': 0.92558}
2025-07-11 07:14:38,339 - INFO - test: {'epoch': 24, 'time_epoch': 3.335, 'loss': 0.87405295, 'lr': 0, 'params': 425270, 'time_iter': 0.05294, 'accuracy': 0.68344, 'f1': 0.68296, 'accuracy-SBM': 0.68334, 'auc': 0.92689}
2025-07-11 07:14:38,340 - INFO - > Epoch 24: took 72.4s (avg 77.1s) | Best so far: epoch 21	train_loss: 0.8129 train_accuracy-SBM: 0.7015	val_loss: 0.8019 val_accuracy-SBM: 0.7053	test_loss: 0.7987 test_accuracy-SBM: 0.7062
2025-07-11 07:15:46,411 - INFO - train: {'epoch': 25, 'time_epoch': 67.81342, 'eta': 5149.43104, 'eta_hours': 1.4304, 'loss': 0.79235163, 'lr': 0.00089457, 'params': 425270, 'time_iter': 0.1085, 'accuracy': 0.70935, 'f1': 0.70934, 'accuracy-SBM': 0.70935, 'auc': 0.93698}
2025-07-11 07:15:49,904 - INFO - val: {'epoch': 25, 'time_epoch': 3.44604, 'loss': 0.84718142, 'lr': 0, 'params': 425270, 'time_iter': 0.0547, 'accuracy': 0.69491, 'f1': 0.69437, 'accuracy-SBM': 0.69444, 'auc': 0.93027}
2025-07-11 07:15:53,400 - INFO - test: {'epoch': 25, 'time_epoch': 3.46074, 'loss': 0.84915083, 'lr': 0, 'params': 425270, 'time_iter': 0.05493, 'accuracy': 0.69534, 'f1': 0.69512, 'accuracy-SBM': 0.69536, 'auc': 0.93011}
2025-07-11 07:15:53,402 - INFO - > Epoch 25: took 75.1s (avg 77.1s) | Best so far: epoch 21	train_loss: 0.8129 train_accuracy-SBM: 0.7015	val_loss: 0.8019 val_accuracy-SBM: 0.7053	test_loss: 0.7987 test_accuracy-SBM: 0.7062
2025-07-11 07:17:00,230 - INFO - train: {'epoch': 26, 'time_epoch': 66.59305, 'eta': 5071.74965, 'eta_hours': 1.40882, 'loss': 0.78776698, 'lr': 0.0008842, 'params': 425270, 'time_iter': 0.10655, 'accuracy': 0.71127, 'f1': 0.71126, 'accuracy-SBM': 0.71127, 'auc': 0.93773}
2025-07-11 07:17:03,672 - INFO - val: {'epoch': 26, 'time_epoch': 3.38832, 'loss': 0.81072819, 'lr': 0, 'params': 425270, 'time_iter': 0.05378, 'accuracy': 0.70553, 'f1': 0.70531, 'accuracy-SBM': 0.70566, 'auc': 0.93486}
2025-07-11 07:17:07,117 - INFO - test: {'epoch': 26, 'time_epoch': 3.41221, 'loss': 0.79890305, 'lr': 0, 'params': 425270, 'time_iter': 0.05416, 'accuracy': 0.70678, 'f1': 0.70642, 'accuracy-SBM': 0.70721, 'auc': 0.937}
2025-07-11 07:17:07,119 - INFO - > Epoch 26: took 73.7s (avg 76.9s) | Best so far: epoch 26	train_loss: 0.7878 train_accuracy-SBM: 0.7113	val_loss: 0.8107 val_accuracy-SBM: 0.7057	test_loss: 0.7989 test_accuracy-SBM: 0.7072
2025-07-11 07:18:14,246 - INFO - train: {'epoch': 27, 'time_epoch': 66.89041, 'eta': 4995.62491, 'eta_hours': 1.38767, 'loss': 0.78106112, 'lr': 0.00087341, 'params': 425270, 'time_iter': 0.10702, 'accuracy': 0.71417, 'f1': 0.71416, 'accuracy-SBM': 0.71417, 'auc': 0.93877}
2025-07-11 07:18:17,666 - INFO - val: {'epoch': 27, 'time_epoch': 3.36409, 'loss': 1.23981805, 'lr': 0, 'params': 425270, 'time_iter': 0.0534, 'accuracy': 0.62264, 'f1': 0.62251, 'accuracy-SBM': 0.62253, 'auc': 0.8659}
2025-07-11 07:18:21,122 - INFO - test: {'epoch': 27, 'time_epoch': 3.42409, 'loss': 1.19776396, 'lr': 0, 'params': 425270, 'time_iter': 0.05435, 'accuracy': 0.62872, 'f1': 0.62885, 'accuracy-SBM': 0.62859, 'auc': 0.8718}
2025-07-11 07:18:21,124 - INFO - > Epoch 27: took 74.0s (avg 76.8s) | Best so far: epoch 26	train_loss: 0.7878 train_accuracy-SBM: 0.7113	val_loss: 0.8107 val_accuracy-SBM: 0.7057	test_loss: 0.7989 test_accuracy-SBM: 0.7072
2025-07-11 07:19:30,333 - INFO - train: {'epoch': 28, 'time_epoch': 68.95193, 'eta': 4925.18418, 'eta_hours': 1.36811, 'loss': 0.77601807, 'lr': 0.00086221, 'params': 425270, 'time_iter': 0.11032, 'accuracy': 0.71573, 'f1': 0.71572, 'accuracy-SBM': 0.71573, 'auc': 0.93957}
2025-07-11 07:19:33,913 - INFO - val: {'epoch': 28, 'time_epoch': 3.53318, 'loss': 0.94674642, 'lr': 0, 'params': 425270, 'time_iter': 0.05608, 'accuracy': 0.68211, 'f1': 0.68149, 'accuracy-SBM': 0.68214, 'auc': 0.91219}
2025-07-11 07:19:37,386 - INFO - test: {'epoch': 28, 'time_epoch': 3.43973, 'loss': 0.97817988, 'lr': 0, 'params': 425270, 'time_iter': 0.0546, 'accuracy': 0.67715, 'f1': 0.67687, 'accuracy-SBM': 0.67702, 'auc': 0.90728}
2025-07-11 07:19:37,388 - INFO - > Epoch 28: took 76.3s (avg 76.8s) | Best so far: epoch 26	train_loss: 0.7878 train_accuracy-SBM: 0.7113	val_loss: 0.8107 val_accuracy-SBM: 0.7057	test_loss: 0.7989 test_accuracy-SBM: 0.7072
2025-07-11 07:20:44,186 - INFO - train: {'epoch': 29, 'time_epoch': 66.55732, 'eta': 4849.25528, 'eta_hours': 1.34702, 'loss': 0.77571896, 'lr': 0.00085062, 'params': 425270, 'time_iter': 0.10649, 'accuracy': 0.71555, 'f1': 0.71554, 'accuracy-SBM': 0.71555, 'auc': 0.9396}
2025-07-11 07:20:47,588 - INFO - val: {'epoch': 29, 'time_epoch': 3.35785, 'loss': 0.79848784, 'lr': 0, 'params': 425270, 'time_iter': 0.0533, 'accuracy': 0.71158, 'f1': 0.71138, 'accuracy-SBM': 0.71133, 'auc': 0.93643}
2025-07-11 07:20:50,975 - INFO - test: {'epoch': 29, 'time_epoch': 3.35295, 'loss': 0.79179308, 'lr': 0, 'params': 425270, 'time_iter': 0.05322, 'accuracy': 0.71352, 'f1': 0.71342, 'accuracy-SBM': 0.71367, 'auc': 0.93775}
2025-07-11 07:20:50,977 - INFO - > Epoch 29: took 73.6s (avg 76.7s) | Best so far: epoch 29	train_loss: 0.7757 train_accuracy-SBM: 0.7156	val_loss: 0.7985 val_accuracy-SBM: 0.7113	test_loss: 0.7918 test_accuracy-SBM: 0.7137
2025-07-11 07:21:58,274 - INFO - train: {'epoch': 30, 'time_epoch': 67.03403, 'eta': 4774.99208, 'eta_hours': 1.32639, 'loss': 0.76880074, 'lr': 0.00083864, 'params': 425270, 'time_iter': 0.10725, 'accuracy': 0.71826, 'f1': 0.71825, 'accuracy-SBM': 0.71826, 'auc': 0.94072}
2025-07-11 07:22:01,408 - INFO - val: {'epoch': 30, 'time_epoch': 3.08843, 'loss': 0.81447397, 'lr': 0, 'params': 425270, 'time_iter': 0.04902, 'accuracy': 0.70473, 'f1': 0.70361, 'accuracy-SBM': 0.70406, 'auc': 0.93501}
2025-07-11 07:22:04,889 - INFO - test: {'epoch': 30, 'time_epoch': 3.44326, 'loss': 0.81289617, 'lr': 0, 'params': 425270, 'time_iter': 0.05465, 'accuracy': 0.70683, 'f1': 0.70615, 'accuracy-SBM': 0.70664, 'auc': 0.93566}
2025-07-11 07:22:04,891 - INFO - > Epoch 30: took 73.9s (avg 76.6s) | Best so far: epoch 29	train_loss: 0.7757 train_accuracy-SBM: 0.7156	val_loss: 0.7985 val_accuracy-SBM: 0.7113	test_loss: 0.7918 test_accuracy-SBM: 0.7137
2025-07-11 07:23:15,462 - INFO - train: {'epoch': 31, 'time_epoch': 70.33722, 'eta': 4708.19998, 'eta_hours': 1.30783, 'loss': 0.76395246, 'lr': 0.00082629, 'params': 425270, 'time_iter': 0.11254, 'accuracy': 0.72014, 'f1': 0.72013, 'accuracy-SBM': 0.72014, 'auc': 0.94145}
2025-07-11 07:23:18,941 - INFO - val: {'epoch': 31, 'time_epoch': 3.43521, 'loss': 0.8141708, 'lr': 0, 'params': 425270, 'time_iter': 0.05453, 'accuracy': 0.70813, 'f1': 0.70838, 'accuracy-SBM': 0.70844, 'auc': 0.93532}
2025-07-11 07:23:22,303 - INFO - test: {'epoch': 31, 'time_epoch': 3.33047, 'loss': 0.79614527, 'lr': 0, 'params': 425270, 'time_iter': 0.05286, 'accuracy': 0.7116, 'f1': 0.71178, 'accuracy-SBM': 0.71162, 'auc': 0.93813}
2025-07-11 07:23:22,305 - INFO - > Epoch 31: took 77.4s (avg 76.6s) | Best so far: epoch 29	train_loss: 0.7757 train_accuracy-SBM: 0.7156	val_loss: 0.7985 val_accuracy-SBM: 0.7113	test_loss: 0.7918 test_accuracy-SBM: 0.7137
2025-07-11 07:24:28,816 - INFO - train: {'epoch': 32, 'time_epoch': 66.27566, 'eta': 4632.94681, 'eta_hours': 1.28693, 'loss': 0.76138311, 'lr': 0.00081359, 'params': 425270, 'time_iter': 0.10604, 'accuracy': 0.72146, 'f1': 0.72145, 'accuracy-SBM': 0.72146, 'auc': 0.94184}
2025-07-11 07:24:32,213 - INFO - val: {'epoch': 32, 'time_epoch': 3.35045, 'loss': 0.74714406, 'lr': 0, 'params': 425270, 'time_iter': 0.05318, 'accuracy': 0.72559, 'f1': 0.72547, 'accuracy-SBM': 0.72534, 'auc': 0.94559}
2025-07-11 07:24:35,587 - INFO - test: {'epoch': 32, 'time_epoch': 3.34151, 'loss': 0.75101806, 'lr': 0, 'params': 425270, 'time_iter': 0.05304, 'accuracy': 0.72455, 'f1': 0.72486, 'accuracy-SBM': 0.72454, 'auc': 0.94487}
2025-07-11 07:24:35,590 - INFO - > Epoch 32: took 73.3s (avg 76.5s) | Best so far: epoch 32	train_loss: 0.7614 train_accuracy-SBM: 0.7215	val_loss: 0.7471 val_accuracy-SBM: 0.7253	test_loss: 0.7510 test_accuracy-SBM: 0.7245
2025-07-11 07:25:42,383 - INFO - train: {'epoch': 33, 'time_epoch': 66.55278, 'eta': 4558.75968, 'eta_hours': 1.26632, 'loss': 0.75881092, 'lr': 0.00080054, 'params': 425270, 'time_iter': 0.10648, 'accuracy': 0.72255, 'f1': 0.72254, 'accuracy-SBM': 0.72255, 'auc': 0.94224}
2025-07-11 07:25:45,973 - INFO - val: {'epoch': 33, 'time_epoch': 3.54322, 'loss': 0.7385501, 'lr': 0, 'params': 425270, 'time_iter': 0.05624, 'accuracy': 0.73075, 'f1': 0.7305, 'accuracy-SBM': 0.73042, 'auc': 0.94589}
2025-07-11 07:25:49,579 - INFO - test: {'epoch': 33, 'time_epoch': 3.57095, 'loss': 0.7324267, 'lr': 0, 'params': 425270, 'time_iter': 0.05668, 'accuracy': 0.73173, 'f1': 0.73167, 'accuracy-SBM': 0.73175, 'auc': 0.94699}
2025-07-11 07:25:49,581 - INFO - > Epoch 33: took 74.0s (avg 76.5s) | Best so far: epoch 33	train_loss: 0.7588 train_accuracy-SBM: 0.7226	val_loss: 0.7386 val_accuracy-SBM: 0.7304	test_loss: 0.7324 test_accuracy-SBM: 0.7318
2025-07-11 07:26:59,362 - INFO - train: {'epoch': 34, 'time_epoch': 69.53998, 'eta': 4490.55646, 'eta_hours': 1.24738, 'loss': 0.75303478, 'lr': 0.00078716, 'params': 425270, 'time_iter': 0.11126, 'accuracy': 0.72462, 'f1': 0.72461, 'accuracy-SBM': 0.72462, 'auc': 0.94311}
2025-07-11 07:27:02,847 - INFO - val: {'epoch': 34, 'time_epoch': 3.44101, 'loss': 0.76158807, 'lr': 0, 'params': 425270, 'time_iter': 0.05462, 'accuracy': 0.72197, 'f1': 0.72219, 'accuracy-SBM': 0.72187, 'auc': 0.94285}
2025-07-11 07:27:06,239 - INFO - test: {'epoch': 34, 'time_epoch': 3.35845, 'loss': 0.7477305, 'lr': 0, 'params': 425270, 'time_iter': 0.05331, 'accuracy': 0.72404, 'f1': 0.72444, 'accuracy-SBM': 0.724, 'auc': 0.94501}
2025-07-11 07:27:06,241 - INFO - > Epoch 34: took 76.7s (avg 76.5s) | Best so far: epoch 33	train_loss: 0.7588 train_accuracy-SBM: 0.7226	val_loss: 0.7386 val_accuracy-SBM: 0.7304	test_loss: 0.7324 test_accuracy-SBM: 0.7318
2025-07-11 07:28:12,712 - INFO - train: {'epoch': 35, 'time_epoch': 66.2362, 'eta': 4416.40558, 'eta_hours': 1.22678, 'loss': 0.75134304, 'lr': 0.00077347, 'params': 425270, 'time_iter': 0.10598, 'accuracy': 0.72447, 'f1': 0.72447, 'accuracy-SBM': 0.72447, 'auc': 0.94339}
2025-07-11 07:28:16,131 - INFO - val: {'epoch': 35, 'time_epoch': 3.37459, 'loss': 0.88798575, 'lr': 0, 'params': 425270, 'time_iter': 0.05356, 'accuracy': 0.6959, 'f1': 0.69585, 'accuracy-SBM': 0.69615, 'auc': 0.92286}
2025-07-11 07:28:19,579 - INFO - test: {'epoch': 35, 'time_epoch': 3.40672, 'loss': 0.84215166, 'lr': 0, 'params': 425270, 'time_iter': 0.05407, 'accuracy': 0.70112, 'f1': 0.70101, 'accuracy-SBM': 0.70146, 'auc': 0.93021}
2025-07-11 07:28:19,581 - INFO - > Epoch 35: took 73.3s (avg 76.4s) | Best so far: epoch 33	train_loss: 0.7588 train_accuracy-SBM: 0.7226	val_loss: 0.7386 val_accuracy-SBM: 0.7304	test_loss: 0.7324 test_accuracy-SBM: 0.7318
2025-07-11 07:29:28,342 - INFO - train: {'epoch': 36, 'time_epoch': 68.49597, 'eta': 4346.53024, 'eta_hours': 1.20737, 'loss': 0.74853724, 'lr': 0.00075948, 'params': 425270, 'time_iter': 0.10959, 'accuracy': 0.7264, 'f1': 0.7264, 'accuracy-SBM': 0.72641, 'auc': 0.9438}
2025-07-11 07:29:32,066 - INFO - val: {'epoch': 36, 'time_epoch': 3.6757, 'loss': 0.7508702, 'lr': 0, 'params': 425270, 'time_iter': 0.05834, 'accuracy': 0.72883, 'f1': 0.72845, 'accuracy-SBM': 0.72858, 'auc': 0.94448}
2025-07-11 07:29:35,694 - INFO - test: {'epoch': 36, 'time_epoch': 3.58485, 'loss': 0.74807906, 'lr': 0, 'params': 425270, 'time_iter': 0.0569, 'accuracy': 0.72817, 'f1': 0.72805, 'accuracy-SBM': 0.72845, 'auc': 0.94513}
2025-07-11 07:29:35,696 - INFO - > Epoch 36: took 76.1s (avg 76.4s) | Best so far: epoch 33	train_loss: 0.7588 train_accuracy-SBM: 0.7226	val_loss: 0.7386 val_accuracy-SBM: 0.7304	test_loss: 0.7324 test_accuracy-SBM: 0.7318
2025-07-11 07:30:43,429 - INFO - train: {'epoch': 37, 'time_epoch': 67.40319, 'eta': 4274.94455, 'eta_hours': 1.18748, 'loss': 0.74496571, 'lr': 0.00074521, 'params': 425270, 'time_iter': 0.10785, 'accuracy': 0.72756, 'f1': 0.72755, 'accuracy-SBM': 0.72756, 'auc': 0.94433}
2025-07-11 07:30:46,795 - INFO - val: {'epoch': 37, 'time_epoch': 3.32288, 'loss': 0.74857858, 'lr': 0, 'params': 425270, 'time_iter': 0.05274, 'accuracy': 0.72527, 'f1': 0.72482, 'accuracy-SBM': 0.72473, 'auc': 0.94478}
2025-07-11 07:30:50,141 - INFO - test: {'epoch': 37, 'time_epoch': 3.31432, 'loss': 0.74248617, 'lr': 0, 'params': 425270, 'time_iter': 0.05261, 'accuracy': 0.72682, 'f1': 0.72664, 'accuracy-SBM': 0.72671, 'auc': 0.94579}
2025-07-11 07:30:50,143 - INFO - > Epoch 37: took 74.4s (avg 76.3s) | Best so far: epoch 33	train_loss: 0.7588 train_accuracy-SBM: 0.7226	val_loss: 0.7386 val_accuracy-SBM: 0.7304	test_loss: 0.7324 test_accuracy-SBM: 0.7318
2025-07-11 07:31:56,938 - INFO - train: {'epoch': 38, 'time_epoch': 66.5632, 'eta': 4202.2595, 'eta_hours': 1.16729, 'loss': 0.74095302, 'lr': 0.00073067, 'params': 425270, 'time_iter': 0.1065, 'accuracy': 0.72879, 'f1': 0.72878, 'accuracy-SBM': 0.72879, 'auc': 0.94496}
2025-07-11 07:32:00,304 - INFO - val: {'epoch': 38, 'time_epoch': 3.32309, 'loss': 0.78260966, 'lr': 0, 'params': 425270, 'time_iter': 0.05275, 'accuracy': 0.723, 'f1': 0.7225, 'accuracy-SBM': 0.72255, 'auc': 0.93978}
2025-07-11 07:32:03,749 - INFO - test: {'epoch': 38, 'time_epoch': 3.40207, 'loss': 0.75760978, 'lr': 0, 'params': 425270, 'time_iter': 0.054, 'accuracy': 0.72873, 'f1': 0.72861, 'accuracy-SBM': 0.72855, 'auc': 0.94314}
2025-07-11 07:32:03,752 - INFO - > Epoch 38: took 73.6s (avg 76.3s) | Best so far: epoch 33	train_loss: 0.7588 train_accuracy-SBM: 0.7226	val_loss: 0.7386 val_accuracy-SBM: 0.7304	test_loss: 0.7324 test_accuracy-SBM: 0.7318
2025-07-11 07:33:14,533 - INFO - train: {'epoch': 39, 'time_epoch': 70.51541, 'eta': 4135.80887, 'eta_hours': 1.14884, 'loss': 0.73915636, 'lr': 0.00071588, 'params': 425270, 'time_iter': 0.11282, 'accuracy': 0.72994, 'f1': 0.72993, 'accuracy-SBM': 0.72994, 'auc': 0.94522}
2025-07-11 07:33:18,375 - INFO - val: {'epoch': 39, 'time_epoch': 3.79185, 'loss': 0.73872775, 'lr': 0, 'params': 425270, 'time_iter': 0.06019, 'accuracy': 0.73566, 'f1': 0.73572, 'accuracy-SBM': 0.73574, 'auc': 0.94557}
2025-07-11 07:33:22,407 - INFO - test: {'epoch': 39, 'time_epoch': 3.99333, 'loss': 0.71705191, 'lr': 0, 'params': 425270, 'time_iter': 0.06339, 'accuracy': 0.73751, 'f1': 0.73752, 'accuracy-SBM': 0.73755, 'auc': 0.94898}
2025-07-11 07:33:22,409 - INFO - > Epoch 39: took 78.7s (avg 76.3s) | Best so far: epoch 39	train_loss: 0.7392 train_accuracy-SBM: 0.7299	val_loss: 0.7387 val_accuracy-SBM: 0.7357	test_loss: 0.7171 test_accuracy-SBM: 0.7376
2025-07-11 07:34:29,895 - INFO - train: {'epoch': 40, 'time_epoch': 67.24895, 'eta': 4064.45944, 'eta_hours': 1.12902, 'loss': 0.73753767, 'lr': 0.00070085, 'params': 425270, 'time_iter': 0.1076, 'accuracy': 0.72979, 'f1': 0.72979, 'accuracy-SBM': 0.72979, 'auc': 0.94549}
2025-07-11 07:34:33,296 - INFO - val: {'epoch': 40, 'time_epoch': 3.35637, 'loss': 0.75061787, 'lr': 0, 'params': 425270, 'time_iter': 0.05328, 'accuracy': 0.73271, 'f1': 0.73247, 'accuracy-SBM': 0.73247, 'auc': 0.94366}
2025-07-11 07:34:36,676 - INFO - test: {'epoch': 40, 'time_epoch': 3.34746, 'loss': 0.74414468, 'lr': 0, 'params': 425270, 'time_iter': 0.05313, 'accuracy': 0.73589, 'f1': 0.73579, 'accuracy-SBM': 0.73587, 'auc': 0.9446}
2025-07-11 07:34:36,678 - INFO - > Epoch 40: took 74.3s (avg 76.3s) | Best so far: epoch 39	train_loss: 0.7392 train_accuracy-SBM: 0.7299	val_loss: 0.7387 val_accuracy-SBM: 0.7357	test_loss: 0.7171 test_accuracy-SBM: 0.7376
2025-07-11 07:35:43,227 - INFO - train: {'epoch': 41, 'time_epoch': 66.30275, 'eta': 3991.99861, 'eta_hours': 1.10889, 'loss': 0.73249457, 'lr': 0.0006856, 'params': 425270, 'time_iter': 0.10608, 'accuracy': 0.73205, 'f1': 0.73204, 'accuracy-SBM': 0.73205, 'auc': 0.94621}
2025-07-11 07:35:46,694 - INFO - val: {'epoch': 41, 'time_epoch': 3.41468, 'loss': 0.7294317, 'lr': 0, 'params': 425270, 'time_iter': 0.0542, 'accuracy': 0.73905, 'f1': 0.73904, 'accuracy-SBM': 0.7389, 'auc': 0.94716}
2025-07-11 07:35:50,075 - INFO - test: {'epoch': 41, 'time_epoch': 3.34927, 'loss': 0.72568091, 'lr': 0, 'params': 425270, 'time_iter': 0.05316, 'accuracy': 0.73838, 'f1': 0.73847, 'accuracy-SBM': 0.7383, 'auc': 0.94774}
2025-07-11 07:35:50,077 - INFO - > Epoch 41: took 73.4s (avg 76.2s) | Best so far: epoch 41	train_loss: 0.7325 train_accuracy-SBM: 0.7320	val_loss: 0.7294 val_accuracy-SBM: 0.7389	test_loss: 0.7257 test_accuracy-SBM: 0.7383
2025-07-11 07:36:57,998 - INFO - train: {'epoch': 42, 'time_epoch': 67.67126, 'eta': 3921.63828, 'eta_hours': 1.08934, 'loss': 0.72738851, 'lr': 0.00067015, 'params': 425270, 'time_iter': 0.10827, 'accuracy': 0.73457, 'f1': 0.73456, 'accuracy-SBM': 0.73457, 'auc': 0.94694}
2025-07-11 07:37:01,616 - INFO - val: {'epoch': 42, 'time_epoch': 3.56963, 'loss': 0.74205185, 'lr': 0, 'params': 425270, 'time_iter': 0.05666, 'accuracy': 0.73276, 'f1': 0.73258, 'accuracy-SBM': 0.73249, 'auc': 0.94556}
2025-07-11 07:37:05,286 - INFO - test: {'epoch': 42, 'time_epoch': 3.63609, 'loss': 0.7306163, 'lr': 0, 'params': 425270, 'time_iter': 0.05772, 'accuracy': 0.73422, 'f1': 0.73414, 'accuracy-SBM': 0.73407, 'auc': 0.94723}
2025-07-11 07:37:05,288 - INFO - > Epoch 42: took 75.2s (avg 76.2s) | Best so far: epoch 41	train_loss: 0.7325 train_accuracy-SBM: 0.7320	val_loss: 0.7294 val_accuracy-SBM: 0.7389	test_loss: 0.7257 test_accuracy-SBM: 0.7383
2025-07-11 07:38:12,921 - INFO - train: {'epoch': 43, 'time_epoch': 67.30722, 'eta': 3850.93684, 'eta_hours': 1.0697, 'loss': 0.72582065, 'lr': 0.00065451, 'params': 425270, 'time_iter': 0.10769, 'accuracy': 0.73466, 'f1': 0.73466, 'accuracy-SBM': 0.73466, 'auc': 0.94719}
2025-07-11 07:38:16,317 - INFO - val: {'epoch': 43, 'time_epoch': 3.34094, 'loss': 0.73067457, 'lr': 0, 'params': 425270, 'time_iter': 0.05303, 'accuracy': 0.73175, 'f1': 0.7313, 'accuracy-SBM': 0.73138, 'auc': 0.94761}
2025-07-11 07:38:19,820 - INFO - test: {'epoch': 43, 'time_epoch': 3.47102, 'loss': 0.72227189, 'lr': 0, 'params': 425270, 'time_iter': 0.0551, 'accuracy': 0.73638, 'f1': 0.73623, 'accuracy-SBM': 0.73627, 'auc': 0.94879}
2025-07-11 07:38:19,822 - INFO - > Epoch 43: took 74.5s (avg 76.1s) | Best so far: epoch 41	train_loss: 0.7325 train_accuracy-SBM: 0.7320	val_loss: 0.7294 val_accuracy-SBM: 0.7389	test_loss: 0.7257 test_accuracy-SBM: 0.7383
2025-07-11 07:39:26,704 - INFO - train: {'epoch': 44, 'time_epoch': 66.65073, 'eta': 3779.58389, 'eta_hours': 1.04988, 'loss': 0.72310226, 'lr': 0.0006387, 'params': 425270, 'time_iter': 0.10664, 'accuracy': 0.73588, 'f1': 0.73586, 'accuracy-SBM': 0.73588, 'auc': 0.94757}
2025-07-11 07:39:30,126 - INFO - val: {'epoch': 44, 'time_epoch': 3.37515, 'loss': 0.73025467, 'lr': 0, 'params': 425270, 'time_iter': 0.05357, 'accuracy': 0.73671, 'f1': 0.73676, 'accuracy-SBM': 0.73671, 'auc': 0.94704}
2025-07-11 07:39:33,618 - INFO - test: {'epoch': 44, 'time_epoch': 3.44618, 'loss': 0.71292564, 'lr': 0, 'params': 425270, 'time_iter': 0.0547, 'accuracy': 0.74033, 'f1': 0.74043, 'accuracy-SBM': 0.74038, 'auc': 0.9495}
2025-07-11 07:39:33,620 - INFO - > Epoch 44: took 73.8s (avg 76.1s) | Best so far: epoch 41	train_loss: 0.7325 train_accuracy-SBM: 0.7320	val_loss: 0.7294 val_accuracy-SBM: 0.7389	test_loss: 0.7257 test_accuracy-SBM: 0.7383
2025-07-11 07:40:42,113 - INFO - train: {'epoch': 45, 'time_epoch': 68.26236, 'eta': 3710.32729, 'eta_hours': 1.03065, 'loss': 0.72076841, 'lr': 0.00062274, 'params': 425270, 'time_iter': 0.10922, 'accuracy': 0.73666, 'f1': 0.73666, 'accuracy-SBM': 0.73666, 'auc': 0.94793}
2025-07-11 07:40:45,531 - INFO - val: {'epoch': 45, 'time_epoch': 3.37324, 'loss': 0.76423172, 'lr': 0, 'params': 425270, 'time_iter': 0.05354, 'accuracy': 0.7285, 'f1': 0.72838, 'accuracy-SBM': 0.72836, 'auc': 0.94194}
2025-07-11 07:40:48,909 - INFO - test: {'epoch': 45, 'time_epoch': 3.34608, 'loss': 0.75759321, 'lr': 0, 'params': 425270, 'time_iter': 0.05311, 'accuracy': 0.72944, 'f1': 0.72947, 'accuracy-SBM': 0.72936, 'auc': 0.94296}
2025-07-11 07:40:48,911 - INFO - > Epoch 45: took 75.3s (avg 76.1s) | Best so far: epoch 41	train_loss: 0.7325 train_accuracy-SBM: 0.7320	val_loss: 0.7294 val_accuracy-SBM: 0.7389	test_loss: 0.7257 test_accuracy-SBM: 0.7383
2025-07-11 07:41:55,218 - INFO - train: {'epoch': 46, 'time_epoch': 66.0704, 'eta': 3638.64122, 'eta_hours': 1.01073, 'loss': 0.71802023, 'lr': 0.00060665, 'params': 425270, 'time_iter': 0.10571, 'accuracy': 0.73771, 'f1': 0.73771, 'accuracy-SBM': 0.73771, 'auc': 0.94833}
2025-07-11 07:41:58,517 - INFO - val: {'epoch': 46, 'time_epoch': 3.25482, 'loss': 0.73245233, 'lr': 0, 'params': 425270, 'time_iter': 0.05166, 'accuracy': 0.7398, 'f1': 0.7399, 'accuracy-SBM': 0.73978, 'auc': 0.94656}
2025-07-11 07:42:01,818 - INFO - test: {'epoch': 46, 'time_epoch': 3.26965, 'loss': 0.70767759, 'lr': 0, 'params': 425270, 'time_iter': 0.0519, 'accuracy': 0.74261, 'f1': 0.74284, 'accuracy-SBM': 0.74264, 'auc': 0.95031}
2025-07-11 07:42:01,820 - INFO - > Epoch 46: took 72.9s (avg 76.0s) | Best so far: epoch 46	train_loss: 0.7180 train_accuracy-SBM: 0.7377	val_loss: 0.7325 val_accuracy-SBM: 0.7398	test_loss: 0.7077 test_accuracy-SBM: 0.7426
2025-07-11 07:43:09,958 - INFO - train: {'epoch': 47, 'time_epoch': 67.88939, 'eta': 3569.15971, 'eta_hours': 0.99143, 'loss': 0.71747337, 'lr': 0.00059044, 'params': 425270, 'time_iter': 0.10862, 'accuracy': 0.73728, 'f1': 0.73728, 'accuracy-SBM': 0.73728, 'auc': 0.94842}
2025-07-11 07:43:13,711 - INFO - val: {'epoch': 47, 'time_epoch': 3.69229, 'loss': 0.74085814, 'lr': 0, 'params': 425270, 'time_iter': 0.05861, 'accuracy': 0.73314, 'f1': 0.7332, 'accuracy-SBM': 0.73311, 'auc': 0.94578}
2025-07-11 07:43:17,275 - INFO - test: {'epoch': 47, 'time_epoch': 3.52538, 'loss': 0.71254573, 'lr': 0, 'params': 425270, 'time_iter': 0.05596, 'accuracy': 0.73879, 'f1': 0.73898, 'accuracy-SBM': 0.73879, 'auc': 0.94975}
2025-07-11 07:43:17,277 - INFO - > Epoch 47: took 75.5s (avg 76.0s) | Best so far: epoch 46	train_loss: 0.7180 train_accuracy-SBM: 0.7377	val_loss: 0.7325 val_accuracy-SBM: 0.7398	test_loss: 0.7077 test_accuracy-SBM: 0.7426
2025-07-11 07:44:25,313 - INFO - train: {'epoch': 48, 'time_epoch': 67.79819, 'eta': 3499.64826, 'eta_hours': 0.97212, 'loss': 0.71026814, 'lr': 0.00057413, 'params': 425270, 'time_iter': 0.10848, 'accuracy': 0.73984, 'f1': 0.73982, 'accuracy-SBM': 0.73983, 'auc': 0.94943}
2025-07-11 07:44:28,755 - INFO - val: {'epoch': 48, 'time_epoch': 3.39838, 'loss': 0.72798731, 'lr': 0, 'params': 425270, 'time_iter': 0.05394, 'accuracy': 0.73753, 'f1': 0.73754, 'accuracy-SBM': 0.73758, 'auc': 0.94805}
2025-07-11 07:44:32,110 - INFO - test: {'epoch': 48, 'time_epoch': 3.31454, 'loss': 0.71299612, 'lr': 0, 'params': 425270, 'time_iter': 0.05261, 'accuracy': 0.73768, 'f1': 0.73771, 'accuracy-SBM': 0.73784, 'auc': 0.95033}
2025-07-11 07:44:32,112 - INFO - > Epoch 48: took 74.8s (avg 76.0s) | Best so far: epoch 46	train_loss: 0.7180 train_accuracy-SBM: 0.7377	val_loss: 0.7325 val_accuracy-SBM: 0.7398	test_loss: 0.7077 test_accuracy-SBM: 0.7426
2025-07-11 07:45:38,902 - INFO - train: {'epoch': 49, 'time_epoch': 66.44881, 'eta': 3428.85597, 'eta_hours': 0.95246, 'loss': 0.70974499, 'lr': 0.00055774, 'params': 425270, 'time_iter': 0.10632, 'accuracy': 0.74034, 'f1': 0.74033, 'accuracy-SBM': 0.74034, 'auc': 0.94952}
2025-07-11 07:45:42,308 - INFO - val: {'epoch': 49, 'time_epoch': 3.36167, 'loss': 0.71590118, 'lr': 0, 'params': 425270, 'time_iter': 0.05336, 'accuracy': 0.73922, 'f1': 0.73947, 'accuracy-SBM': 0.73937, 'auc': 0.95002}
2025-07-11 07:45:45,703 - INFO - test: {'epoch': 49, 'time_epoch': 3.3637, 'loss': 0.70683895, 'lr': 0, 'params': 425270, 'time_iter': 0.05339, 'accuracy': 0.74093, 'f1': 0.74101, 'accuracy-SBM': 0.74086, 'auc': 0.95115}
2025-07-11 07:45:45,705 - INFO - > Epoch 49: took 73.6s (avg 75.9s) | Best so far: epoch 46	train_loss: 0.7180 train_accuracy-SBM: 0.7377	val_loss: 0.7325 val_accuracy-SBM: 0.7398	test_loss: 0.7077 test_accuracy-SBM: 0.7426
2025-07-11 07:46:54,896 - INFO - train: {'epoch': 50, 'time_epoch': 68.81825, 'eta': 3360.51052, 'eta_hours': 0.93348, 'loss': 0.70569935, 'lr': 0.00054129, 'params': 425270, 'time_iter': 0.11011, 'accuracy': 0.74232, 'f1': 0.74231, 'accuracy-SBM': 0.74232, 'auc': 0.95011}
2025-07-11 07:46:58,329 - INFO - val: {'epoch': 50, 'time_epoch': 3.37847, 'loss': 0.70675142, 'lr': 0, 'params': 425270, 'time_iter': 0.05363, 'accuracy': 0.74501, 'f1': 0.74508, 'accuracy-SBM': 0.74511, 'auc': 0.95035}
2025-07-11 07:47:01,999 - INFO - test: {'epoch': 50, 'time_epoch': 3.63086, 'loss': 0.69984373, 'lr': 0, 'params': 425270, 'time_iter': 0.05763, 'accuracy': 0.74455, 'f1': 0.74461, 'accuracy-SBM': 0.74465, 'auc': 0.95142}
2025-07-11 07:47:02,001 - INFO - > Epoch 50: took 76.3s (avg 75.9s) | Best so far: epoch 50	train_loss: 0.7057 train_accuracy-SBM: 0.7423	val_loss: 0.7068 val_accuracy-SBM: 0.7451	test_loss: 0.6998 test_accuracy-SBM: 0.7447
2025-07-11 07:48:09,904 - INFO - train: {'epoch': 51, 'time_epoch': 67.55408, 'eta': 3290.97997, 'eta_hours': 0.91416, 'loss': 0.70190495, 'lr': 0.00052479, 'params': 425270, 'time_iter': 0.10809, 'accuracy': 0.74336, 'f1': 0.74335, 'accuracy-SBM': 0.74336, 'auc': 0.95063}
2025-07-11 07:48:13,323 - INFO - val: {'epoch': 51, 'time_epoch': 3.37387, 'loss': 0.69178148, 'lr': 0, 'params': 425270, 'time_iter': 0.05355, 'accuracy': 0.75004, 'f1': 0.74986, 'accuracy-SBM': 0.74981, 'auc': 0.95218}
2025-07-11 07:48:16,653 - INFO - test: {'epoch': 51, 'time_epoch': 3.29671, 'loss': 0.69134964, 'lr': 0, 'params': 425270, 'time_iter': 0.05233, 'accuracy': 0.74793, 'f1': 0.74785, 'accuracy-SBM': 0.74796, 'auc': 0.95243}
2025-07-11 07:48:16,655 - INFO - > Epoch 51: took 74.7s (avg 75.9s) | Best so far: epoch 51	train_loss: 0.7019 train_accuracy-SBM: 0.7434	val_loss: 0.6918 val_accuracy-SBM: 0.7498	test_loss: 0.6913 test_accuracy-SBM: 0.7480
2025-07-11 07:49:23,055 - INFO - train: {'epoch': 52, 'time_epoch': 66.16253, 'eta': 3220.28998, 'eta_hours': 0.89452, 'loss': 0.70168473, 'lr': 0.00050827, 'params': 425270, 'time_iter': 0.10586, 'accuracy': 0.74417, 'f1': 0.74416, 'accuracy-SBM': 0.74417, 'auc': 0.95066}
2025-07-11 07:49:26,529 - INFO - val: {'epoch': 52, 'time_epoch': 3.42809, 'loss': 0.80067777, 'lr': 0, 'params': 425270, 'time_iter': 0.05441, 'accuracy': 0.73103, 'f1': 0.73105, 'accuracy-SBM': 0.73097, 'auc': 0.93699}
2025-07-11 07:49:29,945 - INFO - test: {'epoch': 52, 'time_epoch': 3.37515, 'loss': 0.76669018, 'lr': 0, 'params': 425270, 'time_iter': 0.05357, 'accuracy': 0.73147, 'f1': 0.73157, 'accuracy-SBM': 0.73159, 'auc': 0.94185}
2025-07-11 07:49:29,947 - INFO - > Epoch 52: took 73.3s (avg 75.8s) | Best so far: epoch 51	train_loss: 0.7019 train_accuracy-SBM: 0.7434	val_loss: 0.6918 val_accuracy-SBM: 0.7498	test_loss: 0.6913 test_accuracy-SBM: 0.7480
2025-07-11 07:50:39,858 - INFO - train: {'epoch': 53, 'time_epoch': 69.66445, 'eta': 3152.7508, 'eta_hours': 0.87576, 'loss': 0.70157892, 'lr': 0.00049173, 'params': 425270, 'time_iter': 0.11146, 'accuracy': 0.7432, 'f1': 0.7432, 'accuracy-SBM': 0.7432, 'auc': 0.9507}
2025-07-11 07:50:43,606 - INFO - val: {'epoch': 53, 'time_epoch': 3.70004, 'loss': 0.71522583, 'lr': 0, 'params': 425270, 'time_iter': 0.05873, 'accuracy': 0.74247, 'f1': 0.74215, 'accuracy-SBM': 0.74215, 'auc': 0.9497}
2025-07-11 07:50:47,255 - INFO - test: {'epoch': 53, 'time_epoch': 3.61436, 'loss': 0.71800869, 'lr': 0, 'params': 425270, 'time_iter': 0.05737, 'accuracy': 0.74076, 'f1': 0.7408, 'accuracy-SBM': 0.74067, 'auc': 0.94935}
2025-07-11 07:50:47,257 - INFO - > Epoch 53: took 77.3s (avg 75.9s) | Best so far: epoch 51	train_loss: 0.7019 train_accuracy-SBM: 0.7434	val_loss: 0.6918 val_accuracy-SBM: 0.7498	test_loss: 0.6913 test_accuracy-SBM: 0.7480
2025-07-11 07:51:53,444 - INFO - train: {'epoch': 54, 'time_epoch': 65.94352, 'eta': 3082.08993, 'eta_hours': 0.85614, 'loss': 0.69584804, 'lr': 0.00047521, 'params': 425270, 'time_iter': 0.10551, 'accuracy': 0.74633, 'f1': 0.74633, 'accuracy-SBM': 0.74633, 'auc': 0.95146}
2025-07-11 07:51:56,989 - INFO - val: {'epoch': 54, 'time_epoch': 3.49697, 'loss': 0.922658, 'lr': 0, 'params': 425270, 'time_iter': 0.05551, 'accuracy': 0.71706, 'f1': 0.71762, 'accuracy-SBM': 0.71686, 'auc': 0.91966}
2025-07-11 07:52:00,362 - INFO - test: {'epoch': 54, 'time_epoch': 3.34088, 'loss': 0.83865423, 'lr': 0, 'params': 425270, 'time_iter': 0.05303, 'accuracy': 0.72699, 'f1': 0.72736, 'accuracy-SBM': 0.72704, 'auc': 0.93154}
2025-07-11 07:52:00,364 - INFO - > Epoch 54: took 73.1s (avg 75.8s) | Best so far: epoch 51	train_loss: 0.7019 train_accuracy-SBM: 0.7434	val_loss: 0.6918 val_accuracy-SBM: 0.7498	test_loss: 0.6913 test_accuracy-SBM: 0.7480
2025-07-11 07:53:08,184 - INFO - train: {'epoch': 55, 'time_epoch': 67.48242, 'eta': 3012.80667, 'eta_hours': 0.83689, 'loss': 0.69352412, 'lr': 0.00045871, 'params': 425270, 'time_iter': 0.10797, 'accuracy': 0.74649, 'f1': 0.74649, 'accuracy-SBM': 0.7465, 'auc': 0.95183}
2025-07-11 07:53:11,610 - INFO - val: {'epoch': 55, 'time_epoch': 3.38127, 'loss': 0.91851056, 'lr': 0, 'params': 425270, 'time_iter': 0.05367, 'accuracy': 0.71232, 'f1': 0.71305, 'accuracy-SBM': 0.71219, 'auc': 0.91986}
2025-07-11 07:53:15,012 - INFO - test: {'epoch': 55, 'time_epoch': 3.37014, 'loss': 0.8872364, 'lr': 0, 'params': 425270, 'time_iter': 0.05349, 'accuracy': 0.71615, 'f1': 0.71713, 'accuracy-SBM': 0.71625, 'auc': 0.92441}
2025-07-11 07:53:15,014 - INFO - > Epoch 55: took 74.6s (avg 75.8s) | Best so far: epoch 51	train_loss: 0.7019 train_accuracy-SBM: 0.7434	val_loss: 0.6918 val_accuracy-SBM: 0.7498	test_loss: 0.6913 test_accuracy-SBM: 0.7480
2025-07-11 07:54:23,502 - INFO - train: {'epoch': 56, 'time_epoch': 68.24498, 'eta': 2944.16187, 'eta_hours': 0.81782, 'loss': 0.69057113, 'lr': 0.00044226, 'params': 425270, 'time_iter': 0.10919, 'accuracy': 0.74774, 'f1': 0.74773, 'accuracy-SBM': 0.74774, 'auc': 0.95222}
2025-07-11 07:54:27,170 - INFO - val: {'epoch': 56, 'time_epoch': 3.62179, 'loss': 0.68613807, 'lr': 0, 'params': 425270, 'time_iter': 0.05749, 'accuracy': 0.7513, 'f1': 0.75109, 'accuracy-SBM': 0.75107, 'auc': 0.95317}
2025-07-11 07:54:30,634 - INFO - test: {'epoch': 56, 'time_epoch': 3.42907, 'loss': 0.68468059, 'lr': 0, 'params': 425270, 'time_iter': 0.05443, 'accuracy': 0.74665, 'f1': 0.74673, 'accuracy-SBM': 0.74673, 'auc': 0.95357}
2025-07-11 07:54:30,636 - INFO - > Epoch 56: took 75.6s (avg 75.8s) | Best so far: epoch 56	train_loss: 0.6906 train_accuracy-SBM: 0.7477	val_loss: 0.6861 val_accuracy-SBM: 0.7511	test_loss: 0.6847 test_accuracy-SBM: 0.7467
2025-07-11 07:55:36,757 - INFO - train: {'epoch': 57, 'time_epoch': 65.88142, 'eta': 2873.81931, 'eta_hours': 0.79828, 'loss': 0.68978526, 'lr': 0.00042587, 'params': 425270, 'time_iter': 0.10541, 'accuracy': 0.74762, 'f1': 0.74761, 'accuracy-SBM': 0.74762, 'auc': 0.95234}
2025-07-11 07:55:40,211 - INFO - val: {'epoch': 57, 'time_epoch': 3.40942, 'loss': 0.68422831, 'lr': 0, 'params': 425270, 'time_iter': 0.05412, 'accuracy': 0.75223, 'f1': 0.75216, 'accuracy-SBM': 0.75221, 'auc': 0.95317}
2025-07-11 07:55:43,667 - INFO - test: {'epoch': 57, 'time_epoch': 3.41446, 'loss': 0.67473908, 'lr': 0, 'params': 425270, 'time_iter': 0.0542, 'accuracy': 0.75266, 'f1': 0.7526, 'accuracy-SBM': 0.75267, 'auc': 0.95455}
2025-07-11 07:55:43,669 - INFO - > Epoch 57: took 73.0s (avg 75.8s) | Best so far: epoch 57	train_loss: 0.6898 train_accuracy-SBM: 0.7476	val_loss: 0.6842 val_accuracy-SBM: 0.7522	test_loss: 0.6747 test_accuracy-SBM: 0.7527
2025-07-11 07:56:51,307 - INFO - train: {'epoch': 58, 'time_epoch': 67.40322, 'eta': 2804.6855, 'eta_hours': 0.77908, 'loss': 0.68556334, 'lr': 0.00040956, 'params': 425270, 'time_iter': 0.10785, 'accuracy': 0.74962, 'f1': 0.74962, 'accuracy-SBM': 0.74963, 'auc': 0.95292}
2025-07-11 07:56:54,803 - INFO - val: {'epoch': 58, 'time_epoch': 3.45069, 'loss': 0.67785531, 'lr': 0, 'params': 425270, 'time_iter': 0.05477, 'accuracy': 0.75163, 'f1': 0.75138, 'accuracy-SBM': 0.75128, 'auc': 0.95448}
2025-07-11 07:56:58,497 - INFO - test: {'epoch': 58, 'time_epoch': 3.6576, 'loss': 0.68528324, 'lr': 0, 'params': 425270, 'time_iter': 0.05806, 'accuracy': 0.74898, 'f1': 0.749, 'accuracy-SBM': 0.74888, 'auc': 0.95354}
2025-07-11 07:56:58,499 - INFO - > Epoch 58: took 74.8s (avg 75.7s) | Best so far: epoch 57	train_loss: 0.6898 train_accuracy-SBM: 0.7476	val_loss: 0.6842 val_accuracy-SBM: 0.7522	test_loss: 0.6747 test_accuracy-SBM: 0.7527
2025-07-11 07:58:08,086 - INFO - train: {'epoch': 59, 'time_epoch': 69.35509, 'eta': 2736.91062, 'eta_hours': 0.76025, 'loss': 0.68288617, 'lr': 0.00039335, 'params': 425270, 'time_iter': 0.11097, 'accuracy': 0.75045, 'f1': 0.75045, 'accuracy-SBM': 0.75045, 'auc': 0.9533}
2025-07-11 07:58:11,492 - INFO - val: {'epoch': 59, 'time_epoch': 3.36093, 'loss': 0.68639984, 'lr': 0, 'params': 425270, 'time_iter': 0.05335, 'accuracy': 0.75096, 'f1': 0.75085, 'accuracy-SBM': 0.75085, 'auc': 0.95293}
2025-07-11 07:58:14,892 - INFO - test: {'epoch': 59, 'time_epoch': 3.36671, 'loss': 0.6898592, 'lr': 0, 'params': 425270, 'time_iter': 0.05344, 'accuracy': 0.75045, 'f1': 0.75049, 'accuracy-SBM': 0.75051, 'auc': 0.95249}
2025-07-11 07:58:14,894 - INFO - > Epoch 59: took 76.4s (avg 75.7s) | Best so far: epoch 57	train_loss: 0.6898 train_accuracy-SBM: 0.7476	val_loss: 0.6842 val_accuracy-SBM: 0.7522	test_loss: 0.6747 test_accuracy-SBM: 0.7527
2025-07-11 07:59:22,606 - INFO - train: {'epoch': 60, 'time_epoch': 67.47844, 'eta': 2667.8841, 'eta_hours': 0.74108, 'loss': 0.67934503, 'lr': 0.00037726, 'params': 425270, 'time_iter': 0.10797, 'accuracy': 0.75195, 'f1': 0.75194, 'accuracy-SBM': 0.75195, 'auc': 0.95377}
2025-07-11 07:59:26,097 - INFO - val: {'epoch': 60, 'time_epoch': 3.44676, 'loss': 0.68013552, 'lr': 0, 'params': 425270, 'time_iter': 0.05471, 'accuracy': 0.75362, 'f1': 0.75365, 'accuracy-SBM': 0.7537, 'auc': 0.95388}
2025-07-11 07:59:29,551 - INFO - test: {'epoch': 60, 'time_epoch': 3.42097, 'loss': 0.6781399, 'lr': 0, 'params': 425270, 'time_iter': 0.0543, 'accuracy': 0.75306, 'f1': 0.75308, 'accuracy-SBM': 0.75316, 'auc': 0.95424}
2025-07-11 07:59:29,553 - INFO - > Epoch 60: took 74.7s (avg 75.7s) | Best so far: epoch 60	train_loss: 0.6793 train_accuracy-SBM: 0.7520	val_loss: 0.6801 val_accuracy-SBM: 0.7537	test_loss: 0.6781 test_accuracy-SBM: 0.7532
2025-07-11 08:00:38,352 - INFO - train: {'epoch': 61, 'time_epoch': 68.55861, 'eta': 2599.56957, 'eta_hours': 0.7221, 'loss': 0.67824981, 'lr': 0.0003613, 'params': 425270, 'time_iter': 0.10969, 'accuracy': 0.75203, 'f1': 0.75202, 'accuracy-SBM': 0.75203, 'auc': 0.95392}
2025-07-11 08:00:42,101 - INFO - val: {'epoch': 61, 'time_epoch': 3.6898, 'loss': 0.69962527, 'lr': 0, 'params': 425270, 'time_iter': 0.05857, 'accuracy': 0.7488, 'f1': 0.74891, 'accuracy-SBM': 0.74888, 'auc': 0.95132}
2025-07-11 08:00:45,804 - INFO - test: {'epoch': 61, 'time_epoch': 3.6672, 'loss': 0.69199012, 'lr': 0, 'params': 425270, 'time_iter': 0.05821, 'accuracy': 0.75027, 'f1': 0.75033, 'accuracy-SBM': 0.75035, 'auc': 0.95238}
2025-07-11 08:00:45,806 - INFO - > Epoch 61: took 76.3s (avg 75.7s) | Best so far: epoch 60	train_loss: 0.6793 train_accuracy-SBM: 0.7520	val_loss: 0.6801 val_accuracy-SBM: 0.7537	test_loss: 0.6781 test_accuracy-SBM: 0.7532
2025-07-11 08:01:53,607 - INFO - train: {'epoch': 62, 'time_epoch': 67.56577, 'eta': 2530.66419, 'eta_hours': 0.70296, 'loss': 0.67425619, 'lr': 0.00034549, 'params': 425270, 'time_iter': 0.10811, 'accuracy': 0.75339, 'f1': 0.75338, 'accuracy-SBM': 0.75339, 'auc': 0.95448}
2025-07-11 08:01:57,059 - INFO - val: {'epoch': 62, 'time_epoch': 3.40797, 'loss': 0.6679505, 'lr': 0, 'params': 425270, 'time_iter': 0.05409, 'accuracy': 0.75829, 'f1': 0.75816, 'accuracy-SBM': 0.75811, 'auc': 0.95565}
2025-07-11 08:02:00,462 - INFO - test: {'epoch': 62, 'time_epoch': 3.36961, 'loss': 0.66979623, 'lr': 0, 'params': 425270, 'time_iter': 0.05349, 'accuracy': 0.7543, 'f1': 0.75432, 'accuracy-SBM': 0.75442, 'auc': 0.95551}
2025-07-11 08:02:00,464 - INFO - > Epoch 62: took 74.7s (avg 75.7s) | Best so far: epoch 62	train_loss: 0.6743 train_accuracy-SBM: 0.7534	val_loss: 0.6680 val_accuracy-SBM: 0.7581	test_loss: 0.6698 test_accuracy-SBM: 0.7544
2025-07-11 08:03:06,480 - INFO - train: {'epoch': 63, 'time_epoch': 65.79011, 'eta': 2460.80186, 'eta_hours': 0.68356, 'loss': 0.67132052, 'lr': 0.00032985, 'params': 425270, 'time_iter': 0.10526, 'accuracy': 0.7544, 'f1': 0.7544, 'accuracy-SBM': 0.7544, 'auc': 0.95486}
2025-07-11 08:03:09,979 - INFO - val: {'epoch': 63, 'time_epoch': 3.45331, 'loss': 0.7028061, 'lr': 0, 'params': 425270, 'time_iter': 0.05481, 'accuracy': 0.74632, 'f1': 0.74617, 'accuracy-SBM': 0.7462, 'auc': 0.95111}
2025-07-11 08:03:13,332 - INFO - test: {'epoch': 63, 'time_epoch': 3.32122, 'loss': 0.69787514, 'lr': 0, 'params': 425270, 'time_iter': 0.05272, 'accuracy': 0.74694, 'f1': 0.74696, 'accuracy-SBM': 0.74701, 'auc': 0.95181}
2025-07-11 08:03:13,334 - INFO - > Epoch 63: took 72.9s (avg 75.7s) | Best so far: epoch 62	train_loss: 0.6743 train_accuracy-SBM: 0.7534	val_loss: 0.6680 val_accuracy-SBM: 0.7581	test_loss: 0.6698 test_accuracy-SBM: 0.7544
2025-07-11 08:04:21,566 - INFO - train: {'epoch': 64, 'time_epoch': 67.97392, 'eta': 2392.24073, 'eta_hours': 0.66451, 'loss': 0.67136202, 'lr': 0.0003144, 'params': 425270, 'time_iter': 0.10876, 'accuracy': 0.75463, 'f1': 0.75462, 'accuracy-SBM': 0.75463, 'auc': 0.95486}
2025-07-11 08:04:25,378 - INFO - val: {'epoch': 64, 'time_epoch': 3.76535, 'loss': 0.67109029, 'lr': 0, 'params': 425270, 'time_iter': 0.05977, 'accuracy': 0.75545, 'f1': 0.75539, 'accuracy-SBM': 0.75534, 'auc': 0.95521}
2025-07-11 08:04:29,211 - INFO - test: {'epoch': 64, 'time_epoch': 3.79931, 'loss': 0.66078236, 'lr': 0, 'params': 425270, 'time_iter': 0.06031, 'accuracy': 0.7582, 'f1': 0.75823, 'accuracy-SBM': 0.75819, 'auc': 0.95657}
2025-07-11 08:04:29,213 - INFO - > Epoch 64: took 75.9s (avg 75.7s) | Best so far: epoch 62	train_loss: 0.6743 train_accuracy-SBM: 0.7534	val_loss: 0.6680 val_accuracy-SBM: 0.7581	test_loss: 0.6698 test_accuracy-SBM: 0.7544
2025-07-11 08:05:38,925 - INFO - train: {'epoch': 65, 'time_epoch': 69.47558, 'eta': 2324.47098, 'eta_hours': 0.64569, 'loss': 0.66577584, 'lr': 0.00029915, 'params': 425270, 'time_iter': 0.11116, 'accuracy': 0.75682, 'f1': 0.75682, 'accuracy-SBM': 0.75682, 'auc': 0.95563}
2025-07-11 08:05:42,340 - INFO - val: {'epoch': 65, 'time_epoch': 3.36389, 'loss': 0.67591038, 'lr': 0, 'params': 425270, 'time_iter': 0.0534, 'accuracy': 0.7558, 'f1': 0.75564, 'accuracy-SBM': 0.7556, 'auc': 0.9548}
2025-07-11 08:05:45,747 - INFO - test: {'epoch': 65, 'time_epoch': 3.36806, 'loss': 0.66631846, 'lr': 0, 'params': 425270, 'time_iter': 0.05346, 'accuracy': 0.7575, 'f1': 0.75742, 'accuracy-SBM': 0.75737, 'auc': 0.95608}
2025-07-11 08:05:45,749 - INFO - > Epoch 65: took 76.5s (avg 75.7s) | Best so far: epoch 62	train_loss: 0.6743 train_accuracy-SBM: 0.7534	val_loss: 0.6680 val_accuracy-SBM: 0.7581	test_loss: 0.6698 test_accuracy-SBM: 0.7544
2025-07-11 08:06:52,479 - INFO - train: {'epoch': 66, 'time_epoch': 66.48346, 'eta': 2255.17657, 'eta_hours': 0.62644, 'loss': 0.66779076, 'lr': 0.00028412, 'params': 425270, 'time_iter': 0.10637, 'accuracy': 0.75622, 'f1': 0.75622, 'accuracy-SBM': 0.75622, 'auc': 0.95536}
2025-07-11 08:06:55,911 - INFO - val: {'epoch': 66, 'time_epoch': 3.38384, 'loss': 0.67004733, 'lr': 0, 'params': 425270, 'time_iter': 0.05371, 'accuracy': 0.7573, 'f1': 0.75731, 'accuracy-SBM': 0.75733, 'auc': 0.95521}
2025-07-11 08:06:59,444 - INFO - test: {'epoch': 66, 'time_epoch': 3.49791, 'loss': 0.65993781, 'lr': 0, 'params': 425270, 'time_iter': 0.05552, 'accuracy': 0.7582, 'f1': 0.75822, 'accuracy-SBM': 0.75836, 'auc': 0.95663}
2025-07-11 08:06:59,445 - INFO - > Epoch 66: took 73.7s (avg 75.7s) | Best so far: epoch 62	train_loss: 0.6743 train_accuracy-SBM: 0.7534	val_loss: 0.6680 val_accuracy-SBM: 0.7581	test_loss: 0.6698 test_accuracy-SBM: 0.7544
2025-07-11 08:08:07,686 - INFO - train: {'epoch': 67, 'time_epoch': 67.98793, 'eta': 2186.67282, 'eta_hours': 0.60741, 'loss': 0.66596391, 'lr': 0.00026933, 'params': 425270, 'time_iter': 0.10878, 'accuracy': 0.75677, 'f1': 0.75677, 'accuracy-SBM': 0.75677, 'auc': 0.95559}
2025-07-11 08:08:11,059 - INFO - val: {'epoch': 67, 'time_epoch': 3.32707, 'loss': 0.68998757, 'lr': 0, 'params': 425270, 'time_iter': 0.05281, 'accuracy': 0.75476, 'f1': 0.75461, 'accuracy-SBM': 0.75455, 'auc': 0.95265}
2025-07-11 08:08:14,449 - INFO - test: {'epoch': 67, 'time_epoch': 3.34341, 'loss': 0.66236621, 'lr': 0, 'params': 425270, 'time_iter': 0.05307, 'accuracy': 0.75995, 'f1': 0.75995, 'accuracy-SBM': 0.75988, 'auc': 0.9562}
2025-07-11 08:08:14,469 - INFO - > Epoch 67: took 75.0s (avg 75.7s) | Best so far: epoch 62	train_loss: 0.6743 train_accuracy-SBM: 0.7534	val_loss: 0.6680 val_accuracy-SBM: 0.7581	test_loss: 0.6698 test_accuracy-SBM: 0.7544
2025-07-11 08:09:18,057 - INFO - train: {'epoch': 68, 'time_epoch': 63.32941, 'eta': 2116.09107, 'eta_hours': 0.5878, 'loss': 0.66253841, 'lr': 0.00025479, 'params': 425270, 'time_iter': 0.10133, 'accuracy': 0.75821, 'f1': 0.75821, 'accuracy-SBM': 0.75821, 'auc': 0.95604}
2025-07-11 08:09:21,111 - INFO - val: {'epoch': 68, 'time_epoch': 3.01014, 'loss': 0.66275486, 'lr': 0, 'params': 425270, 'time_iter': 0.04778, 'accuracy': 0.75951, 'f1': 0.75944, 'accuracy-SBM': 0.75941, 'auc': 0.95623}
2025-07-11 08:09:24,154 - INFO - test: {'epoch': 68, 'time_epoch': 3.01154, 'loss': 0.66259292, 'lr': 0, 'params': 425270, 'time_iter': 0.0478, 'accuracy': 0.75957, 'f1': 0.75956, 'accuracy-SBM': 0.75958, 'auc': 0.95628}
2025-07-11 08:09:24,156 - INFO - > Epoch 68: took 69.7s (avg 75.6s) | Best so far: epoch 68	train_loss: 0.6625 train_accuracy-SBM: 0.7582	val_loss: 0.6628 val_accuracy-SBM: 0.7594	test_loss: 0.6626 test_accuracy-SBM: 0.7596
2025-07-11 08:10:25,365 - INFO - train: {'epoch': 69, 'time_epoch': 60.8857, 'eta': 2044.66923, 'eta_hours': 0.56796, 'loss': 0.65883747, 'lr': 0.00024052, 'params': 425270, 'time_iter': 0.09742, 'accuracy': 0.7595, 'f1': 0.75949, 'accuracy-SBM': 0.7595, 'auc': 0.95654}
2025-07-11 08:10:28,324 - INFO - val: {'epoch': 69, 'time_epoch': 2.9167, 'loss': 0.65881016, 'lr': 0, 'params': 425270, 'time_iter': 0.0463, 'accuracy': 0.76137, 'f1': 0.76125, 'accuracy-SBM': 0.76128, 'auc': 0.95669}
2025-07-11 08:10:31,269 - INFO - test: {'epoch': 69, 'time_epoch': 2.91389, 'loss': 0.66359671, 'lr': 0, 'params': 425270, 'time_iter': 0.04625, 'accuracy': 0.75932, 'f1': 0.75932, 'accuracy-SBM': 0.75941, 'auc': 0.95614}
2025-07-11 08:10:31,271 - INFO - > Epoch 69: took 67.1s (avg 75.4s) | Best so far: epoch 69	train_loss: 0.6588 train_accuracy-SBM: 0.7595	val_loss: 0.6588 val_accuracy-SBM: 0.7613	test_loss: 0.6636 test_accuracy-SBM: 0.7594
2025-07-11 08:11:31,499 - INFO - train: {'epoch': 70, 'time_epoch': 60.00788, 'eta': 1973.18563, 'eta_hours': 0.54811, 'loss': 0.65650049, 'lr': 0.00022653, 'params': 425270, 'time_iter': 0.09601, 'accuracy': 0.75984, 'f1': 0.75983, 'accuracy-SBM': 0.75983, 'auc': 0.95685}
2025-07-11 08:11:34,462 - INFO - val: {'epoch': 70, 'time_epoch': 2.9207, 'loss': 0.66441966, 'lr': 0, 'params': 425270, 'time_iter': 0.04636, 'accuracy': 0.75937, 'f1': 0.7592, 'accuracy-SBM': 0.75915, 'auc': 0.95595}
2025-07-11 08:11:37,398 - INFO - test: {'epoch': 70, 'time_epoch': 2.90249, 'loss': 0.65317687, 'lr': 0, 'params': 425270, 'time_iter': 0.04607, 'accuracy': 0.76181, 'f1': 0.76179, 'accuracy-SBM': 0.76179, 'auc': 0.95744}
2025-07-11 08:11:37,400 - INFO - > Epoch 70: took 66.1s (avg 75.3s) | Best so far: epoch 69	train_loss: 0.6588 train_accuracy-SBM: 0.7595	val_loss: 0.6588 val_accuracy-SBM: 0.7613	test_loss: 0.6636 test_accuracy-SBM: 0.7594
2025-07-11 08:12:38,261 - INFO - train: {'epoch': 71, 'time_epoch': 60.63756, 'eta': 1902.26567, 'eta_hours': 0.52841, 'loss': 0.65581784, 'lr': 0.00021284, 'params': 425270, 'time_iter': 0.09702, 'accuracy': 0.76053, 'f1': 0.76052, 'accuracy-SBM': 0.76053, 'auc': 0.95696}
2025-07-11 08:12:41,238 - INFO - val: {'epoch': 71, 'time_epoch': 2.9353, 'loss': 0.65950969, 'lr': 0, 'params': 425270, 'time_iter': 0.04659, 'accuracy': 0.75986, 'f1': 0.75977, 'accuracy-SBM': 0.75975, 'auc': 0.95664}
2025-07-11 08:12:44,191 - INFO - test: {'epoch': 71, 'time_epoch': 2.92195, 'loss': 0.65466477, 'lr': 0, 'params': 425270, 'time_iter': 0.04638, 'accuracy': 0.76144, 'f1': 0.76147, 'accuracy-SBM': 0.76145, 'auc': 0.95729}
2025-07-11 08:12:44,193 - INFO - > Epoch 71: took 66.8s (avg 75.2s) | Best so far: epoch 69	train_loss: 0.6588 train_accuracy-SBM: 0.7595	val_loss: 0.6588 val_accuracy-SBM: 0.7613	test_loss: 0.6636 test_accuracy-SBM: 0.7594
2025-07-11 08:13:44,981 - INFO - train: {'epoch': 72, 'time_epoch': 60.56285, 'eta': 1831.5998, 'eta_hours': 0.50878, 'loss': 0.65368249, 'lr': 0.00019946, 'params': 425270, 'time_iter': 0.0969, 'accuracy': 0.76096, 'f1': 0.76096, 'accuracy-SBM': 0.76096, 'auc': 0.95723}
2025-07-11 08:13:47,950 - INFO - val: {'epoch': 72, 'time_epoch': 2.92751, 'loss': 0.6561979, 'lr': 0, 'params': 425270, 'time_iter': 0.04647, 'accuracy': 0.76194, 'f1': 0.76183, 'accuracy-SBM': 0.76181, 'auc': 0.95685}
2025-07-11 08:13:50,912 - INFO - test: {'epoch': 72, 'time_epoch': 2.93103, 'loss': 0.64887857, 'lr': 0, 'params': 425270, 'time_iter': 0.04652, 'accuracy': 0.76376, 'f1': 0.76374, 'accuracy-SBM': 0.76377, 'auc': 0.95793}
2025-07-11 08:13:50,914 - INFO - > Epoch 72: took 66.7s (avg 75.1s) | Best so far: epoch 72	train_loss: 0.6537 train_accuracy-SBM: 0.7610	val_loss: 0.6562 val_accuracy-SBM: 0.7618	test_loss: 0.6489 test_accuracy-SBM: 0.7638
2025-07-11 08:14:51,658 - INFO - train: {'epoch': 73, 'time_epoch': 60.32143, 'eta': 1761.12215, 'eta_hours': 0.4892, 'loss': 0.65111931, 'lr': 0.00018641, 'params': 425270, 'time_iter': 0.09651, 'accuracy': 0.76202, 'f1': 0.76202, 'accuracy-SBM': 0.76202, 'auc': 0.95757}
2025-07-11 08:14:54,641 - INFO - val: {'epoch': 73, 'time_epoch': 2.93971, 'loss': 0.6698798, 'lr': 0, 'params': 425270, 'time_iter': 0.04666, 'accuracy': 0.76001, 'f1': 0.75982, 'accuracy-SBM': 0.75983, 'auc': 0.95537}
2025-07-11 08:14:57,613 - INFO - test: {'epoch': 73, 'time_epoch': 2.93429, 'loss': 0.65517299, 'lr': 0, 'params': 425270, 'time_iter': 0.04658, 'accuracy': 0.76183, 'f1': 0.76178, 'accuracy-SBM': 0.76183, 'auc': 0.95731}
2025-07-11 08:14:57,615 - INFO - > Epoch 73: took 66.7s (avg 75.0s) | Best so far: epoch 72	train_loss: 0.6537 train_accuracy-SBM: 0.7610	val_loss: 0.6562 val_accuracy-SBM: 0.7618	test_loss: 0.6489 test_accuracy-SBM: 0.7638
2025-07-11 08:15:58,989 - INFO - train: {'epoch': 74, 'time_epoch': 61.14413, 'eta': 1691.18957, 'eta_hours': 0.46977, 'loss': 0.65086266, 'lr': 0.00017371, 'params': 425270, 'time_iter': 0.09783, 'accuracy': 0.76223, 'f1': 0.76222, 'accuracy-SBM': 0.76223, 'auc': 0.95759}
2025-07-11 08:16:01,990 - INFO - val: {'epoch': 74, 'time_epoch': 2.95982, 'loss': 0.6571894, 'lr': 0, 'params': 425270, 'time_iter': 0.04698, 'accuracy': 0.76143, 'f1': 0.76129, 'accuracy-SBM': 0.76129, 'auc': 0.95672}
2025-07-11 08:16:04,927 - INFO - test: {'epoch': 74, 'time_epoch': 2.906, 'loss': 0.64912011, 'lr': 0, 'params': 425270, 'time_iter': 0.04613, 'accuracy': 0.76256, 'f1': 0.76255, 'accuracy-SBM': 0.76258, 'auc': 0.95791}
2025-07-11 08:16:04,929 - INFO - > Epoch 74: took 67.3s (avg 74.9s) | Best so far: epoch 72	train_loss: 0.6537 train_accuracy-SBM: 0.7610	val_loss: 0.6562 val_accuracy-SBM: 0.7618	test_loss: 0.6489 test_accuracy-SBM: 0.7638
2025-07-11 08:17:05,688 - INFO - train: {'epoch': 75, 'time_epoch': 60.5353, 'eta': 1621.29601, 'eta_hours': 0.45036, 'loss': 0.6489202, 'lr': 0.00016136, 'params': 425270, 'time_iter': 0.09686, 'accuracy': 0.76346, 'f1': 0.76345, 'accuracy-SBM': 0.76346, 'auc': 0.95783}
2025-07-11 08:17:08,658 - INFO - val: {'epoch': 75, 'time_epoch': 2.92874, 'loss': 0.65900344, 'lr': 0, 'params': 425270, 'time_iter': 0.04649, 'accuracy': 0.75984, 'f1': 0.75979, 'accuracy-SBM': 0.75981, 'auc': 0.95663}
2025-07-11 08:17:11,625 - INFO - test: {'epoch': 75, 'time_epoch': 2.9288, 'loss': 0.64903048, 'lr': 0, 'params': 425270, 'time_iter': 0.04649, 'accuracy': 0.76313, 'f1': 0.76313, 'accuracy-SBM': 0.76313, 'auc': 0.95794}
2025-07-11 08:17:11,627 - INFO - > Epoch 75: took 66.7s (avg 74.8s) | Best so far: epoch 72	train_loss: 0.6537 train_accuracy-SBM: 0.7610	val_loss: 0.6562 val_accuracy-SBM: 0.7618	test_loss: 0.6489 test_accuracy-SBM: 0.7638
2025-07-11 08:18:12,322 - INFO - train: {'epoch': 76, 'time_epoch': 60.46481, 'eta': 1551.62445, 'eta_hours': 0.43101, 'loss': 0.64684624, 'lr': 0.00014938, 'params': 425270, 'time_iter': 0.09674, 'accuracy': 0.7639, 'f1': 0.7639, 'accuracy-SBM': 0.7639, 'auc': 0.95812}
2025-07-11 08:18:15,290 - INFO - val: {'epoch': 76, 'time_epoch': 2.92529, 'loss': 0.65796038, 'lr': 0, 'params': 425270, 'time_iter': 0.04643, 'accuracy': 0.76101, 'f1': 0.76086, 'accuracy-SBM': 0.76082, 'auc': 0.95671}
2025-07-11 08:18:18,226 - INFO - test: {'epoch': 76, 'time_epoch': 2.90542, 'loss': 0.65284129, 'lr': 0, 'params': 425270, 'time_iter': 0.04612, 'accuracy': 0.7616, 'f1': 0.7616, 'accuracy-SBM': 0.76161, 'auc': 0.95744}
2025-07-11 08:18:18,228 - INFO - > Epoch 76: took 66.6s (avg 74.7s) | Best so far: epoch 72	train_loss: 0.6537 train_accuracy-SBM: 0.7610	val_loss: 0.6562 val_accuracy-SBM: 0.7618	test_loss: 0.6489 test_accuracy-SBM: 0.7638
2025-07-11 08:19:19,004 - INFO - train: {'epoch': 77, 'time_epoch': 60.55083, 'eta': 1482.21324, 'eta_hours': 0.41173, 'loss': 0.64466092, 'lr': 0.00013779, 'params': 425270, 'time_iter': 0.09688, 'accuracy': 0.76432, 'f1': 0.76432, 'accuracy-SBM': 0.76432, 'auc': 0.95842}
2025-07-11 08:19:21,999 - INFO - val: {'epoch': 77, 'time_epoch': 2.95015, 'loss': 0.65750076, 'lr': 0, 'params': 425270, 'time_iter': 0.04683, 'accuracy': 0.76335, 'f1': 0.76318, 'accuracy-SBM': 0.76314, 'auc': 0.95677}
2025-07-11 08:19:24,976 - INFO - test: {'epoch': 77, 'time_epoch': 2.94662, 'loss': 0.65292739, 'lr': 0, 'params': 425270, 'time_iter': 0.04677, 'accuracy': 0.76285, 'f1': 0.76284, 'accuracy-SBM': 0.76284, 'auc': 0.95745}
2025-07-11 08:19:24,979 - INFO - > Epoch 77: took 66.8s (avg 74.6s) | Best so far: epoch 77	train_loss: 0.6447 train_accuracy-SBM: 0.7643	val_loss: 0.6575 val_accuracy-SBM: 0.7631	test_loss: 0.6529 test_accuracy-SBM: 0.7628
2025-07-11 08:20:25,755 - INFO - train: {'epoch': 78, 'time_epoch': 60.55117, 'eta': 1413.02642, 'eta_hours': 0.39251, 'loss': 0.64281411, 'lr': 0.00012659, 'params': 425270, 'time_iter': 0.09688, 'accuracy': 0.76515, 'f1': 0.76515, 'accuracy-SBM': 0.76515, 'auc': 0.95865}
2025-07-11 08:20:28,729 - INFO - val: {'epoch': 78, 'time_epoch': 2.93267, 'loss': 0.65267223, 'lr': 0, 'params': 425270, 'time_iter': 0.04655, 'accuracy': 0.76341, 'f1': 0.76338, 'accuracy-SBM': 0.7634, 'auc': 0.9574}
2025-07-11 08:20:31,691 - INFO - test: {'epoch': 78, 'time_epoch': 2.9315, 'loss': 0.63897613, 'lr': 0, 'params': 425270, 'time_iter': 0.04653, 'accuracy': 0.76688, 'f1': 0.76688, 'accuracy-SBM': 0.76694, 'auc': 0.95924}
2025-07-11 08:20:31,693 - INFO - > Epoch 78: took 66.7s (avg 74.5s) | Best so far: epoch 78	train_loss: 0.6428 train_accuracy-SBM: 0.7651	val_loss: 0.6527 val_accuracy-SBM: 0.7634	test_loss: 0.6390 test_accuracy-SBM: 0.7669
2025-07-11 08:21:32,182 - INFO - train: {'epoch': 79, 'time_epoch': 60.26765, 'eta': 1343.98462, 'eta_hours': 0.37333, 'loss': 0.64151078, 'lr': 0.0001158, 'params': 425270, 'time_iter': 0.09643, 'accuracy': 0.76595, 'f1': 0.76595, 'accuracy-SBM': 0.76595, 'auc': 0.95882}
2025-07-11 08:21:35,174 - INFO - val: {'epoch': 79, 'time_epoch': 2.95018, 'loss': 0.65691311, 'lr': 0, 'params': 425270, 'time_iter': 0.04683, 'accuracy': 0.7638, 'f1': 0.76363, 'accuracy-SBM': 0.76359, 'auc': 0.95692}
2025-07-11 08:21:38,137 - INFO - test: {'epoch': 79, 'time_epoch': 2.93237, 'loss': 0.6444824, 'lr': 0, 'params': 425270, 'time_iter': 0.04655, 'accuracy': 0.76395, 'f1': 0.76397, 'accuracy-SBM': 0.76396, 'auc': 0.95857}
2025-07-11 08:21:38,139 - INFO - > Epoch 79: took 66.4s (avg 74.4s) | Best so far: epoch 79	train_loss: 0.6415 train_accuracy-SBM: 0.7660	val_loss: 0.6569 val_accuracy-SBM: 0.7636	test_loss: 0.6445 test_accuracy-SBM: 0.7640
2025-07-11 08:22:40,341 - INFO - train: {'epoch': 80, 'time_epoch': 61.97058, 'eta': 1275.55892, 'eta_hours': 0.35432, 'loss': 0.63830383, 'lr': 0.00010543, 'params': 425270, 'time_iter': 0.09915, 'accuracy': 0.76678, 'f1': 0.76678, 'accuracy-SBM': 0.76678, 'auc': 0.95922}
2025-07-11 08:22:43,502 - INFO - val: {'epoch': 80, 'time_epoch': 3.1174, 'loss': 0.65871447, 'lr': 0, 'params': 425270, 'time_iter': 0.04948, 'accuracy': 0.76167, 'f1': 0.7615, 'accuracy-SBM': 0.76146, 'auc': 0.95678}
2025-07-11 08:22:46,774 - INFO - test: {'epoch': 80, 'time_epoch': 3.23497, 'loss': 0.64067514, 'lr': 0, 'params': 425270, 'time_iter': 0.05135, 'accuracy': 0.76677, 'f1': 0.76677, 'accuracy-SBM': 0.76672, 'auc': 0.95908}
2025-07-11 08:22:46,776 - INFO - > Epoch 80: took 68.6s (avg 74.3s) | Best so far: epoch 79	train_loss: 0.6415 train_accuracy-SBM: 0.7660	val_loss: 0.6569 val_accuracy-SBM: 0.7636	test_loss: 0.6445 test_accuracy-SBM: 0.7640
2025-07-11 08:23:51,530 - INFO - train: {'epoch': 81, 'time_epoch': 64.52843, 'eta': 1207.85213, 'eta_hours': 0.33551, 'loss': 0.63855594, 'lr': 9.549e-05, 'params': 425270, 'time_iter': 0.10325, 'accuracy': 0.76676, 'f1': 0.76676, 'accuracy-SBM': 0.76676, 'auc': 0.9592}
2025-07-11 08:23:54,619 - INFO - val: {'epoch': 81, 'time_epoch': 3.04513, 'loss': 0.65158869, 'lr': 0, 'params': 425270, 'time_iter': 0.04834, 'accuracy': 0.76411, 'f1': 0.76396, 'accuracy-SBM': 0.76393, 'auc': 0.9576}
2025-07-11 08:23:57,705 - INFO - test: {'epoch': 81, 'time_epoch': 3.05541, 'loss': 0.64373716, 'lr': 0, 'params': 425270, 'time_iter': 0.0485, 'accuracy': 0.76533, 'f1': 0.76533, 'accuracy-SBM': 0.76528, 'auc': 0.95865}
2025-07-11 08:23:57,707 - INFO - > Epoch 81: took 70.9s (avg 74.2s) | Best so far: epoch 81	train_loss: 0.6386 train_accuracy-SBM: 0.7668	val_loss: 0.6516 val_accuracy-SBM: 0.7639	test_loss: 0.6437 test_accuracy-SBM: 0.7653
2025-07-11 08:24:59,947 - INFO - train: {'epoch': 82, 'time_epoch': 62.01026, 'eta': 1139.70616, 'eta_hours': 0.31659, 'loss': 0.63684368, 'lr': 8.6e-05, 'params': 425270, 'time_iter': 0.09922, 'accuracy': 0.76776, 'f1': 0.76776, 'accuracy-SBM': 0.76776, 'auc': 0.95941}
2025-07-11 08:25:03,108 - INFO - val: {'epoch': 82, 'time_epoch': 3.11948, 'loss': 0.65511113, 'lr': 0, 'params': 425270, 'time_iter': 0.04952, 'accuracy': 0.76269, 'f1': 0.76263, 'accuracy-SBM': 0.76262, 'auc': 0.95718}
2025-07-11 08:25:06,175 - INFO - test: {'epoch': 82, 'time_epoch': 3.03434, 'loss': 0.64689031, 'lr': 0, 'params': 425270, 'time_iter': 0.04816, 'accuracy': 0.76601, 'f1': 0.76598, 'accuracy-SBM': 0.76603, 'auc': 0.95824}
2025-07-11 08:25:06,177 - INFO - > Epoch 82: took 68.5s (avg 74.2s) | Best so far: epoch 81	train_loss: 0.6386 train_accuracy-SBM: 0.7668	val_loss: 0.6516 val_accuracy-SBM: 0.7639	test_loss: 0.6437 test_accuracy-SBM: 0.7653
2025-07-11 08:26:08,901 - INFO - train: {'epoch': 83, 'time_epoch': 62.49669, 'eta': 1071.79894, 'eta_hours': 0.29772, 'loss': 0.63514501, 'lr': 7.695e-05, 'params': 425270, 'time_iter': 0.09999, 'accuracy': 0.76841, 'f1': 0.7684, 'accuracy-SBM': 0.76841, 'auc': 0.95963}
2025-07-11 08:26:12,087 - INFO - val: {'epoch': 83, 'time_epoch': 3.14375, 'loss': 0.65637741, 'lr': 0, 'params': 425270, 'time_iter': 0.0499, 'accuracy': 0.76332, 'f1': 0.76319, 'accuracy-SBM': 0.76319, 'auc': 0.95697}
2025-07-11 08:26:15,216 - INFO - test: {'epoch': 83, 'time_epoch': 3.0969, 'loss': 0.63904072, 'lr': 0, 'params': 425270, 'time_iter': 0.04916, 'accuracy': 0.76633, 'f1': 0.76632, 'accuracy-SBM': 0.76636, 'auc': 0.95924}
2025-07-11 08:26:15,218 - INFO - > Epoch 83: took 69.0s (avg 74.1s) | Best so far: epoch 81	train_loss: 0.6386 train_accuracy-SBM: 0.7668	val_loss: 0.6516 val_accuracy-SBM: 0.7639	test_loss: 0.6437 test_accuracy-SBM: 0.7653
2025-07-11 08:27:16,615 - INFO - train: {'epoch': 84, 'time_epoch': 61.17054, 'eta': 1003.785, 'eta_hours': 0.27883, 'loss': 0.6323047, 'lr': 6.837e-05, 'params': 425270, 'time_iter': 0.09787, 'accuracy': 0.76929, 'f1': 0.76928, 'accuracy-SBM': 0.76929, 'auc': 0.95998}
2025-07-11 08:27:19,599 - INFO - val: {'epoch': 84, 'time_epoch': 2.94256, 'loss': 0.65070892, 'lr': 0, 'params': 425270, 'time_iter': 0.04671, 'accuracy': 0.76524, 'f1': 0.76509, 'accuracy-SBM': 0.76505, 'auc': 0.95768}
2025-07-11 08:27:22,564 - INFO - test: {'epoch': 84, 'time_epoch': 2.93311, 'loss': 0.64010416, 'lr': 0, 'params': 425270, 'time_iter': 0.04656, 'accuracy': 0.76627, 'f1': 0.76627, 'accuracy-SBM': 0.76628, 'auc': 0.95909}
2025-07-11 08:27:22,566 - INFO - > Epoch 84: took 67.3s (avg 74.0s) | Best so far: epoch 84	train_loss: 0.6323 train_accuracy-SBM: 0.7693	val_loss: 0.6507 val_accuracy-SBM: 0.7651	test_loss: 0.6401 test_accuracy-SBM: 0.7663
2025-07-11 08:28:23,323 - INFO - train: {'epoch': 85, 'time_epoch': 60.5303, 'eta': 935.82598, 'eta_hours': 0.25995, 'loss': 0.63319416, 'lr': 6.026e-05, 'params': 425270, 'time_iter': 0.09685, 'accuracy': 0.76878, 'f1': 0.76877, 'accuracy-SBM': 0.76878, 'auc': 0.95989}
2025-07-11 08:28:26,399 - INFO - val: {'epoch': 85, 'time_epoch': 3.03457, 'loss': 0.65035314, 'lr': 0, 'params': 425270, 'time_iter': 0.04817, 'accuracy': 0.76416, 'f1': 0.76406, 'accuracy-SBM': 0.76404, 'auc': 0.95776}
2025-07-11 08:28:29,347 - INFO - test: {'epoch': 85, 'time_epoch': 2.91643, 'loss': 0.64023999, 'lr': 0, 'params': 425270, 'time_iter': 0.04629, 'accuracy': 0.76593, 'f1': 0.76593, 'accuracy-SBM': 0.76598, 'auc': 0.95908}
2025-07-11 08:28:29,349 - INFO - > Epoch 85: took 66.8s (avg 73.9s) | Best so far: epoch 84	train_loss: 0.6323 train_accuracy-SBM: 0.7693	val_loss: 0.6507 val_accuracy-SBM: 0.7651	test_loss: 0.6401 test_accuracy-SBM: 0.7663
2025-07-11 08:29:30,520 - INFO - train: {'epoch': 86, 'time_epoch': 60.93631, 'eta': 868.0984, 'eta_hours': 0.24114, 'loss': 0.63141477, 'lr': 5.264e-05, 'params': 425270, 'time_iter': 0.0975, 'accuracy': 0.76956, 'f1': 0.76956, 'accuracy-SBM': 0.76956, 'auc': 0.96012}
2025-07-11 08:29:33,513 - INFO - val: {'epoch': 86, 'time_epoch': 2.95207, 'loss': 0.64853938, 'lr': 0, 'params': 425270, 'time_iter': 0.04686, 'accuracy': 0.76563, 'f1': 0.76553, 'accuracy-SBM': 0.76554, 'auc': 0.95802}
2025-07-11 08:29:36,471 - INFO - test: {'epoch': 86, 'time_epoch': 2.92694, 'loss': 0.63810146, 'lr': 0, 'params': 425270, 'time_iter': 0.04646, 'accuracy': 0.7677, 'f1': 0.76769, 'accuracy-SBM': 0.76776, 'auc': 0.95939}
2025-07-11 08:29:36,473 - INFO - > Epoch 86: took 67.1s (avg 73.9s) | Best so far: epoch 86	train_loss: 0.6314 train_accuracy-SBM: 0.7696	val_loss: 0.6485 val_accuracy-SBM: 0.7655	test_loss: 0.6381 test_accuracy-SBM: 0.7678
2025-07-11 08:30:37,560 - INFO - train: {'epoch': 87, 'time_epoch': 60.86151, 'eta': 800.51497, 'eta_hours': 0.22237, 'loss': 0.62904264, 'lr': 4.55e-05, 'params': 425270, 'time_iter': 0.09738, 'accuracy': 0.77068, 'f1': 0.77068, 'accuracy-SBM': 0.77068, 'auc': 0.96041}
2025-07-11 08:30:40,516 - INFO - val: {'epoch': 87, 'time_epoch': 2.91427, 'loss': 0.6512328, 'lr': 0, 'params': 425270, 'time_iter': 0.04626, 'accuracy': 0.76546, 'f1': 0.76535, 'accuracy-SBM': 0.76534, 'auc': 0.95759}
2025-07-11 08:30:43,455 - INFO - test: {'epoch': 87, 'time_epoch': 2.90119, 'loss': 0.63788147, 'lr': 0, 'params': 425270, 'time_iter': 0.04605, 'accuracy': 0.76726, 'f1': 0.76725, 'accuracy-SBM': 0.76728, 'auc': 0.95934}
2025-07-11 08:30:43,457 - INFO - > Epoch 87: took 67.0s (avg 73.8s) | Best so far: epoch 86	train_loss: 0.6314 train_accuracy-SBM: 0.7696	val_loss: 0.6485 val_accuracy-SBM: 0.7655	test_loss: 0.6381 test_accuracy-SBM: 0.7678
2025-07-11 08:31:43,903 - INFO - train: {'epoch': 88, 'time_epoch': 60.2187, 'eta': 733.00314, 'eta_hours': 0.20361, 'loss': 0.62813528, 'lr': 3.886e-05, 'params': 425270, 'time_iter': 0.09635, 'accuracy': 0.77059, 'f1': 0.77058, 'accuracy-SBM': 0.77059, 'auc': 0.96052}
2025-07-11 08:31:46,874 - INFO - val: {'epoch': 88, 'time_epoch': 2.92908, 'loss': 0.65008485, 'lr': 0, 'params': 425270, 'time_iter': 0.04649, 'accuracy': 0.76501, 'f1': 0.76495, 'accuracy-SBM': 0.76496, 'auc': 0.95774}
2025-07-11 08:31:49,812 - INFO - test: {'epoch': 88, 'time_epoch': 2.90704, 'loss': 0.63900319, 'lr': 0, 'params': 425270, 'time_iter': 0.04614, 'accuracy': 0.76734, 'f1': 0.76734, 'accuracy-SBM': 0.76736, 'auc': 0.9592}
2025-07-11 08:31:49,815 - INFO - > Epoch 88: took 66.4s (avg 73.7s) | Best so far: epoch 86	train_loss: 0.6314 train_accuracy-SBM: 0.7696	val_loss: 0.6485 val_accuracy-SBM: 0.7655	test_loss: 0.6381 test_accuracy-SBM: 0.7678
2025-07-11 08:32:50,486 - INFO - train: {'epoch': 89, 'time_epoch': 60.44718, 'eta': 665.67877, 'eta_hours': 0.18491, 'loss': 0.62739809, 'lr': 3.272e-05, 'params': 425270, 'time_iter': 0.09672, 'accuracy': 0.77132, 'f1': 0.77131, 'accuracy-SBM': 0.77132, 'auc': 0.9606}
2025-07-11 08:32:53,467 - INFO - val: {'epoch': 89, 'time_epoch': 2.93927, 'loss': 0.65152462, 'lr': 0, 'params': 425270, 'time_iter': 0.04666, 'accuracy': 0.76625, 'f1': 0.76613, 'accuracy-SBM': 0.76612, 'auc': 0.95759}
2025-07-11 08:32:56,466 - INFO - test: {'epoch': 89, 'time_epoch': 2.96716, 'loss': 0.63812176, 'lr': 0, 'params': 425270, 'time_iter': 0.0471, 'accuracy': 0.76746, 'f1': 0.76747, 'accuracy-SBM': 0.76749, 'auc': 0.95937}
2025-07-11 08:32:56,468 - INFO - > Epoch 89: took 66.7s (avg 73.6s) | Best so far: epoch 89	train_loss: 0.6274 train_accuracy-SBM: 0.7713	val_loss: 0.6515 val_accuracy-SBM: 0.7661	test_loss: 0.6381 test_accuracy-SBM: 0.7675
2025-07-11 08:33:57,174 - INFO - train: {'epoch': 90, 'time_epoch': 60.46384, 'eta': 598.5072, 'eta_hours': 0.16625, 'loss': 0.62712562, 'lr': 2.709e-05, 'params': 425270, 'time_iter': 0.09674, 'accuracy': 0.77084, 'f1': 0.77083, 'accuracy-SBM': 0.77084, 'auc': 0.96066}
2025-07-11 08:34:00,148 - INFO - val: {'epoch': 90, 'time_epoch': 2.93224, 'loss': 0.64833008, 'lr': 0, 'params': 425270, 'time_iter': 0.04654, 'accuracy': 0.76647, 'f1': 0.76637, 'accuracy-SBM': 0.76636, 'auc': 0.95799}
2025-07-11 08:34:03,103 - INFO - test: {'epoch': 90, 'time_epoch': 2.92444, 'loss': 0.63847843, 'lr': 0, 'params': 425270, 'time_iter': 0.04642, 'accuracy': 0.7671, 'f1': 0.7671, 'accuracy-SBM': 0.76715, 'auc': 0.95928}
2025-07-11 08:34:03,105 - INFO - > Epoch 90: took 66.6s (avg 73.6s) | Best so far: epoch 90	train_loss: 0.6271 train_accuracy-SBM: 0.7708	val_loss: 0.6483 val_accuracy-SBM: 0.7664	test_loss: 0.6385 test_accuracy-SBM: 0.7671
2025-07-11 08:35:04,817 - INFO - train: {'epoch': 91, 'time_epoch': 61.48755, 'eta': 531.57047, 'eta_hours': 0.14766, 'loss': 0.62650321, 'lr': 2.198e-05, 'params': 425270, 'time_iter': 0.09838, 'accuracy': 0.77126, 'f1': 0.77126, 'accuracy-SBM': 0.77126, 'auc': 0.96073}
2025-07-11 08:35:07,851 - INFO - val: {'epoch': 91, 'time_epoch': 2.98987, 'loss': 0.65361859, 'lr': 0, 'params': 425270, 'time_iter': 0.04746, 'accuracy': 0.76592, 'f1': 0.76582, 'accuracy-SBM': 0.76581, 'auc': 0.95738}
2025-07-11 08:35:10,863 - INFO - test: {'epoch': 91, 'time_epoch': 2.97975, 'loss': 0.63967807, 'lr': 0, 'params': 425270, 'time_iter': 0.0473, 'accuracy': 0.76764, 'f1': 0.76764, 'accuracy-SBM': 0.76768, 'auc': 0.95919}
2025-07-11 08:35:10,865 - INFO - > Epoch 91: took 67.8s (avg 73.5s) | Best so far: epoch 90	train_loss: 0.6271 train_accuracy-SBM: 0.7708	val_loss: 0.6483 val_accuracy-SBM: 0.7664	test_loss: 0.6385 test_accuracy-SBM: 0.7671
2025-07-11 08:36:14,894 - INFO - train: {'epoch': 92, 'time_epoch': 63.69668, 'eta': 464.9172, 'eta_hours': 0.12914, 'loss': 0.62610465, 'lr': 1.74e-05, 'params': 425270, 'time_iter': 0.10191, 'accuracy': 0.77148, 'f1': 0.77148, 'accuracy-SBM': 0.77148, 'auc': 0.96078}
2025-07-11 08:36:18,305 - INFO - val: {'epoch': 92, 'time_epoch': 3.36623, 'loss': 0.65069678, 'lr': 0, 'params': 425270, 'time_iter': 0.05343, 'accuracy': 0.76702, 'f1': 0.7669, 'accuracy-SBM': 0.7669, 'auc': 0.95773}
2025-07-11 08:36:21,709 - INFO - test: {'epoch': 92, 'time_epoch': 3.37181, 'loss': 0.63955363, 'lr': 0, 'params': 425270, 'time_iter': 0.05352, 'accuracy': 0.76674, 'f1': 0.76674, 'accuracy-SBM': 0.76677, 'auc': 0.9592}
2025-07-11 08:36:21,711 - INFO - > Epoch 92: took 70.8s (avg 73.5s) | Best so far: epoch 92	train_loss: 0.6261 train_accuracy-SBM: 0.7715	val_loss: 0.6507 val_accuracy-SBM: 0.7669	test_loss: 0.6396 test_accuracy-SBM: 0.7668
2025-07-11 08:37:25,643 - INFO - train: {'epoch': 93, 'time_epoch': 63.70655, 'eta': 398.32746, 'eta_hours': 0.11065, 'loss': 0.62665229, 'lr': 1.334e-05, 'params': 425270, 'time_iter': 0.10193, 'accuracy': 0.77171, 'f1': 0.77171, 'accuracy-SBM': 0.77171, 'auc': 0.96071}
2025-07-11 08:37:28,650 - INFO - val: {'epoch': 93, 'time_epoch': 2.96466, 'loss': 0.64865688, 'lr': 0, 'params': 425270, 'time_iter': 0.04706, 'accuracy': 0.76624, 'f1': 0.76613, 'accuracy-SBM': 0.76612, 'auc': 0.95801}
2025-07-11 08:37:31,674 - INFO - test: {'epoch': 93, 'time_epoch': 2.99169, 'loss': 0.63757065, 'lr': 0, 'params': 425270, 'time_iter': 0.04749, 'accuracy': 0.76764, 'f1': 0.76765, 'accuracy-SBM': 0.76766, 'auc': 0.95945}
2025-07-11 08:37:31,676 - INFO - > Epoch 93: took 70.0s (avg 73.4s) | Best so far: epoch 92	train_loss: 0.6261 train_accuracy-SBM: 0.7715	val_loss: 0.6507 val_accuracy-SBM: 0.7669	test_loss: 0.6396 test_accuracy-SBM: 0.7668
2025-07-11 08:38:36,746 - INFO - train: {'epoch': 94, 'time_epoch': 64.83825, 'eta': 331.85799, 'eta_hours': 0.09218, 'loss': 0.62470984, 'lr': 9.81e-06, 'params': 425270, 'time_iter': 0.10374, 'accuracy': 0.77197, 'f1': 0.77196, 'accuracy-SBM': 0.77197, 'auc': 0.96095}
2025-07-11 08:38:39,792 - INFO - val: {'epoch': 94, 'time_epoch': 3.00226, 'loss': 0.64647724, 'lr': 0, 'params': 425270, 'time_iter': 0.04765, 'accuracy': 0.76679, 'f1': 0.76666, 'accuracy-SBM': 0.76665, 'auc': 0.95823}
2025-07-11 08:38:42,803 - INFO - test: {'epoch': 94, 'time_epoch': 2.97298, 'loss': 0.6384737, 'lr': 0, 'params': 425270, 'time_iter': 0.04719, 'accuracy': 0.76703, 'f1': 0.76703, 'accuracy-SBM': 0.76705, 'auc': 0.95931}
2025-07-11 08:38:42,805 - INFO - > Epoch 94: took 71.1s (avg 73.4s) | Best so far: epoch 92	train_loss: 0.6261 train_accuracy-SBM: 0.7715	val_loss: 0.6507 val_accuracy-SBM: 0.7669	test_loss: 0.6396 test_accuracy-SBM: 0.7668
2025-07-11 08:39:43,457 - INFO - train: {'epoch': 95, 'time_epoch': 60.42335, 'eta': 265.23855, 'eta_hours': 0.07368, 'loss': 0.62480491, 'lr': 6.82e-06, 'params': 425270, 'time_iter': 0.09668, 'accuracy': 0.77214, 'f1': 0.77214, 'accuracy-SBM': 0.77214, 'auc': 0.96095}
2025-07-11 08:39:46,451 - INFO - val: {'epoch': 95, 'time_epoch': 2.94539, 'loss': 0.64879618, 'lr': 0, 'params': 425270, 'time_iter': 0.04675, 'accuracy': 0.76645, 'f1': 0.76636, 'accuracy-SBM': 0.76636, 'auc': 0.95797}
2025-07-11 08:39:49,406 - INFO - test: {'epoch': 95, 'time_epoch': 2.92441, 'loss': 0.63887178, 'lr': 0, 'params': 425270, 'time_iter': 0.04642, 'accuracy': 0.76718, 'f1': 0.76717, 'accuracy-SBM': 0.76721, 'auc': 0.95926}
2025-07-11 08:39:49,408 - INFO - > Epoch 95: took 66.6s (avg 73.3s) | Best so far: epoch 92	train_loss: 0.6261 train_accuracy-SBM: 0.7715	val_loss: 0.6507 val_accuracy-SBM: 0.7669	test_loss: 0.6396 test_accuracy-SBM: 0.7668
2025-07-11 08:40:50,219 - INFO - train: {'epoch': 96, 'time_epoch': 60.58166, 'eta': 198.75176, 'eta_hours': 0.05521, 'loss': 0.62388735, 'lr': 4.37e-06, 'params': 425270, 'time_iter': 0.09693, 'accuracy': 0.77204, 'f1': 0.77204, 'accuracy-SBM': 0.77204, 'auc': 0.96106}
2025-07-11 08:40:53,218 - INFO - val: {'epoch': 96, 'time_epoch': 2.95796, 'loss': 0.64937989, 'lr': 0, 'params': 425270, 'time_iter': 0.04695, 'accuracy': 0.76686, 'f1': 0.76677, 'accuracy-SBM': 0.76678, 'auc': 0.95793}
2025-07-11 08:40:56,211 - INFO - test: {'epoch': 96, 'time_epoch': 2.96242, 'loss': 0.6380355, 'lr': 0, 'params': 425270, 'time_iter': 0.04702, 'accuracy': 0.76744, 'f1': 0.76744, 'accuracy-SBM': 0.76748, 'auc': 0.95939}
2025-07-11 08:40:56,214 - INFO - > Epoch 96: took 66.8s (avg 73.3s) | Best so far: epoch 92	train_loss: 0.6261 train_accuracy-SBM: 0.7715	val_loss: 0.6507 val_accuracy-SBM: 0.7669	test_loss: 0.6396 test_accuracy-SBM: 0.7668
2025-07-11 08:41:57,737 - INFO - train: {'epoch': 97, 'time_epoch': 61.29569, 'eta': 132.40005, 'eta_hours': 0.03678, 'loss': 0.62265862, 'lr': 2.46e-06, 'params': 425270, 'time_iter': 0.09807, 'accuracy': 0.77284, 'f1': 0.77284, 'accuracy-SBM': 0.77284, 'auc': 0.96121}
2025-07-11 08:42:00,741 - INFO - val: {'epoch': 97, 'time_epoch': 2.96206, 'loss': 0.65082819, 'lr': 0, 'params': 425270, 'time_iter': 0.04702, 'accuracy': 0.7662, 'f1': 0.76611, 'accuracy-SBM': 0.7661, 'auc': 0.95773}
2025-07-11 08:42:03,869 - INFO - test: {'epoch': 97, 'time_epoch': 3.09701, 'loss': 0.63852367, 'lr': 0, 'params': 425270, 'time_iter': 0.04916, 'accuracy': 0.76771, 'f1': 0.76771, 'accuracy-SBM': 0.76775, 'auc': 0.95932}
2025-07-11 08:42:03,871 - INFO - > Epoch 97: took 67.7s (avg 73.2s) | Best so far: epoch 92	train_loss: 0.6261 train_accuracy-SBM: 0.7715	val_loss: 0.6507 val_accuracy-SBM: 0.7669	test_loss: 0.6396 test_accuracy-SBM: 0.7668
2025-07-11 08:43:04,544 - INFO - train: {'epoch': 98, 'time_epoch': 60.44714, 'eta': 66.14192, 'eta_hours': 0.01837, 'loss': 0.62472606, 'lr': 1.09e-06, 'params': 425270, 'time_iter': 0.09672, 'accuracy': 0.7722, 'f1': 0.77219, 'accuracy-SBM': 0.7722, 'auc': 0.96094}
2025-07-11 08:43:07,558 - INFO - val: {'epoch': 98, 'time_epoch': 2.97223, 'loss': 0.64582865, 'lr': 0, 'params': 425270, 'time_iter': 0.04718, 'accuracy': 0.76723, 'f1': 0.76712, 'accuracy-SBM': 0.76711, 'auc': 0.9583}
2025-07-11 08:43:10,538 - INFO - test: {'epoch': 98, 'time_epoch': 2.95049, 'loss': 0.63671148, 'lr': 0, 'params': 425270, 'time_iter': 0.04683, 'accuracy': 0.7681, 'f1': 0.7681, 'accuracy-SBM': 0.7681, 'auc': 0.95949}
2025-07-11 08:43:10,540 - INFO - > Epoch 98: took 66.7s (avg 73.1s) | Best so far: epoch 98	train_loss: 0.6247 train_accuracy-SBM: 0.7722	val_loss: 0.6458 val_accuracy-SBM: 0.7671	test_loss: 0.6367 test_accuracy-SBM: 0.7681
2025-07-11 08:44:11,238 - INFO - train: {'epoch': 99, 'time_epoch': 60.47827, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.6233098, 'lr': 2.7e-07, 'params': 425270, 'time_iter': 0.09677, 'accuracy': 0.7725, 'f1': 0.7725, 'accuracy-SBM': 0.7725, 'auc': 0.96113}
2025-07-11 08:44:14,212 - INFO - val: {'epoch': 99, 'time_epoch': 2.93173, 'loss': 0.65058393, 'lr': 0, 'params': 425270, 'time_iter': 0.04654, 'accuracy': 0.76653, 'f1': 0.76642, 'accuracy-SBM': 0.76641, 'auc': 0.95779}
2025-07-11 08:44:17,160 - INFO - test: {'epoch': 99, 'time_epoch': 2.91783, 'loss': 0.63841881, 'lr': 0, 'params': 425270, 'time_iter': 0.04631, 'accuracy': 0.76782, 'f1': 0.76781, 'accuracy-SBM': 0.76782, 'auc': 0.95935}
2025-07-11 08:44:17,329 - INFO - > Epoch 99: took 66.6s (avg 73.1s) | Best so far: epoch 98	train_loss: 0.6247 train_accuracy-SBM: 0.7722	val_loss: 0.6458 val_accuracy-SBM: 0.7671	test_loss: 0.6367 test_accuracy-SBM: 0.7681
2025-07-11 08:44:17,329 - INFO - Avg time per epoch: 73.07s
2025-07-11 08:44:17,329 - INFO - Total train loop time: 2.03h
2025-07-11 08:44:17,330 - INFO - Task done, results saved in results/Cluster/Cluster-GINE-41
2025-07-11 08:44:17,330 - INFO - Total time: 7373.59s (2.05h)
2025-07-11 08:44:17,353 - INFO - Results aggregated across runs saved in results/Cluster/Cluster-GINE-41/agg
2025-07-11 08:44:17,353 - INFO - === OPTIMIZED TRAINING COMPLETED SUCCESSFULLY! ===
2025-07-11 08:44:17,353 - INFO - Results saved in: results/Cluster/Cluster-GINE-41
2025-07-11 08:44:17,353 - INFO - Test results JSON files saved in: results/Cluster/Cluster-GINE-41/test_results/
Completed seed 41. Results saved in results/Cluster/Cluster-GINE-41
----------------------------------------
Submitting next job for seed 45
Submitted batch job 5348378
