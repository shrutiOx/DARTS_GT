Lmod has detected the following error: The following module(s) are unknown:
"cuda/12.0"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore-cache load "cuda/12.0"

Also make sure that all modulefiles written in TCL start with the string
#%Module



              total        used        free      shared  buff/cache   available
Mem:          376Gi        11Gi       310Gi       2.9Gi        53Gi       358Gi
Swap:         1.9Gi       196Mi       1.7Gi
Sun Aug  3 04:38:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:1C:00.0 Off |                    0 |
| N/A   30C    P0             42W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Running experiment with seed: 41
Starting training for seed 41...
Changed working directory to: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/RAND_GT
Loading config from: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/RAND_GT/confignas.yaml
Using device: cuda
2025-08-03 04:40:46,385 - INFO - GPU Mem: 34.1GB
2025-08-03 04:40:46,386 - INFO - Run directory: results/Cluster/Cluster-GINE-41
2025-08-03 04:40:46,386 - INFO - Seed: 41
2025-08-03 04:40:46,386 - INFO - === OPTIMIZED MOE CONFIGURATION ===
2025-08-03 04:40:46,386 - INFO - Routing mode: none
2025-08-03 04:40:46,386 - INFO - Expert types: ['GINE', 'CustomGatedGCN', 'GATV2']
2025-08-03 04:40:46,386 - INFO - Number of layers: 16
2025-08-03 04:40:46,386 - INFO - Uncertainty enabled: False
2025-08-03 04:40:46,386 - INFO - Training mode: custom
2025-08-03 04:40:46,386 - INFO - Optimization: Uncertainty only for last layer + test/val
2025-08-03 04:40:46,386 - INFO - Additional features: Router weights logging + JSON export
2025-08-03 04:41:02,336 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-08-03 04:41:02,374 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-08-03 04:41:02,376 - INFO -   undirected: True
2025-08-03 04:41:02,376 - INFO -   num graphs: 12000
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-08-03 04:41:02,376 - INFO -   avg num_nodes/graph: 117
2025-08-03 04:41:02,376 - INFO -   num node features: 7
2025-08-03 04:41:02,377 - INFO -   num edge features: 0
2025-08-03 04:41:02,378 - INFO -   num classes: 6
2025-08-03 04:41:02,378 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-08-03 04:41:02,378 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-08-03 04:41:02,387 - INFO -   ...estimated to be undirected: True

  0%|          | 0/12000 [00:00<?, ?it/s]
 14%|█▎        | 1620/12000 [00:10<01:04, 161.89it/s]
 27%|██▋       | 3277/12000 [00:20<00:53, 164.10it/s]
 40%|████      | 4801/12000 [00:30<00:45, 158.75it/s]
 54%|█████▍    | 6464/12000 [00:40<00:34, 161.72it/s]
 68%|██████▊   | 8100/12000 [00:50<00:24, 162.37it/s]
 81%|████████  | 9701/12000 [01:00<00:14, 161.59it/s]
 95%|█████████▍| 11343/12000 [01:10<00:04, 162.42it/s]
100%|██████████| 12000/12000 [01:13<00:00, 162.16it/s]
2025-08-03 04:42:17,181 - INFO - Done! Took 00:01:14.80
2025-08-03 04:42:17,203 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GINE
RANDOMGTLayer: Randomly selected GNN type: GINE
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GATV2
RANDOMGTLayer: Randomly selected GNN type: GATV2
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GATV2
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GATV2
RANDOMGTLayer: Randomly selected GNN type: GATV2
RANDOMGTLayer: Randomly selected GNN type: GINE
RANDOMGTLayer: Randomly selected GNN type: GINE
RANDOMGTLayer: Randomly selected GNN type: GATV2
2025-08-03 04:42:17,577 - INFO - Created model type: <class 'torch_geometric.graphgym.model_builder.GraphGymModule'>
2025-08-03 04:42:17,577 - INFO - Inner model type: <class 'graphgps.network.RANDOM_GTModel_EDGE.RANDOM_GTModelEDGE'>
2025-08-03 04:42:17,577 - INFO - Inner model has get_darts_model: False
2025-08-03 04:42:17,582 - INFO - GraphGymModule(
  (model): RANDOM_GTModelEDGE(
    (encoder): FeatureEncoder(
      (node_encoder): LapPENodeEncoder(
        (linear_x): Linear(in_features=7, out_features=32, bias=True)
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (edge_encoder): DummyEdgeEncoder(
        (encoder): Embedding(1, 48)
      )
    )
    (layers): Sequential(
      (0): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (1): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (2): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (3): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (4): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (5): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (6): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (7): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (8): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (9): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (10): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (11): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (12): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (13): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (14): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (15): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (post_mp): GNNInductiveNodeHead(
      (layer_post_mp): MLP(
        (model): Sequential(
          (0): GeneralMultiLayer(
            (Layer_0): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
            (Layer_1): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
          )
          (1): Linear(
            (model): Linear(48, 6, bias=True)
          )
        )
      )
    )
  )
)
2025-08-03 04:42:17,587 - INFO - Number of parameters: 483,158
2025-08-03 04:42:17,587 - INFO - Starting optimized training: 2025-08-03 04:42:17.587766
2025-08-03 04:42:23,724 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
2025-08-03 04:42:23,725 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-08-03 04:42:23,727 - INFO -   undirected: True
2025-08-03 04:42:23,727 - INFO -   num graphs: 12000
2025-08-03 04:42:23,727 - INFO -   avg num_nodes/graph: 117
2025-08-03 04:42:23,727 - INFO -   num node features: 7
2025-08-03 04:42:23,727 - INFO -   num edge features: 0
2025-08-03 04:42:23,729 - INFO -   num classes: 6
2025-08-03 04:42:23,729 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-08-03 04:42:23,729 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-08-03 04:42:23,738 - INFO -   ...estimated to be undirected: True

  0%|          | 0/12000 [00:00<?, ?it/s]
 13%|█▎        | 1598/12000 [00:10<01:05, 159.72it/s]
 27%|██▋       | 3196/12000 [00:20<00:55, 159.76it/s]
 40%|████      | 4848/12000 [00:30<00:44, 162.21it/s]
 54%|█████▍    | 6495/12000 [00:40<00:33, 163.17it/s]
 67%|██████▋   | 8087/12000 [00:50<00:24, 161.73it/s]
 81%|████████  | 9740/12000 [01:00<00:13, 162.94it/s]
 95%|█████████▍| 11390/12000 [01:10<00:03, 163.58it/s]
100%|██████████| 12000/12000 [01:13<00:00, 162.88it/s]
2025-08-03 04:43:38,195 - INFO - Done! Took 00:01:14.47
2025-08-03 04:43:38,221 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-08-03 04:43:38,225 - INFO - === STARTING OPTIMIZED TRAINING ===
2025-08-03 04:43:38,225 - INFO - Start from epoch 0
2025-08-03 04:45:21,539 - INFO - train: {'epoch': 0, 'time_epoch': 101.95723, 'eta': 10093.76559, 'eta_hours': 2.80382, 'loss': 1.80073528, 'lr': 0.0, 'params': 483158, 'time_iter': 0.16313, 'accuracy': 0.16665, 'f1': 0.04778, 'accuracy-SBM': 0.16667, 'auc': 0.50063}
2025-08-03 04:45:21,546 - INFO - ...computing epoch stats took: 1.33s
2025-08-03 04:45:26,510 - INFO - val: {'epoch': 0, 'time_epoch': 4.92035, 'loss': 1.80095751, 'lr': 0, 'params': 483158, 'time_iter': 0.0781, 'accuracy': 0.16714, 'f1': 0.04774, 'accuracy-SBM': 0.16667, 'auc': 0.50049}
2025-08-03 04:45:26,512 - INFO - ...computing epoch stats took: 0.04s
2025-08-03 04:45:31,470 - INFO - test: {'epoch': 0, 'time_epoch': 4.92212, 'loss': 1.80077396, 'lr': 0, 'params': 483158, 'time_iter': 0.07813, 'accuracy': 0.16728, 'f1': 0.04777, 'accuracy-SBM': 0.16667, 'auc': 0.50171}
2025-08-03 04:45:31,497 - INFO - ...computing epoch stats took: 0.06s
2025-08-03 04:45:31,498 - INFO - > Epoch 0: took 113.3s (avg 113.3s) | Best so far: epoch 0	train_loss: 1.8007 train_accuracy-SBM: 0.1667	val_loss: 1.8010 val_accuracy-SBM: 0.1667	test_loss: 1.8008 test_accuracy-SBM: 0.1667
2025-08-03 04:47:10,403 - INFO - train: {'epoch': 1, 'time_epoch': 98.50771, 'eta': 9822.78215, 'eta_hours': 2.72855, 'loss': 1.67822921, 'lr': 0.0002, 'params': 483158, 'time_iter': 0.15761, 'accuracy': 0.30452, 'f1': 0.24587, 'accuracy-SBM': 0.30438, 'auc': 0.66155}
2025-08-03 04:47:10,409 - INFO - ...computing epoch stats took: 0.38s
2025-08-03 04:47:15,247 - INFO - val: {'epoch': 1, 'time_epoch': 4.77113, 'loss': 1.69727832, 'lr': 0, 'params': 483158, 'time_iter': 0.07573, 'accuracy': 0.28842, 'f1': 0.20068, 'accuracy-SBM': 0.28834, 'auc': 0.6544}
2025-08-03 04:47:15,249 - INFO - ...computing epoch stats took: 0.07s
2025-08-03 04:47:20,081 - INFO - test: {'epoch': 1, 'time_epoch': 4.78293, 'loss': 1.69841829, 'lr': 0, 'params': 483158, 'time_iter': 0.07592, 'accuracy': 0.28606, 'f1': 0.19881, 'accuracy-SBM': 0.28597, 'auc': 0.65337}
2025-08-03 04:47:20,083 - INFO - ...computing epoch stats took: 0.05s
2025-08-03 04:47:20,084 - INFO - > Epoch 1: took 108.6s (avg 110.9s) | Best so far: epoch 1	train_loss: 1.6782 train_accuracy-SBM: 0.3044	val_loss: 1.6973 val_accuracy-SBM: 0.2883	test_loss: 1.6984 test_accuracy-SBM: 0.2860
2025-08-03 04:49:01,342 - INFO - train: {'epoch': 2, 'time_epoch': 100.85119, 'eta': 9742.555, 'eta_hours': 2.70627, 'loss': 1.51807024, 'lr': 0.0004, 'params': 483158, 'time_iter': 0.16136, 'accuracy': 0.37247, 'f1': 0.33426, 'accuracy-SBM': 0.37231, 'auc': 0.73405}
2025-08-03 04:49:01,349 - INFO - ...computing epoch stats took: 0.39s
2025-08-03 04:49:05,980 - INFO - val: {'epoch': 2, 'time_epoch': 4.5542, 'loss': 1.66514675, 'lr': 0, 'params': 483158, 'time_iter': 0.07229, 'accuracy': 0.28097, 'f1': 0.25412, 'accuracy-SBM': 0.28036, 'auc': 0.70484}
2025-08-03 04:49:05,984 - INFO - ...computing epoch stats took: 0.08s
2025-08-03 04:49:10,526 - INFO - test: {'epoch': 2, 'time_epoch': 4.49368, 'loss': 1.65909094, 'lr': 0, 'params': 483158, 'time_iter': 0.07133, 'accuracy': 0.28646, 'f1': 0.26029, 'accuracy-SBM': 0.28674, 'auc': 0.70495}
2025-08-03 04:49:10,529 - INFO - ...computing epoch stats took: 0.05s
2025-08-03 04:49:10,530 - INFO - > Epoch 2: took 110.4s (avg 110.8s) | Best so far: epoch 1	train_loss: 1.6782 train_accuracy-SBM: 0.3044	val_loss: 1.6973 val_accuracy-SBM: 0.2883	test_loss: 1.6984 test_accuracy-SBM: 0.2860
2025-08-03 04:50:43,910 - INFO - train: {'epoch': 3, 'time_epoch': 93.01398, 'eta': 9463.92278, 'eta_hours': 2.62887, 'loss': 1.42796373, 'lr': 0.0006, 'params': 483158, 'time_iter': 0.14882, 'accuracy': 0.40036, 'f1': 0.37103, 'accuracy-SBM': 0.40019, 'auc': 0.76641}
2025-08-03 04:50:48,375 - INFO - val: {'epoch': 3, 'time_epoch': 4.39417, 'loss': 1.48432658, 'lr': 0, 'params': 483158, 'time_iter': 0.06975, 'accuracy': 0.38961, 'f1': 0.38254, 'accuracy-SBM': 0.38909, 'auc': 0.76578}
2025-08-03 04:50:52,650 - INFO - test: {'epoch': 3, 'time_epoch': 4.24175, 'loss': 1.4784469, 'lr': 0, 'params': 483158, 'time_iter': 0.06733, 'accuracy': 0.39446, 'f1': 0.3883, 'accuracy-SBM': 0.39482, 'auc': 0.76721}
2025-08-03 04:50:52,652 - INFO - > Epoch 3: took 102.1s (avg 108.6s) | Best so far: epoch 3	train_loss: 1.4280 train_accuracy-SBM: 0.4002	val_loss: 1.4843 val_accuracy-SBM: 0.3891	test_loss: 1.4784 test_accuracy-SBM: 0.3948
2025-08-03 04:52:24,762 - INFO - train: {'epoch': 4, 'time_epoch': 91.84613, 'eta': 9237.34872, 'eta_hours': 2.56593, 'loss': 1.27781848, 'lr': 0.0008, 'params': 483158, 'time_iter': 0.14695, 'accuracy': 0.47718, 'f1': 0.46034, 'accuracy-SBM': 0.47705, 'auc': 0.81482}
2025-08-03 04:52:28,945 - INFO - val: {'epoch': 4, 'time_epoch': 4.11815, 'loss': 1.35553846, 'lr': 0, 'params': 483158, 'time_iter': 0.06537, 'accuracy': 0.443, 'f1': 0.41878, 'accuracy-SBM': 0.44279, 'auc': 0.8003}
2025-08-03 04:52:33,381 - INFO - test: {'epoch': 4, 'time_epoch': 4.40132, 'loss': 1.34774356, 'lr': 0, 'params': 483158, 'time_iter': 0.06986, 'accuracy': 0.44274, 'f1': 0.42044, 'accuracy-SBM': 0.4437, 'auc': 0.80196}
2025-08-03 04:52:33,383 - INFO - > Epoch 4: took 100.7s (avg 107.0s) | Best so far: epoch 4	train_loss: 1.2778 train_accuracy-SBM: 0.4770	val_loss: 1.3555 val_accuracy-SBM: 0.4428	test_loss: 1.3477 test_accuracy-SBM: 0.4437
2025-08-03 04:54:06,008 - INFO - train: {'epoch': 5, 'time_epoch': 92.36207, 'eta': 9063.767, 'eta_hours': 2.51771, 'loss': 1.17120123, 'lr': 0.001, 'params': 483158, 'time_iter': 0.14778, 'accuracy': 0.52877, 'f1': 0.51511, 'accuracy-SBM': 0.52869, 'auc': 0.85038}
2025-08-03 04:54:10,457 - INFO - val: {'epoch': 5, 'time_epoch': 4.39811, 'loss': 1.12455924, 'lr': 0, 'params': 483158, 'time_iter': 0.06981, 'accuracy': 0.55036, 'f1': 0.53755, 'accuracy-SBM': 0.55142, 'auc': 0.86755}
2025-08-03 04:54:14,985 - INFO - test: {'epoch': 5, 'time_epoch': 4.49046, 'loss': 1.1188322, 'lr': 0, 'params': 483158, 'time_iter': 0.07128, 'accuracy': 0.55203, 'f1': 0.53728, 'accuracy-SBM': 0.55333, 'auc': 0.86829}
2025-08-03 04:54:15,039 - INFO - > Epoch 5: took 101.7s (avg 106.1s) | Best so far: epoch 5	train_loss: 1.1712 train_accuracy-SBM: 0.5287	val_loss: 1.1246 val_accuracy-SBM: 0.5514	test_loss: 1.1188 test_accuracy-SBM: 0.5533
2025-08-03 04:55:47,199 - INFO - train: {'epoch': 6, 'time_epoch': 91.90346, 'eta': 8907.29786, 'eta_hours': 2.47425, 'loss': 1.04216997, 'lr': 0.00099973, 'params': 483158, 'time_iter': 0.14705, 'accuracy': 0.57995, 'f1': 0.56848, 'accuracy-SBM': 0.57992, 'auc': 0.88324}
2025-08-03 04:55:51,670 - INFO - val: {'epoch': 6, 'time_epoch': 4.41284, 'loss': 1.02797502, 'lr': 0, 'params': 483158, 'time_iter': 0.07005, 'accuracy': 0.58139, 'f1': 0.56151, 'accuracy-SBM': 0.58267, 'auc': 0.88873}
2025-08-03 04:55:56,123 - INFO - test: {'epoch': 6, 'time_epoch': 4.40997, 'loss': 1.02287895, 'lr': 0, 'params': 483158, 'time_iter': 0.07, 'accuracy': 0.58514, 'f1': 0.56497, 'accuracy-SBM': 0.58695, 'auc': 0.88845}
2025-08-03 04:55:56,125 - INFO - > Epoch 6: took 101.1s (avg 105.4s) | Best so far: epoch 6	train_loss: 1.0422 train_accuracy-SBM: 0.5799	val_loss: 1.0280 val_accuracy-SBM: 0.5827	test_loss: 1.0229 test_accuracy-SBM: 0.5869
2025-08-03 04:57:28,756 - INFO - train: {'epoch': 7, 'time_epoch': 92.24288, 'eta': 8770.87357, 'eta_hours': 2.43635, 'loss': 0.99917502, 'lr': 0.00099891, 'params': 483158, 'time_iter': 0.14759, 'accuracy': 0.59299, 'f1': 0.57941, 'accuracy-SBM': 0.59297, 'auc': 0.89162}
2025-08-03 04:57:33,336 - INFO - val: {'epoch': 7, 'time_epoch': 4.40282, 'loss': 1.0704234, 'lr': 0, 'params': 483158, 'time_iter': 0.06989, 'accuracy': 0.57677, 'f1': 0.56281, 'accuracy-SBM': 0.57722, 'auc': 0.87866}
2025-08-03 04:57:37,491 - INFO - test: {'epoch': 7, 'time_epoch': 4.11905, 'loss': 1.03561147, 'lr': 0, 'params': 483158, 'time_iter': 0.06538, 'accuracy': 0.5802, 'f1': 0.56581, 'accuracy-SBM': 0.58177, 'auc': 0.88501}
2025-08-03 04:57:37,493 - INFO - > Epoch 7: took 101.4s (avg 104.9s) | Best so far: epoch 6	train_loss: 1.0422 train_accuracy-SBM: 0.5799	val_loss: 1.0280 val_accuracy-SBM: 0.5827	test_loss: 1.0229 test_accuracy-SBM: 0.5869
2025-08-03 04:59:09,420 - INFO - train: {'epoch': 8, 'time_epoch': 91.66705, 'eta': 8638.44504, 'eta_hours': 2.39957, 'loss': 0.97865157, 'lr': 0.00099754, 'params': 483158, 'time_iter': 0.14667, 'accuracy': 0.59851, 'f1': 0.58716, 'accuracy-SBM': 0.59848, 'auc': 0.8954}
2025-08-03 04:59:13,899 - INFO - val: {'epoch': 8, 'time_epoch': 4.42388, 'loss': 0.9559545, 'lr': 0, 'params': 483158, 'time_iter': 0.07022, 'accuracy': 0.60391, 'f1': 0.56124, 'accuracy-SBM': 0.60473, 'auc': 0.90152}
2025-08-03 04:59:18,774 - INFO - test: {'epoch': 8, 'time_epoch': 4.71768, 'loss': 0.95282431, 'lr': 0, 'params': 483158, 'time_iter': 0.07488, 'accuracy': 0.60352, 'f1': 0.56141, 'accuracy-SBM': 0.6056, 'auc': 0.90142}
2025-08-03 04:59:18,776 - INFO - > Epoch 8: took 101.3s (avg 104.5s) | Best so far: epoch 8	train_loss: 0.9787 train_accuracy-SBM: 0.5985	val_loss: 0.9560 val_accuracy-SBM: 0.6047	test_loss: 0.9528 test_accuracy-SBM: 0.6056
2025-08-03 05:00:50,238 - INFO - train: {'epoch': 9, 'time_epoch': 91.16801, 'eta': 8509.6775, 'eta_hours': 2.3638, 'loss': 0.96479156, 'lr': 0.00099563, 'params': 483158, 'time_iter': 0.14587, 'accuracy': 0.60295, 'f1': 0.59116, 'accuracy-SBM': 0.60292, 'auc': 0.89781}
2025-08-03 05:00:54,716 - INFO - val: {'epoch': 9, 'time_epoch': 4.42644, 'loss': 0.95104187, 'lr': 0, 'params': 483158, 'time_iter': 0.07026, 'accuracy': 0.60797, 'f1': 0.59119, 'accuracy-SBM': 0.60882, 'auc': 0.90193}
2025-08-03 05:00:59,003 - INFO - test: {'epoch': 9, 'time_epoch': 4.25173, 'loss': 0.95592943, 'lr': 0, 'params': 483158, 'time_iter': 0.06749, 'accuracy': 0.605, 'f1': 0.58592, 'accuracy-SBM': 0.60643, 'auc': 0.90028}
2025-08-03 05:00:59,005 - INFO - > Epoch 9: took 100.2s (avg 104.1s) | Best so far: epoch 9	train_loss: 0.9648 train_accuracy-SBM: 0.6029	val_loss: 0.9510 val_accuracy-SBM: 0.6088	test_loss: 0.9559 test_accuracy-SBM: 0.6064
2025-08-03 05:02:31,079 - INFO - train: {'epoch': 10, 'time_epoch': 91.68957, 'eta': 8391.96609, 'eta_hours': 2.3311, 'loss': 0.95405068, 'lr': 0.00099318, 'params': 483158, 'time_iter': 0.1467, 'accuracy': 0.60626, 'f1': 0.59492, 'accuracy-SBM': 0.60624, 'auc': 0.89982}
2025-08-03 05:02:35,414 - INFO - val: {'epoch': 10, 'time_epoch': 4.28446, 'loss': 0.93099342, 'lr': 0, 'params': 483158, 'time_iter': 0.06801, 'accuracy': 0.61143, 'f1': 0.59159, 'accuracy-SBM': 0.61234, 'auc': 0.90604}
2025-08-03 05:02:40,152 - INFO - test: {'epoch': 10, 'time_epoch': 4.41701, 'loss': 0.93122856, 'lr': 0, 'params': 483158, 'time_iter': 0.07011, 'accuracy': 0.61061, 'f1': 0.59073, 'accuracy-SBM': 0.61251, 'auc': 0.90525}
2025-08-03 05:02:40,517 - INFO - > Epoch 10: took 101.5s (avg 103.8s) | Best so far: epoch 10	train_loss: 0.9541 train_accuracy-SBM: 0.6062	val_loss: 0.9310 val_accuracy-SBM: 0.6123	test_loss: 0.9312 test_accuracy-SBM: 0.6125
2025-08-03 05:04:13,466 - INFO - train: {'epoch': 11, 'time_epoch': 92.46002, 'eta': 8284.24165, 'eta_hours': 2.30118, 'loss': 0.94664053, 'lr': 0.00099019, 'params': 483158, 'time_iter': 0.14794, 'accuracy': 0.60821, 'f1': 0.59747, 'accuracy-SBM': 0.60819, 'auc': 0.90116}
2025-08-03 05:04:17,891 - INFO - val: {'epoch': 11, 'time_epoch': 4.37156, 'loss': 0.94168023, 'lr': 0, 'params': 483158, 'time_iter': 0.06939, 'accuracy': 0.60667, 'f1': 0.58934, 'accuracy-SBM': 0.60757, 'auc': 0.90368}
2025-08-03 05:04:22,346 - INFO - test: {'epoch': 11, 'time_epoch': 4.37301, 'loss': 0.94295984, 'lr': 0, 'params': 483158, 'time_iter': 0.06941, 'accuracy': 0.60496, 'f1': 0.58662, 'accuracy-SBM': 0.60655, 'auc': 0.90277}
2025-08-03 05:04:22,368 - INFO - > Epoch 11: took 101.9s (avg 103.7s) | Best so far: epoch 10	train_loss: 0.9541 train_accuracy-SBM: 0.6062	val_loss: 0.9310 val_accuracy-SBM: 0.6123	test_loss: 0.9312 test_accuracy-SBM: 0.6125
2025-08-03 05:05:55,352 - INFO - train: {'epoch': 12, 'time_epoch': 92.61, 'eta': 8179.86929, 'eta_hours': 2.27219, 'loss': 0.93950781, 'lr': 0.00098666, 'params': 483158, 'time_iter': 0.14818, 'accuracy': 0.61053, 'f1': 0.59962, 'accuracy-SBM': 0.61051, 'auc': 0.90231}
2025-08-03 05:05:59,795 - INFO - val: {'epoch': 12, 'time_epoch': 4.39142, 'loss': 0.91926632, 'lr': 0, 'params': 483158, 'time_iter': 0.06971, 'accuracy': 0.61569, 'f1': 0.59777, 'accuracy-SBM': 0.61632, 'auc': 0.90728}
2025-08-03 05:06:04,233 - INFO - test: {'epoch': 12, 'time_epoch': 4.40042, 'loss': 0.92010679, 'lr': 0, 'params': 483158, 'time_iter': 0.06985, 'accuracy': 0.61383, 'f1': 0.59611, 'accuracy-SBM': 0.61565, 'auc': 0.9065}
2025-08-03 05:06:04,490 - INFO - > Epoch 12: took 102.1s (avg 103.6s) | Best so far: epoch 12	train_loss: 0.9395 train_accuracy-SBM: 0.6105	val_loss: 0.9193 val_accuracy-SBM: 0.6163	test_loss: 0.9201 test_accuracy-SBM: 0.6157
2025-08-03 05:07:39,710 - INFO - train: {'epoch': 13, 'time_epoch': 94.762, 'eta': 8090.39667, 'eta_hours': 2.24733, 'loss': 0.9333609, 'lr': 0.0009826, 'params': 483158, 'time_iter': 0.15162, 'accuracy': 0.61262, 'f1': 0.60158, 'accuracy-SBM': 0.6126, 'auc': 0.90335}
2025-08-03 05:07:44,164 - INFO - val: {'epoch': 13, 'time_epoch': 4.39502, 'loss': 0.91638986, 'lr': 0, 'params': 483158, 'time_iter': 0.06976, 'accuracy': 0.61745, 'f1': 0.59572, 'accuracy-SBM': 0.61809, 'auc': 0.90752}
2025-08-03 05:07:48,847 - INFO - test: {'epoch': 13, 'time_epoch': 4.4113, 'loss': 0.91021658, 'lr': 0, 'params': 483158, 'time_iter': 0.07002, 'accuracy': 0.61702, 'f1': 0.59592, 'accuracy-SBM': 0.61882, 'auc': 0.90793}
2025-08-03 05:07:48,850 - INFO - > Epoch 13: took 104.4s (avg 103.6s) | Best so far: epoch 13	train_loss: 0.9334 train_accuracy-SBM: 0.6126	val_loss: 0.9164 val_accuracy-SBM: 0.6181	test_loss: 0.9102 test_accuracy-SBM: 0.6188
2025-08-03 05:09:20,796 - INFO - train: {'epoch': 14, 'time_epoch': 91.5769, 'eta': 7982.16989, 'eta_hours': 2.21727, 'loss': 0.92725042, 'lr': 0.00097802, 'params': 483158, 'time_iter': 0.14652, 'accuracy': 0.61474, 'f1': 0.60313, 'accuracy-SBM': 0.61471, 'auc': 0.90443}
2025-08-03 05:09:25,271 - INFO - val: {'epoch': 14, 'time_epoch': 4.34729, 'loss': 0.93353195, 'lr': 0, 'params': 483158, 'time_iter': 0.069, 'accuracy': 0.61271, 'f1': 0.58202, 'accuracy-SBM': 0.61356, 'auc': 0.90435}
2025-08-03 05:09:29,477 - INFO - test: {'epoch': 14, 'time_epoch': 4.00155, 'loss': 0.91748557, 'lr': 0, 'params': 483158, 'time_iter': 0.06352, 'accuracy': 0.61462, 'f1': 0.58389, 'accuracy-SBM': 0.61656, 'auc': 0.90635}
2025-08-03 05:09:29,508 - INFO - > Epoch 14: took 100.7s (avg 103.4s) | Best so far: epoch 13	train_loss: 0.9334 train_accuracy-SBM: 0.6126	val_loss: 0.9164 val_accuracy-SBM: 0.6181	test_loss: 0.9102 test_accuracy-SBM: 0.6188
2025-08-03 05:11:01,318 - INFO - train: {'epoch': 15, 'time_epoch': 91.54496, 'eta': 7875.85665, 'eta_hours': 2.18774, 'loss': 0.9240417, 'lr': 0.00097291, 'params': 483158, 'time_iter': 0.14647, 'accuracy': 0.6153, 'f1': 0.60384, 'accuracy-SBM': 0.61527, 'auc': 0.90506}
2025-08-03 05:11:05,639 - INFO - val: {'epoch': 15, 'time_epoch': 4.23407, 'loss': 0.92003211, 'lr': 0, 'params': 483158, 'time_iter': 0.06721, 'accuracy': 0.61506, 'f1': 0.5802, 'accuracy-SBM': 0.61543, 'auc': 0.90724}
2025-08-03 05:11:09,961 - INFO - test: {'epoch': 15, 'time_epoch': 4.27369, 'loss': 0.92352983, 'lr': 0, 'params': 483158, 'time_iter': 0.06784, 'accuracy': 0.61682, 'f1': 0.58021, 'accuracy-SBM': 0.61791, 'auc': 0.90619}
2025-08-03 05:11:09,963 - INFO - > Epoch 15: took 100.5s (avg 103.2s) | Best so far: epoch 13	train_loss: 0.9334 train_accuracy-SBM: 0.6126	val_loss: 0.9164 val_accuracy-SBM: 0.6181	test_loss: 0.9102 test_accuracy-SBM: 0.6188
2025-08-03 05:12:40,497 - INFO - train: {'epoch': 16, 'time_epoch': 90.16835, 'eta': 7764.55977, 'eta_hours': 2.15682, 'loss': 0.91872591, 'lr': 0.00096728, 'params': 483158, 'time_iter': 0.14427, 'accuracy': 0.61651, 'f1': 0.60604, 'accuracy-SBM': 0.61648, 'auc': 0.90599}
2025-08-03 05:12:44,839 - INFO - val: {'epoch': 16, 'time_epoch': 4.28309, 'loss': 0.91114338, 'lr': 0, 'params': 483158, 'time_iter': 0.06799, 'accuracy': 0.61661, 'f1': 0.59788, 'accuracy-SBM': 0.61749, 'auc': 0.90792}
2025-08-03 05:12:49,170 - INFO - test: {'epoch': 16, 'time_epoch': 4.27712, 'loss': 0.9181051, 'lr': 0, 'params': 483158, 'time_iter': 0.06789, 'accuracy': 0.61484, 'f1': 0.59625, 'accuracy-SBM': 0.61664, 'auc': 0.90624}
2025-08-03 05:12:49,172 - INFO - > Epoch 16: took 99.2s (avg 103.0s) | Best so far: epoch 13	train_loss: 0.9334 train_accuracy-SBM: 0.6126	val_loss: 0.9164 val_accuracy-SBM: 0.6181	test_loss: 0.9102 test_accuracy-SBM: 0.6188
2025-08-03 05:14:19,314 - INFO - train: {'epoch': 17, 'time_epoch': 89.86189, 'eta': 7654.21442, 'eta_hours': 2.12617, 'loss': 0.91715074, 'lr': 0.00096114, 'params': 483158, 'time_iter': 0.14378, 'accuracy': 0.61668, 'f1': 0.60599, 'accuracy-SBM': 0.61665, 'auc': 0.90626}
2025-08-03 05:14:23,699 - INFO - val: {'epoch': 17, 'time_epoch': 4.31284, 'loss': 0.90534292, 'lr': 0, 'params': 483158, 'time_iter': 0.06846, 'accuracy': 0.61844, 'f1': 0.60244, 'accuracy-SBM': 0.61901, 'auc': 0.90917}
2025-08-03 05:14:28,052 - INFO - test: {'epoch': 17, 'time_epoch': 4.30542, 'loss': 0.90339355, 'lr': 0, 'params': 483158, 'time_iter': 0.06834, 'accuracy': 0.61989, 'f1': 0.60268, 'accuracy-SBM': 0.62112, 'auc': 0.90911}
2025-08-03 05:14:28,184 - INFO - > Epoch 17: took 99.0s (avg 102.8s) | Best so far: epoch 17	train_loss: 0.9172 train_accuracy-SBM: 0.6167	val_loss: 0.9053 val_accuracy-SBM: 0.6190	test_loss: 0.9034 test_accuracy-SBM: 0.6211
2025-08-03 05:15:58,067 - INFO - train: {'epoch': 18, 'time_epoch': 89.62808, 'eta': 7545.02847, 'eta_hours': 2.09584, 'loss': 0.91230834, 'lr': 0.0009545, 'params': 483158, 'time_iter': 0.1434, 'accuracy': 0.61908, 'f1': 0.6086, 'accuracy-SBM': 0.61906, 'auc': 0.90708}
2025-08-03 05:16:02,697 - INFO - val: {'epoch': 18, 'time_epoch': 4.56073, 'loss': 0.90184285, 'lr': 0, 'params': 483158, 'time_iter': 0.07239, 'accuracy': 0.61958, 'f1': 0.60049, 'accuracy-SBM': 0.62047, 'auc': 0.90993}
2025-08-03 05:16:07,369 - INFO - test: {'epoch': 18, 'time_epoch': 4.61448, 'loss': 0.89687045, 'lr': 0, 'params': 483158, 'time_iter': 0.07325, 'accuracy': 0.62299, 'f1': 0.60398, 'accuracy-SBM': 0.62443, 'auc': 0.91011}
2025-08-03 05:16:07,372 - INFO - > Epoch 18: took 99.2s (avg 102.6s) | Best so far: epoch 18	train_loss: 0.9123 train_accuracy-SBM: 0.6191	val_loss: 0.9018 val_accuracy-SBM: 0.6205	test_loss: 0.8969 test_accuracy-SBM: 0.6244
2025-08-03 05:17:39,037 - INFO - train: {'epoch': 19, 'time_epoch': 91.39761, 'eta': 7444.87642, 'eta_hours': 2.06802, 'loss': 0.91228236, 'lr': 0.00094736, 'params': 483158, 'time_iter': 0.14624, 'accuracy': 0.61852, 'f1': 0.60803, 'accuracy-SBM': 0.6185, 'auc': 0.90708}
2025-08-03 05:17:43,380 - INFO - val: {'epoch': 19, 'time_epoch': 4.29384, 'loss': 0.91248458, 'lr': 0, 'params': 483158, 'time_iter': 0.06816, 'accuracy': 0.62036, 'f1': 0.61132, 'accuracy-SBM': 0.62094, 'auc': 0.90781}
2025-08-03 05:17:47,651 - INFO - test: {'epoch': 19, 'time_epoch': 4.23251, 'loss': 0.89984859, 'lr': 0, 'params': 483158, 'time_iter': 0.06718, 'accuracy': 0.62163, 'f1': 0.61159, 'accuracy-SBM': 0.623, 'auc': 0.90936}
2025-08-03 05:17:47,702 - INFO - > Epoch 19: took 100.3s (avg 102.5s) | Best so far: epoch 19	train_loss: 0.9123 train_accuracy-SBM: 0.6185	val_loss: 0.9125 val_accuracy-SBM: 0.6209	test_loss: 0.8998 test_accuracy-SBM: 0.6230
2025-08-03 05:19:16,178 - INFO - train: {'epoch': 20, 'time_epoch': 88.21767, 'eta': 7333.59549, 'eta_hours': 2.03711, 'loss': 0.90735815, 'lr': 0.00093974, 'params': 483158, 'time_iter': 0.14115, 'accuracy': 0.61984, 'f1': 0.60894, 'accuracy-SBM': 0.61981, 'auc': 0.90792}
2025-08-03 05:19:20,476 - INFO - val: {'epoch': 20, 'time_epoch': 4.24955, 'loss': 0.90821358, 'lr': 0, 'params': 483158, 'time_iter': 0.06745, 'accuracy': 0.61893, 'f1': 0.61001, 'accuracy-SBM': 0.61982, 'auc': 0.90944}
2025-08-03 05:19:24,729 - INFO - test: {'epoch': 20, 'time_epoch': 4.20906, 'loss': 0.91105997, 'lr': 0, 'params': 483158, 'time_iter': 0.06681, 'accuracy': 0.61448, 'f1': 0.60521, 'accuracy-SBM': 0.61614, 'auc': 0.90813}
2025-08-03 05:19:24,731 - INFO - > Epoch 20: took 97.0s (avg 102.2s) | Best so far: epoch 19	train_loss: 0.9123 train_accuracy-SBM: 0.6185	val_loss: 0.9125 val_accuracy-SBM: 0.6209	test_loss: 0.8998 test_accuracy-SBM: 0.6230
2025-08-03 05:20:53,440 - INFO - train: {'epoch': 21, 'time_epoch': 88.46412, 'eta': 7225.28501, 'eta_hours': 2.00702, 'loss': 0.903007, 'lr': 0.00093163, 'params': 483158, 'time_iter': 0.14154, 'accuracy': 0.62225, 'f1': 0.61199, 'accuracy-SBM': 0.62223, 'auc': 0.90876}
2025-08-03 05:20:57,701 - INFO - val: {'epoch': 21, 'time_epoch': 4.21305, 'loss': 0.90688738, 'lr': 0, 'params': 483158, 'time_iter': 0.06687, 'accuracy': 0.62031, 'f1': 0.61193, 'accuracy-SBM': 0.62105, 'auc': 0.9096}
2025-08-03 05:21:01,972 - INFO - test: {'epoch': 21, 'time_epoch': 4.23529, 'loss': 0.89643817, 'lr': 0, 'params': 483158, 'time_iter': 0.06723, 'accuracy': 0.62387, 'f1': 0.61423, 'accuracy-SBM': 0.62504, 'auc': 0.9106}
2025-08-03 05:21:02,054 - INFO - > Epoch 21: took 97.3s (avg 102.0s) | Best so far: epoch 21	train_loss: 0.9030 train_accuracy-SBM: 0.6222	val_loss: 0.9069 val_accuracy-SBM: 0.6210	test_loss: 0.8964 test_accuracy-SBM: 0.6250
2025-08-03 05:22:29,566 - INFO - train: {'epoch': 22, 'time_epoch': 87.12482, 'eta': 7114.21653, 'eta_hours': 1.97617, 'loss': 0.90065619, 'lr': 0.00092305, 'params': 483158, 'time_iter': 0.1394, 'accuracy': 0.62102, 'f1': 0.61037, 'accuracy-SBM': 0.621, 'auc': 0.90907}
2025-08-03 05:22:33,844 - INFO - val: {'epoch': 22, 'time_epoch': 4.22826, 'loss': 0.91485405, 'lr': 0, 'params': 483158, 'time_iter': 0.06712, 'accuracy': 0.619, 'f1': 0.60903, 'accuracy-SBM': 0.61962, 'auc': 0.90793}
2025-08-03 05:22:38,089 - INFO - test: {'epoch': 22, 'time_epoch': 4.20965, 'loss': 0.90235371, 'lr': 0, 'params': 483158, 'time_iter': 0.06682, 'accuracy': 0.62455, 'f1': 0.61465, 'accuracy-SBM': 0.62591, 'auc': 0.90919}
2025-08-03 05:22:38,102 - INFO - > Epoch 22: took 96.0s (avg 101.7s) | Best so far: epoch 21	train_loss: 0.9030 train_accuracy-SBM: 0.6222	val_loss: 0.9069 val_accuracy-SBM: 0.6210	test_loss: 0.8964 test_accuracy-SBM: 0.6250
2025-08-03 05:24:06,055 - INFO - train: {'epoch': 23, 'time_epoch': 87.59614, 'eta': 7006.63589, 'eta_hours': 1.94629, 'loss': 0.89815554, 'lr': 0.000914, 'params': 483158, 'time_iter': 0.14015, 'accuracy': 0.62288, 'f1': 0.61264, 'accuracy-SBM': 0.62286, 'auc': 0.90958}
2025-08-03 05:24:10,311 - INFO - val: {'epoch': 23, 'time_epoch': 4.20591, 'loss': 0.89405245, 'lr': 0, 'params': 483158, 'time_iter': 0.06676, 'accuracy': 0.62471, 'f1': 0.6154, 'accuracy-SBM': 0.62556, 'auc': 0.91157}
2025-08-03 05:24:14,607 - INFO - test: {'epoch': 23, 'time_epoch': 4.25237, 'loss': 0.89557561, 'lr': 0, 'params': 483158, 'time_iter': 0.0675, 'accuracy': 0.62336, 'f1': 0.61341, 'accuracy-SBM': 0.62483, 'auc': 0.91061}
2025-08-03 05:24:14,614 - INFO - > Epoch 23: took 96.5s (avg 101.5s) | Best so far: epoch 23	train_loss: 0.8982 train_accuracy-SBM: 0.6229	val_loss: 0.8941 val_accuracy-SBM: 0.6256	test_loss: 0.8956 test_accuracy-SBM: 0.6248
2025-08-03 05:25:42,687 - INFO - train: {'epoch': 24, 'time_epoch': 87.82245, 'eta': 6901.33294, 'eta_hours': 1.91704, 'loss': 0.89227341, 'lr': 0.00090451, 'params': 483158, 'time_iter': 0.14052, 'accuracy': 0.62526, 'f1': 0.61504, 'accuracy-SBM': 0.62523, 'auc': 0.91053}
2025-08-03 05:25:46,968 - INFO - val: {'epoch': 24, 'time_epoch': 4.23183, 'loss': 0.8804569, 'lr': 0, 'params': 483158, 'time_iter': 0.06717, 'accuracy': 0.62761, 'f1': 0.62036, 'accuracy-SBM': 0.62813, 'auc': 0.91347}
2025-08-03 05:25:51,218 - INFO - test: {'epoch': 24, 'time_epoch': 4.21469, 'loss': 0.89160462, 'lr': 0, 'params': 483158, 'time_iter': 0.0669, 'accuracy': 0.62392, 'f1': 0.61561, 'accuracy-SBM': 0.62512, 'auc': 0.91099}
2025-08-03 05:25:51,221 - INFO - > Epoch 24: took 96.6s (avg 101.3s) | Best so far: epoch 24	train_loss: 0.8923 train_accuracy-SBM: 0.6252	val_loss: 0.8805 val_accuracy-SBM: 0.6281	test_loss: 0.8916 test_accuracy-SBM: 0.6251
2025-08-03 05:27:19,457 - INFO - train: {'epoch': 25, 'time_epoch': 87.98494, 'eta': 6797.8371, 'eta_hours': 1.88829, 'loss': 0.89178331, 'lr': 0.00089457, 'params': 483158, 'time_iter': 0.14078, 'accuracy': 0.62511, 'f1': 0.61515, 'accuracy-SBM': 0.62509, 'auc': 0.9106}
2025-08-03 05:27:23,759 - INFO - val: {'epoch': 25, 'time_epoch': 4.25283, 'loss': 0.88874926, 'lr': 0, 'params': 483158, 'time_iter': 0.06751, 'accuracy': 0.626, 'f1': 0.60049, 'accuracy-SBM': 0.62684, 'auc': 0.91254}
2025-08-03 05:27:28,017 - INFO - test: {'epoch': 25, 'time_epoch': 4.21285, 'loss': 0.88774342, 'lr': 0, 'params': 483158, 'time_iter': 0.06687, 'accuracy': 0.62532, 'f1': 0.60026, 'accuracy-SBM': 0.62724, 'auc': 0.91198}
2025-08-03 05:27:28,019 - INFO - > Epoch 25: took 96.8s (avg 101.1s) | Best so far: epoch 24	train_loss: 0.8923 train_accuracy-SBM: 0.6252	val_loss: 0.8805 val_accuracy-SBM: 0.6281	test_loss: 0.8916 test_accuracy-SBM: 0.6251
2025-08-03 05:28:56,409 - INFO - train: {'epoch': 26, 'time_epoch': 88.13304, 'eta': 6695.89063, 'eta_hours': 1.85997, 'loss': 0.88649244, 'lr': 0.0008842, 'params': 483158, 'time_iter': 0.14101, 'accuracy': 0.62651, 'f1': 0.61578, 'accuracy-SBM': 0.62648, 'auc': 0.91153}
2025-08-03 05:29:00,761 - INFO - val: {'epoch': 26, 'time_epoch': 4.30084, 'loss': 0.88749656, 'lr': 0, 'params': 483158, 'time_iter': 0.06827, 'accuracy': 0.62645, 'f1': 0.60991, 'accuracy-SBM': 0.62691, 'auc': 0.91177}
2025-08-03 05:29:04,801 - INFO - test: {'epoch': 26, 'time_epoch': 4.00686, 'loss': 0.87820427, 'lr': 0, 'params': 483158, 'time_iter': 0.0636, 'accuracy': 0.62898, 'f1': 0.61037, 'accuracy-SBM': 0.63, 'auc': 0.91285}
2025-08-03 05:29:04,994 - INFO - > Epoch 26: took 97.0s (avg 101.0s) | Best so far: epoch 24	train_loss: 0.8923 train_accuracy-SBM: 0.6252	val_loss: 0.8805 val_accuracy-SBM: 0.6281	test_loss: 0.8916 test_accuracy-SBM: 0.6251
2025-08-03 05:30:33,705 - INFO - train: {'epoch': 27, 'time_epoch': 88.44673, 'eta': 6595.73746, 'eta_hours': 1.83215, 'loss': 0.8846032, 'lr': 0.00087341, 'params': 483158, 'time_iter': 0.14151, 'accuracy': 0.62756, 'f1': 0.6178, 'accuracy-SBM': 0.62753, 'auc': 0.91187}
2025-08-03 05:30:37,951 - INFO - val: {'epoch': 27, 'time_epoch': 4.18561, 'loss': 0.87540646, 'lr': 0, 'params': 483158, 'time_iter': 0.06644, 'accuracy': 0.62881, 'f1': 0.61756, 'accuracy-SBM': 0.62938, 'auc': 0.91435}
2025-08-03 05:30:42,028 - INFO - test: {'epoch': 27, 'time_epoch': 4.03467, 'loss': 0.87678055, 'lr': 0, 'params': 483158, 'time_iter': 0.06404, 'accuracy': 0.62953, 'f1': 0.61843, 'accuracy-SBM': 0.63097, 'auc': 0.91363}
2025-08-03 05:30:42,031 - INFO - > Epoch 27: took 97.0s (avg 100.8s) | Best so far: epoch 27	train_loss: 0.8846 train_accuracy-SBM: 0.6275	val_loss: 0.8754 val_accuracy-SBM: 0.6294	test_loss: 0.8768 test_accuracy-SBM: 0.6310
2025-08-03 05:32:10,877 - INFO - train: {'epoch': 28, 'time_epoch': 88.6029, 'eta': 6496.774, 'eta_hours': 1.80466, 'loss': 0.8817561, 'lr': 0.00086221, 'params': 483158, 'time_iter': 0.14176, 'accuracy': 0.62744, 'f1': 0.61753, 'accuracy-SBM': 0.62742, 'auc': 0.91242}
2025-08-03 05:32:15,141 - INFO - val: {'epoch': 28, 'time_epoch': 4.2155, 'loss': 0.88372335, 'lr': 0, 'params': 483158, 'time_iter': 0.06691, 'accuracy': 0.62909, 'f1': 0.60231, 'accuracy-SBM': 0.62938, 'auc': 0.91282}
2025-08-03 05:32:19,380 - INFO - test: {'epoch': 28, 'time_epoch': 4.20417, 'loss': 0.87310028, 'lr': 0, 'params': 483158, 'time_iter': 0.06673, 'accuracy': 0.62937, 'f1': 0.60109, 'accuracy-SBM': 0.63019, 'auc': 0.91389}
2025-08-03 05:32:19,382 - INFO - > Epoch 28: took 97.4s (avg 100.7s) | Best so far: epoch 27	train_loss: 0.8846 train_accuracy-SBM: 0.6275	val_loss: 0.8754 val_accuracy-SBM: 0.6294	test_loss: 0.8768 test_accuracy-SBM: 0.6310
2025-08-03 05:33:46,661 - INFO - train: {'epoch': 29, 'time_epoch': 87.032, 'eta': 6394.8358, 'eta_hours': 1.77634, 'loss': 0.87750396, 'lr': 0.00085062, 'params': 483158, 'time_iter': 0.13925, 'accuracy': 0.62948, 'f1': 0.6197, 'accuracy-SBM': 0.62946, 'auc': 0.91316}
2025-08-03 05:33:50,953 - INFO - val: {'epoch': 29, 'time_epoch': 4.24125, 'loss': 0.8683822, 'lr': 0, 'params': 483158, 'time_iter': 0.06732, 'accuracy': 0.63193, 'f1': 0.61629, 'accuracy-SBM': 0.63273, 'auc': 0.9151}
2025-08-03 05:33:55,118 - INFO - test: {'epoch': 29, 'time_epoch': 4.13035, 'loss': 0.86235304, 'lr': 0, 'params': 483158, 'time_iter': 0.06556, 'accuracy': 0.63217, 'f1': 0.61597, 'accuracy-SBM': 0.63398, 'auc': 0.91591}
2025-08-03 05:33:55,120 - INFO - > Epoch 29: took 95.7s (avg 100.6s) | Best so far: epoch 29	train_loss: 0.8775 train_accuracy-SBM: 0.6295	val_loss: 0.8684 val_accuracy-SBM: 0.6327	test_loss: 0.8624 test_accuracy-SBM: 0.6340
2025-08-03 05:35:23,348 - INFO - train: {'epoch': 30, 'time_epoch': 87.97369, 'eta': 6295.95531, 'eta_hours': 1.74888, 'loss': 0.87330398, 'lr': 0.00083864, 'params': 483158, 'time_iter': 0.14076, 'accuracy': 0.63055, 'f1': 0.62077, 'accuracy-SBM': 0.63053, 'auc': 0.9138}
2025-08-03 05:35:27,633 - INFO - val: {'epoch': 30, 'time_epoch': 4.23575, 'loss': 0.87512584, 'lr': 0, 'params': 483158, 'time_iter': 0.06723, 'accuracy': 0.62942, 'f1': 0.61605, 'accuracy-SBM': 0.63004, 'auc': 0.91416}
2025-08-03 05:35:31,905 - INFO - test: {'epoch': 30, 'time_epoch': 4.23627, 'loss': 0.86717839, 'lr': 0, 'params': 483158, 'time_iter': 0.06724, 'accuracy': 0.63283, 'f1': 0.61904, 'accuracy-SBM': 0.63395, 'auc': 0.91492}
2025-08-03 05:35:31,906 - INFO - > Epoch 30: took 96.8s (avg 100.4s) | Best so far: epoch 29	train_loss: 0.8775 train_accuracy-SBM: 0.6295	val_loss: 0.8684 val_accuracy-SBM: 0.6327	test_loss: 0.8624 test_accuracy-SBM: 0.6340
2025-08-03 05:37:00,191 - INFO - train: {'epoch': 31, 'time_epoch': 88.02616, 'eta': 6197.86799, 'eta_hours': 1.72163, 'loss': 0.87192792, 'lr': 0.00082629, 'params': 483158, 'time_iter': 0.14084, 'accuracy': 0.63254, 'f1': 0.62227, 'accuracy-SBM': 0.63251, 'auc': 0.91408}
2025-08-03 05:37:04,523 - INFO - val: {'epoch': 31, 'time_epoch': 4.28358, 'loss': 0.87675427, 'lr': 0, 'params': 483158, 'time_iter': 0.06799, 'accuracy': 0.62808, 'f1': 0.59654, 'accuracy-SBM': 0.62849, 'auc': 0.91407}
2025-08-03 05:37:08,833 - INFO - test: {'epoch': 31, 'time_epoch': 4.27328, 'loss': 0.86748589, 'lr': 0, 'params': 483158, 'time_iter': 0.06783, 'accuracy': 0.63381, 'f1': 0.60084, 'accuracy-SBM': 0.63473, 'auc': 0.915}
2025-08-03 05:37:08,834 - INFO - > Epoch 31: took 96.9s (avg 100.3s) | Best so far: epoch 29	train_loss: 0.8775 train_accuracy-SBM: 0.6295	val_loss: 0.8684 val_accuracy-SBM: 0.6327	test_loss: 0.8624 test_accuracy-SBM: 0.6340
2025-08-03 05:38:37,925 - INFO - train: {'epoch': 32, 'time_epoch': 88.72669, 'eta': 6101.81274, 'eta_hours': 1.69495, 'loss': 0.86942359, 'lr': 0.00081359, 'params': 483158, 'time_iter': 0.14196, 'accuracy': 0.6322, 'f1': 0.62275, 'accuracy-SBM': 0.63218, 'auc': 0.91453}
2025-08-03 05:38:42,258 - INFO - val: {'epoch': 32, 'time_epoch': 4.24207, 'loss': 0.87584632, 'lr': 0, 'params': 483158, 'time_iter': 0.06733, 'accuracy': 0.63021, 'f1': 0.61757, 'accuracy-SBM': 0.6308, 'auc': 0.91419}
2025-08-03 05:38:46,527 - INFO - test: {'epoch': 32, 'time_epoch': 4.23127, 'loss': 0.86683077, 'lr': 0, 'params': 483158, 'time_iter': 0.06716, 'accuracy': 0.63384, 'f1': 0.6203, 'accuracy-SBM': 0.63517, 'auc': 0.91518}
2025-08-03 05:38:46,529 - INFO - > Epoch 32: took 97.7s (avg 100.3s) | Best so far: epoch 29	train_loss: 0.8775 train_accuracy-SBM: 0.6295	val_loss: 0.8684 val_accuracy-SBM: 0.6327	test_loss: 0.8624 test_accuracy-SBM: 0.6340
2025-08-03 05:40:15,300 - INFO - train: {'epoch': 33, 'time_epoch': 88.51525, 'eta': 6005.77814, 'eta_hours': 1.66827, 'loss': 0.86625576, 'lr': 0.00080054, 'params': 483158, 'time_iter': 0.14162, 'accuracy': 0.63256, 'f1': 0.62283, 'accuracy-SBM': 0.63253, 'auc': 0.91508}
2025-08-03 05:40:19,427 - INFO - val: {'epoch': 33, 'time_epoch': 4.0761, 'loss': 0.861749, 'lr': 0, 'params': 483158, 'time_iter': 0.0647, 'accuracy': 0.63466, 'f1': 0.62301, 'accuracy-SBM': 0.63519, 'auc': 0.91637}
2025-08-03 05:40:23,604 - INFO - test: {'epoch': 33, 'time_epoch': 4.13021, 'loss': 0.85971681, 'lr': 0, 'params': 483158, 'time_iter': 0.06556, 'accuracy': 0.63425, 'f1': 0.62127, 'accuracy-SBM': 0.6355, 'auc': 0.91611}
2025-08-03 05:40:23,757 - INFO - > Epoch 33: took 97.2s (avg 100.2s) | Best so far: epoch 33	train_loss: 0.8663 train_accuracy-SBM: 0.6325	val_loss: 0.8617 val_accuracy-SBM: 0.6352	test_loss: 0.8597 test_accuracy-SBM: 0.6355
2025-08-03 05:41:52,646 - INFO - train: {'epoch': 34, 'time_epoch': 88.63285, 'eta': 5910.39161, 'eta_hours': 1.64178, 'loss': 0.86318269, 'lr': 0.00078716, 'params': 483158, 'time_iter': 0.14181, 'accuracy': 0.63403, 'f1': 0.62448, 'accuracy-SBM': 0.634, 'auc': 0.91549}
2025-08-03 05:41:56,983 - INFO - val: {'epoch': 34, 'time_epoch': 4.2506, 'loss': 0.86495009, 'lr': 0, 'params': 483158, 'time_iter': 0.06747, 'accuracy': 0.63094, 'f1': 0.59696, 'accuracy-SBM': 0.63201, 'auc': 0.91584}
2025-08-03 05:42:01,283 - INFO - test: {'epoch': 34, 'time_epoch': 4.16133, 'loss': 0.85407974, 'lr': 0, 'params': 483158, 'time_iter': 0.06605, 'accuracy': 0.63565, 'f1': 0.60207, 'accuracy-SBM': 0.63773, 'auc': 0.91715}
2025-08-03 05:42:01,547 - INFO - > Epoch 34: took 97.8s (avg 100.1s) | Best so far: epoch 33	train_loss: 0.8663 train_accuracy-SBM: 0.6325	val_loss: 0.8617 val_accuracy-SBM: 0.6352	test_loss: 0.8597 test_accuracy-SBM: 0.6355
2025-08-03 05:43:29,962 - INFO - train: {'epoch': 35, 'time_epoch': 88.14365, 'eta': 5814.51059, 'eta_hours': 1.61514, 'loss': 0.86043897, 'lr': 0.00077347, 'params': 483158, 'time_iter': 0.14103, 'accuracy': 0.63523, 'f1': 0.62556, 'accuracy-SBM': 0.63521, 'auc': 0.91603}
2025-08-03 05:43:34,216 - INFO - val: {'epoch': 35, 'time_epoch': 4.20653, 'loss': 0.8548136, 'lr': 0, 'params': 483158, 'time_iter': 0.06677, 'accuracy': 0.63502, 'f1': 0.62523, 'accuracy-SBM': 0.63573, 'auc': 0.91746}
2025-08-03 05:43:38,776 - INFO - test: {'epoch': 35, 'time_epoch': 4.24227, 'loss': 0.85440156, 'lr': 0, 'params': 483158, 'time_iter': 0.06734, 'accuracy': 0.63652, 'f1': 0.62685, 'accuracy-SBM': 0.63814, 'auc': 0.91711}
2025-08-03 05:43:39,123 - INFO - > Epoch 35: took 97.6s (avg 100.0s) | Best so far: epoch 35	train_loss: 0.8604 train_accuracy-SBM: 0.6352	val_loss: 0.8548 val_accuracy-SBM: 0.6357	test_loss: 0.8544 test_accuracy-SBM: 0.6381
2025-08-03 05:45:08,768 - INFO - train: {'epoch': 36, 'time_epoch': 89.27979, 'eta': 5720.98232, 'eta_hours': 1.58916, 'loss': 0.85725225, 'lr': 0.00075948, 'params': 483158, 'time_iter': 0.14285, 'accuracy': 0.63626, 'f1': 0.62647, 'accuracy-SBM': 0.63624, 'auc': 0.9166}
2025-08-03 05:45:13,133 - INFO - val: {'epoch': 36, 'time_epoch': 4.30363, 'loss': 0.86480916, 'lr': 0, 'params': 483158, 'time_iter': 0.06831, 'accuracy': 0.63442, 'f1': 0.60986, 'accuracy-SBM': 0.63538, 'auc': 0.91633}
2025-08-03 05:45:17,452 - INFO - test: {'epoch': 36, 'time_epoch': 4.27729, 'loss': 0.85507903, 'lr': 0, 'params': 483158, 'time_iter': 0.06789, 'accuracy': 0.6376, 'f1': 0.61482, 'accuracy-SBM': 0.63949, 'auc': 0.91726}
2025-08-03 05:45:17,454 - INFO - > Epoch 36: took 98.3s (avg 100.0s) | Best so far: epoch 35	train_loss: 0.8604 train_accuracy-SBM: 0.6352	val_loss: 0.8548 val_accuracy-SBM: 0.6357	test_loss: 0.8544 test_accuracy-SBM: 0.6381
2025-08-03 05:46:45,587 - INFO - train: {'epoch': 37, 'time_epoch': 87.75677, 'eta': 5625.19273, 'eta_hours': 1.56255, 'loss': 0.8542783, 'lr': 0.00074521, 'params': 483158, 'time_iter': 0.14041, 'accuracy': 0.63683, 'f1': 0.62736, 'accuracy-SBM': 0.63681, 'auc': 0.91704}
2025-08-03 05:46:49,884 - INFO - val: {'epoch': 37, 'time_epoch': 4.23036, 'loss': 0.86704236, 'lr': 0, 'params': 483158, 'time_iter': 0.06715, 'accuracy': 0.63281, 'f1': 0.62427, 'accuracy-SBM': 0.63334, 'auc': 0.91602}
2025-08-03 05:46:54,126 - INFO - test: {'epoch': 37, 'time_epoch': 4.20653, 'loss': 0.86033603, 'lr': 0, 'params': 483158, 'time_iter': 0.06677, 'accuracy': 0.63532, 'f1': 0.62696, 'accuracy-SBM': 0.63677, 'auc': 0.91657}
2025-08-03 05:46:54,128 - INFO - > Epoch 37: took 96.7s (avg 99.9s) | Best so far: epoch 35	train_loss: 0.8604 train_accuracy-SBM: 0.6352	val_loss: 0.8548 val_accuracy-SBM: 0.6357	test_loss: 0.8544 test_accuracy-SBM: 0.6381
2025-08-03 05:48:21,791 - INFO - train: {'epoch': 38, 'time_epoch': 87.40081, 'eta': 5529.25832, 'eta_hours': 1.53591, 'loss': 0.85390426, 'lr': 0.00073067, 'params': 483158, 'time_iter': 0.13984, 'accuracy': 0.63684, 'f1': 0.6273, 'accuracy-SBM': 0.63682, 'auc': 0.91712}
2025-08-03 05:48:26,107 - INFO - val: {'epoch': 38, 'time_epoch': 4.26056, 'loss': 0.86111708, 'lr': 0, 'params': 483158, 'time_iter': 0.06763, 'accuracy': 0.63425, 'f1': 0.60707, 'accuracy-SBM': 0.63522, 'auc': 0.91683}
2025-08-03 05:48:30,423 - INFO - test: {'epoch': 38, 'time_epoch': 4.27463, 'loss': 0.85130885, 'lr': 0, 'params': 483158, 'time_iter': 0.06785, 'accuracy': 0.63671, 'f1': 0.61012, 'accuracy-SBM': 0.63869, 'auc': 0.91774}
2025-08-03 05:48:30,425 - INFO - > Epoch 38: took 96.3s (avg 99.8s) | Best so far: epoch 35	train_loss: 0.8604 train_accuracy-SBM: 0.6352	val_loss: 0.8548 val_accuracy-SBM: 0.6357	test_loss: 0.8544 test_accuracy-SBM: 0.6381
2025-08-03 05:49:59,173 - INFO - train: {'epoch': 39, 'time_epoch': 88.48199, 'eta': 5435.37235, 'eta_hours': 1.50983, 'loss': 0.84911734, 'lr': 0.00071588, 'params': 483158, 'time_iter': 0.14157, 'accuracy': 0.63834, 'f1': 0.62872, 'accuracy-SBM': 0.63832, 'auc': 0.91787}
2025-08-03 05:50:03,526 - INFO - val: {'epoch': 39, 'time_epoch': 4.30332, 'loss': 0.85498426, 'lr': 0, 'params': 483158, 'time_iter': 0.06831, 'accuracy': 0.63434, 'f1': 0.61103, 'accuracy-SBM': 0.63533, 'auc': 0.91744}
2025-08-03 05:50:07,885 - INFO - test: {'epoch': 39, 'time_epoch': 4.29161, 'loss': 0.84696797, 'lr': 0, 'params': 483158, 'time_iter': 0.06812, 'accuracy': 0.63734, 'f1': 0.61455, 'accuracy-SBM': 0.63927, 'auc': 0.91828}
2025-08-03 05:50:07,887 - INFO - > Epoch 39: took 97.5s (avg 99.7s) | Best so far: epoch 35	train_loss: 0.8604 train_accuracy-SBM: 0.6352	val_loss: 0.8548 val_accuracy-SBM: 0.6357	test_loss: 0.8544 test_accuracy-SBM: 0.6381
2025-08-03 05:51:36,382 - INFO - train: {'epoch': 40, 'time_epoch': 88.13273, 'eta': 5341.24741, 'eta_hours': 1.48368, 'loss': 0.84615095, 'lr': 0.00070085, 'params': 483158, 'time_iter': 0.14101, 'accuracy': 0.63893, 'f1': 0.62872, 'accuracy-SBM': 0.6389, 'auc': 0.91831}
2025-08-03 05:51:40,661 - INFO - val: {'epoch': 40, 'time_epoch': 4.22997, 'loss': 0.85732412, 'lr': 0, 'params': 483158, 'time_iter': 0.06714, 'accuracy': 0.63575, 'f1': 0.62263, 'accuracy-SBM': 0.63659, 'auc': 0.91756}
2025-08-03 05:51:44,872 - INFO - test: {'epoch': 40, 'time_epoch': 4.17187, 'loss': 0.8539855, 'lr': 0, 'params': 483158, 'time_iter': 0.06622, 'accuracy': 0.63692, 'f1': 0.62513, 'accuracy-SBM': 0.63866, 'auc': 0.91734}
2025-08-03 05:51:44,874 - INFO - > Epoch 40: took 97.0s (avg 99.7s) | Best so far: epoch 40	train_loss: 0.8462 train_accuracy-SBM: 0.6389	val_loss: 0.8573 val_accuracy-SBM: 0.6366	test_loss: 0.8540 test_accuracy-SBM: 0.6387
2025-08-03 05:53:13,616 - INFO - train: {'epoch': 41, 'time_epoch': 88.37936, 'eta': 5247.7484, 'eta_hours': 1.45771, 'loss': 0.84401891, 'lr': 0.0006856, 'params': 483158, 'time_iter': 0.14141, 'accuracy': 0.63985, 'f1': 0.63056, 'accuracy-SBM': 0.63982, 'auc': 0.91878}
2025-08-03 05:53:17,903 - INFO - val: {'epoch': 41, 'time_epoch': 4.23508, 'loss': 0.86926716, 'lr': 0, 'params': 483158, 'time_iter': 0.06722, 'accuracy': 0.63422, 'f1': 0.62355, 'accuracy-SBM': 0.63475, 'auc': 0.91618}
2025-08-03 05:53:22,215 - INFO - test: {'epoch': 41, 'time_epoch': 4.23794, 'loss': 0.86172108, 'lr': 0, 'params': 483158, 'time_iter': 0.06727, 'accuracy': 0.63744, 'f1': 0.62709, 'accuracy-SBM': 0.63872, 'auc': 0.91682}
2025-08-03 05:53:22,217 - INFO - > Epoch 41: took 97.3s (avg 99.6s) | Best so far: epoch 40	train_loss: 0.8462 train_accuracy-SBM: 0.6389	val_loss: 0.8573 val_accuracy-SBM: 0.6366	test_loss: 0.8540 test_accuracy-SBM: 0.6387
2025-08-03 05:54:49,996 - INFO - train: {'epoch': 42, 'time_epoch': 87.51565, 'eta': 5153.34259, 'eta_hours': 1.43148, 'loss': 0.8399767, 'lr': 0.00067015, 'params': 483158, 'time_iter': 0.14003, 'accuracy': 0.64183, 'f1': 0.63292, 'accuracy-SBM': 0.64181, 'auc': 0.91943}
2025-08-03 05:54:54,051 - INFO - val: {'epoch': 42, 'time_epoch': 4.00982, 'loss': 0.85474152, 'lr': 0, 'params': 483158, 'time_iter': 0.06365, 'accuracy': 0.63858, 'f1': 0.62357, 'accuracy-SBM': 0.63883, 'auc': 0.91783}
2025-08-03 05:54:58,075 - INFO - test: {'epoch': 42, 'time_epoch': 3.99161, 'loss': 0.84664867, 'lr': 0, 'params': 483158, 'time_iter': 0.06336, 'accuracy': 0.63839, 'f1': 0.62249, 'accuracy-SBM': 0.63929, 'auc': 0.91861}
2025-08-03 05:54:58,077 - INFO - > Epoch 42: took 95.9s (avg 99.5s) | Best so far: epoch 42	train_loss: 0.8400 train_accuracy-SBM: 0.6418	val_loss: 0.8547 val_accuracy-SBM: 0.6388	test_loss: 0.8466 test_accuracy-SBM: 0.6393
2025-08-03 05:56:26,684 - INFO - train: {'epoch': 43, 'time_epoch': 88.35538, 'eta': 5060.31871, 'eta_hours': 1.40564, 'loss': 0.83712799, 'lr': 0.00065451, 'params': 483158, 'time_iter': 0.14137, 'accuracy': 0.64161, 'f1': 0.63261, 'accuracy-SBM': 0.64159, 'auc': 0.91981}
2025-08-03 05:56:30,846 - INFO - val: {'epoch': 43, 'time_epoch': 4.11646, 'loss': 0.8519141, 'lr': 0, 'params': 483158, 'time_iter': 0.06534, 'accuracy': 0.63795, 'f1': 0.62428, 'accuracy-SBM': 0.63882, 'auc': 0.91852}
2025-08-03 05:56:35,235 - INFO - test: {'epoch': 43, 'time_epoch': 4.16506, 'loss': 0.85065203, 'lr': 0, 'params': 483158, 'time_iter': 0.06611, 'accuracy': 0.6366, 'f1': 0.62438, 'accuracy-SBM': 0.63835, 'auc': 0.91826}
2025-08-03 05:56:35,411 - INFO - > Epoch 43: took 97.3s (avg 99.5s) | Best so far: epoch 42	train_loss: 0.8400 train_accuracy-SBM: 0.6418	val_loss: 0.8547 val_accuracy-SBM: 0.6388	test_loss: 0.8466 test_accuracy-SBM: 0.6393
2025-08-03 05:58:02,755 - INFO - train: {'epoch': 44, 'time_epoch': 87.09471, 'eta': 4965.9615, 'eta_hours': 1.37943, 'loss': 0.83616135, 'lr': 0.0006387, 'params': 483158, 'time_iter': 0.13935, 'accuracy': 0.6429, 'f1': 0.63398, 'accuracy-SBM': 0.64288, 'auc': 0.92001}
2025-08-03 05:58:07,019 - INFO - val: {'epoch': 44, 'time_epoch': 4.21529, 'loss': 0.84383622, 'lr': 0, 'params': 483158, 'time_iter': 0.06691, 'accuracy': 0.63817, 'f1': 0.62064, 'accuracy-SBM': 0.63905, 'auc': 0.91941}
2025-08-03 05:58:11,274 - INFO - test: {'epoch': 44, 'time_epoch': 4.22055, 'loss': 0.84380042, 'lr': 0, 'params': 483158, 'time_iter': 0.06699, 'accuracy': 0.64033, 'f1': 0.62374, 'accuracy-SBM': 0.64218, 'auc': 0.91911}
2025-08-03 05:58:11,277 - INFO - > Epoch 44: took 95.9s (avg 99.4s) | Best so far: epoch 44	train_loss: 0.8362 train_accuracy-SBM: 0.6429	val_loss: 0.8438 val_accuracy-SBM: 0.6391	test_loss: 0.8438 test_accuracy-SBM: 0.6422
2025-08-03 05:59:38,521 - INFO - train: {'epoch': 45, 'time_epoch': 86.99572, 'eta': 4871.80385, 'eta_hours': 1.35328, 'loss': 0.83442049, 'lr': 0.00062274, 'params': 483158, 'time_iter': 0.13919, 'accuracy': 0.6429, 'f1': 0.63379, 'accuracy-SBM': 0.64288, 'auc': 0.92029}
2025-08-03 05:59:42,779 - INFO - val: {'epoch': 45, 'time_epoch': 4.20991, 'loss': 0.8523898, 'lr': 0, 'params': 483158, 'time_iter': 0.06682, 'accuracy': 0.63763, 'f1': 0.62417, 'accuracy-SBM': 0.63845, 'auc': 0.91823}
2025-08-03 05:59:47,026 - INFO - test: {'epoch': 45, 'time_epoch': 4.2043, 'loss': 0.84762951, 'lr': 0, 'params': 483158, 'time_iter': 0.06673, 'accuracy': 0.6416, 'f1': 0.62886, 'accuracy-SBM': 0.64327, 'auc': 0.91863}
2025-08-03 05:59:47,028 - INFO - > Epoch 45: took 95.8s (avg 99.3s) | Best so far: epoch 44	train_loss: 0.8362 train_accuracy-SBM: 0.6429	val_loss: 0.8438 val_accuracy-SBM: 0.6391	test_loss: 0.8438 test_accuracy-SBM: 0.6422
2025-08-03 06:01:14,607 - INFO - train: {'epoch': 46, 'time_epoch': 87.33142, 'eta': 4778.32952, 'eta_hours': 1.32731, 'loss': 0.82973814, 'lr': 0.00060665, 'params': 483158, 'time_iter': 0.13973, 'accuracy': 0.64492, 'f1': 0.63526, 'accuracy-SBM': 0.6449, 'auc': 0.92101}
2025-08-03 06:01:18,594 - INFO - val: {'epoch': 46, 'time_epoch': 3.94143, 'loss': 0.84770266, 'lr': 0, 'params': 483158, 'time_iter': 0.06256, 'accuracy': 0.64214, 'f1': 0.61133, 'accuracy-SBM': 0.64264, 'auc': 0.919}
2025-08-03 06:01:22,554 - INFO - test: {'epoch': 46, 'time_epoch': 3.92582, 'loss': 0.84076142, 'lr': 0, 'params': 483158, 'time_iter': 0.06231, 'accuracy': 0.64322, 'f1': 0.61136, 'accuracy-SBM': 0.64417, 'auc': 0.91959}
2025-08-03 06:01:22,619 - INFO - > Epoch 46: took 95.6s (avg 99.2s) | Best so far: epoch 46	train_loss: 0.8297 train_accuracy-SBM: 0.6449	val_loss: 0.8477 val_accuracy-SBM: 0.6426	test_loss: 0.8408 test_accuracy-SBM: 0.6442
2025-08-03 06:02:50,591 - INFO - train: {'epoch': 47, 'time_epoch': 87.72049, 'eta': 4685.53264, 'eta_hours': 1.30154, 'loss': 0.82605895, 'lr': 0.00059044, 'params': 483158, 'time_iter': 0.14035, 'accuracy': 0.64593, 'f1': 0.63709, 'accuracy-SBM': 0.6459, 'auc': 0.92158}
2025-08-03 06:02:54,866 - INFO - val: {'epoch': 47, 'time_epoch': 4.22592, 'loss': 0.8388081, 'lr': 0, 'params': 483158, 'time_iter': 0.06708, 'accuracy': 0.63953, 'f1': 0.62278, 'accuracy-SBM': 0.64023, 'auc': 0.9202}
2025-08-03 06:02:59,057 - INFO - test: {'epoch': 47, 'time_epoch': 4.15564, 'loss': 0.83055135, 'lr': 0, 'params': 483158, 'time_iter': 0.06596, 'accuracy': 0.64573, 'f1': 0.62813, 'accuracy-SBM': 0.64676, 'auc': 0.921}
2025-08-03 06:02:59,058 - INFO - > Epoch 47: took 96.4s (avg 99.2s) | Best so far: epoch 46	train_loss: 0.8297 train_accuracy-SBM: 0.6449	val_loss: 0.8477 val_accuracy-SBM: 0.6426	test_loss: 0.8408 test_accuracy-SBM: 0.6442
2025-08-03 06:04:27,640 - INFO - train: {'epoch': 48, 'time_epoch': 88.22156, 'eta': 4593.46447, 'eta_hours': 1.27596, 'loss': 0.8242163, 'lr': 0.00057413, 'params': 483158, 'time_iter': 0.14115, 'accuracy': 0.6464, 'f1': 0.63749, 'accuracy-SBM': 0.64638, 'auc': 0.92188}
2025-08-03 06:04:31,920 - INFO - val: {'epoch': 48, 'time_epoch': 4.2314, 'loss': 0.84959504, 'lr': 0, 'params': 483158, 'time_iter': 0.06717, 'accuracy': 0.63982, 'f1': 0.63298, 'accuracy-SBM': 0.64048, 'auc': 0.91876}
2025-08-03 06:04:36,271 - INFO - test: {'epoch': 48, 'time_epoch': 4.22311, 'loss': 0.83208904, 'lr': 0, 'params': 483158, 'time_iter': 0.06703, 'accuracy': 0.64267, 'f1': 0.6357, 'accuracy-SBM': 0.64411, 'auc': 0.92077}
2025-08-03 06:04:36,458 - INFO - > Epoch 48: took 97.4s (avg 99.1s) | Best so far: epoch 46	train_loss: 0.8297 train_accuracy-SBM: 0.6449	val_loss: 0.8477 val_accuracy-SBM: 0.6426	test_loss: 0.8408 test_accuracy-SBM: 0.6442
2025-08-03 06:06:05,044 - INFO - train: {'epoch': 49, 'time_epoch': 88.33495, 'eta': 4501.66356, 'eta_hours': 1.25046, 'loss': 0.82237886, 'lr': 0.00055774, 'params': 483158, 'time_iter': 0.14134, 'accuracy': 0.64747, 'f1': 0.63879, 'accuracy-SBM': 0.64745, 'auc': 0.92209}
2025-08-03 06:06:09,383 - INFO - val: {'epoch': 49, 'time_epoch': 4.27944, 'loss': 0.83870657, 'lr': 0, 'params': 483158, 'time_iter': 0.06793, 'accuracy': 0.64295, 'f1': 0.61475, 'accuracy-SBM': 0.64342, 'auc': 0.92028}
2025-08-03 06:06:13,697 - INFO - test: {'epoch': 49, 'time_epoch': 4.27748, 'loss': 0.83304052, 'lr': 0, 'params': 483158, 'time_iter': 0.0679, 'accuracy': 0.64606, 'f1': 0.61665, 'accuracy-SBM': 0.64693, 'auc': 0.92063}
2025-08-03 06:06:13,699 - INFO - > Epoch 49: took 97.2s (avg 99.1s) | Best so far: epoch 49	train_loss: 0.8224 train_accuracy-SBM: 0.6474	val_loss: 0.8387 val_accuracy-SBM: 0.6434	test_loss: 0.8330 test_accuracy-SBM: 0.6469
2025-08-03 06:07:43,196 - INFO - train: {'epoch': 50, 'time_epoch': 89.23868, 'eta': 4410.86685, 'eta_hours': 1.22524, 'loss': 0.81866782, 'lr': 0.00054129, 'params': 483158, 'time_iter': 0.14278, 'accuracy': 0.64792, 'f1': 0.63912, 'accuracy-SBM': 0.6479, 'auc': 0.92277}
2025-08-03 06:07:47,599 - INFO - val: {'epoch': 50, 'time_epoch': 4.26462, 'loss': 0.84032189, 'lr': 0, 'params': 483158, 'time_iter': 0.06769, 'accuracy': 0.64251, 'f1': 0.62809, 'accuracy-SBM': 0.64302, 'auc': 0.92037}
2025-08-03 06:07:51,901 - INFO - test: {'epoch': 50, 'time_epoch': 4.26649, 'loss': 0.84173467, 'lr': 0, 'params': 483158, 'time_iter': 0.06772, 'accuracy': 0.64168, 'f1': 0.6264, 'accuracy-SBM': 0.64275, 'auc': 0.91968}
2025-08-03 06:07:51,903 - INFO - > Epoch 50: took 98.2s (avg 99.1s) | Best so far: epoch 49	train_loss: 0.8224 train_accuracy-SBM: 0.6474	val_loss: 0.8387 val_accuracy-SBM: 0.6434	test_loss: 0.8330 test_accuracy-SBM: 0.6469
2025-08-03 06:09:20,935 - INFO - train: {'epoch': 51, 'time_epoch': 88.77011, 'eta': 4319.69755, 'eta_hours': 1.19992, 'loss': 0.81760853, 'lr': 0.00052479, 'params': 483158, 'time_iter': 0.14203, 'accuracy': 0.64818, 'f1': 0.63968, 'accuracy-SBM': 0.64816, 'auc': 0.92293}
2025-08-03 06:09:25,301 - INFO - val: {'epoch': 51, 'time_epoch': 4.29153, 'loss': 0.85193621, 'lr': 0, 'params': 483158, 'time_iter': 0.06812, 'accuracy': 0.64191, 'f1': 0.62345, 'accuracy-SBM': 0.64286, 'auc': 0.91933}
2025-08-03 06:09:29,675 - INFO - test: {'epoch': 51, 'time_epoch': 4.28412, 'loss': 0.84477974, 'lr': 0, 'params': 483158, 'time_iter': 0.068, 'accuracy': 0.64174, 'f1': 0.62336, 'accuracy-SBM': 0.64359, 'auc': 0.91988}
2025-08-03 06:09:29,677 - INFO - > Epoch 51: took 97.8s (avg 99.1s) | Best so far: epoch 49	train_loss: 0.8224 train_accuracy-SBM: 0.6474	val_loss: 0.8387 val_accuracy-SBM: 0.6434	test_loss: 0.8330 test_accuracy-SBM: 0.6469
2025-08-03 06:10:58,326 - INFO - train: {'epoch': 52, 'time_epoch': 88.14293, 'eta': 4228.06261, 'eta_hours': 1.17446, 'loss': 0.81584771, 'lr': 0.00050827, 'params': 483158, 'time_iter': 0.14103, 'accuracy': 0.64856, 'f1': 0.63997, 'accuracy-SBM': 0.64854, 'auc': 0.92317}
2025-08-03 06:11:02,651 - INFO - val: {'epoch': 52, 'time_epoch': 4.24777, 'loss': 0.83842189, 'lr': 0, 'params': 483158, 'time_iter': 0.06742, 'accuracy': 0.64321, 'f1': 0.63204, 'accuracy-SBM': 0.64406, 'auc': 0.92073}
2025-08-03 06:11:06,918 - INFO - test: {'epoch': 52, 'time_epoch': 4.23179, 'loss': 0.8295096, 'lr': 0, 'params': 483158, 'time_iter': 0.06717, 'accuracy': 0.64347, 'f1': 0.63276, 'accuracy-SBM': 0.64511, 'auc': 0.92147}
2025-08-03 06:11:07,144 - INFO - > Epoch 52: took 97.5s (avg 99.0s) | Best so far: epoch 52	train_loss: 0.8158 train_accuracy-SBM: 0.6485	val_loss: 0.8384 val_accuracy-SBM: 0.6441	test_loss: 0.8295 test_accuracy-SBM: 0.6451
2025-08-03 06:12:33,424 - INFO - train: {'epoch': 53, 'time_epoch': 85.93016, 'eta': 4134.67204, 'eta_hours': 1.14852, 'loss': 0.81352617, 'lr': 0.00049173, 'params': 483158, 'time_iter': 0.13749, 'accuracy': 0.64968, 'f1': 0.64017, 'accuracy-SBM': 0.64965, 'auc': 0.92352}
2025-08-03 06:12:37,541 - INFO - val: {'epoch': 53, 'time_epoch': 4.03563, 'loss': 0.84124714, 'lr': 0, 'params': 483158, 'time_iter': 0.06406, 'accuracy': 0.6401, 'f1': 0.63157, 'accuracy-SBM': 0.64071, 'auc': 0.9201}
2025-08-03 06:12:41,654 - INFO - test: {'epoch': 53, 'time_epoch': 4.03897, 'loss': 0.84507489, 'lr': 0, 'params': 483158, 'time_iter': 0.06411, 'accuracy': 0.6413, 'f1': 0.63255, 'accuracy-SBM': 0.64245, 'auc': 0.91901}
2025-08-03 06:12:41,852 - INFO - > Epoch 53: took 94.7s (avg 99.0s) | Best so far: epoch 52	train_loss: 0.8158 train_accuracy-SBM: 0.6485	val_loss: 0.8384 val_accuracy-SBM: 0.6441	test_loss: 0.8295 test_accuracy-SBM: 0.6451
2025-08-03 06:14:07,504 - INFO - train: {'epoch': 54, 'time_epoch': 85.32309, 'eta': 4041.05607, 'eta_hours': 1.12252, 'loss': 0.80864093, 'lr': 0.00047521, 'params': 483158, 'time_iter': 0.13652, 'accuracy': 0.65099, 'f1': 0.64274, 'accuracy-SBM': 0.65096, 'auc': 0.92429}
2025-08-03 06:14:11,546 - INFO - val: {'epoch': 54, 'time_epoch': 3.87885, 'loss': 0.83516747, 'lr': 0, 'params': 483158, 'time_iter': 0.06157, 'accuracy': 0.64184, 'f1': 0.63254, 'accuracy-SBM': 0.64264, 'auc': 0.92124}
2025-08-03 06:14:15,584 - INFO - test: {'epoch': 54, 'time_epoch': 3.97546, 'loss': 0.82519608, 'lr': 0, 'params': 483158, 'time_iter': 0.0631, 'accuracy': 0.64761, 'f1': 0.63833, 'accuracy-SBM': 0.64921, 'auc': 0.92205}
2025-08-03 06:14:15,832 - INFO - > Epoch 54: took 94.0s (avg 98.9s) | Best so far: epoch 52	train_loss: 0.8158 train_accuracy-SBM: 0.6485	val_loss: 0.8384 val_accuracy-SBM: 0.6441	test_loss: 0.8295 test_accuracy-SBM: 0.6451
2025-08-03 06:15:43,715 - INFO - train: {'epoch': 55, 'time_epoch': 87.50382, 'eta': 3949.44971, 'eta_hours': 1.09707, 'loss': 0.80733053, 'lr': 0.00045871, 'params': 483158, 'time_iter': 0.14001, 'accuracy': 0.65212, 'f1': 0.64354, 'accuracy-SBM': 0.6521, 'auc': 0.92448}
2025-08-03 06:15:47,911 - INFO - val: {'epoch': 55, 'time_epoch': 4.13878, 'loss': 0.83588991, 'lr': 0, 'params': 483158, 'time_iter': 0.06569, 'accuracy': 0.64525, 'f1': 0.63242, 'accuracy-SBM': 0.64581, 'auc': 0.92081}
2025-08-03 06:15:52,145 - INFO - test: {'epoch': 55, 'time_epoch': 3.9022, 'loss': 0.8329883, 'lr': 0, 'params': 483158, 'time_iter': 0.06194, 'accuracy': 0.64622, 'f1': 0.63244, 'accuracy-SBM': 0.64738, 'auc': 0.92087}
2025-08-03 06:15:52,463 - INFO - > Epoch 55: took 96.6s (avg 98.8s) | Best so far: epoch 55	train_loss: 0.8073 train_accuracy-SBM: 0.6521	val_loss: 0.8359 val_accuracy-SBM: 0.6458	test_loss: 0.8330 test_accuracy-SBM: 0.6474
2025-08-03 06:17:20,535 - INFO - train: {'epoch': 56, 'time_epoch': 87.81657, 'eta': 3858.22322, 'eta_hours': 1.07173, 'loss': 0.80452476, 'lr': 0.00044226, 'params': 483158, 'time_iter': 0.14051, 'accuracy': 0.65271, 'f1': 0.6443, 'accuracy-SBM': 0.65269, 'auc': 0.92486}
2025-08-03 06:17:24,800 - INFO - val: {'epoch': 56, 'time_epoch': 4.21659, 'loss': 0.82908718, 'lr': 0, 'params': 483158, 'time_iter': 0.06693, 'accuracy': 0.64407, 'f1': 0.63465, 'accuracy-SBM': 0.64488, 'auc': 0.92199}
2025-08-03 06:17:28,822 - INFO - test: {'epoch': 56, 'time_epoch': 3.97143, 'loss': 0.82918458, 'lr': 0, 'params': 483158, 'time_iter': 0.06304, 'accuracy': 0.64524, 'f1': 0.63646, 'accuracy-SBM': 0.64685, 'auc': 0.92158}
2025-08-03 06:17:28,968 - INFO - > Epoch 56: took 96.5s (avg 98.8s) | Best so far: epoch 55	train_loss: 0.8073 train_accuracy-SBM: 0.6521	val_loss: 0.8359 val_accuracy-SBM: 0.6458	test_loss: 0.8330 test_accuracy-SBM: 0.6474
2025-08-03 06:18:57,530 - INFO - train: {'epoch': 57, 'time_epoch': 88.31012, 'eta': 3767.47172, 'eta_hours': 1.04652, 'loss': 0.80257053, 'lr': 0.00042587, 'params': 483158, 'time_iter': 0.1413, 'accuracy': 0.65333, 'f1': 0.6447, 'accuracy-SBM': 0.6533, 'auc': 0.92518}
2025-08-03 06:19:01,811 - INFO - val: {'epoch': 57, 'time_epoch': 4.23338, 'loss': 0.83484448, 'lr': 0, 'params': 483158, 'time_iter': 0.0672, 'accuracy': 0.64479, 'f1': 0.6375, 'accuracy-SBM': 0.64543, 'auc': 0.92125}
2025-08-03 06:19:05,855 - INFO - test: {'epoch': 57, 'time_epoch': 4.00933, 'loss': 0.82713234, 'lr': 0, 'params': 483158, 'time_iter': 0.06364, 'accuracy': 0.64595, 'f1': 0.63852, 'accuracy-SBM': 0.64743, 'auc': 0.92179}
2025-08-03 06:19:05,856 - INFO - > Epoch 57: took 96.9s (avg 98.8s) | Best so far: epoch 55	train_loss: 0.8073 train_accuracy-SBM: 0.6521	val_loss: 0.8359 val_accuracy-SBM: 0.6458	test_loss: 0.8330 test_accuracy-SBM: 0.6474
2025-08-03 06:20:34,547 - INFO - train: {'epoch': 58, 'time_epoch': 88.43747, 'eta': 3676.89148, 'eta_hours': 1.02136, 'loss': 0.80177808, 'lr': 0.00040956, 'params': 483158, 'time_iter': 0.1415, 'accuracy': 0.65313, 'f1': 0.6444, 'accuracy-SBM': 0.6531, 'auc': 0.92528}
2025-08-03 06:20:38,656 - INFO - val: {'epoch': 58, 'time_epoch': 4.03705, 'loss': 0.83560787, 'lr': 0, 'params': 483158, 'time_iter': 0.06408, 'accuracy': 0.64249, 'f1': 0.62943, 'accuracy-SBM': 0.64306, 'auc': 0.92107}
2025-08-03 06:20:42,709 - INFO - test: {'epoch': 58, 'time_epoch': 4.0178, 'loss': 0.82291536, 'lr': 0, 'params': 483158, 'time_iter': 0.06377, 'accuracy': 0.64709, 'f1': 0.63327, 'accuracy-SBM': 0.64822, 'auc': 0.9223}
2025-08-03 06:20:42,711 - INFO - > Epoch 58: took 96.9s (avg 98.7s) | Best so far: epoch 55	train_loss: 0.8073 train_accuracy-SBM: 0.6521	val_loss: 0.8359 val_accuracy-SBM: 0.6458	test_loss: 0.8330 test_accuracy-SBM: 0.6474
2025-08-03 06:22:10,710 - INFO - train: {'epoch': 59, 'time_epoch': 87.74272, 'eta': 3585.9195, 'eta_hours': 0.99609, 'loss': 0.79721268, 'lr': 0.00039335, 'params': 483158, 'time_iter': 0.14039, 'accuracy': 0.65522, 'f1': 0.64693, 'accuracy-SBM': 0.65519, 'auc': 0.926}
2025-08-03 06:22:14,989 - INFO - val: {'epoch': 59, 'time_epoch': 4.12802, 'loss': 0.82971757, 'lr': 0, 'params': 483158, 'time_iter': 0.06552, 'accuracy': 0.64518, 'f1': 0.63627, 'accuracy-SBM': 0.64594, 'auc': 0.92172}
2025-08-03 06:22:19,099 - INFO - test: {'epoch': 59, 'time_epoch': 4.06752, 'loss': 0.82454169, 'lr': 0, 'params': 483158, 'time_iter': 0.06456, 'accuracy': 0.64591, 'f1': 0.637, 'accuracy-SBM': 0.64744, 'auc': 0.92211}
2025-08-03 06:22:19,100 - INFO - > Epoch 59: took 96.4s (avg 98.7s) | Best so far: epoch 59	train_loss: 0.7972 train_accuracy-SBM: 0.6552	val_loss: 0.8297 val_accuracy-SBM: 0.6459	test_loss: 0.8245 test_accuracy-SBM: 0.6474
2025-08-03 06:23:48,261 - INFO - train: {'epoch': 60, 'time_epoch': 88.91227, 'eta': 3495.80113, 'eta_hours': 0.97106, 'loss': 0.79477727, 'lr': 0.00037726, 'params': 483158, 'time_iter': 0.14226, 'accuracy': 0.65581, 'f1': 0.64716, 'accuracy-SBM': 0.65578, 'auc': 0.92633}
2025-08-03 06:23:52,456 - INFO - val: {'epoch': 60, 'time_epoch': 4.14739, 'loss': 0.83329806, 'lr': 0, 'params': 483158, 'time_iter': 0.06583, 'accuracy': 0.64725, 'f1': 0.63246, 'accuracy-SBM': 0.64769, 'auc': 0.92156}
2025-08-03 06:23:56,459 - INFO - test: {'epoch': 60, 'time_epoch': 3.97032, 'loss': 0.8288463, 'lr': 0, 'params': 483158, 'time_iter': 0.06302, 'accuracy': 0.64935, 'f1': 0.63325, 'accuracy-SBM': 0.65037, 'auc': 0.92195}
2025-08-03 06:23:56,461 - INFO - > Epoch 60: took 97.4s (avg 98.7s) | Best so far: epoch 60	train_loss: 0.7948 train_accuracy-SBM: 0.6558	val_loss: 0.8333 val_accuracy-SBM: 0.6477	test_loss: 0.8288 test_accuracy-SBM: 0.6504
2025-08-03 06:25:25,261 - INFO - train: {'epoch': 61, 'time_epoch': 88.42083, 'eta': 3405.42047, 'eta_hours': 0.94595, 'loss': 0.79455647, 'lr': 0.0003613, 'params': 483158, 'time_iter': 0.14147, 'accuracy': 0.65611, 'f1': 0.64784, 'accuracy-SBM': 0.65609, 'auc': 0.9264}
2025-08-03 06:25:29,505 - INFO - val: {'epoch': 61, 'time_epoch': 4.19735, 'loss': 0.8299155, 'lr': 0, 'params': 483158, 'time_iter': 0.06662, 'accuracy': 0.64743, 'f1': 0.63492, 'accuracy-SBM': 0.64796, 'auc': 0.92195}
2025-08-03 06:25:33,643 - INFO - test: {'epoch': 61, 'time_epoch': 4.1041, 'loss': 0.81920054, 'lr': 0, 'params': 483158, 'time_iter': 0.06514, 'accuracy': 0.6494, 'f1': 0.63604, 'accuracy-SBM': 0.65052, 'auc': 0.92292}
2025-08-03 06:25:33,645 - INFO - > Epoch 61: took 97.2s (avg 98.6s) | Best so far: epoch 61	train_loss: 0.7946 train_accuracy-SBM: 0.6561	val_loss: 0.8299 val_accuracy-SBM: 0.6480	test_loss: 0.8192 test_accuracy-SBM: 0.6505
2025-08-03 06:27:03,102 - INFO - train: {'epoch': 62, 'time_epoch': 89.18635, 'eta': 3315.55161, 'eta_hours': 0.92099, 'loss': 0.79235474, 'lr': 0.00034549, 'params': 483158, 'time_iter': 0.1427, 'accuracy': 0.65626, 'f1': 0.64818, 'accuracy-SBM': 0.65623, 'auc': 0.92668}
2025-08-03 06:27:07,411 - INFO - val: {'epoch': 62, 'time_epoch': 4.2594, 'loss': 0.8364596, 'lr': 0, 'params': 483158, 'time_iter': 0.06761, 'accuracy': 0.64225, 'f1': 0.61574, 'accuracy-SBM': 0.64289, 'auc': 0.92079}
2025-08-03 06:27:11,691 - INFO - test: {'epoch': 62, 'time_epoch': 4.24469, 'loss': 0.82431616, 'lr': 0, 'params': 483158, 'time_iter': 0.06738, 'accuracy': 0.64772, 'f1': 0.62029, 'accuracy-SBM': 0.64862, 'auc': 0.92227}
2025-08-03 06:27:11,693 - INFO - > Epoch 62: took 98.0s (avg 98.6s) | Best so far: epoch 61	train_loss: 0.7946 train_accuracy-SBM: 0.6561	val_loss: 0.8299 val_accuracy-SBM: 0.6480	test_loss: 0.8192 test_accuracy-SBM: 0.6505
2025-08-03 06:28:39,482 - INFO - train: {'epoch': 63, 'time_epoch': 87.51292, 'eta': 3224.76278, 'eta_hours': 0.89577, 'loss': 0.78762102, 'lr': 0.00032985, 'params': 483158, 'time_iter': 0.14002, 'accuracy': 0.65835, 'f1': 0.64992, 'accuracy-SBM': 0.65833, 'auc': 0.9274}
2025-08-03 06:28:43,764 - INFO - val: {'epoch': 63, 'time_epoch': 4.23567, 'loss': 0.82502854, 'lr': 0, 'params': 483158, 'time_iter': 0.06723, 'accuracy': 0.64799, 'f1': 0.63582, 'accuracy-SBM': 0.64869, 'auc': 0.92255}
2025-08-03 06:28:47,831 - INFO - test: {'epoch': 63, 'time_epoch': 4.03344, 'loss': 0.82240871, 'lr': 0, 'params': 483158, 'time_iter': 0.06402, 'accuracy': 0.64765, 'f1': 0.63543, 'accuracy-SBM': 0.64936, 'auc': 0.92254}
2025-08-03 06:28:47,833 - INFO - > Epoch 63: took 96.1s (avg 98.6s) | Best so far: epoch 63	train_loss: 0.7876 train_accuracy-SBM: 0.6583	val_loss: 0.8250 val_accuracy-SBM: 0.6487	test_loss: 0.8224 test_accuracy-SBM: 0.6494
2025-08-03 06:30:15,520 - INFO - train: {'epoch': 64, 'time_epoch': 87.43246, 'eta': 3134.03142, 'eta_hours': 0.87056, 'loss': 0.78826986, 'lr': 0.0003144, 'params': 483158, 'time_iter': 0.13989, 'accuracy': 0.65785, 'f1': 0.64919, 'accuracy-SBM': 0.65783, 'auc': 0.92728}
2025-08-03 06:30:19,805 - INFO - val: {'epoch': 64, 'time_epoch': 4.23523, 'loss': 0.82620773, 'lr': 0, 'params': 483158, 'time_iter': 0.06723, 'accuracy': 0.64918, 'f1': 0.64077, 'accuracy-SBM': 0.64992, 'auc': 0.9225}
2025-08-03 06:30:24,043 - INFO - test: {'epoch': 64, 'time_epoch': 4.20387, 'loss': 0.81748783, 'lr': 0, 'params': 483158, 'time_iter': 0.06673, 'accuracy': 0.64936, 'f1': 0.64081, 'accuracy-SBM': 0.65094, 'auc': 0.92338}
2025-08-03 06:30:24,046 - INFO - > Epoch 64: took 96.2s (avg 98.6s) | Best so far: epoch 64	train_loss: 0.7883 train_accuracy-SBM: 0.6578	val_loss: 0.8262 val_accuracy-SBM: 0.6499	test_loss: 0.8175 test_accuracy-SBM: 0.6509
2025-08-03 06:31:52,137 - INFO - train: {'epoch': 65, 'time_epoch': 87.82192, 'eta': 3043.60066, 'eta_hours': 0.84544, 'loss': 0.78485337, 'lr': 0.00029915, 'params': 483158, 'time_iter': 0.14052, 'accuracy': 0.65865, 'f1': 0.65047, 'accuracy-SBM': 0.65863, 'auc': 0.92777}
2025-08-03 06:31:56,414 - INFO - val: {'epoch': 65, 'time_epoch': 4.22656, 'loss': 0.82350834, 'lr': 0, 'params': 483158, 'time_iter': 0.06709, 'accuracy': 0.64987, 'f1': 0.64202, 'accuracy-SBM': 0.65055, 'auc': 0.92318}
2025-08-03 06:32:01,130 - INFO - test: {'epoch': 65, 'time_epoch': 4.22987, 'loss': 0.82425345, 'lr': 0, 'params': 483158, 'time_iter': 0.06714, 'accuracy': 0.65087, 'f1': 0.64296, 'accuracy-SBM': 0.65232, 'auc': 0.92272}
2025-08-03 06:32:01,132 - INFO - > Epoch 65: took 97.1s (avg 98.5s) | Best so far: epoch 65	train_loss: 0.7849 train_accuracy-SBM: 0.6586	val_loss: 0.8235 val_accuracy-SBM: 0.6505	test_loss: 0.8243 test_accuracy-SBM: 0.6523
2025-08-03 06:33:29,168 - INFO - train: {'epoch': 66, 'time_epoch': 87.76093, 'eta': 2953.21774, 'eta_hours': 0.82034, 'loss': 0.78113185, 'lr': 0.00028412, 'params': 483158, 'time_iter': 0.14042, 'accuracy': 0.65965, 'f1': 0.6513, 'accuracy-SBM': 0.65962, 'auc': 0.9283}
2025-08-03 06:33:33,332 - INFO - val: {'epoch': 66, 'time_epoch': 4.11766, 'loss': 0.82757512, 'lr': 0, 'params': 483158, 'time_iter': 0.06536, 'accuracy': 0.64608, 'f1': 0.63591, 'accuracy-SBM': 0.64667, 'auc': 0.9223}
2025-08-03 06:33:37,440 - INFO - test: {'epoch': 66, 'time_epoch': 4.05007, 'loss': 0.82046908, 'lr': 0, 'params': 483158, 'time_iter': 0.06429, 'accuracy': 0.64861, 'f1': 0.63801, 'accuracy-SBM': 0.64979, 'auc': 0.92292}
2025-08-03 06:33:37,442 - INFO - > Epoch 66: took 96.3s (avg 98.5s) | Best so far: epoch 65	train_loss: 0.7849 train_accuracy-SBM: 0.6586	val_loss: 0.8235 val_accuracy-SBM: 0.6505	test_loss: 0.8243 test_accuracy-SBM: 0.6523
2025-08-03 06:35:05,767 - INFO - train: {'epoch': 67, 'time_epoch': 88.06887, 'eta': 2863.05684, 'eta_hours': 0.79529, 'loss': 0.7808894, 'lr': 0.00026933, 'params': 483158, 'time_iter': 0.14091, 'accuracy': 0.65994, 'f1': 0.65173, 'accuracy-SBM': 0.65991, 'auc': 0.9283}
2025-08-03 06:35:10,039 - INFO - val: {'epoch': 67, 'time_epoch': 4.22325, 'loss': 0.82793844, 'lr': 0, 'params': 483158, 'time_iter': 0.06704, 'accuracy': 0.64805, 'f1': 0.64067, 'accuracy-SBM': 0.64862, 'auc': 0.92263}
2025-08-03 06:35:14,305 - INFO - test: {'epoch': 67, 'time_epoch': 4.21262, 'loss': 0.82117071, 'lr': 0, 'params': 483158, 'time_iter': 0.06687, 'accuracy': 0.6493, 'f1': 0.64121, 'accuracy-SBM': 0.65073, 'auc': 0.92307}
2025-08-03 06:35:14,312 - INFO - > Epoch 67: took 96.9s (avg 98.5s) | Best so far: epoch 65	train_loss: 0.7849 train_accuracy-SBM: 0.6586	val_loss: 0.8235 val_accuracy-SBM: 0.6505	test_loss: 0.8243 test_accuracy-SBM: 0.6523
2025-08-03 06:36:42,571 - INFO - train: {'epoch': 68, 'time_epoch': 88.00043, 'eta': 2772.92584, 'eta_hours': 0.77026, 'loss': 0.77878394, 'lr': 0.00025479, 'params': 483158, 'time_iter': 0.1408, 'accuracy': 0.66127, 'f1': 0.65289, 'accuracy-SBM': 0.66125, 'auc': 0.92866}
2025-08-03 06:36:46,894 - INFO - val: {'epoch': 68, 'time_epoch': 4.21582, 'loss': 0.8171401, 'lr': 0, 'params': 483158, 'time_iter': 0.06692, 'accuracy': 0.65027, 'f1': 0.64304, 'accuracy-SBM': 0.65093, 'auc': 0.92393}
2025-08-03 06:36:51,286 - INFO - test: {'epoch': 68, 'time_epoch': 4.21579, 'loss': 0.81887154, 'lr': 0, 'params': 483158, 'time_iter': 0.06692, 'accuracy': 0.65218, 'f1': 0.64479, 'accuracy-SBM': 0.65363, 'auc': 0.9234}
2025-08-03 06:36:51,288 - INFO - > Epoch 68: took 97.0s (avg 98.4s) | Best so far: epoch 68	train_loss: 0.7788 train_accuracy-SBM: 0.6613	val_loss: 0.8171 val_accuracy-SBM: 0.6509	test_loss: 0.8189 test_accuracy-SBM: 0.6536
2025-08-03 06:38:19,231 - INFO - train: {'epoch': 69, 'time_epoch': 87.69242, 'eta': 2682.7237, 'eta_hours': 0.7452, 'loss': 0.77489726, 'lr': 0.00024052, 'params': 483158, 'time_iter': 0.14031, 'accuracy': 0.66215, 'f1': 0.65299, 'accuracy-SBM': 0.66212, 'auc': 0.92917}
2025-08-03 06:38:23,507 - INFO - val: {'epoch': 69, 'time_epoch': 4.22125, 'loss': 0.82471058, 'lr': 0, 'params': 483158, 'time_iter': 0.067, 'accuracy': 0.65021, 'f1': 0.62874, 'accuracy-SBM': 0.65064, 'auc': 0.9231}
2025-08-03 06:38:27,473 - INFO - test: {'epoch': 69, 'time_epoch': 3.93247, 'loss': 0.81773435, 'lr': 0, 'params': 483158, 'time_iter': 0.06242, 'accuracy': 0.65082, 'f1': 0.62956, 'accuracy-SBM': 0.65179, 'auc': 0.92358}
2025-08-03 06:38:27,475 - INFO - > Epoch 69: took 96.2s (avg 98.4s) | Best so far: epoch 68	train_loss: 0.7788 train_accuracy-SBM: 0.6613	val_loss: 0.8171 val_accuracy-SBM: 0.6509	test_loss: 0.8189 test_accuracy-SBM: 0.6536
2025-08-03 06:39:55,181 - INFO - train: {'epoch': 70, 'time_epoch': 87.44827, 'eta': 2592.49254, 'eta_hours': 0.72014, 'loss': 0.77616011, 'lr': 0.00022653, 'params': 483158, 'time_iter': 0.13992, 'accuracy': 0.66225, 'f1': 0.65434, 'accuracy-SBM': 0.66222, 'auc': 0.92904}
2025-08-03 06:39:59,500 - INFO - val: {'epoch': 70, 'time_epoch': 4.26962, 'loss': 0.82024156, 'lr': 0, 'params': 483158, 'time_iter': 0.06777, 'accuracy': 0.64997, 'f1': 0.63751, 'accuracy-SBM': 0.65051, 'auc': 0.92339}
2025-08-03 06:40:03,662 - INFO - test: {'epoch': 70, 'time_epoch': 4.12655, 'loss': 0.82031878, 'lr': 0, 'params': 483158, 'time_iter': 0.0655, 'accuracy': 0.65127, 'f1': 0.63836, 'accuracy-SBM': 0.65234, 'auc': 0.92307}
2025-08-03 06:40:03,664 - INFO - > Epoch 70: took 96.2s (avg 98.4s) | Best so far: epoch 68	train_loss: 0.7788 train_accuracy-SBM: 0.6613	val_loss: 0.8171 val_accuracy-SBM: 0.6509	test_loss: 0.8189 test_accuracy-SBM: 0.6536
2025-08-03 06:41:31,503 - INFO - train: {'epoch': 71, 'time_epoch': 87.58423, 'eta': 2502.39156, 'eta_hours': 0.69511, 'loss': 0.77180776, 'lr': 0.00021284, 'params': 483158, 'time_iter': 0.14013, 'accuracy': 0.6633, 'f1': 0.65534, 'accuracy-SBM': 0.66328, 'auc': 0.92966}
2025-08-03 06:41:35,830 - INFO - val: {'epoch': 71, 'time_epoch': 4.26916, 'loss': 0.82601151, 'lr': 0, 'params': 483158, 'time_iter': 0.06776, 'accuracy': 0.64883, 'f1': 0.63864, 'accuracy-SBM': 0.64953, 'auc': 0.92305}
2025-08-03 06:41:40,186 - INFO - test: {'epoch': 71, 'time_epoch': 4.24596, 'loss': 0.81757339, 'lr': 0, 'params': 483158, 'time_iter': 0.0674, 'accuracy': 0.65187, 'f1': 0.64159, 'accuracy-SBM': 0.65348, 'auc': 0.92383}
2025-08-03 06:41:40,204 - INFO - > Epoch 71: took 96.5s (avg 98.4s) | Best so far: epoch 68	train_loss: 0.7788 train_accuracy-SBM: 0.6613	val_loss: 0.8171 val_accuracy-SBM: 0.6509	test_loss: 0.8189 test_accuracy-SBM: 0.6536
2025-08-03 06:43:09,265 - INFO - train: {'epoch': 72, 'time_epoch': 88.80699, 'eta': 2412.81177, 'eta_hours': 0.67023, 'loss': 0.7698712, 'lr': 0.00019946, 'params': 483158, 'time_iter': 0.14209, 'accuracy': 0.66439, 'f1': 0.65659, 'accuracy-SBM': 0.66437, 'auc': 0.92989}
2025-08-03 06:43:13,573 - INFO - val: {'epoch': 72, 'time_epoch': 4.2581, 'loss': 0.82792639, 'lr': 0, 'params': 483158, 'time_iter': 0.06759, 'accuracy': 0.64836, 'f1': 0.63834, 'accuracy-SBM': 0.64905, 'auc': 0.92253}
2025-08-03 06:43:17,784 - INFO - test: {'epoch': 72, 'time_epoch': 4.17118, 'loss': 0.81813584, 'lr': 0, 'params': 483158, 'time_iter': 0.06621, 'accuracy': 0.64948, 'f1': 0.63977, 'accuracy-SBM': 0.65107, 'auc': 0.92347}
2025-08-03 06:43:17,806 - INFO - > Epoch 72: took 97.6s (avg 98.4s) | Best so far: epoch 68	train_loss: 0.7788 train_accuracy-SBM: 0.6613	val_loss: 0.8171 val_accuracy-SBM: 0.6509	test_loss: 0.8189 test_accuracy-SBM: 0.6536
2025-08-03 06:44:45,402 - INFO - train: {'epoch': 73, 'time_epoch': 87.34744, 'eta': 2322.74006, 'eta_hours': 0.64521, 'loss': 0.76906443, 'lr': 0.00018641, 'params': 483158, 'time_iter': 0.13976, 'accuracy': 0.6641, 'f1': 0.65602, 'accuracy-SBM': 0.66407, 'auc': 0.93001}
2025-08-03 06:44:49,671 - INFO - val: {'epoch': 73, 'time_epoch': 4.22042, 'loss': 0.82451744, 'lr': 0, 'params': 483158, 'time_iter': 0.06699, 'accuracy': 0.64886, 'f1': 0.64115, 'accuracy-SBM': 0.64961, 'auc': 0.923}
2025-08-03 06:44:53,920 - INFO - test: {'epoch': 73, 'time_epoch': 4.21049, 'loss': 0.81799724, 'lr': 0, 'params': 483158, 'time_iter': 0.06683, 'accuracy': 0.65032, 'f1': 0.64232, 'accuracy-SBM': 0.6518, 'auc': 0.92339}
2025-08-03 06:44:53,922 - INFO - > Epoch 73: took 96.1s (avg 98.3s) | Best so far: epoch 68	train_loss: 0.7788 train_accuracy-SBM: 0.6613	val_loss: 0.8171 val_accuracy-SBM: 0.6509	test_loss: 0.8189 test_accuracy-SBM: 0.6536
2025-08-03 06:46:21,651 - INFO - train: {'epoch': 74, 'time_epoch': 87.36645, 'eta': 2232.74734, 'eta_hours': 0.62021, 'loss': 0.7670139, 'lr': 0.00017371, 'params': 483158, 'time_iter': 0.13979, 'accuracy': 0.66465, 'f1': 0.65676, 'accuracy-SBM': 0.66462, 'auc': 0.93032}
2025-08-03 06:46:25,833 - INFO - val: {'epoch': 74, 'time_epoch': 4.07209, 'loss': 0.83455934, 'lr': 0, 'params': 483158, 'time_iter': 0.06464, 'accuracy': 0.64902, 'f1': 0.63914, 'accuracy-SBM': 0.64984, 'auc': 0.92193}
2025-08-03 06:46:29,876 - INFO - test: {'epoch': 74, 'time_epoch': 3.9834, 'loss': 0.82077043, 'lr': 0, 'params': 483158, 'time_iter': 0.06323, 'accuracy': 0.64911, 'f1': 0.63843, 'accuracy-SBM': 0.65077, 'auc': 0.92323}
2025-08-03 06:46:29,970 - INFO - > Epoch 74: took 96.0s (avg 98.3s) | Best so far: epoch 68	train_loss: 0.7788 train_accuracy-SBM: 0.6613	val_loss: 0.8171 val_accuracy-SBM: 0.6509	test_loss: 0.8189 test_accuracy-SBM: 0.6536
2025-08-03 06:47:57,656 - INFO - train: {'epoch': 75, 'time_epoch': 87.2344, 'eta': 2142.78203, 'eta_hours': 0.59522, 'loss': 0.76471684, 'lr': 0.00016136, 'params': 483158, 'time_iter': 0.13958, 'accuracy': 0.66609, 'f1': 0.65769, 'accuracy-SBM': 0.66607, 'auc': 0.93063}
2025-08-03 06:48:01,922 - INFO - val: {'epoch': 75, 'time_epoch': 4.21621, 'loss': 0.8171937, 'lr': 0, 'params': 483158, 'time_iter': 0.06692, 'accuracy': 0.65169, 'f1': 0.6445, 'accuracy-SBM': 0.65238, 'auc': 0.92385}
2025-08-03 06:48:06,910 - INFO - test: {'epoch': 75, 'time_epoch': 4.2385, 'loss': 0.81424194, 'lr': 0, 'params': 483158, 'time_iter': 0.06728, 'accuracy': 0.65186, 'f1': 0.64414, 'accuracy-SBM': 0.65332, 'auc': 0.92401}
2025-08-03 06:48:07,151 - INFO - > Epoch 75: took 97.2s (avg 98.3s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 06:49:34,779 - INFO - train: {'epoch': 76, 'time_epoch': 87.37661, 'eta': 2052.93012, 'eta_hours': 0.57026, 'loss': 0.76417668, 'lr': 0.00014938, 'params': 483158, 'time_iter': 0.1398, 'accuracy': 0.66598, 'f1': 0.65802, 'accuracy-SBM': 0.66596, 'auc': 0.93069}
2025-08-03 06:49:38,949 - INFO - val: {'epoch': 76, 'time_epoch': 4.0982, 'loss': 0.81886889, 'lr': 0, 'params': 483158, 'time_iter': 0.06505, 'accuracy': 0.6486, 'f1': 0.64085, 'accuracy-SBM': 0.64937, 'auc': 0.92368}
2025-08-03 06:49:42,970 - INFO - test: {'epoch': 76, 'time_epoch': 3.98757, 'loss': 0.81617921, 'lr': 0, 'params': 483158, 'time_iter': 0.06329, 'accuracy': 0.64985, 'f1': 0.64169, 'accuracy-SBM': 0.65143, 'auc': 0.92379}
2025-08-03 06:49:42,972 - INFO - > Epoch 76: took 95.8s (avg 98.2s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 06:51:11,250 - INFO - train: {'epoch': 77, 'time_epoch': 88.02802, 'eta': 1963.32542, 'eta_hours': 0.54537, 'loss': 0.76230686, 'lr': 0.00013779, 'params': 483158, 'time_iter': 0.14084, 'accuracy': 0.6664, 'f1': 0.65846, 'accuracy-SBM': 0.66638, 'auc': 0.93094}
2025-08-03 06:51:15,541 - INFO - val: {'epoch': 77, 'time_epoch': 4.24211, 'loss': 0.82420266, 'lr': 0, 'params': 483158, 'time_iter': 0.06734, 'accuracy': 0.64828, 'f1': 0.63892, 'accuracy-SBM': 0.64889, 'auc': 0.92318}
2025-08-03 06:51:19,817 - INFO - test: {'epoch': 77, 'time_epoch': 4.23871, 'loss': 0.81446334, 'lr': 0, 'params': 483158, 'time_iter': 0.06728, 'accuracy': 0.65314, 'f1': 0.64357, 'accuracy-SBM': 0.65441, 'auc': 0.92404}
2025-08-03 06:51:19,819 - INFO - > Epoch 77: took 96.8s (avg 98.2s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 06:52:49,222 - INFO - train: {'epoch': 78, 'time_epoch': 89.13208, 'eta': 1874.05412, 'eta_hours': 0.52057, 'loss': 0.76030709, 'lr': 0.00012659, 'params': 483158, 'time_iter': 0.14261, 'accuracy': 0.66696, 'f1': 0.65883, 'accuracy-SBM': 0.66694, 'auc': 0.93121}
2025-08-03 06:52:53,616 - INFO - val: {'epoch': 78, 'time_epoch': 4.34319, 'loss': 0.81966884, 'lr': 0, 'params': 483158, 'time_iter': 0.06894, 'accuracy': 0.6503, 'f1': 0.63892, 'accuracy-SBM': 0.65109, 'auc': 0.92349}
2025-08-03 06:52:57,989 - INFO - test: {'epoch': 78, 'time_epoch': 4.3118, 'loss': 0.81293866, 'lr': 0, 'params': 483158, 'time_iter': 0.06844, 'accuracy': 0.65048, 'f1': 0.6388, 'accuracy-SBM': 0.65221, 'auc': 0.92407}
2025-08-03 06:52:57,991 - INFO - > Epoch 78: took 98.2s (avg 98.2s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 06:54:26,876 - INFO - train: {'epoch': 79, 'time_epoch': 88.6032, 'eta': 1784.65408, 'eta_hours': 0.49574, 'loss': 0.76021605, 'lr': 0.0001158, 'params': 483158, 'time_iter': 0.14177, 'accuracy': 0.6673, 'f1': 0.65796, 'accuracy-SBM': 0.66728, 'auc': 0.93122}
2025-08-03 06:54:31,187 - INFO - val: {'epoch': 79, 'time_epoch': 4.26166, 'loss': 0.82101321, 'lr': 0, 'params': 483158, 'time_iter': 0.06765, 'accuracy': 0.64894, 'f1': 0.64137, 'accuracy-SBM': 0.64961, 'auc': 0.92351}
2025-08-03 06:54:35,476 - INFO - test: {'epoch': 79, 'time_epoch': 4.2523, 'loss': 0.81174875, 'lr': 0, 'params': 483158, 'time_iter': 0.0675, 'accuracy': 0.65386, 'f1': 0.64625, 'accuracy-SBM': 0.6553, 'auc': 0.92445}
2025-08-03 06:54:35,567 - INFO - > Epoch 79: took 97.6s (avg 98.2s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 06:56:03,483 - INFO - train: {'epoch': 80, 'time_epoch': 87.64987, 'eta': 1695.0501, 'eta_hours': 0.47085, 'loss': 0.75681461, 'lr': 0.00010543, 'params': 483158, 'time_iter': 0.14024, 'accuracy': 0.6683, 'f1': 0.66043, 'accuracy-SBM': 0.66828, 'auc': 0.93172}
2025-08-03 06:56:07,771 - INFO - val: {'epoch': 80, 'time_epoch': 4.23896, 'loss': 0.82685559, 'lr': 0, 'params': 483158, 'time_iter': 0.06729, 'accuracy': 0.64811, 'f1': 0.64125, 'accuracy-SBM': 0.64878, 'auc': 0.92305}
2025-08-03 06:56:11,829 - INFO - test: {'epoch': 80, 'time_epoch': 4.02545, 'loss': 0.81152853, 'lr': 0, 'params': 483158, 'time_iter': 0.0639, 'accuracy': 0.65372, 'f1': 0.6467, 'accuracy-SBM': 0.65509, 'auc': 0.9247}
2025-08-03 06:56:11,831 - INFO - > Epoch 80: took 96.3s (avg 98.2s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 06:57:39,813 - INFO - train: {'epoch': 81, 'time_epoch': 87.61238, 'eta': 1605.48554, 'eta_hours': 0.44597, 'loss': 0.75608197, 'lr': 9.549e-05, 'params': 483158, 'time_iter': 0.14018, 'accuracy': 0.66892, 'f1': 0.6608, 'accuracy-SBM': 0.6689, 'auc': 0.93173}
2025-08-03 06:57:44,082 - INFO - val: {'epoch': 81, 'time_epoch': 4.22113, 'loss': 0.82207546, 'lr': 0, 'params': 483158, 'time_iter': 0.067, 'accuracy': 0.64915, 'f1': 0.64192, 'accuracy-SBM': 0.64984, 'auc': 0.9233}
2025-08-03 06:57:48,322 - INFO - test: {'epoch': 81, 'time_epoch': 4.20306, 'loss': 0.81394837, 'lr': 0, 'params': 483158, 'time_iter': 0.06672, 'accuracy': 0.65108, 'f1': 0.64296, 'accuracy-SBM': 0.6526, 'auc': 0.92403}
2025-08-03 06:57:48,324 - INFO - > Epoch 81: took 96.5s (avg 98.2s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 06:59:16,681 - INFO - train: {'epoch': 82, 'time_epoch': 87.89123, 'eta': 1516.02515, 'eta_hours': 0.42112, 'loss': 0.75489491, 'lr': 8.6e-05, 'params': 483158, 'time_iter': 0.14063, 'accuracy': 0.66888, 'f1': 0.66005, 'accuracy-SBM': 0.66886, 'auc': 0.93193}
2025-08-03 06:59:20,969 - INFO - val: {'epoch': 82, 'time_epoch': 4.23909, 'loss': 0.8208211, 'lr': 0, 'params': 483158, 'time_iter': 0.06729, 'accuracy': 0.65014, 'f1': 0.64271, 'accuracy-SBM': 0.65077, 'auc': 0.92352}
2025-08-03 06:59:25,217 - INFO - test: {'epoch': 82, 'time_epoch': 4.2123, 'loss': 0.81655646, 'lr': 0, 'params': 483158, 'time_iter': 0.06686, 'accuracy': 0.6507, 'f1': 0.64312, 'accuracy-SBM': 0.65204, 'auc': 0.92375}
2025-08-03 06:59:25,219 - INFO - > Epoch 82: took 96.9s (avg 98.2s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:00:52,868 - INFO - train: {'epoch': 83, 'time_epoch': 87.39725, 'eta': 1426.50802, 'eta_hours': 0.39625, 'loss': 0.75524302, 'lr': 7.695e-05, 'params': 483158, 'time_iter': 0.13984, 'accuracy': 0.66924, 'f1': 0.66121, 'accuracy-SBM': 0.66922, 'auc': 0.93191}
2025-08-03 07:00:57,147 - INFO - val: {'epoch': 83, 'time_epoch': 4.23023, 'loss': 0.82078631, 'lr': 0, 'params': 483158, 'time_iter': 0.06715, 'accuracy': 0.65009, 'f1': 0.64013, 'accuracy-SBM': 0.65086, 'auc': 0.92358}
2025-08-03 07:01:01,613 - INFO - test: {'epoch': 83, 'time_epoch': 4.24314, 'loss': 0.81389981, 'lr': 0, 'params': 483158, 'time_iter': 0.06735, 'accuracy': 0.64957, 'f1': 0.63883, 'accuracy-SBM': 0.65125, 'auc': 0.92418}
2025-08-03 07:01:01,615 - INFO - > Epoch 83: took 96.4s (avg 98.1s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:02:30,261 - INFO - train: {'epoch': 84, 'time_epoch': 88.38672, 'eta': 1337.21538, 'eta_hours': 0.37145, 'loss': 0.75312581, 'lr': 6.837e-05, 'params': 483158, 'time_iter': 0.14142, 'accuracy': 0.66923, 'f1': 0.66032, 'accuracy-SBM': 0.6692, 'auc': 0.93223}
2025-08-03 07:02:34,397 - INFO - val: {'epoch': 84, 'time_epoch': 4.08949, 'loss': 0.82039404, 'lr': 0, 'params': 483158, 'time_iter': 0.06491, 'accuracy': 0.65065, 'f1': 0.64392, 'accuracy-SBM': 0.65129, 'auc': 0.92379}
2025-08-03 07:02:38,530 - INFO - test: {'epoch': 84, 'time_epoch': 4.0989, 'loss': 0.81494845, 'lr': 0, 'params': 483158, 'time_iter': 0.06506, 'accuracy': 0.65212, 'f1': 0.64502, 'accuracy-SBM': 0.65345, 'auc': 0.9241}
2025-08-03 07:02:38,532 - INFO - > Epoch 84: took 96.9s (avg 98.1s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:04:06,089 - INFO - train: {'epoch': 85, 'time_epoch': 87.30752, 'eta': 1247.76812, 'eta_hours': 0.3466, 'loss': 0.75273947, 'lr': 6.026e-05, 'params': 483158, 'time_iter': 0.13969, 'accuracy': 0.66936, 'f1': 0.66153, 'accuracy-SBM': 0.66933, 'auc': 0.93225}
2025-08-03 07:04:10,362 - INFO - val: {'epoch': 85, 'time_epoch': 4.22416, 'loss': 0.82590205, 'lr': 0, 'params': 483158, 'time_iter': 0.06705, 'accuracy': 0.64728, 'f1': 0.63958, 'accuracy-SBM': 0.64797, 'auc': 0.92321}
2025-08-03 07:04:14,626 - INFO - test: {'epoch': 85, 'time_epoch': 4.2262, 'loss': 0.81931208, 'lr': 0, 'params': 483158, 'time_iter': 0.06708, 'accuracy': 0.65102, 'f1': 0.64325, 'accuracy-SBM': 0.65247, 'auc': 0.92378}
2025-08-03 07:04:14,628 - INFO - > Epoch 85: took 96.1s (avg 98.1s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:05:43,525 - INFO - train: {'epoch': 86, 'time_epoch': 88.63267, 'eta': 1158.56807, 'eta_hours': 0.32182, 'loss': 0.75134067, 'lr': 5.264e-05, 'params': 483158, 'time_iter': 0.14181, 'accuracy': 0.67033, 'f1': 0.66268, 'accuracy-SBM': 0.67031, 'auc': 0.93244}
2025-08-03 07:05:47,772 - INFO - val: {'epoch': 86, 'time_epoch': 4.19613, 'loss': 0.8214994, 'lr': 0, 'params': 483158, 'time_iter': 0.06661, 'accuracy': 0.64989, 'f1': 0.64303, 'accuracy-SBM': 0.6506, 'auc': 0.9237}
2025-08-03 07:05:51,929 - INFO - test: {'epoch': 86, 'time_epoch': 4.12287, 'loss': 0.81809174, 'lr': 0, 'params': 483158, 'time_iter': 0.06544, 'accuracy': 0.65016, 'f1': 0.64305, 'accuracy-SBM': 0.65166, 'auc': 0.92379}
2025-08-03 07:05:51,932 - INFO - > Epoch 86: took 97.3s (avg 98.1s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:07:20,619 - INFO - train: {'epoch': 87, 'time_epoch': 88.43498, 'eta': 1069.35395, 'eta_hours': 0.29704, 'loss': 0.74979124, 'lr': 4.55e-05, 'params': 483158, 'time_iter': 0.1415, 'accuracy': 0.67085, 'f1': 0.66276, 'accuracy-SBM': 0.67083, 'auc': 0.93267}
2025-08-03 07:07:24,918 - INFO - val: {'epoch': 87, 'time_epoch': 4.24966, 'loss': 0.82227619, 'lr': 0, 'params': 483158, 'time_iter': 0.06745, 'accuracy': 0.65, 'f1': 0.64274, 'accuracy-SBM': 0.65067, 'auc': 0.92337}
2025-08-03 07:07:29,168 - INFO - test: {'epoch': 87, 'time_epoch': 4.2148, 'loss': 0.81453039, 'lr': 0, 'params': 483158, 'time_iter': 0.0669, 'accuracy': 0.65265, 'f1': 0.64502, 'accuracy-SBM': 0.65409, 'auc': 0.92404}
2025-08-03 07:07:29,404 - INFO - > Epoch 87: took 97.5s (avg 98.1s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:08:57,568 - INFO - train: {'epoch': 88, 'time_epoch': 87.89796, 'eta': 980.09097, 'eta_hours': 0.27225, 'loss': 0.74890112, 'lr': 3.886e-05, 'params': 483158, 'time_iter': 0.14064, 'accuracy': 0.67115, 'f1': 0.66339, 'accuracy-SBM': 0.67113, 'auc': 0.93279}
2025-08-03 07:09:01,879 - INFO - val: {'epoch': 88, 'time_epoch': 4.26231, 'loss': 0.82363944, 'lr': 0, 'params': 483158, 'time_iter': 0.06766, 'accuracy': 0.64911, 'f1': 0.64218, 'accuracy-SBM': 0.64974, 'auc': 0.92339}
2025-08-03 07:09:06,164 - INFO - test: {'epoch': 88, 'time_epoch': 4.24961, 'loss': 0.81651714, 'lr': 0, 'params': 483158, 'time_iter': 0.06745, 'accuracy': 0.65256, 'f1': 0.64511, 'accuracy-SBM': 0.65389, 'auc': 0.92394}
2025-08-03 07:09:06,168 - INFO - > Epoch 88: took 96.8s (avg 98.1s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:10:34,710 - INFO - train: {'epoch': 89, 'time_epoch': 88.28808, 'eta': 890.90167, 'eta_hours': 0.24747, 'loss': 0.74800389, 'lr': 3.272e-05, 'params': 483158, 'time_iter': 0.14126, 'accuracy': 0.67062, 'f1': 0.66289, 'accuracy-SBM': 0.6706, 'auc': 0.9329}
2025-08-03 07:10:38,931 - INFO - val: {'epoch': 89, 'time_epoch': 4.17365, 'loss': 0.82146993, 'lr': 0, 'params': 483158, 'time_iter': 0.06625, 'accuracy': 0.6507, 'f1': 0.64399, 'accuracy-SBM': 0.65137, 'auc': 0.92377}
2025-08-03 07:10:42,994 - INFO - test: {'epoch': 89, 'time_epoch': 4.01377, 'loss': 0.81400209, 'lr': 0, 'params': 483158, 'time_iter': 0.06371, 'accuracy': 0.65245, 'f1': 0.64528, 'accuracy-SBM': 0.65393, 'auc': 0.92431}
2025-08-03 07:10:43,006 - INFO - > Epoch 89: took 96.8s (avg 98.1s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:12:10,745 - INFO - train: {'epoch': 90, 'time_epoch': 87.46227, 'eta': 801.6505, 'eta_hours': 0.22268, 'loss': 0.74717408, 'lr': 2.709e-05, 'params': 483158, 'time_iter': 0.13994, 'accuracy': 0.67198, 'f1': 0.6642, 'accuracy-SBM': 0.67195, 'auc': 0.93302}
2025-08-03 07:12:15,111 - INFO - val: {'epoch': 90, 'time_epoch': 4.31539, 'loss': 0.82351435, 'lr': 0, 'params': 483158, 'time_iter': 0.0685, 'accuracy': 0.64979, 'f1': 0.64247, 'accuracy-SBM': 0.65053, 'auc': 0.92346}
2025-08-03 07:12:19,843 - INFO - test: {'epoch': 90, 'time_epoch': 4.30876, 'loss': 0.8143861, 'lr': 0, 'params': 483158, 'time_iter': 0.06839, 'accuracy': 0.65284, 'f1': 0.64517, 'accuracy-SBM': 0.65431, 'auc': 0.92427}
2025-08-03 07:12:20,111 - INFO - > Epoch 90: took 97.1s (avg 98.0s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:13:49,020 - INFO - train: {'epoch': 91, 'time_epoch': 88.64958, 'eta': 712.54147, 'eta_hours': 0.19793, 'loss': 0.7464728, 'lr': 2.198e-05, 'params': 483158, 'time_iter': 0.14184, 'accuracy': 0.67177, 'f1': 0.66365, 'accuracy-SBM': 0.67175, 'auc': 0.93309}
2025-08-03 07:13:53,386 - INFO - val: {'epoch': 91, 'time_epoch': 4.23223, 'loss': 0.82243991, 'lr': 0, 'params': 483158, 'time_iter': 0.06718, 'accuracy': 0.64935, 'f1': 0.64288, 'accuracy-SBM': 0.65003, 'auc': 0.92357}
2025-08-03 07:13:57,870 - INFO - test: {'epoch': 91, 'time_epoch': 4.25023, 'loss': 0.81170474, 'lr': 0, 'params': 483158, 'time_iter': 0.06746, 'accuracy': 0.65247, 'f1': 0.64568, 'accuracy-SBM': 0.65389, 'auc': 0.9246}
2025-08-03 07:13:58,146 - INFO - > Epoch 91: took 98.0s (avg 98.0s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:15:26,311 - INFO - train: {'epoch': 92, 'time_epoch': 87.88761, 'eta': 623.38496, 'eta_hours': 0.17316, 'loss': 0.74690376, 'lr': 1.74e-05, 'params': 483158, 'time_iter': 0.14062, 'accuracy': 0.67192, 'f1': 0.66423, 'accuracy-SBM': 0.67189, 'auc': 0.93302}
2025-08-03 07:15:30,678 - INFO - val: {'epoch': 92, 'time_epoch': 4.3165, 'loss': 0.82171183, 'lr': 0, 'params': 483158, 'time_iter': 0.06852, 'accuracy': 0.64986, 'f1': 0.64261, 'accuracy-SBM': 0.65054, 'auc': 0.92361}
2025-08-03 07:15:34,975 - INFO - test: {'epoch': 92, 'time_epoch': 4.26067, 'loss': 0.81528923, 'lr': 0, 'params': 483158, 'time_iter': 0.06763, 'accuracy': 0.65191, 'f1': 0.6444, 'accuracy-SBM': 0.65339, 'auc': 0.92409}
2025-08-03 07:15:34,977 - INFO - > Epoch 92: took 96.8s (avg 98.0s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:17:03,291 - INFO - train: {'epoch': 93, 'time_epoch': 87.96745, 'eta': 534.26055, 'eta_hours': 0.14841, 'loss': 0.7460898, 'lr': 1.334e-05, 'params': 483158, 'time_iter': 0.14075, 'accuracy': 0.67126, 'f1': 0.66331, 'accuracy-SBM': 0.67123, 'auc': 0.93314}
2025-08-03 07:17:07,243 - INFO - val: {'epoch': 93, 'time_epoch': 3.90566, 'loss': 0.82011567, 'lr': 0, 'params': 483158, 'time_iter': 0.06199, 'accuracy': 0.65028, 'f1': 0.64317, 'accuracy-SBM': 0.651, 'auc': 0.92374}
2025-08-03 07:17:11,238 - INFO - test: {'epoch': 93, 'time_epoch': 3.95773, 'loss': 0.81323617, 'lr': 0, 'params': 483158, 'time_iter': 0.06282, 'accuracy': 0.65317, 'f1': 0.64593, 'accuracy-SBM': 0.65465, 'auc': 0.92431}
2025-08-03 07:17:11,240 - INFO - > Epoch 93: took 96.3s (avg 98.0s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:18:39,132 - INFO - train: {'epoch': 94, 'time_epoch': 87.6366, 'eta': 445.14308, 'eta_hours': 0.12365, 'loss': 0.74529321, 'lr': 9.81e-06, 'params': 483158, 'time_iter': 0.14022, 'accuracy': 0.67216, 'f1': 0.66439, 'accuracy-SBM': 0.67213, 'auc': 0.93322}
2025-08-03 07:18:43,451 - INFO - val: {'epoch': 94, 'time_epoch': 4.27028, 'loss': 0.82034122, 'lr': 0, 'params': 483158, 'time_iter': 0.06778, 'accuracy': 0.65022, 'f1': 0.64298, 'accuracy-SBM': 0.65089, 'auc': 0.9237}
2025-08-03 07:18:47,970 - INFO - test: {'epoch': 94, 'time_epoch': 4.48367, 'loss': 0.81308156, 'lr': 0, 'params': 483158, 'time_iter': 0.07117, 'accuracy': 0.65299, 'f1': 0.64559, 'accuracy-SBM': 0.65446, 'auc': 0.92429}
2025-08-03 07:18:47,972 - INFO - > Epoch 94: took 96.7s (avg 98.0s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:20:15,009 - INFO - train: {'epoch': 95, 'time_epoch': 86.77659, 'eta': 356.02063, 'eta_hours': 0.09889, 'loss': 0.74494813, 'lr': 6.82e-06, 'params': 483158, 'time_iter': 0.13884, 'accuracy': 0.67247, 'f1': 0.66473, 'accuracy-SBM': 0.67244, 'auc': 0.9333}
2025-08-03 07:20:19,292 - INFO - val: {'epoch': 95, 'time_epoch': 4.23384, 'loss': 0.82164654, 'lr': 0, 'params': 483158, 'time_iter': 0.0672, 'accuracy': 0.65052, 'f1': 0.64322, 'accuracy-SBM': 0.65121, 'auc': 0.9237}
2025-08-03 07:20:23,574 - INFO - test: {'epoch': 95, 'time_epoch': 4.23824, 'loss': 0.81354753, 'lr': 0, 'params': 483158, 'time_iter': 0.06727, 'accuracy': 0.65313, 'f1': 0.64563, 'accuracy-SBM': 0.65459, 'auc': 0.9244}
2025-08-03 07:20:23,649 - INFO - > Epoch 95: took 95.7s (avg 98.0s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:21:52,031 - INFO - train: {'epoch': 96, 'time_epoch': 88.1268, 'eta': 266.98831, 'eta_hours': 0.07416, 'loss': 0.74592515, 'lr': 4.37e-06, 'params': 483158, 'time_iter': 0.141, 'accuracy': 0.67167, 'f1': 0.66395, 'accuracy-SBM': 0.67165, 'auc': 0.93315}
2025-08-03 07:21:56,189 - INFO - val: {'epoch': 96, 'time_epoch': 4.11141, 'loss': 0.82286482, 'lr': 0, 'params': 483158, 'time_iter': 0.06526, 'accuracy': 0.6495, 'f1': 0.64254, 'accuracy-SBM': 0.65021, 'auc': 0.92364}
2025-08-03 07:22:00,309 - INFO - test: {'epoch': 96, 'time_epoch': 4.08391, 'loss': 0.81432088, 'lr': 0, 'params': 483158, 'time_iter': 0.06482, 'accuracy': 0.65378, 'f1': 0.64643, 'accuracy-SBM': 0.65522, 'auc': 0.9244}
2025-08-03 07:22:00,311 - INFO - > Epoch 96: took 96.7s (avg 98.0s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:23:28,744 - INFO - train: {'epoch': 97, 'time_epoch': 88.18762, 'eta': 177.97571, 'eta_hours': 0.04944, 'loss': 0.74529381, 'lr': 2.46e-06, 'params': 483158, 'time_iter': 0.1411, 'accuracy': 0.6723, 'f1': 0.66448, 'accuracy-SBM': 0.67227, 'auc': 0.93328}
2025-08-03 07:23:32,983 - INFO - val: {'epoch': 97, 'time_epoch': 4.19068, 'loss': 0.82163107, 'lr': 0, 'params': 483158, 'time_iter': 0.06652, 'accuracy': 0.65017, 'f1': 0.64331, 'accuracy-SBM': 0.65084, 'auc': 0.92368}
2025-08-03 07:23:37,225 - INFO - test: {'epoch': 97, 'time_epoch': 4.19817, 'loss': 0.81493335, 'lr': 0, 'params': 483158, 'time_iter': 0.06664, 'accuracy': 0.65266, 'f1': 0.6454, 'accuracy-SBM': 0.65409, 'auc': 0.92426}
2025-08-03 07:23:37,227 - INFO - > Epoch 97: took 96.9s (avg 97.9s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:25:05,500 - INFO - train: {'epoch': 98, 'time_epoch': 88.01519, 'eta': 88.97803, 'eta_hours': 0.02472, 'loss': 0.74444658, 'lr': 1.09e-06, 'params': 483158, 'time_iter': 0.14082, 'accuracy': 0.6725, 'f1': 0.66484, 'accuracy-SBM': 0.67248, 'auc': 0.93338}
2025-08-03 07:25:09,778 - INFO - val: {'epoch': 98, 'time_epoch': 4.22983, 'loss': 0.82200391, 'lr': 0, 'params': 483158, 'time_iter': 0.06714, 'accuracy': 0.64967, 'f1': 0.64279, 'accuracy-SBM': 0.65033, 'auc': 0.92362}
2025-08-03 07:25:14,032 - INFO - test: {'epoch': 98, 'time_epoch': 4.21017, 'loss': 0.81385773, 'lr': 0, 'params': 483158, 'time_iter': 0.06683, 'accuracy': 0.65263, 'f1': 0.64555, 'accuracy-SBM': 0.65409, 'auc': 0.92431}
2025-08-03 07:25:14,034 - INFO - > Epoch 98: took 96.8s (avg 97.9s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:26:41,533 - INFO - train: {'epoch': 99, 'time_epoch': 87.24383, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.74427383, 'lr': 2.7e-07, 'params': 483158, 'time_iter': 0.13959, 'accuracy': 0.67229, 'f1': 0.6646, 'accuracy-SBM': 0.67226, 'auc': 0.93338}
2025-08-03 07:26:45,838 - INFO - val: {'epoch': 99, 'time_epoch': 4.25606, 'loss': 0.81960142, 'lr': 0, 'params': 483158, 'time_iter': 0.06756, 'accuracy': 0.64988, 'f1': 0.6428, 'accuracy-SBM': 0.65059, 'auc': 0.92382}
2025-08-03 07:26:50,120 - INFO - test: {'epoch': 99, 'time_epoch': 4.24557, 'loss': 0.81257405, 'lr': 0, 'params': 483158, 'time_iter': 0.06739, 'accuracy': 0.6528, 'f1': 0.6454, 'accuracy-SBM': 0.65428, 'auc': 0.92442}
2025-08-03 07:26:50,329 - INFO - > Epoch 99: took 96.1s (avg 97.9s) | Best so far: epoch 75	train_loss: 0.7647 train_accuracy-SBM: 0.6661	val_loss: 0.8172 val_accuracy-SBM: 0.6524	test_loss: 0.8142 test_accuracy-SBM: 0.6533
2025-08-03 07:26:50,329 - INFO - Avg time per epoch: 97.92s
2025-08-03 07:26:50,329 - INFO - Total train loop time: 2.72h
2025-08-03 07:26:51,915 - INFO - ============================================================
2025-08-03 07:26:51,915 - INFO - Starting PK-Explainer Analysis
2025-08-03 07:26:51,915 - INFO - ============================================================
2025-08-03 07:26:51,993 - INFO - Saved model state to results/Cluster/Cluster-GINE-41/model_for_ablation.pt
2025-08-03 07:26:51,993 - INFO - 
----------------------------------------
Submitting next job for seed 45
Submitted batch job 5411992
