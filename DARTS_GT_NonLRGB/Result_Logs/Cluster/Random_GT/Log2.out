Lmod has detected the following error: The following module(s) are unknown:
"cuda/12.0"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore-cache load "cuda/12.0"

Also make sure that all modulefiles written in TCL start with the string
#%Module



              total        used        free      shared  buff/cache   available
Mem:          376Gi        12Gi       308Gi       3.0Gi        54Gi       357Gi
Swap:         1.9Gi       196Mi       1.7Gi
Sun Aug  3 07:27:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:1C:00.0 Off |                    0 |
| N/A   37C    P0             63W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Running experiment with seed: 45
Starting training for seed 45...
Changed working directory to: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/RAND_GT
Loading config from: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/RAND_GT/confignas.yaml
Using device: cuda
2025-08-03 07:28:46,376 - INFO - GPU Mem: 34.1GB
2025-08-03 07:28:46,376 - INFO - Run directory: results/Cluster/Cluster-GINE-45
2025-08-03 07:28:46,376 - INFO - Seed: 45
2025-08-03 07:28:46,376 - INFO - === OPTIMIZED MOE CONFIGURATION ===
2025-08-03 07:28:46,376 - INFO - Routing mode: none
2025-08-03 07:28:46,377 - INFO - Expert types: ['GINE', 'CustomGatedGCN', 'GATV2']
2025-08-03 07:28:46,377 - INFO - Number of layers: 16
2025-08-03 07:28:46,377 - INFO - Uncertainty enabled: False
2025-08-03 07:28:46,377 - INFO - Training mode: custom
2025-08-03 07:28:46,377 - INFO - Optimization: Uncertainty only for last layer + test/val
2025-08-03 07:28:46,377 - INFO - Additional features: Router weights logging + JSON export
2025-08-03 07:28:59,803 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-08-03 07:28:59,809 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-08-03 07:28:59,877 - INFO -   undirected: True
2025-08-03 07:28:59,878 - INFO -   num graphs: 12000
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-08-03 07:28:59,878 - INFO -   avg num_nodes/graph: 117
2025-08-03 07:28:59,878 - INFO -   num node features: 7
2025-08-03 07:28:59,878 - INFO -   num edge features: 0
2025-08-03 07:28:59,880 - INFO -   num classes: 6
2025-08-03 07:28:59,880 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-08-03 07:28:59,880 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-08-03 07:28:59,888 - INFO -   ...estimated to be undirected: True

  0%|          | 0/12000 [00:00<?, ?it/s]
 15%|█▌        | 1820/12000 [00:10<00:55, 181.92it/s]
 31%|███       | 3694/12000 [00:20<00:44, 185.11it/s]
 47%|████▋     | 5598/12000 [00:30<00:34, 187.52it/s]
 63%|██████▎   | 7546/12000 [00:40<00:23, 190.37it/s]
 79%|███████▉  | 9501/12000 [00:50<00:13, 191.33it/s]
 95%|█████████▌| 11449/12000 [01:00<00:02, 192.49it/s]
100%|██████████| 12000/12000 [01:02<00:00, 190.59it/s]
2025-08-03 07:30:03,577 - INFO - Done! Took 00:01:03.70
2025-08-03 07:30:03,599 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GINE
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GINE
RANDOMGTLayer: Randomly selected GNN type: GINE
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GINE
RANDOMGTLayer: Randomly selected GNN type: GINE
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GINE
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GATV2
2025-08-03 07:30:04,035 - INFO - Created model type: <class 'torch_geometric.graphgym.model_builder.GraphGymModule'>
2025-08-03 07:30:04,036 - INFO - Inner model type: <class 'graphgps.network.RANDOM_GTModel_EDGE.RANDOM_GTModelEDGE'>
2025-08-03 07:30:04,036 - INFO - Inner model has get_darts_model: False
2025-08-03 07:30:04,040 - INFO - GraphGymModule(
  (model): RANDOM_GTModelEDGE(
    (encoder): FeatureEncoder(
      (node_encoder): LapPENodeEncoder(
        (linear_x): Linear(in_features=7, out_features=32, bias=True)
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (edge_encoder): DummyEdgeEncoder(
        (encoder): Embedding(1, 48)
      )
    )
    (layers): Sequential(
      (0): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (1): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (2): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (3): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (4): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (5): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (6): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (7): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (8): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (9): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (10): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (11): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (12): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (13): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (14): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (15): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (post_mp): GNNInductiveNodeHead(
      (layer_post_mp): MLP(
        (model): Sequential(
          (0): GeneralMultiLayer(
            (Layer_0): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
            (Layer_1): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
          )
          (1): Linear(
            (model): Linear(48, 6, bias=True)
          )
        )
      )
    )
  )
)
2025-08-03 07:30:04,045 - INFO - Number of parameters: 492,902
2025-08-03 07:30:04,045 - INFO - Starting optimized training: 2025-08-03 07:30:04.045792
2025-08-03 07:30:09,548 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
2025-08-03 07:30:09,549 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-08-03 07:30:09,550 - INFO -   undirected: True
2025-08-03 07:30:09,550 - INFO -   num graphs: 12000
2025-08-03 07:30:09,551 - INFO -   avg num_nodes/graph: 117
2025-08-03 07:30:09,551 - INFO -   num node features: 7
2025-08-03 07:30:09,551 - INFO -   num edge features: 0
2025-08-03 07:30:09,553 - INFO -   num classes: 6
2025-08-03 07:30:09,553 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-08-03 07:30:09,553 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-08-03 07:30:09,561 - INFO -   ...estimated to be undirected: True

  0%|          | 0/12000 [00:00<?, ?it/s]
 15%|█▌        | 1844/12000 [00:10<00:55, 184.35it/s]
 30%|███       | 3649/12000 [00:20<00:45, 182.05it/s]
 46%|████▌     | 5471/12000 [00:30<00:35, 182.11it/s]
 62%|██████▏   | 7450/12000 [00:40<00:24, 188.34it/s]
 78%|███████▊  | 9356/12000 [00:50<00:13, 189.13it/s]
 95%|█████████▌| 11408/12000 [01:00<00:03, 194.59it/s]
100%|██████████| 12000/12000 [01:02<00:00, 190.69it/s]
2025-08-03 07:31:13,218 - INFO - Done! Took 00:01:03.67
2025-08-03 07:31:13,243 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-08-03 07:31:13,263 - INFO - === STARTING OPTIMIZED TRAINING ===
2025-08-03 07:31:13,263 - INFO - Start from epoch 0
2025-08-03 07:32:44,138 - INFO - train: {'epoch': 0, 'time_epoch': 90.21473, 'eta': 8931.25865, 'eta_hours': 2.48091, 'loss': 1.79711667, 'lr': 0.0, 'params': 492902, 'time_iter': 0.14434, 'accuracy': 0.16268, 'f1': 0.06942, 'accuracy-SBM': 0.16242, 'auc': 0.494}
2025-08-03 07:32:44,146 - INFO - ...computing epoch stats took: 0.64s
2025-08-03 07:32:48,595 - INFO - val: {'epoch': 0, 'time_epoch': 4.40729, 'loss': 1.79693089, 'lr': 0, 'params': 492902, 'time_iter': 0.06996, 'accuracy': 0.16393, 'f1': 0.06659, 'accuracy-SBM': 0.16049, 'auc': 0.49036}
2025-08-03 07:32:48,597 - INFO - ...computing epoch stats took: 0.04s
2025-08-03 07:32:53,031 - INFO - test: {'epoch': 0, 'time_epoch': 4.40285, 'loss': 1.79729586, 'lr': 0, 'params': 492902, 'time_iter': 0.06989, 'accuracy': 0.16078, 'f1': 0.06326, 'accuracy-SBM': 0.16051, 'auc': 0.49205}
2025-08-03 07:32:53,033 - INFO - ...computing epoch stats took: 0.03s
2025-08-03 07:32:53,033 - INFO - > Epoch 0: took 99.8s (avg 99.8s) | Best so far: epoch 0	train_loss: 1.7971 train_accuracy-SBM: 0.1624	val_loss: 1.7969 val_accuracy-SBM: 0.1605	test_loss: 1.7973 test_accuracy-SBM: 0.1605
2025-08-03 07:34:20,126 - INFO - train: {'epoch': 1, 'time_epoch': 86.52822, 'eta': 8660.40471, 'eta_hours': 2.40567, 'loss': 1.6684775, 'lr': 0.0002, 'params': 492902, 'time_iter': 0.13845, 'accuracy': 0.32071, 'f1': 0.2449, 'accuracy-SBM': 0.32046, 'auc': 0.6661}
2025-08-03 07:34:20,132 - INFO - ...computing epoch stats took: 0.55s
2025-08-03 07:34:24,326 - INFO - val: {'epoch': 1, 'time_epoch': 4.14613, 'loss': 1.61927364, 'lr': 0, 'params': 492902, 'time_iter': 0.06581, 'accuracy': 0.35146, 'f1': 0.315, 'accuracy-SBM': 0.34979, 'auc': 0.69699}
2025-08-03 07:34:24,342 - INFO - ...computing epoch stats took: 0.06s
2025-08-03 07:34:28,536 - INFO - test: {'epoch': 1, 'time_epoch': 4.15242, 'loss': 1.62183191, 'lr': 0, 'params': 492902, 'time_iter': 0.06591, 'accuracy': 0.35005, 'f1': 0.31272, 'accuracy-SBM': 0.34965, 'auc': 0.69208}
2025-08-03 07:34:28,538 - INFO - ...computing epoch stats took: 0.04s
2025-08-03 07:34:28,539 - INFO - > Epoch 1: took 95.5s (avg 97.6s) | Best so far: epoch 1	train_loss: 1.6685 train_accuracy-SBM: 0.3205	val_loss: 1.6193 val_accuracy-SBM: 0.3498	test_loss: 1.6218 test_accuracy-SBM: 0.3497
2025-08-03 07:35:57,415 - INFO - train: {'epoch': 2, 'time_epoch': 88.57667, 'eta': 8578.66786, 'eta_hours': 2.38296, 'loss': 1.51132592, 'lr': 0.0004, 'params': 492902, 'time_iter': 0.14172, 'accuracy': 0.37909, 'f1': 0.32879, 'accuracy-SBM': 0.37885, 'auc': 0.72563}
2025-08-03 07:35:57,657 - INFO - ...computing epoch stats took: 0.52s
2025-08-03 07:36:02,036 - INFO - val: {'epoch': 2, 'time_epoch': 4.31786, 'loss': 1.59209507, 'lr': 0, 'params': 492902, 'time_iter': 0.06854, 'accuracy': 0.33283, 'f1': 0.24966, 'accuracy-SBM': 0.33046, 'auc': 0.69434}
2025-08-03 07:36:02,039 - INFO - ...computing epoch stats took: 0.06s
2025-08-03 07:36:06,300 - INFO - test: {'epoch': 2, 'time_epoch': 4.21569, 'loss': 1.58827725, 'lr': 0, 'params': 492902, 'time_iter': 0.06692, 'accuracy': 0.33415, 'f1': 0.25211, 'accuracy-SBM': 0.33398, 'auc': 0.69874}
2025-08-03 07:36:06,302 - INFO - ...computing epoch stats took: 0.04s
2025-08-03 07:36:06,303 - INFO - > Epoch 2: took 97.8s (avg 97.7s) | Best so far: epoch 1	train_loss: 1.6685 train_accuracy-SBM: 0.3205	val_loss: 1.6193 val_accuracy-SBM: 0.3498	test_loss: 1.6218 test_accuracy-SBM: 0.3497
2025-08-03 07:37:32,952 - INFO - train: {'epoch': 3, 'time_epoch': 86.20096, 'eta': 8436.49409, 'eta_hours': 2.34347, 'loss': 1.36502589, 'lr': 0.0006, 'params': 492902, 'time_iter': 0.13792, 'accuracy': 0.45467, 'f1': 0.41608, 'accuracy-SBM': 0.45446, 'auc': 0.79065}
2025-08-03 07:37:37,034 - INFO - val: {'epoch': 3, 'time_epoch': 4.01809, 'loss': 1.2863266, 'lr': 0, 'params': 492902, 'time_iter': 0.06378, 'accuracy': 0.48923, 'f1': 0.459, 'accuracy-SBM': 0.48686, 'auc': 0.82215}
2025-08-03 07:37:40,991 - INFO - test: {'epoch': 3, 'time_epoch': 3.92306, 'loss': 1.27887538, 'lr': 0, 'params': 492902, 'time_iter': 0.06227, 'accuracy': 0.49173, 'f1': 0.46217, 'accuracy-SBM': 0.49074, 'auc': 0.82551}
2025-08-03 07:37:40,995 - INFO - > Epoch 3: took 94.7s (avg 96.9s) | Best so far: epoch 3	train_loss: 1.3650 train_accuracy-SBM: 0.4545	val_loss: 1.2863 val_accuracy-SBM: 0.4869	test_loss: 1.2789 test_accuracy-SBM: 0.4907
2025-08-03 07:39:06,654 - INFO - train: {'epoch': 4, 'time_epoch': 85.40086, 'eta': 8301.50745, 'eta_hours': 2.30597, 'loss': 1.24527215, 'lr': 0.0008, 'params': 492902, 'time_iter': 0.13664, 'accuracy': 0.49883, 'f1': 0.47979, 'accuracy-SBM': 0.49868, 'auc': 0.82993}
2025-08-03 07:39:10,961 - INFO - val: {'epoch': 4, 'time_epoch': 4.25421, 'loss': 1.18739929, 'lr': 0, 'params': 492902, 'time_iter': 0.06753, 'accuracy': 0.52426, 'f1': 0.50662, 'accuracy-SBM': 0.5226, 'auc': 0.85123}
2025-08-03 07:39:15,214 - INFO - test: {'epoch': 4, 'time_epoch': 4.21597, 'loss': 1.17962763, 'lr': 0, 'params': 492902, 'time_iter': 0.06692, 'accuracy': 0.52746, 'f1': 0.51072, 'accuracy-SBM': 0.5273, 'auc': 0.85458}
2025-08-03 07:39:15,216 - INFO - > Epoch 4: took 94.2s (avg 96.4s) | Best so far: epoch 4	train_loss: 1.2453 train_accuracy-SBM: 0.4987	val_loss: 1.1874 val_accuracy-SBM: 0.5226	test_loss: 1.1796 test_accuracy-SBM: 0.5273
2025-08-03 07:40:43,043 - INFO - train: {'epoch': 5, 'time_epoch': 87.56544, 'eta': 8216.96115, 'eta_hours': 2.28249, 'loss': 1.12768076, 'lr': 0.001, 'params': 492902, 'time_iter': 0.1401, 'accuracy': 0.54913, 'f1': 0.53715, 'accuracy-SBM': 0.54905, 'auc': 0.86523}
2025-08-03 07:40:47,357 - INFO - val: {'epoch': 5, 'time_epoch': 4.26266, 'loss': 1.33975561, 'lr': 0, 'params': 492902, 'time_iter': 0.06766, 'accuracy': 0.47903, 'f1': 0.45057, 'accuracy-SBM': 0.47834, 'auc': 0.81797}
2025-08-03 07:40:51,652 - INFO - test: {'epoch': 5, 'time_epoch': 4.25783, 'loss': 1.31382148, 'lr': 0, 'params': 492902, 'time_iter': 0.06758, 'accuracy': 0.48642, 'f1': 0.45904, 'accuracy-SBM': 0.48722, 'auc': 0.82472}
2025-08-03 07:40:51,654 - INFO - > Epoch 5: took 96.4s (avg 96.4s) | Best so far: epoch 4	train_loss: 1.2453 train_accuracy-SBM: 0.4987	val_loss: 1.1874 val_accuracy-SBM: 0.5226	test_loss: 1.1796 test_accuracy-SBM: 0.5273
2025-08-03 07:42:19,483 - INFO - train: {'epoch': 6, 'time_epoch': 87.56599, 'eta': 8131.5596, 'eta_hours': 2.25877, 'loss': 1.02880702, 'lr': 0.00099973, 'params': 492902, 'time_iter': 0.14011, 'accuracy': 0.58346, 'f1': 0.57058, 'accuracy-SBM': 0.58342, 'auc': 0.88651}
2025-08-03 07:42:23,811 - INFO - val: {'epoch': 6, 'time_epoch': 4.25898, 'loss': 1.01672557, 'lr': 0, 'params': 492902, 'time_iter': 0.0676, 'accuracy': 0.59533, 'f1': 0.57867, 'accuracy-SBM': 0.59351, 'auc': 0.8896}
2025-08-03 07:42:27,864 - INFO - test: {'epoch': 6, 'time_epoch': 4.00933, 'loss': 0.98881908, 'lr': 0, 'params': 492902, 'time_iter': 0.06364, 'accuracy': 0.59949, 'f1': 0.58246, 'accuracy-SBM': 0.59939, 'auc': 0.8956}
2025-08-03 07:42:27,893 - INFO - > Epoch 6: took 96.2s (avg 96.4s) | Best so far: epoch 6	train_loss: 1.0288 train_accuracy-SBM: 0.5834	val_loss: 1.0167 val_accuracy-SBM: 0.5935	test_loss: 0.9888 test_accuracy-SBM: 0.5994
2025-08-03 07:43:56,354 - INFO - train: {'epoch': 7, 'time_epoch': 88.20032, 'eta': 8052.9117, 'eta_hours': 2.23692, 'loss': 0.99253827, 'lr': 0.00099891, 'params': 492902, 'time_iter': 0.14112, 'accuracy': 0.59347, 'f1': 0.58134, 'accuracy-SBM': 0.59344, 'auc': 0.89323}
2025-08-03 07:44:00,730 - INFO - val: {'epoch': 7, 'time_epoch': 4.29942, 'loss': 0.98426864, 'lr': 0, 'params': 492902, 'time_iter': 0.06824, 'accuracy': 0.59952, 'f1': 0.58588, 'accuracy-SBM': 0.59876, 'auc': 0.8962}
2025-08-03 07:44:04,964 - INFO - test: {'epoch': 7, 'time_epoch': 4.18458, 'loss': 0.9759386, 'lr': 0, 'params': 492902, 'time_iter': 0.06642, 'accuracy': 0.60244, 'f1': 0.59012, 'accuracy-SBM': 0.60271, 'auc': 0.898}
2025-08-03 07:44:04,970 - INFO - > Epoch 7: took 97.1s (avg 96.5s) | Best so far: epoch 7	train_loss: 0.9925 train_accuracy-SBM: 0.5934	val_loss: 0.9843 val_accuracy-SBM: 0.5988	test_loss: 0.9759 test_accuracy-SBM: 0.6027
2025-08-03 07:45:32,752 - INFO - train: {'epoch': 8, 'time_epoch': 87.40242, 'eta': 7964.07341, 'eta_hours': 2.21224, 'loss': 0.96964111, 'lr': 0.00099754, 'params': 492902, 'time_iter': 0.13984, 'accuracy': 0.60123, 'f1': 0.58973, 'accuracy-SBM': 0.6012, 'auc': 0.89729}
2025-08-03 07:45:37,055 - INFO - val: {'epoch': 8, 'time_epoch': 4.25208, 'loss': 0.95443113, 'lr': 0, 'params': 492902, 'time_iter': 0.06749, 'accuracy': 0.60782, 'f1': 0.58502, 'accuracy-SBM': 0.60586, 'auc': 0.90115}
2025-08-03 07:45:41,326 - INFO - test: {'epoch': 8, 'time_epoch': 4.22416, 'loss': 0.94715354, 'lr': 0, 'params': 492902, 'time_iter': 0.06705, 'accuracy': 0.61059, 'f1': 0.58954, 'accuracy-SBM': 0.61048, 'auc': 0.90266}
2025-08-03 07:45:41,329 - INFO - > Epoch 8: took 96.4s (avg 96.5s) | Best so far: epoch 8	train_loss: 0.9696 train_accuracy-SBM: 0.6012	val_loss: 0.9544 val_accuracy-SBM: 0.6059	test_loss: 0.9472 test_accuracy-SBM: 0.6105
2025-08-03 07:47:08,930 - INFO - train: {'epoch': 9, 'time_epoch': 87.33962, 'eta': 7874.95711, 'eta_hours': 2.18749, 'loss': 0.96047239, 'lr': 0.00099563, 'params': 492902, 'time_iter': 0.13974, 'accuracy': 0.60511, 'f1': 0.59362, 'accuracy-SBM': 0.60507, 'auc': 0.89899}
2025-08-03 07:47:13,258 - INFO - val: {'epoch': 9, 'time_epoch': 4.27246, 'loss': 0.94694769, 'lr': 0, 'params': 492902, 'time_iter': 0.06782, 'accuracy': 0.61301, 'f1': 0.56227, 'accuracy-SBM': 0.61044, 'auc': 0.90214}
2025-08-03 07:47:17,572 - INFO - test: {'epoch': 9, 'time_epoch': 4.27336, 'loss': 0.94810112, 'lr': 0, 'params': 492902, 'time_iter': 0.06783, 'accuracy': 0.6095, 'f1': 0.56041, 'accuracy-SBM': 0.60947, 'auc': 0.90229}
2025-08-03 07:47:17,574 - INFO - > Epoch 9: took 96.2s (avg 96.4s) | Best so far: epoch 9	train_loss: 0.9605 train_accuracy-SBM: 0.6051	val_loss: 0.9469 val_accuracy-SBM: 0.6104	test_loss: 0.9481 test_accuracy-SBM: 0.6095
2025-08-03 07:48:46,085 - INFO - train: {'epoch': 10, 'time_epoch': 88.14628, 'eta': 7792.69044, 'eta_hours': 2.16464, 'loss': 0.9455339, 'lr': 0.00099318, 'params': 492902, 'time_iter': 0.14103, 'accuracy': 0.60858, 'f1': 0.59707, 'accuracy-SBM': 0.60855, 'auc': 0.90149}
2025-08-03 07:48:50,573 - INFO - val: {'epoch': 10, 'time_epoch': 4.28568, 'loss': 0.94879472, 'lr': 0, 'params': 492902, 'time_iter': 0.06803, 'accuracy': 0.60247, 'f1': 0.57653, 'accuracy-SBM': 0.6011, 'auc': 0.90327}
2025-08-03 07:48:54,920 - INFO - test: {'epoch': 10, 'time_epoch': 4.29431, 'loss': 0.94881612, 'lr': 0, 'params': 492902, 'time_iter': 0.06816, 'accuracy': 0.60789, 'f1': 0.58146, 'accuracy-SBM': 0.60787, 'auc': 0.90307}
2025-08-03 07:48:54,922 - INFO - > Epoch 10: took 97.3s (avg 96.5s) | Best so far: epoch 9	train_loss: 0.9605 train_accuracy-SBM: 0.6051	val_loss: 0.9469 val_accuracy-SBM: 0.6104	test_loss: 0.9481 test_accuracy-SBM: 0.6095
2025-08-03 07:50:23,225 - INFO - train: {'epoch': 11, 'time_epoch': 88.04594, 'eta': 7708.70801, 'eta_hours': 2.14131, 'loss': 0.93953738, 'lr': 0.00099019, 'params': 492902, 'time_iter': 0.14087, 'accuracy': 0.61064, 'f1': 0.59923, 'accuracy-SBM': 0.6106, 'auc': 0.90255}
2025-08-03 07:50:27,505 - INFO - val: {'epoch': 11, 'time_epoch': 4.23005, 'loss': 0.92100675, 'lr': 0, 'params': 492902, 'time_iter': 0.06714, 'accuracy': 0.61789, 'f1': 0.6079, 'accuracy-SBM': 0.6163, 'auc': 0.90605}
2025-08-03 07:50:31,781 - INFO - test: {'epoch': 11, 'time_epoch': 4.23017, 'loss': 0.91700159, 'lr': 0, 'params': 492902, 'time_iter': 0.06715, 'accuracy': 0.61614, 'f1': 0.6059, 'accuracy-SBM': 0.61603, 'auc': 0.90675}
2025-08-03 07:50:31,784 - INFO - > Epoch 11: took 96.9s (avg 96.5s) | Best so far: epoch 11	train_loss: 0.9395 train_accuracy-SBM: 0.6106	val_loss: 0.9210 val_accuracy-SBM: 0.6163	test_loss: 0.9170 test_accuracy-SBM: 0.6160
2025-08-03 07:51:59,255 - INFO - train: {'epoch': 12, 'time_epoch': 87.20625, 'eta': 7618.48093, 'eta_hours': 2.11624, 'loss': 0.935866, 'lr': 0.00098666, 'params': 492902, 'time_iter': 0.13953, 'accuracy': 0.61185, 'f1': 0.59969, 'accuracy-SBM': 0.61181, 'auc': 0.90323}
2025-08-03 07:52:03,558 - INFO - val: {'epoch': 12, 'time_epoch': 4.25159, 'loss': 0.91524344, 'lr': 0, 'params': 492902, 'time_iter': 0.06749, 'accuracy': 0.62138, 'f1': 0.60726, 'accuracy-SBM': 0.61961, 'auc': 0.90779}
2025-08-03 07:52:07,914 - INFO - test: {'epoch': 12, 'time_epoch': 4.31191, 'loss': 0.91808883, 'lr': 0, 'params': 492902, 'time_iter': 0.06844, 'accuracy': 0.61607, 'f1': 0.60301, 'accuracy-SBM': 0.61591, 'auc': 0.90744}
2025-08-03 07:52:07,916 - INFO - > Epoch 12: took 96.1s (avg 96.5s) | Best so far: epoch 12	train_loss: 0.9359 train_accuracy-SBM: 0.6118	val_loss: 0.9152 val_accuracy-SBM: 0.6196	test_loss: 0.9181 test_accuracy-SBM: 0.6159
2025-08-03 07:53:37,094 - INFO - train: {'epoch': 13, 'time_epoch': 88.90706, 'eta': 7539.13328, 'eta_hours': 2.0942, 'loss': 0.92634533, 'lr': 0.0009826, 'params': 492902, 'time_iter': 0.14225, 'accuracy': 0.61468, 'f1': 0.60345, 'accuracy-SBM': 0.61465, 'auc': 0.90486}
2025-08-03 07:53:41,373 - INFO - val: {'epoch': 13, 'time_epoch': 4.22936, 'loss': 0.90447178, 'lr': 0, 'params': 492902, 'time_iter': 0.06713, 'accuracy': 0.62075, 'f1': 0.5727, 'accuracy-SBM': 0.62088, 'auc': 0.9096}
2025-08-03 07:53:45,592 - INFO - test: {'epoch': 13, 'time_epoch': 4.1824, 'loss': 0.90933932, 'lr': 0, 'params': 492902, 'time_iter': 0.06639, 'accuracy': 0.61991, 'f1': 0.57121, 'accuracy-SBM': 0.62051, 'auc': 0.9091}
2025-08-03 07:53:45,594 - INFO - > Epoch 13: took 97.7s (avg 96.6s) | Best so far: epoch 13	train_loss: 0.9263 train_accuracy-SBM: 0.6147	val_loss: 0.9045 val_accuracy-SBM: 0.6209	test_loss: 0.9093 test_accuracy-SBM: 0.6205
2025-08-03 07:55:12,616 - INFO - train: {'epoch': 14, 'time_epoch': 86.75786, 'eta': 7446.33224, 'eta_hours': 2.06843, 'loss': 0.92039036, 'lr': 0.00097802, 'params': 492902, 'time_iter': 0.13881, 'accuracy': 0.61668, 'f1': 0.60131, 'accuracy-SBM': 0.61665, 'auc': 0.9059}
2025-08-03 07:55:16,785 - INFO - val: {'epoch': 14, 'time_epoch': 4.12169, 'loss': 0.90745377, 'lr': 0, 'params': 492902, 'time_iter': 0.06542, 'accuracy': 0.62379, 'f1': 0.61228, 'accuracy-SBM': 0.62223, 'auc': 0.90888}
2025-08-03 07:55:20,685 - INFO - test: {'epoch': 14, 'time_epoch': 3.86567, 'loss': 0.89724971, 'lr': 0, 'params': 492902, 'time_iter': 0.06136, 'accuracy': 0.62584, 'f1': 0.61517, 'accuracy-SBM': 0.62581, 'auc': 0.91087}
2025-08-03 07:55:21,098 - INFO - > Epoch 14: took 95.5s (avg 96.5s) | Best so far: epoch 14	train_loss: 0.9204 train_accuracy-SBM: 0.6167	val_loss: 0.9075 val_accuracy-SBM: 0.6222	test_loss: 0.8972 test_accuracy-SBM: 0.6258
2025-08-03 07:56:49,646 - INFO - train: {'epoch': 15, 'time_epoch': 88.26529, 'eta': 7362.20059, 'eta_hours': 2.04506, 'loss': 0.91911421, 'lr': 0.00097291, 'params': 492902, 'time_iter': 0.14122, 'accuracy': 0.61615, 'f1': 0.60438, 'accuracy-SBM': 0.61611, 'auc': 0.90619}
2025-08-03 07:56:53,976 - INFO - val: {'epoch': 15, 'time_epoch': 4.24017, 'loss': 0.90163179, 'lr': 0, 'params': 492902, 'time_iter': 0.0673, 'accuracy': 0.63035, 'f1': 0.59051, 'accuracy-SBM': 0.62764, 'auc': 0.9104}
2025-08-03 07:56:58,253 - INFO - test: {'epoch': 15, 'time_epoch': 4.23278, 'loss': 0.89215138, 'lr': 0, 'params': 492902, 'time_iter': 0.06719, 'accuracy': 0.63035, 'f1': 0.59, 'accuracy-SBM': 0.63001, 'auc': 0.91223}
2025-08-03 07:56:58,256 - INFO - > Epoch 15: took 97.2s (avg 96.6s) | Best so far: epoch 15	train_loss: 0.9191 train_accuracy-SBM: 0.6161	val_loss: 0.9016 val_accuracy-SBM: 0.6276	test_loss: 0.8922 test_accuracy-SBM: 0.6300
2025-08-03 07:58:24,488 - INFO - train: {'epoch': 16, 'time_epoch': 85.96059, 'eta': 7266.33026, 'eta_hours': 2.01843, 'loss': 0.91325335, 'lr': 0.00096728, 'params': 492902, 'time_iter': 0.13754, 'accuracy': 0.6185, 'f1': 0.6076, 'accuracy-SBM': 0.61846, 'auc': 0.90726}
2025-08-03 07:58:28,694 - INFO - val: {'epoch': 16, 'time_epoch': 4.15464, 'loss': 0.89030887, 'lr': 0, 'params': 492902, 'time_iter': 0.06595, 'accuracy': 0.62847, 'f1': 0.61747, 'accuracy-SBM': 0.62718, 'auc': 0.91157}
2025-08-03 07:58:32,887 - INFO - test: {'epoch': 16, 'time_epoch': 4.15615, 'loss': 0.88883275, 'lr': 0, 'params': 492902, 'time_iter': 0.06597, 'accuracy': 0.63009, 'f1': 0.62052, 'accuracy-SBM': 0.63014, 'auc': 0.91199}
2025-08-03 07:58:32,890 - INFO - > Epoch 16: took 94.6s (avg 96.4s) | Best so far: epoch 15	train_loss: 0.9191 train_accuracy-SBM: 0.6161	val_loss: 0.9016 val_accuracy-SBM: 0.6276	test_loss: 0.8922 test_accuracy-SBM: 0.6300
2025-08-03 07:59:58,106 - INFO - train: {'epoch': 17, 'time_epoch': 84.9576, 'eta': 7166.99184, 'eta_hours': 1.99083, 'loss': 0.90630858, 'lr': 0.00096114, 'params': 492902, 'time_iter': 0.13593, 'accuracy': 0.62083, 'f1': 0.60767, 'accuracy-SBM': 0.62079, 'auc': 0.90836}
2025-08-03 08:00:02,262 - INFO - val: {'epoch': 17, 'time_epoch': 4.10708, 'loss': 0.88648242, 'lr': 0, 'params': 492902, 'time_iter': 0.06519, 'accuracy': 0.62735, 'f1': 0.61747, 'accuracy-SBM': 0.62598, 'auc': 0.9125}
2025-08-03 08:00:06,394 - INFO - test: {'epoch': 17, 'time_epoch': 4.09603, 'loss': 0.8813423, 'lr': 0, 'params': 492902, 'time_iter': 0.06502, 'accuracy': 0.62788, 'f1': 0.61909, 'accuracy-SBM': 0.62782, 'auc': 0.91372}
2025-08-03 08:00:06,396 - INFO - > Epoch 17: took 93.5s (avg 96.3s) | Best so far: epoch 15	train_loss: 0.9191 train_accuracy-SBM: 0.6161	val_loss: 0.9016 val_accuracy-SBM: 0.6276	test_loss: 0.8922 test_accuracy-SBM: 0.6300
2025-08-03 08:01:31,894 - INFO - train: {'epoch': 18, 'time_epoch': 85.24142, 'eta': 7070.37717, 'eta_hours': 1.96399, 'loss': 0.90130301, 'lr': 0.0009545, 'params': 492902, 'time_iter': 0.13639, 'accuracy': 0.62264, 'f1': 0.61028, 'accuracy-SBM': 0.62261, 'auc': 0.90926}
2025-08-03 08:01:35,850 - INFO - val: {'epoch': 18, 'time_epoch': 3.91103, 'loss': 0.88459144, 'lr': 0, 'params': 492902, 'time_iter': 0.06208, 'accuracy': 0.62971, 'f1': 0.60865, 'accuracy-SBM': 0.62782, 'auc': 0.91242}
2025-08-03 08:01:39,788 - INFO - test: {'epoch': 18, 'time_epoch': 3.90329, 'loss': 0.88974455, 'lr': 0, 'params': 492902, 'time_iter': 0.06196, 'accuracy': 0.62786, 'f1': 0.60831, 'accuracy-SBM': 0.62789, 'auc': 0.91185}
2025-08-03 08:01:39,790 - INFO - > Epoch 18: took 93.4s (avg 96.1s) | Best so far: epoch 18	train_loss: 0.9013 train_accuracy-SBM: 0.6226	val_loss: 0.8846 val_accuracy-SBM: 0.6278	test_loss: 0.8897 test_accuracy-SBM: 0.6279
2025-08-03 08:03:03,924 - INFO - train: {'epoch': 19, 'time_epoch': 83.8864, 'eta': 6969.47975, 'eta_hours': 1.93597, 'loss': 0.89817124, 'lr': 0.00094736, 'params': 492902, 'time_iter': 0.13422, 'accuracy': 0.62234, 'f1': 0.6113, 'accuracy-SBM': 0.6223, 'auc': 0.90984}
2025-08-03 08:03:07,953 - INFO - val: {'epoch': 19, 'time_epoch': 3.98288, 'loss': 0.86564197, 'lr': 0, 'params': 492902, 'time_iter': 0.06322, 'accuracy': 0.63465, 'f1': 0.62028, 'accuracy-SBM': 0.63346, 'auc': 0.91576}
2025-08-03 08:03:11,775 - INFO - test: {'epoch': 19, 'time_epoch': 3.78939, 'loss': 0.86911644, 'lr': 0, 'params': 492902, 'time_iter': 0.06015, 'accuracy': 0.63215, 'f1': 0.61813, 'accuracy-SBM': 0.63236, 'auc': 0.91518}
2025-08-03 08:03:11,777 - INFO - > Epoch 19: took 92.0s (avg 95.9s) | Best so far: epoch 19	train_loss: 0.8982 train_accuracy-SBM: 0.6223	val_loss: 0.8656 val_accuracy-SBM: 0.6335	test_loss: 0.8691 test_accuracy-SBM: 0.6324
2025-08-03 08:04:36,137 - INFO - train: {'epoch': 20, 'time_epoch': 83.99943, 'eta': 6870.62764, 'eta_hours': 1.90851, 'loss': 0.89549228, 'lr': 0.00093974, 'params': 492902, 'time_iter': 0.1344, 'accuracy': 0.62455, 'f1': 0.61254, 'accuracy-SBM': 0.62452, 'auc': 0.91032}
2025-08-03 08:04:40,371 - INFO - val: {'epoch': 20, 'time_epoch': 4.17184, 'loss': 0.85719321, 'lr': 0, 'params': 492902, 'time_iter': 0.06622, 'accuracy': 0.635, 'f1': 0.60048, 'accuracy-SBM': 0.63475, 'auc': 0.91696}
2025-08-03 08:04:44,582 - INFO - test: {'epoch': 20, 'time_epoch': 4.17344, 'loss': 0.85745492, 'lr': 0, 'params': 492902, 'time_iter': 0.06625, 'accuracy': 0.63843, 'f1': 0.60462, 'accuracy-SBM': 0.63886, 'auc': 0.91704}
2025-08-03 08:04:44,584 - INFO - > Epoch 20: took 92.8s (avg 95.8s) | Best so far: epoch 20	train_loss: 0.8955 train_accuracy-SBM: 0.6245	val_loss: 0.8572 val_accuracy-SBM: 0.6348	test_loss: 0.8575 test_accuracy-SBM: 0.6389
2025-08-03 08:06:08,722 - INFO - train: {'epoch': 21, 'time_epoch': 83.88319, 'eta': 6772.71362, 'eta_hours': 1.88131, 'loss': 0.88794468, 'lr': 0.00093163, 'params': 492902, 'time_iter': 0.13421, 'accuracy': 0.62694, 'f1': 0.61574, 'accuracy-SBM': 0.62691, 'auc': 0.91167}
2025-08-03 08:06:12,865 - INFO - val: {'epoch': 21, 'time_epoch': 4.09341, 'loss': 0.85163802, 'lr': 0, 'params': 492902, 'time_iter': 0.06497, 'accuracy': 0.63504, 'f1': 0.61387, 'accuracy-SBM': 0.63445, 'auc': 0.91794}
2025-08-03 08:06:16,971 - INFO - test: {'epoch': 21, 'time_epoch': 4.0669, 'loss': 0.85818223, 'lr': 0, 'params': 492902, 'time_iter': 0.06455, 'accuracy': 0.63614, 'f1': 0.61448, 'accuracy-SBM': 0.63643, 'auc': 0.91687}
2025-08-03 08:06:16,973 - INFO - > Epoch 21: took 92.4s (avg 95.6s) | Best so far: epoch 20	train_loss: 0.8955 train_accuracy-SBM: 0.6245	val_loss: 0.8572 val_accuracy-SBM: 0.6348	test_loss: 0.8575 test_accuracy-SBM: 0.6389
2025-08-03 08:07:42,474 - INFO - train: {'epoch': 22, 'time_epoch': 85.23966, 'eta': 6680.5609, 'eta_hours': 1.85571, 'loss': 0.8820523, 'lr': 0.00092305, 'params': 492902, 'time_iter': 0.13638, 'accuracy': 0.62833, 'f1': 0.6172, 'accuracy-SBM': 0.62829, 'auc': 0.91269}
2025-08-03 08:07:46,591 - INFO - val: {'epoch': 22, 'time_epoch': 4.07015, 'loss': 0.87666333, 'lr': 0, 'params': 492902, 'time_iter': 0.06461, 'accuracy': 0.63103, 'f1': 0.6176, 'accuracy-SBM': 0.63014, 'auc': 0.91409}
2025-08-03 08:07:50,571 - INFO - test: {'epoch': 22, 'time_epoch': 3.94215, 'loss': 0.87399291, 'lr': 0, 'params': 492902, 'time_iter': 0.06257, 'accuracy': 0.63228, 'f1': 0.6201, 'accuracy-SBM': 0.63236, 'auc': 0.91437}
2025-08-03 08:07:50,586 - INFO - > Epoch 22: took 93.6s (avg 95.5s) | Best so far: epoch 20	train_loss: 0.8955 train_accuracy-SBM: 0.6245	val_loss: 0.8572 val_accuracy-SBM: 0.6348	test_loss: 0.8575 test_accuracy-SBM: 0.6389
2025-08-03 08:09:14,881 - INFO - train: {'epoch': 23, 'time_epoch': 84.04016, 'eta': 6585.18587, 'eta_hours': 1.82922, 'loss': 0.87977161, 'lr': 0.000914, 'params': 492902, 'time_iter': 0.13446, 'accuracy': 0.62878, 'f1': 0.61844, 'accuracy-SBM': 0.62874, 'auc': 0.91319}
2025-08-03 08:09:19,052 - INFO - val: {'epoch': 23, 'time_epoch': 4.11324, 'loss': 0.85885688, 'lr': 0, 'params': 492902, 'time_iter': 0.06529, 'accuracy': 0.63648, 'f1': 0.62269, 'accuracy-SBM': 0.63481, 'auc': 0.91669}
2025-08-03 08:09:23,187 - INFO - test: {'epoch': 23, 'time_epoch': 4.0991, 'loss': 0.86200563, 'lr': 0, 'params': 492902, 'time_iter': 0.06507, 'accuracy': 0.63565, 'f1': 0.6233, 'accuracy-SBM': 0.63556, 'auc': 0.91625}
2025-08-03 08:09:23,189 - INFO - > Epoch 23: took 92.6s (avg 95.4s) | Best so far: epoch 23	train_loss: 0.8798 train_accuracy-SBM: 0.6287	val_loss: 0.8589 val_accuracy-SBM: 0.6348	test_loss: 0.8620 test_accuracy-SBM: 0.6356
2025-08-03 08:10:47,853 - INFO - train: {'epoch': 24, 'time_epoch': 84.41168, 'eta': 6491.83219, 'eta_hours': 1.80329, 'loss': 0.87667349, 'lr': 0.00090451, 'params': 492902, 'time_iter': 0.13506, 'accuracy': 0.63076, 'f1': 0.61989, 'accuracy-SBM': 0.63073, 'auc': 0.91355}
2025-08-03 08:10:52,010 - INFO - val: {'epoch': 24, 'time_epoch': 4.10854, 'loss': 0.86701405, 'lr': 0, 'params': 492902, 'time_iter': 0.06521, 'accuracy': 0.63733, 'f1': 0.62784, 'accuracy-SBM': 0.63616, 'auc': 0.91559}
2025-08-03 08:10:56,158 - INFO - test: {'epoch': 24, 'time_epoch': 4.10991, 'loss': 0.86717233, 'lr': 0, 'params': 492902, 'time_iter': 0.06524, 'accuracy': 0.63523, 'f1': 0.62609, 'accuracy-SBM': 0.63535, 'auc': 0.9155}
2025-08-03 08:10:56,161 - INFO - > Epoch 24: took 93.0s (avg 95.3s) | Best so far: epoch 24	train_loss: 0.8767 train_accuracy-SBM: 0.6307	val_loss: 0.8670 val_accuracy-SBM: 0.6362	test_loss: 0.8672 test_accuracy-SBM: 0.6353
2025-08-03 08:12:20,810 - INFO - train: {'epoch': 25, 'time_epoch': 84.28776, 'eta': 6398.81364, 'eta_hours': 1.77745, 'loss': 0.87410686, 'lr': 0.00089457, 'params': 492902, 'time_iter': 0.13486, 'accuracy': 0.63156, 'f1': 0.62019, 'accuracy-SBM': 0.63153, 'auc': 0.91402}
2025-08-03 08:12:24,696 - INFO - val: {'epoch': 25, 'time_epoch': 3.8375, 'loss': 0.85015671, 'lr': 0, 'params': 492902, 'time_iter': 0.06091, 'accuracy': 0.63791, 'f1': 0.62644, 'accuracy-SBM': 0.63703, 'auc': 0.9185}
2025-08-03 08:12:28,616 - INFO - test: {'epoch': 25, 'time_epoch': 3.88679, 'loss': 0.84638626, 'lr': 0, 'params': 492902, 'time_iter': 0.0617, 'accuracy': 0.63973, 'f1': 0.62841, 'accuracy-SBM': 0.63984, 'auc': 0.91906}
2025-08-03 08:12:28,618 - INFO - > Epoch 25: took 92.5s (avg 95.2s) | Best so far: epoch 25	train_loss: 0.8741 train_accuracy-SBM: 0.6315	val_loss: 0.8502 val_accuracy-SBM: 0.6370	test_loss: 0.8464 test_accuracy-SBM: 0.6398
2025-08-03 08:13:53,562 - INFO - train: {'epoch': 26, 'time_epoch': 84.68779, 'eta': 6307.5234, 'eta_hours': 1.75209, 'loss': 0.86954345, 'lr': 0.0008842, 'params': 492902, 'time_iter': 0.1355, 'accuracy': 0.63264, 'f1': 0.62145, 'accuracy-SBM': 0.63261, 'auc': 0.91489}
2025-08-03 08:13:57,746 - INFO - val: {'epoch': 26, 'time_epoch': 4.13419, 'loss': 0.83625171, 'lr': 0, 'params': 492902, 'time_iter': 0.06562, 'accuracy': 0.6443, 'f1': 0.62346, 'accuracy-SBM': 0.64377, 'auc': 0.92056}
2025-08-03 08:14:01,930 - INFO - test: {'epoch': 26, 'time_epoch': 4.13154, 'loss': 0.83675202, 'lr': 0, 'params': 492902, 'time_iter': 0.06558, 'accuracy': 0.64504, 'f1': 0.62384, 'accuracy-SBM': 0.64536, 'auc': 0.92041}
2025-08-03 08:14:01,933 - INFO - > Epoch 26: took 93.3s (avg 95.1s) | Best so far: epoch 26	train_loss: 0.8695 train_accuracy-SBM: 0.6326	val_loss: 0.8363 val_accuracy-SBM: 0.6438	test_loss: 0.8368 test_accuracy-SBM: 0.6454
2025-08-03 08:15:27,106 - INFO - train: {'epoch': 27, 'time_epoch': 84.90571, 'eta': 6217.26512, 'eta_hours': 1.72702, 'loss': 0.86329699, 'lr': 0.00087341, 'params': 492902, 'time_iter': 0.13585, 'accuracy': 0.63471, 'f1': 0.62525, 'accuracy-SBM': 0.63468, 'auc': 0.9159}
2025-08-03 08:15:31,312 - INFO - val: {'epoch': 27, 'time_epoch': 4.14879, 'loss': 0.85479744, 'lr': 0, 'params': 492902, 'time_iter': 0.06585, 'accuracy': 0.63918, 'f1': 0.62314, 'accuracy-SBM': 0.63879, 'auc': 0.9175}
2025-08-03 08:15:35,454 - INFO - test: {'epoch': 27, 'time_epoch': 4.10443, 'loss': 0.8586323, 'lr': 0, 'params': 492902, 'time_iter': 0.06515, 'accuracy': 0.63971, 'f1': 0.62427, 'accuracy-SBM': 0.64008, 'auc': 0.91732}
2025-08-03 08:15:35,456 - INFO - > Epoch 27: took 93.5s (avg 95.1s) | Best so far: epoch 26	train_loss: 0.8695 train_accuracy-SBM: 0.6326	val_loss: 0.8363 val_accuracy-SBM: 0.6438	test_loss: 0.8368 test_accuracy-SBM: 0.6454
2025-08-03 08:17:00,566 - INFO - train: {'epoch': 28, 'time_epoch': 84.84947, 'eta': 6127.2383, 'eta_hours': 1.70201, 'loss': 0.8594174, 'lr': 0.00086221, 'params': 492902, 'time_iter': 0.13576, 'accuracy': 0.63623, 'f1': 0.6254, 'accuracy-SBM': 0.6362, 'auc': 0.91647}
2025-08-03 08:17:04,766 - INFO - val: {'epoch': 28, 'time_epoch': 4.13799, 'loss': 0.85007617, 'lr': 0, 'params': 492902, 'time_iter': 0.06568, 'accuracy': 0.63918, 'f1': 0.62654, 'accuracy-SBM': 0.6384, 'auc': 0.91826}
2025-08-03 08:17:08,951 - INFO - test: {'epoch': 28, 'time_epoch': 4.13401, 'loss': 0.8393596, 'lr': 0, 'params': 492902, 'time_iter': 0.06562, 'accuracy': 0.64193, 'f1': 0.62995, 'accuracy-SBM': 0.64212, 'auc': 0.91994}
2025-08-03 08:17:08,962 - INFO - > Epoch 28: took 93.5s (avg 95.0s) | Best so far: epoch 26	train_loss: 0.8695 train_accuracy-SBM: 0.6326	val_loss: 0.8363 val_accuracy-SBM: 0.6438	test_loss: 0.8368 test_accuracy-SBM: 0.6454
2025-08-03 08:18:34,284 - INFO - train: {'epoch': 29, 'time_epoch': 85.05919, 'eta': 6038.04597, 'eta_hours': 1.67723, 'loss': 0.85668345, 'lr': 0.00085062, 'params': 492902, 'time_iter': 0.13609, 'accuracy': 0.63779, 'f1': 0.62744, 'accuracy-SBM': 0.63776, 'auc': 0.91694}
2025-08-03 08:18:38,475 - INFO - val: {'epoch': 29, 'time_epoch': 4.14104, 'loss': 0.84693441, 'lr': 0, 'params': 492902, 'time_iter': 0.06573, 'accuracy': 0.63944, 'f1': 0.63041, 'accuracy-SBM': 0.63845, 'auc': 0.91892}
2025-08-03 08:18:42,647 - INFO - test: {'epoch': 29, 'time_epoch': 4.1359, 'loss': 0.85301829, 'lr': 0, 'params': 492902, 'time_iter': 0.06565, 'accuracy': 0.6373, 'f1': 0.62919, 'accuracy-SBM': 0.63745, 'auc': 0.9184}
2025-08-03 08:18:42,649 - INFO - > Epoch 29: took 93.7s (avg 95.0s) | Best so far: epoch 26	train_loss: 0.8695 train_accuracy-SBM: 0.6326	val_loss: 0.8363 val_accuracy-SBM: 0.6438	test_loss: 0.8368 test_accuracy-SBM: 0.6454
2025-08-03 08:20:07,505 - INFO - train: {'epoch': 30, 'time_epoch': 84.47819, 'eta': 5947.82709, 'eta_hours': 1.65217, 'loss': 0.85386787, 'lr': 0.00083864, 'params': 492902, 'time_iter': 0.13517, 'accuracy': 0.63794, 'f1': 0.62807, 'accuracy-SBM': 0.63791, 'auc': 0.91746}
2025-08-03 08:20:11,656 - INFO - val: {'epoch': 30, 'time_epoch': 4.10108, 'loss': 0.85173476, 'lr': 0, 'params': 492902, 'time_iter': 0.0651, 'accuracy': 0.63758, 'f1': 0.61286, 'accuracy-SBM': 0.63678, 'auc': 0.91835}
2025-08-03 08:20:15,784 - INFO - test: {'epoch': 30, 'time_epoch': 4.0921, 'loss': 0.84863905, 'lr': 0, 'params': 492902, 'time_iter': 0.06495, 'accuracy': 0.64403, 'f1': 0.6199, 'accuracy-SBM': 0.64422, 'auc': 0.9186}
2025-08-03 08:20:15,786 - INFO - > Epoch 30: took 93.1s (avg 94.9s) | Best so far: epoch 26	train_loss: 0.8695 train_accuracy-SBM: 0.6326	val_loss: 0.8363 val_accuracy-SBM: 0.6438	test_loss: 0.8368 test_accuracy-SBM: 0.6454
2025-08-03 08:21:41,065 - INFO - train: {'epoch': 31, 'time_epoch': 85.02832, 'eta': 5859.13605, 'eta_hours': 1.62754, 'loss': 0.8523192, 'lr': 0.00082629, 'params': 492902, 'time_iter': 0.13605, 'accuracy': 0.63926, 'f1': 0.62898, 'accuracy-SBM': 0.63923, 'auc': 0.91775}
2025-08-03 08:21:45,078 - INFO - val: {'epoch': 31, 'time_epoch': 3.9646, 'loss': 0.82674823, 'lr': 0, 'params': 492902, 'time_iter': 0.06293, 'accuracy': 0.64798, 'f1': 0.63656, 'accuracy-SBM': 0.64718, 'auc': 0.92201}
2025-08-03 08:21:49,046 - INFO - test: {'epoch': 31, 'time_epoch': 3.93103, 'loss': 0.82326502, 'lr': 0, 'params': 492902, 'time_iter': 0.0624, 'accuracy': 0.6476, 'f1': 0.63669, 'accuracy-SBM': 0.64779, 'auc': 0.92257}
2025-08-03 08:21:49,048 - INFO - > Epoch 31: took 93.3s (avg 94.9s) | Best so far: epoch 31	train_loss: 0.8523 train_accuracy-SBM: 0.6392	val_loss: 0.8267 val_accuracy-SBM: 0.6472	test_loss: 0.8233 test_accuracy-SBM: 0.6478
2025-08-03 08:23:14,586 - INFO - train: {'epoch': 32, 'time_epoch': 85.27582, 'eta': 5771.16948, 'eta_hours': 1.6031, 'loss': 0.84834295, 'lr': 0.00081359, 'params': 492902, 'time_iter': 0.13644, 'accuracy': 0.64183, 'f1': 0.632, 'accuracy-SBM': 0.6418, 'auc': 0.91845}
2025-08-03 08:23:18,517 - INFO - val: {'epoch': 32, 'time_epoch': 3.88417, 'loss': 0.8457485, 'lr': 0, 'params': 492902, 'time_iter': 0.06165, 'accuracy': 0.63885, 'f1': 0.617, 'accuracy-SBM': 0.63821, 'auc': 0.91889}
2025-08-03 08:23:22,452 - INFO - test: {'epoch': 32, 'time_epoch': 3.90164, 'loss': 0.83756272, 'lr': 0, 'params': 492902, 'time_iter': 0.06193, 'accuracy': 0.64146, 'f1': 0.61971, 'accuracy-SBM': 0.64169, 'auc': 0.9204}
2025-08-03 08:23:22,454 - INFO - > Epoch 32: took 93.4s (avg 94.8s) | Best so far: epoch 31	train_loss: 0.8523 train_accuracy-SBM: 0.6392	val_loss: 0.8267 val_accuracy-SBM: 0.6472	test_loss: 0.8233 test_accuracy-SBM: 0.6478
2025-08-03 08:24:47,557 - INFO - train: {'epoch': 33, 'time_epoch': 84.85992, 'eta': 5682.55385, 'eta_hours': 1.57849, 'loss': 0.84780525, 'lr': 0.00080054, 'params': 492902, 'time_iter': 0.13578, 'accuracy': 0.64143, 'f1': 0.63177, 'accuracy-SBM': 0.6414, 'auc': 0.91859}
2025-08-03 08:24:51,365 - INFO - val: {'epoch': 33, 'time_epoch': 3.73847, 'loss': 0.82934607, 'lr': 0, 'params': 492902, 'time_iter': 0.05934, 'accuracy': 0.64482, 'f1': 0.62751, 'accuracy-SBM': 0.644, 'auc': 0.92183}
2025-08-03 08:24:55,207 - INFO - test: {'epoch': 33, 'time_epoch': 3.80564, 'loss': 0.83146316, 'lr': 0, 'params': 492902, 'time_iter': 0.06041, 'accuracy': 0.64774, 'f1': 0.63102, 'accuracy-SBM': 0.64787, 'auc': 0.92166}
2025-08-03 08:24:55,209 - INFO - > Epoch 33: took 92.8s (avg 94.8s) | Best so far: epoch 31	train_loss: 0.8523 train_accuracy-SBM: 0.6392	val_loss: 0.8267 val_accuracy-SBM: 0.6472	test_loss: 0.8233 test_accuracy-SBM: 0.6478
2025-08-03 08:26:19,695 - INFO - train: {'epoch': 34, 'time_epoch': 84.22664, 'eta': 5592.97676, 'eta_hours': 1.5536, 'loss': 0.83892821, 'lr': 0.00078716, 'params': 492902, 'time_iter': 0.13476, 'accuracy': 0.64492, 'f1': 0.63521, 'accuracy-SBM': 0.64488, 'auc': 0.92}
2025-08-03 08:26:23,908 - INFO - val: {'epoch': 34, 'time_epoch': 4.16316, 'loss': 0.83275373, 'lr': 0, 'params': 492902, 'time_iter': 0.06608, 'accuracy': 0.64704, 'f1': 0.63787, 'accuracy-SBM': 0.64603, 'auc': 0.92103}
2025-08-03 08:26:28,035 - INFO - test: {'epoch': 34, 'time_epoch': 4.08793, 'loss': 0.82874026, 'lr': 0, 'params': 492902, 'time_iter': 0.06489, 'accuracy': 0.65114, 'f1': 0.64271, 'accuracy-SBM': 0.65129, 'auc': 0.92197}
2025-08-03 08:26:28,037 - INFO - > Epoch 34: took 92.8s (avg 94.7s) | Best so far: epoch 31	train_loss: 0.8523 train_accuracy-SBM: 0.6392	val_loss: 0.8267 val_accuracy-SBM: 0.6472	test_loss: 0.8233 test_accuracy-SBM: 0.6478
2025-08-03 08:27:52,822 - INFO - train: {'epoch': 35, 'time_epoch': 84.53119, 'eta': 5504.23834, 'eta_hours': 1.52896, 'loss': 0.83867416, 'lr': 0.00077347, 'params': 492902, 'time_iter': 0.13525, 'accuracy': 0.64743, 'f1': 0.63892, 'accuracy-SBM': 0.6474, 'auc': 0.92014}
2025-08-03 08:27:56,952 - INFO - val: {'epoch': 35, 'time_epoch': 4.06716, 'loss': 0.81810736, 'lr': 0, 'params': 492902, 'time_iter': 0.06456, 'accuracy': 0.65193, 'f1': 0.64054, 'accuracy-SBM': 0.65119, 'auc': 0.92338}
2025-08-03 08:28:01,045 - INFO - test: {'epoch': 35, 'time_epoch': 4.05652, 'loss': 0.81577187, 'lr': 0, 'params': 492902, 'time_iter': 0.06439, 'accuracy': 0.65546, 'f1': 0.64406, 'accuracy-SBM': 0.65565, 'auc': 0.92414}
2025-08-03 08:28:01,066 - INFO - > Epoch 35: took 93.0s (avg 94.7s) | Best so far: epoch 35	train_loss: 0.8387 train_accuracy-SBM: 0.6474	val_loss: 0.8181 val_accuracy-SBM: 0.6512	test_loss: 0.8158 test_accuracy-SBM: 0.6556
2025-08-03 08:29:24,843 - INFO - train: {'epoch': 36, 'time_epoch': 83.51856, 'eta': 5414.00311, 'eta_hours': 1.50389, 'loss': 0.83810795, 'lr': 0.00075948, 'params': 492902, 'time_iter': 0.13363, 'accuracy': 0.65068, 'f1': 0.64227, 'accuracy-SBM': 0.65065, 'auc': 0.92052}
2025-08-03 08:29:29,023 - INFO - val: {'epoch': 36, 'time_epoch': 4.12952, 'loss': 0.81269358, 'lr': 0, 'params': 492902, 'time_iter': 0.06555, 'accuracy': 0.67186, 'f1': 0.66462, 'accuracy-SBM': 0.67105, 'auc': 0.926}
2025-08-03 08:29:33,186 - INFO - test: {'epoch': 36, 'time_epoch': 4.12641, 'loss': 0.82018315, 'lr': 0, 'params': 492902, 'time_iter': 0.0655, 'accuracy': 0.66962, 'f1': 0.6627, 'accuracy-SBM': 0.66974, 'auc': 0.92482}
2025-08-03 08:29:33,188 - INFO - > Epoch 36: took 92.1s (avg 94.6s) | Best so far: epoch 36	train_loss: 0.8381 train_accuracy-SBM: 0.6506	val_loss: 0.8127 val_accuracy-SBM: 0.6711	test_loss: 0.8202 test_accuracy-SBM: 0.6697
2025-08-03 08:30:58,745 - INFO - train: {'epoch': 37, 'time_epoch': 85.1014, 'eta': 5326.70393, 'eta_hours': 1.47964, 'loss': 0.77445683, 'lr': 0.00074521, 'params': 492902, 'time_iter': 0.13616, 'accuracy': 0.71348, 'f1': 0.71208, 'accuracy-SBM': 0.71347, 'auc': 0.93833}
2025-08-03 08:31:02,954 - INFO - val: {'epoch': 37, 'time_epoch': 4.16136, 'loss': 0.73816031, 'lr': 0, 'params': 492902, 'time_iter': 0.06605, 'accuracy': 0.73978, 'f1': 0.73976, 'accuracy-SBM': 0.73974, 'auc': 0.94648}
2025-08-03 08:31:07,145 - INFO - test: {'epoch': 37, 'time_epoch': 4.15154, 'loss': 0.72928521, 'lr': 0, 'params': 492902, 'time_iter': 0.0659, 'accuracy': 0.74377, 'f1': 0.74388, 'accuracy-SBM': 0.74383, 'auc': 0.94768}
2025-08-03 08:31:07,162 - INFO - > Epoch 37: took 94.0s (avg 94.6s) | Best so far: epoch 37	train_loss: 0.7745 train_accuracy-SBM: 0.7135	val_loss: 0.7382 val_accuracy-SBM: 0.7397	test_loss: 0.7293 test_accuracy-SBM: 0.7438
2025-08-03 08:32:32,893 - INFO - train: {'epoch': 38, 'time_epoch': 85.45305, 'eta': 5240.06747, 'eta_hours': 1.45557, 'loss': 0.70084711, 'lr': 0.00073067, 'params': 492902, 'time_iter': 0.13672, 'accuracy': 0.74842, 'f1': 0.7484, 'accuracy-SBM': 0.74841, 'auc': 0.9509}
2025-08-03 08:32:37,089 - INFO - val: {'epoch': 38, 'time_epoch': 4.14654, 'loss': 0.6749163, 'lr': 0, 'params': 492902, 'time_iter': 0.06582, 'accuracy': 0.75778, 'f1': 0.75761, 'accuracy-SBM': 0.75759, 'auc': 0.95474}
2025-08-03 08:32:41,226 - INFO - test: {'epoch': 38, 'time_epoch': 4.10106, 'loss': 0.67025864, 'lr': 0, 'params': 492902, 'time_iter': 0.0651, 'accuracy': 0.75856, 'f1': 0.75848, 'accuracy-SBM': 0.75858, 'auc': 0.95558}
2025-08-03 08:32:41,228 - INFO - > Epoch 38: took 94.1s (avg 94.6s) | Best so far: epoch 38	train_loss: 0.7008 train_accuracy-SBM: 0.7484	val_loss: 0.6749 val_accuracy-SBM: 0.7576	test_loss: 0.6703 test_accuracy-SBM: 0.7586
2025-08-03 08:34:06,446 - INFO - train: {'epoch': 39, 'time_epoch': 84.95056, 'eta': 5152.73645, 'eta_hours': 1.43132, 'loss': 0.68319062, 'lr': 0.00071588, 'params': 492902, 'time_iter': 0.13592, 'accuracy': 0.75308, 'f1': 0.75307, 'accuracy-SBM': 0.75308, 'auc': 0.95335}
2025-08-03 08:34:10,546 - INFO - val: {'epoch': 39, 'time_epoch': 4.05281, 'loss': 0.6606045, 'lr': 0, 'params': 492902, 'time_iter': 0.06433, 'accuracy': 0.76194, 'f1': 0.7617, 'accuracy-SBM': 0.76156, 'auc': 0.95664}
2025-08-03 08:34:14,485 - INFO - test: {'epoch': 39, 'time_epoch': 3.90433, 'loss': 0.65537803, 'lr': 0, 'params': 492902, 'time_iter': 0.06197, 'accuracy': 0.76425, 'f1': 0.76423, 'accuracy-SBM': 0.7642, 'auc': 0.95749}
2025-08-03 08:34:14,487 - INFO - > Epoch 39: took 93.3s (avg 94.5s) | Best so far: epoch 39	train_loss: 0.6832 train_accuracy-SBM: 0.7531	val_loss: 0.6606 val_accuracy-SBM: 0.7616	test_loss: 0.6554 test_accuracy-SBM: 0.7642
2025-08-03 08:35:39,485 - INFO - train: {'epoch': 40, 'time_epoch': 84.73428, 'eta': 5065.21031, 'eta_hours': 1.407, 'loss': 0.67340249, 'lr': 0.00070085, 'params': 492902, 'time_iter': 0.13557, 'accuracy': 0.75647, 'f1': 0.75646, 'accuracy-SBM': 0.75646, 'auc': 0.95466}
2025-08-03 08:35:43,704 - INFO - val: {'epoch': 40, 'time_epoch': 4.14955, 'loss': 0.65829344, 'lr': 0, 'params': 492902, 'time_iter': 0.06587, 'accuracy': 0.76423, 'f1': 0.76419, 'accuracy-SBM': 0.76413, 'auc': 0.95697}
2025-08-03 08:35:47,880 - INFO - test: {'epoch': 40, 'time_epoch': 4.13754, 'loss': 0.64454277, 'lr': 0, 'params': 492902, 'time_iter': 0.06568, 'accuracy': 0.76667, 'f1': 0.76665, 'accuracy-SBM': 0.76662, 'auc': 0.95881}
2025-08-03 08:35:47,882 - INFO - > Epoch 40: took 93.4s (avg 94.5s) | Best so far: epoch 40	train_loss: 0.6734 train_accuracy-SBM: 0.7565	val_loss: 0.6583 val_accuracy-SBM: 0.7641	test_loss: 0.6445 test_accuracy-SBM: 0.7666
2025-08-03 08:37:13,053 - INFO - train: {'epoch': 41, 'time_epoch': 84.90226, 'eta': 4978.04909, 'eta_hours': 1.38279, 'loss': 0.66796184, 'lr': 0.0006856, 'params': 492902, 'time_iter': 0.13584, 'accuracy': 0.75861, 'f1': 0.75861, 'accuracy-SBM': 0.7586, 'auc': 0.95537}
2025-08-03 08:37:17,242 - INFO - val: {'epoch': 41, 'time_epoch': 4.1399, 'loss': 0.64831394, 'lr': 0, 'params': 492902, 'time_iter': 0.06571, 'accuracy': 0.76794, 'f1': 0.76788, 'accuracy-SBM': 0.76787, 'auc': 0.95809}
2025-08-03 08:37:21,424 - INFO - test: {'epoch': 41, 'time_epoch': 4.13623, 'loss': 0.6366865, 'lr': 0, 'params': 492902, 'time_iter': 0.06565, 'accuracy': 0.76941, 'f1': 0.76939, 'accuracy-SBM': 0.76946, 'auc': 0.95964}
2025-08-03 08:37:21,426 - INFO - > Epoch 41: took 93.5s (avg 94.5s) | Best so far: epoch 41	train_loss: 0.6680 train_accuracy-SBM: 0.7586	val_loss: 0.6483 val_accuracy-SBM: 0.7679	test_loss: 0.6367 test_accuracy-SBM: 0.7695
2025-08-03 08:38:45,979 - INFO - train: {'epoch': 42, 'time_epoch': 84.29842, 'eta': 4890.1925, 'eta_hours': 1.35839, 'loss': 0.65868979, 'lr': 0.00067015, 'params': 492902, 'time_iter': 0.13488, 'accuracy': 0.76133, 'f1': 0.76133, 'accuracy-SBM': 0.76133, 'auc': 0.95661}
2025-08-03 08:38:50,093 - INFO - val: {'epoch': 42, 'time_epoch': 4.06482, 'loss': 0.64004521, 'lr': 0, 'params': 492902, 'time_iter': 0.06452, 'accuracy': 0.76838, 'f1': 0.76808, 'accuracy-SBM': 0.76802, 'auc': 0.95934}
2025-08-03 08:38:54,182 - INFO - test: {'epoch': 42, 'time_epoch': 4.04351, 'loss': 0.63405908, 'lr': 0, 'params': 492902, 'time_iter': 0.06418, 'accuracy': 0.77032, 'f1': 0.77028, 'accuracy-SBM': 0.77022, 'auc': 0.96013}
2025-08-03 08:38:54,185 - INFO - > Epoch 42: took 92.8s (avg 94.4s) | Best so far: epoch 42	train_loss: 0.6587 train_accuracy-SBM: 0.7613	val_loss: 0.6400 val_accuracy-SBM: 0.7680	test_loss: 0.6341 test_accuracy-SBM: 0.7702
2025-08-03 08:40:18,849 - INFO - train: {'epoch': 43, 'time_epoch': 84.40451, 'eta': 4802.63267, 'eta_hours': 1.33406, 'loss': 0.65462877, 'lr': 0.00065451, 'params': 492902, 'time_iter': 0.13505, 'accuracy': 0.76285, 'f1': 0.76285, 'accuracy-SBM': 0.76285, 'auc': 0.95713}
2025-08-03 08:40:23,041 - INFO - val: {'epoch': 43, 'time_epoch': 4.14288, 'loss': 0.63946757, 'lr': 0, 'params': 492902, 'time_iter': 0.06576, 'accuracy': 0.76914, 'f1': 0.76922, 'accuracy-SBM': 0.76915, 'auc': 0.95926}
2025-08-03 08:40:26,966 - INFO - test: {'epoch': 43, 'time_epoch': 3.89099, 'loss': 0.63318102, 'lr': 0, 'params': 492902, 'time_iter': 0.06176, 'accuracy': 0.77163, 'f1': 0.77166, 'accuracy-SBM': 0.77172, 'auc': 0.9601}
2025-08-03 08:40:26,976 - INFO - > Epoch 43: took 92.8s (avg 94.4s) | Best so far: epoch 43	train_loss: 0.6546 train_accuracy-SBM: 0.7629	val_loss: 0.6395 val_accuracy-SBM: 0.7692	test_loss: 0.6332 test_accuracy-SBM: 0.7717
2025-08-03 08:41:52,045 - INFO - train: {'epoch': 44, 'time_epoch': 84.80606, 'eta': 4715.70386, 'eta_hours': 1.30992, 'loss': 0.64871694, 'lr': 0.0006387, 'params': 492902, 'time_iter': 0.13569, 'accuracy': 0.76405, 'f1': 0.76405, 'accuracy-SBM': 0.76405, 'auc': 0.95793}
2025-08-03 08:41:56,214 - INFO - val: {'epoch': 44, 'time_epoch': 4.11912, 'loss': 0.64373867, 'lr': 0, 'params': 492902, 'time_iter': 0.06538, 'accuracy': 0.76691, 'f1': 0.76688, 'accuracy-SBM': 0.76683, 'auc': 0.95861}
2025-08-03 08:42:00,379 - INFO - test: {'epoch': 44, 'time_epoch': 4.10828, 'loss': 0.63329319, 'lr': 0, 'params': 492902, 'time_iter': 0.06521, 'accuracy': 0.77053, 'f1': 0.77047, 'accuracy-SBM': 0.77056, 'auc': 0.96008}
2025-08-03 08:42:00,400 - INFO - > Epoch 44: took 93.4s (avg 94.4s) | Best so far: epoch 43	train_loss: 0.6546 train_accuracy-SBM: 0.7629	val_loss: 0.6395 val_accuracy-SBM: 0.7692	test_loss: 0.6332 test_accuracy-SBM: 0.7717
2025-08-03 08:43:25,782 - INFO - train: {'epoch': 45, 'time_epoch': 85.00866, 'eta': 4629.10518, 'eta_hours': 1.28586, 'loss': 0.64593971, 'lr': 0.00062274, 'params': 492902, 'time_iter': 0.13601, 'accuracy': 0.76553, 'f1': 0.76553, 'accuracy-SBM': 0.76553, 'auc': 0.95827}
2025-08-03 08:43:29,955 - INFO - val: {'epoch': 45, 'time_epoch': 4.12369, 'loss': 0.63695609, 'lr': 0, 'params': 492902, 'time_iter': 0.06546, 'accuracy': 0.76943, 'f1': 0.76937, 'accuracy-SBM': 0.76937, 'auc': 0.95949}
2025-08-03 08:43:33,876 - INFO - test: {'epoch': 45, 'time_epoch': 3.8846, 'loss': 0.62693015, 'lr': 0, 'params': 492902, 'time_iter': 0.06166, 'accuracy': 0.77359, 'f1': 0.77356, 'accuracy-SBM': 0.77359, 'auc': 0.96086}
2025-08-03 08:43:34,320 - INFO - > Epoch 45: took 93.9s (avg 94.4s) | Best so far: epoch 45	train_loss: 0.6459 train_accuracy-SBM: 0.7655	val_loss: 0.6370 val_accuracy-SBM: 0.7694	test_loss: 0.6269 test_accuracy-SBM: 0.7736
2025-08-03 08:45:00,293 - INFO - train: {'epoch': 46, 'time_epoch': 85.5972, 'eta': 4543.23783, 'eta_hours': 1.26201, 'loss': 0.64036143, 'lr': 0.00060665, 'params': 492902, 'time_iter': 0.13696, 'accuracy': 0.76822, 'f1': 0.76822, 'accuracy-SBM': 0.76822, 'auc': 0.95896}
2025-08-03 08:45:04,383 - INFO - val: {'epoch': 46, 'time_epoch': 4.03327, 'loss': 0.64441793, 'lr': 0, 'params': 492902, 'time_iter': 0.06402, 'accuracy': 0.76877, 'f1': 0.76857, 'accuracy-SBM': 0.76857, 'auc': 0.95835}
2025-08-03 08:45:08,500 - INFO - test: {'epoch': 46, 'time_epoch': 4.07942, 'loss': 0.62691916, 'lr': 0, 'params': 492902, 'time_iter': 0.06475, 'accuracy': 0.77096, 'f1': 0.77089, 'accuracy-SBM': 0.77087, 'auc': 0.96088}
2025-08-03 08:45:08,515 - INFO - > Epoch 46: took 94.2s (avg 94.4s) | Best so far: epoch 45	train_loss: 0.6459 train_accuracy-SBM: 0.7655	val_loss: 0.6370 val_accuracy-SBM: 0.7694	test_loss: 0.6269 test_accuracy-SBM: 0.7736
2025-08-03 08:46:34,576 - INFO - train: {'epoch': 47, 'time_epoch': 85.79814, 'eta': 4457.59942, 'eta_hours': 1.23822, 'loss': 0.63839028, 'lr': 0.00059044, 'params': 492902, 'time_iter': 0.13728, 'accuracy': 0.76824, 'f1': 0.76825, 'accuracy-SBM': 0.76824, 'auc': 0.95923}
2025-08-03 08:46:38,785 - INFO - val: {'epoch': 47, 'time_epoch': 4.15743, 'loss': 0.63674978, 'lr': 0, 'params': 492902, 'time_iter': 0.06599, 'accuracy': 0.77002, 'f1': 0.77004, 'accuracy-SBM': 0.76999, 'auc': 0.9596}
2025-08-03 08:46:42,896 - INFO - test: {'epoch': 47, 'time_epoch': 4.0752, 'loss': 0.62736206, 'lr': 0, 'params': 492902, 'time_iter': 0.06469, 'accuracy': 0.77312, 'f1': 0.77319, 'accuracy-SBM': 0.77318, 'auc': 0.96085}
2025-08-03 08:46:42,898 - INFO - > Epoch 47: took 94.4s (avg 94.4s) | Best so far: epoch 47	train_loss: 0.6384 train_accuracy-SBM: 0.7682	val_loss: 0.6367 val_accuracy-SBM: 0.7700	test_loss: 0.6274 test_accuracy-SBM: 0.7732
2025-08-03 08:48:07,516 - INFO - train: {'epoch': 48, 'time_epoch': 84.36251, 'eta': 4370.46027, 'eta_hours': 1.21402, 'loss': 0.63286803, 'lr': 0.00057413, 'params': 492902, 'time_iter': 0.13498, 'accuracy': 0.77032, 'f1': 0.77032, 'accuracy-SBM': 0.77032, 'auc': 0.95993}
2025-08-03 08:48:11,686 - INFO - val: {'epoch': 48, 'time_epoch': 4.11888, 'loss': 0.62963404, 'lr': 0, 'params': 492902, 'time_iter': 0.06538, 'accuracy': 0.77511, 'f1': 0.77495, 'accuracy-SBM': 0.77492, 'auc': 0.96028}
2025-08-03 08:48:15,831 - INFO - test: {'epoch': 48, 'time_epoch': 4.10838, 'loss': 0.62653295, 'lr': 0, 'params': 492902, 'time_iter': 0.06521, 'accuracy': 0.77225, 'f1': 0.77226, 'accuracy-SBM': 0.77225, 'auc': 0.96069}
2025-08-03 08:48:15,833 - INFO - > Epoch 48: took 92.9s (avg 94.3s) | Best so far: epoch 48	train_loss: 0.6329 train_accuracy-SBM: 0.7703	val_loss: 0.6296 val_accuracy-SBM: 0.7749	test_loss: 0.6265 test_accuracy-SBM: 0.7722
2025-08-03 08:49:41,736 - INFO - train: {'epoch': 49, 'time_epoch': 85.63851, 'eta': 4284.70818, 'eta_hours': 1.1902, 'loss': 0.62913996, 'lr': 0.00055774, 'params': 492902, 'time_iter': 0.13702, 'accuracy': 0.77238, 'f1': 0.77238, 'accuracy-SBM': 0.77238, 'auc': 0.9604}
2025-08-03 08:49:45,921 - INFO - val: {'epoch': 49, 'time_epoch': 4.13576, 'loss': 0.63314484, 'lr': 0, 'params': 492902, 'time_iter': 0.06565, 'accuracy': 0.77198, 'f1': 0.77187, 'accuracy-SBM': 0.77191, 'auc': 0.96014}
2025-08-03 08:49:50,089 - INFO - test: {'epoch': 49, 'time_epoch': 4.12286, 'loss': 0.62804916, 'lr': 0, 'params': 492902, 'time_iter': 0.06544, 'accuracy': 0.77264, 'f1': 0.7727, 'accuracy-SBM': 0.77255, 'auc': 0.96069}
2025-08-03 08:49:50,091 - INFO - > Epoch 49: took 94.3s (avg 94.3s) | Best so far: epoch 48	train_loss: 0.6329 train_accuracy-SBM: 0.7703	val_loss: 0.6296 val_accuracy-SBM: 0.7749	test_loss: 0.6265 test_accuracy-SBM: 0.7722
2025-08-03 08:51:14,453 - INFO - train: {'epoch': 50, 'time_epoch': 84.10488, 'eta': 4197.48707, 'eta_hours': 1.16597, 'loss': 0.62692229, 'lr': 0.00054129, 'params': 492902, 'time_iter': 0.13457, 'accuracy': 0.77285, 'f1': 0.77285, 'accuracy-SBM': 0.77285, 'auc': 0.96069}
2025-08-03 08:51:18,586 - INFO - val: {'epoch': 50, 'time_epoch': 4.08368, 'loss': 0.62960291, 'lr': 0, 'params': 492902, 'time_iter': 0.06482, 'accuracy': 0.77435, 'f1': 0.77426, 'accuracy-SBM': 0.77419, 'auc': 0.9604}
2025-08-03 08:51:22,697 - INFO - test: {'epoch': 50, 'time_epoch': 4.07508, 'loss': 0.62412264, 'lr': 0, 'params': 492902, 'time_iter': 0.06468, 'accuracy': 0.7755, 'f1': 0.77548, 'accuracy-SBM': 0.77555, 'auc': 0.96112}
2025-08-03 08:51:22,699 - INFO - > Epoch 50: took 92.6s (avg 94.3s) | Best so far: epoch 48	train_loss: 0.6329 train_accuracy-SBM: 0.7703	val_loss: 0.6296 val_accuracy-SBM: 0.7749	test_loss: 0.6265 test_accuracy-SBM: 0.7722
2025-08-03 08:52:46,560 - INFO - train: {'epoch': 51, 'time_epoch': 83.60551, 'eta': 4109.92484, 'eta_hours': 1.14165, 'loss': 0.62370353, 'lr': 0.00052479, 'params': 492902, 'time_iter': 0.13377, 'accuracy': 0.77397, 'f1': 0.77397, 'accuracy-SBM': 0.77397, 'auc': 0.96107}
2025-08-03 08:52:50,694 - INFO - val: {'epoch': 51, 'time_epoch': 4.08674, 'loss': 0.62200676, 'lr': 0, 'params': 492902, 'time_iter': 0.06487, 'accuracy': 0.77521, 'f1': 0.77506, 'accuracy-SBM': 0.77501, 'auc': 0.96128}
2025-08-03 08:52:54,676 - INFO - test: {'epoch': 51, 'time_epoch': 3.93905, 'loss': 0.6159922, 'lr': 0, 'params': 492902, 'time_iter': 0.06252, 'accuracy': 0.77673, 'f1': 0.77667, 'accuracy-SBM': 0.77672, 'auc': 0.96213}
2025-08-03 08:52:54,678 - INFO - > Epoch 51: took 92.0s (avg 94.3s) | Best so far: epoch 51	train_loss: 0.6237 train_accuracy-SBM: 0.7740	val_loss: 0.6220 val_accuracy-SBM: 0.7750	test_loss: 0.6160 test_accuracy-SBM: 0.7767
2025-08-03 08:54:18,909 - INFO - train: {'epoch': 52, 'time_epoch': 83.96976, 'eta': 4022.83493, 'eta_hours': 1.11745, 'loss': 0.62170941, 'lr': 0.00050827, 'params': 492902, 'time_iter': 0.13435, 'accuracy': 0.7742, 'f1': 0.77421, 'accuracy-SBM': 0.7742, 'auc': 0.96132}
2025-08-03 08:54:23,068 - INFO - val: {'epoch': 52, 'time_epoch': 4.10957, 'loss': 0.62887728, 'lr': 0, 'params': 492902, 'time_iter': 0.06523, 'accuracy': 0.77529, 'f1': 0.77517, 'accuracy-SBM': 0.77515, 'auc': 0.96057}
2025-08-03 08:54:27,208 - INFO - test: {'epoch': 52, 'time_epoch': 4.10386, 'loss': 0.62195566, 'lr': 0, 'params': 492902, 'time_iter': 0.06514, 'accuracy': 0.77562, 'f1': 0.77558, 'accuracy-SBM': 0.77565, 'auc': 0.96151}
2025-08-03 08:54:27,210 - INFO - > Epoch 52: took 92.5s (avg 94.2s) | Best so far: epoch 52	train_loss: 0.6217 train_accuracy-SBM: 0.7742	val_loss: 0.6289 val_accuracy-SBM: 0.7752	test_loss: 0.6220 test_accuracy-SBM: 0.7756
2025-08-03 08:55:52,012 - INFO - train: {'epoch': 53, 'time_epoch': 84.54794, 'eta': 3936.35312, 'eta_hours': 1.09343, 'loss': 0.6169182, 'lr': 0.00049173, 'params': 492902, 'time_iter': 0.13528, 'accuracy': 0.77566, 'f1': 0.77566, 'accuracy-SBM': 0.77566, 'auc': 0.96192}
2025-08-03 08:55:56,137 - INFO - val: {'epoch': 53, 'time_epoch': 4.07557, 'loss': 0.63146545, 'lr': 0, 'params': 492902, 'time_iter': 0.06469, 'accuracy': 0.77139, 'f1': 0.77141, 'accuracy-SBM': 0.77141, 'auc': 0.96038}
2025-08-03 08:56:00,232 - INFO - test: {'epoch': 53, 'time_epoch': 4.05999, 'loss': 0.62500991, 'lr': 0, 'params': 492902, 'time_iter': 0.06444, 'accuracy': 0.77516, 'f1': 0.77509, 'accuracy-SBM': 0.77519, 'auc': 0.96112}
2025-08-03 08:56:00,234 - INFO - > Epoch 53: took 93.0s (avg 94.2s) | Best so far: epoch 52	train_loss: 0.6217 train_accuracy-SBM: 0.7742	val_loss: 0.6289 val_accuracy-SBM: 0.7752	test_loss: 0.6220 test_accuracy-SBM: 0.7756
2025-08-03 08:57:24,889 - INFO - train: {'epoch': 54, 'time_epoch': 84.37914, 'eta': 3849.80351, 'eta_hours': 1.06939, 'loss': 0.61582148, 'lr': 0.00047521, 'params': 492902, 'time_iter': 0.13501, 'accuracy': 0.77648, 'f1': 0.77649, 'accuracy-SBM': 0.77648, 'auc': 0.96206}
2025-08-03 08:57:29,023 - INFO - val: {'epoch': 54, 'time_epoch': 4.07555, 'loss': 0.61681909, 'lr': 0, 'params': 492902, 'time_iter': 0.06469, 'accuracy': 0.77593, 'f1': 0.77583, 'accuracy-SBM': 0.77588, 'auc': 0.96208}
2025-08-03 08:57:33,110 - INFO - test: {'epoch': 54, 'time_epoch': 4.05113, 'loss': 0.61592973, 'lr': 0, 'params': 492902, 'time_iter': 0.0643, 'accuracy': 0.77732, 'f1': 0.77739, 'accuracy-SBM': 0.77737, 'auc': 0.96214}
2025-08-03 08:57:33,113 - INFO - > Epoch 54: took 92.9s (avg 94.2s) | Best so far: epoch 54	train_loss: 0.6158 train_accuracy-SBM: 0.7765	val_loss: 0.6168 val_accuracy-SBM: 0.7759	test_loss: 0.6159 test_accuracy-SBM: 0.7774
2025-08-03 08:58:57,960 - INFO - train: {'epoch': 55, 'time_epoch': 84.59239, 'eta': 3763.49898, 'eta_hours': 1.04542, 'loss': 0.61197629, 'lr': 0.00045871, 'params': 492902, 'time_iter': 0.13535, 'accuracy': 0.77765, 'f1': 0.77765, 'accuracy-SBM': 0.77765, 'auc': 0.96254}
2025-08-03 08:59:02,154 - INFO - val: {'epoch': 55, 'time_epoch': 4.13583, 'loss': 0.62211918, 'lr': 0, 'params': 492902, 'time_iter': 0.06565, 'accuracy': 0.77539, 'f1': 0.77538, 'accuracy-SBM': 0.7754, 'auc': 0.96141}
2025-08-03 08:59:06,325 - INFO - test: {'epoch': 55, 'time_epoch': 4.13428, 'loss': 0.61560667, 'lr': 0, 'params': 492902, 'time_iter': 0.06562, 'accuracy': 0.77831, 'f1': 0.7783, 'accuracy-SBM': 0.77833, 'auc': 0.96216}
2025-08-03 08:59:06,327 - INFO - > Epoch 55: took 93.2s (avg 94.2s) | Best so far: epoch 54	train_loss: 0.6158 train_accuracy-SBM: 0.7765	val_loss: 0.6168 val_accuracy-SBM: 0.7759	test_loss: 0.6159 test_accuracy-SBM: 0.7774
2025-08-03 09:00:29,199 - INFO - train: {'epoch': 56, 'time_epoch': 82.59281, 'eta': 3675.74607, 'eta_hours': 1.02104, 'loss': 0.60928039, 'lr': 0.00044226, 'params': 492902, 'time_iter': 0.13215, 'accuracy': 0.7784, 'f1': 0.7784, 'accuracy-SBM': 0.7784, 'auc': 0.96288}
2025-08-03 09:00:33,334 - INFO - val: {'epoch': 56, 'time_epoch': 4.07738, 'loss': 0.62075136, 'lr': 0, 'params': 492902, 'time_iter': 0.06472, 'accuracy': 0.77774, 'f1': 0.77762, 'accuracy-SBM': 0.77761, 'auc': 0.9614}
2025-08-03 09:00:37,444 - INFO - test: {'epoch': 56, 'time_epoch': 4.07377, 'loss': 0.61207619, 'lr': 0, 'params': 492902, 'time_iter': 0.06466, 'accuracy': 0.78047, 'f1': 0.78045, 'accuracy-SBM': 0.78044, 'auc': 0.96251}
2025-08-03 09:00:37,446 - INFO - > Epoch 56: took 91.1s (avg 94.1s) | Best so far: epoch 56	train_loss: 0.6093 train_accuracy-SBM: 0.7784	val_loss: 0.6208 val_accuracy-SBM: 0.7776	test_loss: 0.6121 test_accuracy-SBM: 0.7804
2025-08-03 09:01:58,676 - INFO - train: {'epoch': 57, 'time_epoch': 80.95705, 'eta': 3586.98657, 'eta_hours': 0.99639, 'loss': 0.60515443, 'lr': 0.00042587, 'params': 492902, 'time_iter': 0.12953, 'accuracy': 0.78084, 'f1': 0.78085, 'accuracy-SBM': 0.78084, 'auc': 0.96336}
2025-08-03 09:02:02,616 - INFO - val: {'epoch': 57, 'time_epoch': 3.89655, 'loss': 0.62344985, 'lr': 0, 'params': 492902, 'time_iter': 0.06185, 'accuracy': 0.77876, 'f1': 0.77853, 'accuracy-SBM': 0.77854, 'auc': 0.96121}
2025-08-03 09:02:06,321 - INFO - test: {'epoch': 57, 'time_epoch': 3.67248, 'loss': 0.60440312, 'lr': 0, 'params': 492902, 'time_iter': 0.05829, 'accuracy': 0.78133, 'f1': 0.78132, 'accuracy-SBM': 0.78129, 'auc': 0.9636}
2025-08-03 09:02:06,326 - INFO - > Epoch 57: took 88.9s (avg 94.0s) | Best so far: epoch 57	train_loss: 0.6052 train_accuracy-SBM: 0.7808	val_loss: 0.6234 val_accuracy-SBM: 0.7785	test_loss: 0.6044 test_accuracy-SBM: 0.7813
2025-08-03 09:03:27,991 - INFO - train: {'epoch': 58, 'time_epoch': 81.40281, 'eta': 3498.80133, 'eta_hours': 0.97189, 'loss': 0.60288459, 'lr': 0.00040956, 'params': 492902, 'time_iter': 0.13024, 'accuracy': 0.7807, 'f1': 0.7807, 'accuracy-SBM': 0.7807, 'auc': 0.96364}
2025-08-03 09:03:32,135 - INFO - val: {'epoch': 58, 'time_epoch': 4.09505, 'loss': 0.61689712, 'lr': 0, 'params': 492902, 'time_iter': 0.065, 'accuracy': 0.77917, 'f1': 0.77905, 'accuracy-SBM': 0.77901, 'auc': 0.9619}
2025-08-03 09:03:36,223 - INFO - test: {'epoch': 58, 'time_epoch': 4.05068, 'loss': 0.61522555, 'lr': 0, 'params': 492902, 'time_iter': 0.0643, 'accuracy': 0.77825, 'f1': 0.77828, 'accuracy-SBM': 0.77823, 'auc': 0.96217}
2025-08-03 09:03:36,225 - INFO - > Epoch 58: took 89.9s (avg 93.9s) | Best so far: epoch 58	train_loss: 0.6029 train_accuracy-SBM: 0.7807	val_loss: 0.6169 val_accuracy-SBM: 0.7790	test_loss: 0.6152 test_accuracy-SBM: 0.7782
2025-08-03 09:04:56,913 - INFO - train: {'epoch': 59, 'time_epoch': 80.43759, 'eta': 3410.1987, 'eta_hours': 0.94728, 'loss': 0.60068689, 'lr': 0.00039335, 'params': 492902, 'time_iter': 0.1287, 'accuracy': 0.78168, 'f1': 0.78168, 'accuracy-SBM': 0.78168, 'auc': 0.9639}
2025-08-03 09:05:01,041 - INFO - val: {'epoch': 59, 'time_epoch': 4.07993, 'loss': 0.61213702, 'lr': 0, 'params': 492902, 'time_iter': 0.06476, 'accuracy': 0.78068, 'f1': 0.78062, 'accuracy-SBM': 0.78053, 'auc': 0.96248}
2025-08-03 09:05:05,355 - INFO - test: {'epoch': 59, 'time_epoch': 4.17791, 'loss': 0.61229682, 'lr': 0, 'params': 492902, 'time_iter': 0.06632, 'accuracy': 0.77931, 'f1': 0.77929, 'accuracy-SBM': 0.77931, 'auc': 0.96254}
2025-08-03 09:05:05,357 - INFO - > Epoch 59: took 89.1s (avg 93.9s) | Best so far: epoch 59	train_loss: 0.6007 train_accuracy-SBM: 0.7817	val_loss: 0.6121 val_accuracy-SBM: 0.7805	test_loss: 0.6123 test_accuracy-SBM: 0.7793
2025-08-03 09:06:27,384 - INFO - train: {'epoch': 60, 'time_epoch': 81.78349, 'eta': 3322.72426, 'eta_hours': 0.92298, 'loss': 0.59734972, 'lr': 0.00037726, 'params': 492902, 'time_iter': 0.13085, 'accuracy': 0.78231, 'f1': 0.78231, 'accuracy-SBM': 0.78231, 'auc': 0.9643}
2025-08-03 09:06:31,407 - INFO - val: {'epoch': 60, 'time_epoch': 3.97423, 'loss': 0.61425391, 'lr': 0, 'params': 492902, 'time_iter': 0.06308, 'accuracy': 0.78118, 'f1': 0.78102, 'accuracy-SBM': 0.781, 'auc': 0.96215}
2025-08-03 09:06:35,473 - INFO - test: {'epoch': 60, 'time_epoch': 4.03058, 'loss': 0.60816098, 'lr': 0, 'params': 492902, 'time_iter': 0.06398, 'accuracy': 0.78189, 'f1': 0.78187, 'accuracy-SBM': 0.78182, 'auc': 0.96302}
2025-08-03 09:06:35,475 - INFO - > Epoch 60: took 90.1s (avg 93.8s) | Best so far: epoch 60	train_loss: 0.5973 train_accuracy-SBM: 0.7823	val_loss: 0.6143 val_accuracy-SBM: 0.7810	test_loss: 0.6082 test_accuracy-SBM: 0.7818
2025-08-03 09:07:58,954 - INFO - train: {'epoch': 61, 'time_epoch': 83.22597, 'eta': 3236.3175, 'eta_hours': 0.89898, 'loss': 0.59477164, 'lr': 0.0003613, 'params': 492902, 'time_iter': 0.13316, 'accuracy': 0.78384, 'f1': 0.78384, 'accuracy-SBM': 0.78384, 'auc': 0.9646}
2025-08-03 09:08:02,893 - INFO - val: {'epoch': 61, 'time_epoch': 3.89412, 'loss': 0.61574622, 'lr': 0, 'params': 492902, 'time_iter': 0.06181, 'accuracy': 0.78262, 'f1': 0.78259, 'accuracy-SBM': 0.78253, 'auc': 0.96227}
2025-08-03 09:08:06,845 - INFO - test: {'epoch': 61, 'time_epoch': 3.90937, 'loss': 0.60996033, 'lr': 0, 'params': 492902, 'time_iter': 0.06205, 'accuracy': 0.78092, 'f1': 0.78093, 'accuracy-SBM': 0.78091, 'auc': 0.96303}
2025-08-03 09:08:06,847 - INFO - > Epoch 61: took 91.4s (avg 93.8s) | Best so far: epoch 61	train_loss: 0.5948 train_accuracy-SBM: 0.7838	val_loss: 0.6157 val_accuracy-SBM: 0.7825	test_loss: 0.6100 test_accuracy-SBM: 0.7809
2025-08-03 09:09:20,732 - INFO - train: {'epoch': 62, 'time_epoch': 73.65456, 'eta': 3144.39042, 'eta_hours': 0.87344, 'loss': 0.59464453, 'lr': 0.00034549, 'params': 492902, 'time_iter': 0.11785, 'accuracy': 0.784, 'f1': 0.784, 'accuracy-SBM': 0.784, 'auc': 0.96463}
2025-08-03 09:09:24,177 - INFO - val: {'epoch': 62, 'time_epoch': 3.39289, 'loss': 0.60942426, 'lr': 0, 'params': 492902, 'time_iter': 0.05386, 'accuracy': 0.78241, 'f1': 0.78235, 'accuracy-SBM': 0.78232, 'auc': 0.96287}
2025-08-03 09:09:27,608 - INFO - test: {'epoch': 62, 'time_epoch': 3.39738, 'loss': 0.6092373, 'lr': 0, 'params': 492902, 'time_iter': 0.05393, 'accuracy': 0.78117, 'f1': 0.7812, 'accuracy-SBM': 0.7812, 'auc': 0.96294}
2025-08-03 09:09:27,611 - INFO - > Epoch 62: took 80.8s (avg 93.6s) | Best so far: epoch 61	train_loss: 0.5948 train_accuracy-SBM: 0.7838	val_loss: 0.6157 val_accuracy-SBM: 0.7825	test_loss: 0.6100 test_accuracy-SBM: 0.7809
2025-08-03 09:10:40,293 - INFO - train: {'epoch': 63, 'time_epoch': 72.45176, 'eta': 3052.35777, 'eta_hours': 0.84788, 'loss': 0.59172016, 'lr': 0.00032985, 'params': 492902, 'time_iter': 0.11592, 'accuracy': 0.78488, 'f1': 0.78489, 'accuracy-SBM': 0.78488, 'auc': 0.96497}
2025-08-03 09:10:43,880 - INFO - val: {'epoch': 63, 'time_epoch': 3.43843, 'loss': 0.62171149, 'lr': 0, 'params': 492902, 'time_iter': 0.05458, 'accuracy': 0.7792, 'f1': 0.77914, 'accuracy-SBM': 0.7791, 'auc': 0.96138}
2025-08-03 09:10:47,337 - INFO - test: {'epoch': 63, 'time_epoch': 3.41221, 'loss': 0.60930755, 'lr': 0, 'params': 492902, 'time_iter': 0.05416, 'accuracy': 0.78226, 'f1': 0.78224, 'accuracy-SBM': 0.78228, 'auc': 0.96287}
2025-08-03 09:10:47,340 - INFO - > Epoch 63: took 79.7s (avg 93.3s) | Best so far: epoch 61	train_loss: 0.5948 train_accuracy-SBM: 0.7838	val_loss: 0.6157 val_accuracy-SBM: 0.7825	test_loss: 0.6100 test_accuracy-SBM: 0.7809
2025-08-03 09:11:58,984 - INFO - train: {'epoch': 64, 'time_epoch': 71.41962, 'eta': 2960.37185, 'eta_hours': 0.82233, 'loss': 0.58986956, 'lr': 0.0003144, 'params': 492902, 'time_iter': 0.11427, 'accuracy': 0.78597, 'f1': 0.78598, 'accuracy-SBM': 0.78597, 'auc': 0.96519}
2025-08-03 09:12:02,462 - INFO - val: {'epoch': 64, 'time_epoch': 3.43507, 'loss': 0.61827689, 'lr': 0, 'params': 492902, 'time_iter': 0.05452, 'accuracy': 0.78121, 'f1': 0.78116, 'accuracy-SBM': 0.78117, 'auc': 0.96178}
2025-08-03 09:12:05,883 - INFO - test: {'epoch': 64, 'time_epoch': 3.38783, 'loss': 0.61270956, 'lr': 0, 'params': 492902, 'time_iter': 0.05378, 'accuracy': 0.78096, 'f1': 0.781, 'accuracy-SBM': 0.78102, 'auc': 0.96258}
2025-08-03 09:12:05,885 - INFO - > Epoch 64: took 78.5s (avg 93.1s) | Best so far: epoch 61	train_loss: 0.5948 train_accuracy-SBM: 0.7838	val_loss: 0.6157 val_accuracy-SBM: 0.7825	test_loss: 0.6100 test_accuracy-SBM: 0.7809
2025-08-03 09:13:17,861 - INFO - train: {'epoch': 65, 'time_epoch': 71.74412, 'eta': 2869.17632, 'eta_hours': 0.79699, 'loss': 0.58623807, 'lr': 0.00029915, 'params': 492902, 'time_iter': 0.11479, 'accuracy': 0.78759, 'f1': 0.78759, 'accuracy-SBM': 0.78759, 'auc': 0.96561}
2025-08-03 09:13:21,241 - INFO - val: {'epoch': 65, 'time_epoch': 3.31526, 'loss': 0.61850309, 'lr': 0, 'params': 492902, 'time_iter': 0.05262, 'accuracy': 0.78022, 'f1': 0.78012, 'accuracy-SBM': 0.78008, 'auc': 0.96196}
2025-08-03 09:13:24,556 - INFO - test: {'epoch': 65, 'time_epoch': 3.28423, 'loss': 0.61037245, 'lr': 0, 'params': 492902, 'time_iter': 0.05213, 'accuracy': 0.78298, 'f1': 0.78297, 'accuracy-SBM': 0.78298, 'auc': 0.96291}
2025-08-03 09:13:24,558 - INFO - > Epoch 65: took 78.7s (avg 92.9s) | Best so far: epoch 61	train_loss: 0.5948 train_accuracy-SBM: 0.7838	val_loss: 0.6157 val_accuracy-SBM: 0.7825	test_loss: 0.6100 test_accuracy-SBM: 0.7809
2025-08-03 09:14:34,808 - INFO - train: {'epoch': 66, 'time_epoch': 70.02217, 'eta': 2777.7133, 'eta_hours': 0.77159, 'loss': 0.58218086, 'lr': 0.00028412, 'params': 492902, 'time_iter': 0.11204, 'accuracy': 0.78886, 'f1': 0.78886, 'accuracy-SBM': 0.78886, 'auc': 0.9661}
2025-08-03 09:14:38,244 - INFO - val: {'epoch': 66, 'time_epoch': 3.39144, 'loss': 0.61108209, 'lr': 0, 'params': 492902, 'time_iter': 0.05383, 'accuracy': 0.78325, 'f1': 0.78316, 'accuracy-SBM': 0.78311, 'auc': 0.96277}
2025-08-03 09:14:41,657 - INFO - test: {'epoch': 66, 'time_epoch': 3.38171, 'loss': 0.61200467, 'lr': 0, 'params': 492902, 'time_iter': 0.05368, 'accuracy': 0.78107, 'f1': 0.7811, 'accuracy-SBM': 0.78113, 'auc': 0.96279}
2025-08-03 09:14:41,660 - INFO - > Epoch 66: took 77.1s (avg 92.7s) | Best so far: epoch 66	train_loss: 0.5822 train_accuracy-SBM: 0.7889	val_loss: 0.6111 val_accuracy-SBM: 0.7831	test_loss: 0.6120 test_accuracy-SBM: 0.7811
2025-08-03 09:15:54,846 - INFO - train: {'epoch': 67, 'time_epoch': 72.93037, 'eta': 2688.24946, 'eta_hours': 0.74674, 'loss': 0.58045124, 'lr': 0.00026933, 'params': 492902, 'time_iter': 0.11669, 'accuracy': 0.78923, 'f1': 0.78924, 'accuracy-SBM': 0.78923, 'auc': 0.9663}
2025-08-03 09:15:58,306 - INFO - val: {'epoch': 67, 'time_epoch': 3.40589, 'loss': 0.60866486, 'lr': 0, 'params': 492902, 'time_iter': 0.05406, 'accuracy': 0.78489, 'f1': 0.78475, 'accuracy-SBM': 0.78479, 'auc': 0.963}
2025-08-03 09:16:01,965 - INFO - test: {'epoch': 67, 'time_epoch': 3.41177, 'loss': 0.60357692, 'lr': 0, 'params': 492902, 'time_iter': 0.05416, 'accuracy': 0.78351, 'f1': 0.78351, 'accuracy-SBM': 0.78351, 'auc': 0.96364}
2025-08-03 09:16:02,062 - INFO - > Epoch 67: took 80.4s (avg 92.5s) | Best so far: epoch 67	train_loss: 0.5805 train_accuracy-SBM: 0.7892	val_loss: 0.6087 val_accuracy-SBM: 0.7848	test_loss: 0.6036 test_accuracy-SBM: 0.7835
2025-08-03 09:17:14,925 - INFO - train: {'epoch': 68, 'time_epoch': 72.53184, 'eta': 2599.0858, 'eta_hours': 0.72197, 'loss': 0.58001098, 'lr': 0.00025479, 'params': 492902, 'time_iter': 0.11605, 'accuracy': 0.78915, 'f1': 0.78916, 'accuracy-SBM': 0.78915, 'auc': 0.96635}
2025-08-03 09:17:18,377 - INFO - val: {'epoch': 68, 'time_epoch': 3.40797, 'loss': 0.61195881, 'lr': 0, 'params': 492902, 'time_iter': 0.05409, 'accuracy': 0.78253, 'f1': 0.78236, 'accuracy-SBM': 0.78235, 'auc': 0.96277}
2025-08-03 09:17:21,799 - INFO - test: {'epoch': 68, 'time_epoch': 3.3874, 'loss': 0.60051928, 'lr': 0, 'params': 492902, 'time_iter': 0.05377, 'accuracy': 0.78642, 'f1': 0.78639, 'accuracy-SBM': 0.78638, 'auc': 0.96405}
2025-08-03 09:17:21,808 - INFO - > Epoch 68: took 79.7s (avg 92.3s) | Best so far: epoch 67	train_loss: 0.5805 train_accuracy-SBM: 0.7892	val_loss: 0.6087 val_accuracy-SBM: 0.7848	test_loss: 0.6036 test_accuracy-SBM: 0.7835
2025-08-03 09:18:34,101 - INFO - train: {'epoch': 69, 'time_epoch': 72.06349, 'eta': 2510.19661, 'eta_hours': 0.69728, 'loss': 0.57747993, 'lr': 0.00024052, 'params': 492902, 'time_iter': 0.1153, 'accuracy': 0.78989, 'f1': 0.7899, 'accuracy-SBM': 0.78989, 'auc': 0.96665}
2025-08-03 09:18:37,638 - INFO - val: {'epoch': 69, 'time_epoch': 3.48859, 'loss': 0.61218895, 'lr': 0, 'params': 492902, 'time_iter': 0.05537, 'accuracy': 0.78318, 'f1': 0.78312, 'accuracy-SBM': 0.78317, 'auc': 0.96259}
2025-08-03 09:18:41,086 - INFO - test: {'epoch': 69, 'time_epoch': 3.40591, 'loss': 0.60283092, 'lr': 0, 'params': 492902, 'time_iter': 0.05406, 'accuracy': 0.78461, 'f1': 0.78464, 'accuracy-SBM': 0.78462, 'auc': 0.96379}
2025-08-03 09:18:41,525 - INFO - > Epoch 69: took 79.7s (avg 92.1s) | Best so far: epoch 67	train_loss: 0.5805 train_accuracy-SBM: 0.7892	val_loss: 0.6087 val_accuracy-SBM: 0.7848	test_loss: 0.6036 test_accuracy-SBM: 0.7835
2025-08-03 09:19:53,446 - INFO - train: {'epoch': 70, 'time_epoch': 71.65939, 'eta': 2421.61633, 'eta_hours': 0.67267, 'loss': 0.57247316, 'lr': 0.00022653, 'params': 492902, 'time_iter': 0.11466, 'accuracy': 0.7921, 'f1': 0.7921, 'accuracy-SBM': 0.7921, 'auc': 0.96721}
2025-08-03 09:19:56,958 - INFO - val: {'epoch': 70, 'time_epoch': 3.46672, 'loss': 0.61559178, 'lr': 0, 'params': 492902, 'time_iter': 0.05503, 'accuracy': 0.7845, 'f1': 0.78442, 'accuracy-SBM': 0.78436, 'auc': 0.9622}
2025-08-03 09:20:00,429 - INFO - test: {'epoch': 70, 'time_epoch': 3.42273, 'loss': 0.60772608, 'lr': 0, 'params': 492902, 'time_iter': 0.05433, 'accuracy': 0.78345, 'f1': 0.7834, 'accuracy-SBM': 0.78345, 'auc': 0.9632}
2025-08-03 09:20:00,438 - INFO - > Epoch 70: took 78.9s (avg 91.9s) | Best so far: epoch 67	train_loss: 0.5805 train_accuracy-SBM: 0.7892	val_loss: 0.6087 val_accuracy-SBM: 0.7848	test_loss: 0.6036 test_accuracy-SBM: 0.7835
2025-08-03 09:21:12,033 - INFO - train: {'epoch': 71, 'time_epoch': 71.36884, 'eta': 2333.39308, 'eta_hours': 0.64816, 'loss': 0.57153466, 'lr': 0.00021284, 'params': 492902, 'time_iter': 0.11419, 'accuracy': 0.79192, 'f1': 0.79192, 'accuracy-SBM': 0.79192, 'auc': 0.96733}
2025-08-03 09:21:15,688 - INFO - val: {'epoch': 71, 'time_epoch': 3.61134, 'loss': 0.61280972, 'lr': 0, 'params': 492902, 'time_iter': 0.05732, 'accuracy': 0.78372, 'f1': 0.78365, 'accuracy-SBM': 0.78361, 'auc': 0.96251}
2025-08-03 09:21:19,282 - INFO - test: {'epoch': 71, 'time_epoch': 3.5579, 'loss': 0.6064413, 'lr': 0, 'params': 492902, 'time_iter': 0.05647, 'accuracy': 0.78272, 'f1': 0.78275, 'accuracy-SBM': 0.78276, 'auc': 0.96337}
2025-08-03 09:21:19,285 - INFO - > Epoch 71: took 78.8s (avg 91.7s) | Best so far: epoch 67	train_loss: 0.5805 train_accuracy-SBM: 0.7892	val_loss: 0.6087 val_accuracy-SBM: 0.7848	test_loss: 0.6036 test_accuracy-SBM: 0.7835
2025-08-03 09:22:32,876 - INFO - train: {'epoch': 72, 'time_epoch': 73.24315, 'eta': 2246.32484, 'eta_hours': 0.62398, 'loss': 0.5685604, 'lr': 0.00019946, 'params': 492902, 'time_iter': 0.11719, 'accuracy': 0.79345, 'f1': 0.79346, 'accuracy-SBM': 0.79345, 'auc': 0.96768}
2025-08-03 09:22:36,493 - INFO - val: {'epoch': 72, 'time_epoch': 3.57399, 'loss': 0.61659991, 'lr': 0, 'params': 492902, 'time_iter': 0.05673, 'accuracy': 0.78285, 'f1': 0.78277, 'accuracy-SBM': 0.78274, 'auc': 0.96242}
2025-08-03 09:22:40,027 - INFO - test: {'epoch': 72, 'time_epoch': 3.50229, 'loss': 0.61318629, 'lr': 0, 'params': 492902, 'time_iter': 0.05559, 'accuracy': 0.78178, 'f1': 0.78177, 'accuracy-SBM': 0.7818, 'auc': 0.9628}
2025-08-03 09:22:40,029 - INFO - > Epoch 72: took 80.7s (avg 91.6s) | Best so far: epoch 67	train_loss: 0.5805 train_accuracy-SBM: 0.7892	val_loss: 0.6087 val_accuracy-SBM: 0.7848	test_loss: 0.6036 test_accuracy-SBM: 0.7835
2025-08-03 09:23:53,935 - INFO - train: {'epoch': 73, 'time_epoch': 73.65914, 'eta': 2159.77641, 'eta_hours': 0.59994, 'loss': 0.56753852, 'lr': 0.00018641, 'params': 492902, 'time_iter': 0.11785, 'accuracy': 0.7939, 'f1': 0.7939, 'accuracy-SBM': 0.7939, 'auc': 0.96776}
2025-08-03 09:23:57,593 - INFO - val: {'epoch': 73, 'time_epoch': 3.61482, 'loss': 0.61678613, 'lr': 0, 'params': 492902, 'time_iter': 0.05738, 'accuracy': 0.78372, 'f1': 0.78365, 'accuracy-SBM': 0.7836, 'auc': 0.96206}
2025-08-03 09:24:01,228 - INFO - test: {'epoch': 73, 'time_epoch': 3.60253, 'loss': 0.6081042, 'lr': 0, 'params': 492902, 'time_iter': 0.05718, 'accuracy': 0.78323, 'f1': 0.78323, 'accuracy-SBM': 0.78328, 'auc': 0.96319}
2025-08-03 09:24:01,231 - INFO - > Epoch 73: took 81.2s (avg 91.5s) | Best so far: epoch 67	train_loss: 0.5805 train_accuracy-SBM: 0.7892	val_loss: 0.6087 val_accuracy-SBM: 0.7848	test_loss: 0.6036 test_accuracy-SBM: 0.7835
2025-08-03 09:25:15,564 - INFO - train: {'epoch': 74, 'time_epoch': 74.10615, 'eta': 2073.72069, 'eta_hours': 0.57603, 'loss': 0.56494199, 'lr': 0.00017371, 'params': 492902, 'time_iter': 0.11857, 'accuracy': 0.79444, 'f1': 0.79444, 'accuracy-SBM': 0.79444, 'auc': 0.96809}
2025-08-03 09:25:19,193 - INFO - val: {'epoch': 74, 'time_epoch': 3.58578, 'loss': 0.61493435, 'lr': 0, 'params': 492902, 'time_iter': 0.05692, 'accuracy': 0.7845, 'f1': 0.78441, 'accuracy-SBM': 0.78437, 'auc': 0.96229}
2025-08-03 09:25:22,801 - INFO - test: {'epoch': 74, 'time_epoch': 3.57725, 'loss': 0.60261964, 'lr': 0, 'params': 492902, 'time_iter': 0.05678, 'accuracy': 0.78472, 'f1': 0.7847, 'accuracy-SBM': 0.78472, 'auc': 0.96378}
2025-08-03 09:25:23,000 - INFO - > Epoch 74: took 81.8s (avg 91.3s) | Best so far: epoch 67	train_loss: 0.5805 train_accuracy-SBM: 0.7892	val_loss: 0.6087 val_accuracy-SBM: 0.7848	test_loss: 0.6036 test_accuracy-SBM: 0.7835
2025-08-03 09:26:37,017 - INFO - train: {'epoch': 75, 'time_epoch': 73.78409, 'eta': 1987.87774, 'eta_hours': 0.55219, 'loss': 0.5645607, 'lr': 0.00016136, 'params': 492902, 'time_iter': 0.11805, 'accuracy': 0.79487, 'f1': 0.79487, 'accuracy-SBM': 0.79487, 'auc': 0.96811}
2025-08-03 09:26:40,654 - INFO - val: {'epoch': 75, 'time_epoch': 3.59328, 'loss': 0.61270514, 'lr': 0, 'params': 492902, 'time_iter': 0.05704, 'accuracy': 0.78519, 'f1': 0.7851, 'accuracy-SBM': 0.78508, 'auc': 0.96261}
2025-08-03 09:26:44,279 - INFO - test: {'epoch': 75, 'time_epoch': 3.59429, 'loss': 0.60400462, 'lr': 0, 'params': 492902, 'time_iter': 0.05705, 'accuracy': 0.78446, 'f1': 0.78446, 'accuracy-SBM': 0.78449, 'auc': 0.96374}
2025-08-03 09:26:44,281 - INFO - > Epoch 75: took 81.3s (avg 91.2s) | Best so far: epoch 75	train_loss: 0.5646 train_accuracy-SBM: 0.7949	val_loss: 0.6127 val_accuracy-SBM: 0.7851	test_loss: 0.6040 test_accuracy-SBM: 0.7845
2025-08-03 09:27:58,492 - INFO - train: {'epoch': 76, 'time_epoch': 73.77332, 'eta': 1902.34478, 'eta_hours': 0.52843, 'loss': 0.56073578, 'lr': 0.00014938, 'params': 492902, 'time_iter': 0.11804, 'accuracy': 0.79598, 'f1': 0.79598, 'accuracy-SBM': 0.79598, 'auc': 0.96855}
2025-08-03 09:28:02,139 - INFO - val: {'epoch': 76, 'time_epoch': 3.60222, 'loss': 0.61149908, 'lr': 0, 'params': 492902, 'time_iter': 0.05718, 'accuracy': 0.78484, 'f1': 0.7847, 'accuracy-SBM': 0.78465, 'auc': 0.9628}
2025-08-03 09:28:05,725 - INFO - test: {'epoch': 76, 'time_epoch': 3.55453, 'loss': 0.60553112, 'lr': 0, 'params': 492902, 'time_iter': 0.05642, 'accuracy': 0.78459, 'f1': 0.78461, 'accuracy-SBM': 0.78458, 'auc': 0.96353}
2025-08-03 09:28:05,727 - INFO - > Epoch 76: took 81.4s (avg 91.1s) | Best so far: epoch 75	train_loss: 0.5646 train_accuracy-SBM: 0.7949	val_loss: 0.6127 val_accuracy-SBM: 0.7851	test_loss: 0.6040 test_accuracy-SBM: 0.7845
2025-08-03 09:29:18,887 - INFO - train: {'epoch': 77, 'time_epoch': 72.93224, 'eta': 1816.87613, 'eta_hours': 0.50469, 'loss': 0.55978792, 'lr': 0.00013779, 'params': 492902, 'time_iter': 0.11669, 'accuracy': 0.7969, 'f1': 0.7969, 'accuracy-SBM': 0.7969, 'auc': 0.96866}
2025-08-03 09:29:22,352 - INFO - val: {'epoch': 77, 'time_epoch': 3.42202, 'loss': 0.60615611, 'lr': 0, 'params': 492902, 'time_iter': 0.05432, 'accuracy': 0.78621, 'f1': 0.78606, 'accuracy-SBM': 0.78606, 'auc': 0.96333}
2025-08-03 09:29:25,776 - INFO - test: {'epoch': 77, 'time_epoch': 3.38526, 'loss': 0.61189233, 'lr': 0, 'params': 492902, 'time_iter': 0.05373, 'accuracy': 0.78315, 'f1': 0.78314, 'accuracy-SBM': 0.78311, 'auc': 0.96265}
2025-08-03 09:29:25,778 - INFO - > Epoch 77: took 80.1s (avg 90.9s) | Best so far: epoch 77	train_loss: 0.5598 train_accuracy-SBM: 0.7969	val_loss: 0.6062 val_accuracy-SBM: 0.7861	test_loss: 0.6119 test_accuracy-SBM: 0.7831
2025-08-03 09:30:37,580 - INFO - train: {'epoch': 78, 'time_epoch': 71.38829, 'eta': 1731.31443, 'eta_hours': 0.48092, 'loss': 0.55868795, 'lr': 0.00012659, 'params': 492902, 'time_iter': 0.11422, 'accuracy': 0.79724, 'f1': 0.79724, 'accuracy-SBM': 0.79724, 'auc': 0.96878}
2025-08-03 09:30:41,043 - INFO - val: {'epoch': 78, 'time_epoch': 3.41835, 'loss': 0.60874798, 'lr': 0, 'params': 492902, 'time_iter': 0.05426, 'accuracy': 0.78608, 'f1': 0.78593, 'accuracy-SBM': 0.78592, 'auc': 0.9631}
2025-08-03 09:30:44,467 - INFO - test: {'epoch': 78, 'time_epoch': 3.39336, 'loss': 0.60300745, 'lr': 0, 'params': 492902, 'time_iter': 0.05386, 'accuracy': 0.78511, 'f1': 0.78509, 'accuracy-SBM': 0.78509, 'auc': 0.96378}
2025-08-03 09:30:44,469 - INFO - > Epoch 78: took 78.7s (avg 90.8s) | Best so far: epoch 77	train_loss: 0.5598 train_accuracy-SBM: 0.7969	val_loss: 0.6062 val_accuracy-SBM: 0.7861	test_loss: 0.6119 test_accuracy-SBM: 0.7831
2025-08-03 09:31:55,907 - INFO - train: {'epoch': 79, 'time_epoch': 71.21445, 'eta': 1646.06362, 'eta_hours': 0.45724, 'loss': 0.55694338, 'lr': 0.0001158, 'params': 492902, 'time_iter': 0.11394, 'accuracy': 0.79749, 'f1': 0.79749, 'accuracy-SBM': 0.79749, 'auc': 0.96898}
2025-08-03 09:31:59,368 - INFO - val: {'epoch': 79, 'time_epoch': 3.41834, 'loss': 0.61241296, 'lr': 0, 'params': 492902, 'time_iter': 0.05426, 'accuracy': 0.78518, 'f1': 0.78508, 'accuracy-SBM': 0.78505, 'auc': 0.96287}
2025-08-03 09:32:02,824 - INFO - test: {'epoch': 79, 'time_epoch': 3.42504, 'loss': 0.60521723, 'lr': 0, 'params': 492902, 'time_iter': 0.05437, 'accuracy': 0.78436, 'f1': 0.78435, 'accuracy-SBM': 0.78434, 'auc': 0.96369}
2025-08-03 09:32:02,826 - INFO - > Epoch 79: took 78.4s (avg 90.6s) | Best so far: epoch 77	train_loss: 0.5598 train_accuracy-SBM: 0.7969	val_loss: 0.6062 val_accuracy-SBM: 0.7861	test_loss: 0.6119 test_accuracy-SBM: 0.7831
2025-08-03 09:33:14,397 - INFO - train: {'epoch': 80, 'time_epoch': 71.34677, 'eta': 1561.19041, 'eta_hours': 0.43366, 'loss': 0.5551726, 'lr': 0.00010543, 'params': 492902, 'time_iter': 0.11415, 'accuracy': 0.79807, 'f1': 0.79807, 'accuracy-SBM': 0.79807, 'auc': 0.96918}
2025-08-03 09:33:17,883 - INFO - val: {'epoch': 80, 'time_epoch': 3.40301, 'loss': 0.61297288, 'lr': 0, 'params': 492902, 'time_iter': 0.05402, 'accuracy': 0.78527, 'f1': 0.78513, 'accuracy-SBM': 0.7851, 'auc': 0.96281}
2025-08-03 09:33:21,340 - INFO - test: {'epoch': 80, 'time_epoch': 3.40195, 'loss': 0.6081389, 'lr': 0, 'params': 492902, 'time_iter': 0.054, 'accuracy': 0.78429, 'f1': 0.78426, 'accuracy-SBM': 0.78426, 'auc': 0.96336}
2025-08-03 09:33:21,352 - INFO - > Epoch 80: took 78.5s (avg 90.5s) | Best so far: epoch 77	train_loss: 0.5598 train_accuracy-SBM: 0.7969	val_loss: 0.6062 val_accuracy-SBM: 0.7861	test_loss: 0.6119 test_accuracy-SBM: 0.7831
2025-08-03 09:34:34,344 - INFO - train: {'epoch': 81, 'time_epoch': 72.76902, 'eta': 1476.95932, 'eta_hours': 0.41027, 'loss': 0.55521282, 'lr': 9.549e-05, 'params': 492902, 'time_iter': 0.11643, 'accuracy': 0.79825, 'f1': 0.79825, 'accuracy-SBM': 0.79825, 'auc': 0.96916}
2025-08-03 09:34:37,791 - INFO - val: {'epoch': 81, 'time_epoch': 3.40498, 'loss': 0.6098488, 'lr': 0, 'params': 492902, 'time_iter': 0.05405, 'accuracy': 0.78602, 'f1': 0.78592, 'accuracy-SBM': 0.78589, 'auc': 0.96321}
2025-08-03 09:34:41,192 - INFO - test: {'epoch': 81, 'time_epoch': 3.36681, 'loss': 0.60582977, 'lr': 0, 'params': 492902, 'time_iter': 0.05344, 'accuracy': 0.78545, 'f1': 0.78543, 'accuracy-SBM': 0.78544, 'auc': 0.96367}
2025-08-03 09:34:41,194 - INFO - > Epoch 81: took 79.8s (avg 90.3s) | Best so far: epoch 77	train_loss: 0.5598 train_accuracy-SBM: 0.7969	val_loss: 0.6062 val_accuracy-SBM: 0.7861	test_loss: 0.6119 test_accuracy-SBM: 0.7831
2025-08-03 09:35:52,823 - INFO - train: {'epoch': 82, 'time_epoch': 71.40306, 'eta': 1392.72465, 'eta_hours': 0.38687, 'loss': 0.55149215, 'lr': 8.6e-05, 'params': 492902, 'time_iter': 0.11424, 'accuracy': 0.79927, 'f1': 0.79927, 'accuracy-SBM': 0.79927, 'auc': 0.96959}
2025-08-03 09:35:56,260 - INFO - val: {'epoch': 82, 'time_epoch': 3.3933, 'loss': 0.61426588, 'lr': 0, 'params': 492902, 'time_iter': 0.05386, 'accuracy': 0.78644, 'f1': 0.78636, 'accuracy-SBM': 0.78632, 'auc': 0.96286}
2025-08-03 09:35:59,689 - INFO - test: {'epoch': 82, 'time_epoch': 3.39607, 'loss': 0.61184538, 'lr': 0, 'params': 492902, 'time_iter': 0.05391, 'accuracy': 0.78501, 'f1': 0.78501, 'accuracy-SBM': 0.78504, 'auc': 0.96313}
2025-08-03 09:35:59,691 - INFO - > Epoch 82: took 78.5s (avg 90.2s) | Best so far: epoch 82	train_loss: 0.5515 train_accuracy-SBM: 0.7993	val_loss: 0.6143 val_accuracy-SBM: 0.7863	test_loss: 0.6118 test_accuracy-SBM: 0.7850
2025-08-03 09:37:12,645 - INFO - train: {'epoch': 83, 'time_epoch': 72.72544, 'eta': 1309.04738, 'eta_hours': 0.36362, 'loss': 0.55036057, 'lr': 7.695e-05, 'params': 492902, 'time_iter': 0.11636, 'accuracy': 0.80006, 'f1': 0.80006, 'accuracy-SBM': 0.80006, 'auc': 0.96973}
2025-08-03 09:37:16,111 - INFO - val: {'epoch': 83, 'time_epoch': 3.40823, 'loss': 0.61266202, 'lr': 0, 'params': 492902, 'time_iter': 0.0541, 'accuracy': 0.78477, 'f1': 0.78466, 'accuracy-SBM': 0.78463, 'auc': 0.96268}
2025-08-03 09:37:19,537 - INFO - test: {'epoch': 83, 'time_epoch': 3.39354, 'loss': 0.60432185, 'lr': 0, 'params': 492902, 'time_iter': 0.05387, 'accuracy': 0.78587, 'f1': 0.78583, 'accuracy-SBM': 0.78584, 'auc': 0.96365}
2025-08-03 09:37:19,539 - INFO - > Epoch 83: took 79.8s (avg 90.1s) | Best so far: epoch 82	train_loss: 0.5515 train_accuracy-SBM: 0.7993	val_loss: 0.6143 val_accuracy-SBM: 0.7863	test_loss: 0.6118 test_accuracy-SBM: 0.7850
2025-08-03 09:38:31,721 - INFO - train: {'epoch': 84, 'time_epoch': 71.95108, 'eta': 1225.49115, 'eta_hours': 0.34041, 'loss': 0.54992409, 'lr': 6.837e-05, 'params': 492902, 'time_iter': 0.11512, 'accuracy': 0.80024, 'f1': 0.80024, 'accuracy-SBM': 0.80024, 'auc': 0.96974}
2025-08-03 09:38:35,319 - INFO - val: {'epoch': 84, 'time_epoch': 3.55409, 'loss': 0.60866754, 'lr': 0, 'params': 492902, 'time_iter': 0.05641, 'accuracy': 0.78611, 'f1': 0.78603, 'accuracy-SBM': 0.78599, 'auc': 0.96312}
2025-08-03 09:38:38,957 - INFO - test: {'epoch': 84, 'time_epoch': 3.60228, 'loss': 0.60745801, 'lr': 0, 'params': 492902, 'time_iter': 0.05718, 'accuracy': 0.78457, 'f1': 0.78455, 'accuracy-SBM': 0.78456, 'auc': 0.96333}
2025-08-03 09:38:38,970 - INFO - > Epoch 84: took 79.4s (avg 89.9s) | Best so far: epoch 82	train_loss: 0.5515 train_accuracy-SBM: 0.7993	val_loss: 0.6143 val_accuracy-SBM: 0.7863	test_loss: 0.6118 test_accuracy-SBM: 0.7850
2025-08-03 09:39:53,219 - INFO - train: {'epoch': 85, 'time_epoch': 74.01967, 'eta': 1142.54155, 'eta_hours': 0.31737, 'loss': 0.5489772, 'lr': 6.026e-05, 'params': 492902, 'time_iter': 0.11843, 'accuracy': 0.80039, 'f1': 0.80039, 'accuracy-SBM': 0.80039, 'auc': 0.96987}
2025-08-03 09:39:56,923 - INFO - val: {'epoch': 85, 'time_epoch': 3.59766, 'loss': 0.61314403, 'lr': 0, 'params': 492902, 'time_iter': 0.05711, 'accuracy': 0.78522, 'f1': 0.78508, 'accuracy-SBM': 0.78509, 'auc': 0.96262}
2025-08-03 09:40:00,534 - INFO - test: {'epoch': 85, 'time_epoch': 3.57847, 'loss': 0.6050025, 'lr': 0, 'params': 492902, 'time_iter': 0.0568, 'accuracy': 0.78566, 'f1': 0.78565, 'accuracy-SBM': 0.78566, 'auc': 0.9636}
2025-08-03 09:40:00,536 - INFO - > Epoch 85: took 81.6s (avg 89.9s) | Best so far: epoch 82	train_loss: 0.5515 train_accuracy-SBM: 0.7993	val_loss: 0.6143 val_accuracy-SBM: 0.7863	test_loss: 0.6118 test_accuracy-SBM: 0.7850
2025-08-03 09:41:14,601 - INFO - train: {'epoch': 86, 'time_epoch': 73.73867, 'eta': 1059.75524, 'eta_hours': 0.29438, 'loss': 0.54659742, 'lr': 5.264e-05, 'params': 492902, 'time_iter': 0.11798, 'accuracy': 0.80114, 'f1': 0.80114, 'accuracy-SBM': 0.80114, 'auc': 0.97013}
2025-08-03 09:41:18,171 - INFO - val: {'epoch': 86, 'time_epoch': 3.52625, 'loss': 0.60841155, 'lr': 0, 'params': 492902, 'time_iter': 0.05597, 'accuracy': 0.78718, 'f1': 0.78706, 'accuracy-SBM': 0.78703, 'auc': 0.96311}
2025-08-03 09:41:21,713 - INFO - test: {'epoch': 86, 'time_epoch': 3.50874, 'loss': 0.60936522, 'lr': 0, 'params': 492902, 'time_iter': 0.05569, 'accuracy': 0.7842, 'f1': 0.78419, 'accuracy-SBM': 0.78418, 'auc': 0.96303}
2025-08-03 09:41:21,715 - INFO - > Epoch 86: took 81.2s (avg 89.8s) | Best so far: epoch 86	train_loss: 0.5466 train_accuracy-SBM: 0.8011	val_loss: 0.6084 val_accuracy-SBM: 0.7870	test_loss: 0.6094 test_accuracy-SBM: 0.7842
2025-08-03 09:42:35,106 - INFO - train: {'epoch': 87, 'time_epoch': 73.16341, 'eta': 977.09612, 'eta_hours': 0.27142, 'loss': 0.5478466, 'lr': 4.55e-05, 'params': 492902, 'time_iter': 0.11706, 'accuracy': 0.80103, 'f1': 0.80103, 'accuracy-SBM': 0.80103, 'auc': 0.96999}
2025-08-03 09:42:38,714 - INFO - val: {'epoch': 87, 'time_epoch': 3.56473, 'loss': 0.60786785, 'lr': 0, 'params': 492902, 'time_iter': 0.05658, 'accuracy': 0.78674, 'f1': 0.78666, 'accuracy-SBM': 0.7866, 'auc': 0.96327}
2025-08-03 09:42:42,530 - INFO - test: {'epoch': 87, 'time_epoch': 3.56552, 'loss': 0.60798301, 'lr': 0, 'params': 492902, 'time_iter': 0.0566, 'accuracy': 0.78473, 'f1': 0.78472, 'accuracy-SBM': 0.78475, 'auc': 0.96326}
2025-08-03 09:42:42,533 - INFO - > Epoch 87: took 80.8s (avg 89.7s) | Best so far: epoch 86	train_loss: 0.5466 train_accuracy-SBM: 0.8011	val_loss: 0.6084 val_accuracy-SBM: 0.7870	test_loss: 0.6094 test_accuracy-SBM: 0.7842
2025-08-03 09:43:56,127 - INFO - train: {'epoch': 88, 'time_epoch': 73.3464, 'eta': 894.67301, 'eta_hours': 0.24852, 'loss': 0.54631702, 'lr': 3.886e-05, 'params': 492902, 'time_iter': 0.11735, 'accuracy': 0.80126, 'f1': 0.80126, 'accuracy-SBM': 0.80126, 'auc': 0.97015}
2025-08-03 09:43:59,742 - INFO - val: {'epoch': 88, 'time_epoch': 3.55779, 'loss': 0.61076546, 'lr': 0, 'params': 492902, 'time_iter': 0.05647, 'accuracy': 0.78675, 'f1': 0.78661, 'accuracy-SBM': 0.78661, 'auc': 0.96289}
2025-08-03 09:44:03,345 - INFO - test: {'epoch': 88, 'time_epoch': 3.57186, 'loss': 0.605791, 'lr': 0, 'params': 492902, 'time_iter': 0.0567, 'accuracy': 0.7858, 'f1': 0.78577, 'accuracy-SBM': 0.78577, 'auc': 0.96348}
2025-08-03 09:44:03,347 - INFO - > Epoch 88: took 80.8s (avg 89.6s) | Best so far: epoch 86	train_loss: 0.5466 train_accuracy-SBM: 0.8011	val_loss: 0.6084 val_accuracy-SBM: 0.7870	test_loss: 0.6094 test_accuracy-SBM: 0.7842
2025-08-03 09:45:17,490 - INFO - train: {'epoch': 89, 'time_epoch': 73.80898, 'eta': 812.503, 'eta_hours': 0.2257, 'loss': 0.54419064, 'lr': 3.272e-05, 'params': 492902, 'time_iter': 0.11809, 'accuracy': 0.80209, 'f1': 0.80209, 'accuracy-SBM': 0.80209, 'auc': 0.97038}
2025-08-03 09:45:21,120 - INFO - val: {'epoch': 89, 'time_epoch': 3.57664, 'loss': 0.61508271, 'lr': 0, 'params': 492902, 'time_iter': 0.05677, 'accuracy': 0.78587, 'f1': 0.78577, 'accuracy-SBM': 0.78574, 'auc': 0.9627}
2025-08-03 09:45:24,696 - INFO - test: {'epoch': 89, 'time_epoch': 3.53439, 'loss': 0.61055579, 'lr': 0, 'params': 492902, 'time_iter': 0.0561, 'accuracy': 0.78465, 'f1': 0.78464, 'accuracy-SBM': 0.78464, 'auc': 0.96323}
2025-08-03 09:45:24,738 - INFO - > Epoch 89: took 81.4s (avg 89.5s) | Best so far: epoch 86	train_loss: 0.5466 train_accuracy-SBM: 0.8011	val_loss: 0.6084 val_accuracy-SBM: 0.7870	test_loss: 0.6094 test_accuracy-SBM: 0.7842
2025-08-03 09:46:38,180 - INFO - train: {'epoch': 90, 'time_epoch': 73.20669, 'eta': 730.45717, 'eta_hours': 0.2029, 'loss': 0.54529818, 'lr': 2.709e-05, 'params': 492902, 'time_iter': 0.11713, 'accuracy': 0.80137, 'f1': 0.80137, 'accuracy-SBM': 0.80137, 'auc': 0.97028}
2025-08-03 09:46:41,804 - INFO - val: {'epoch': 90, 'time_epoch': 3.56349, 'loss': 0.61447548, 'lr': 0, 'params': 492902, 'time_iter': 0.05656, 'accuracy': 0.78615, 'f1': 0.78603, 'accuracy-SBM': 0.78601, 'auc': 0.96277}
2025-08-03 09:46:45,378 - INFO - test: {'epoch': 90, 'time_epoch': 3.52183, 'loss': 0.60953923, 'lr': 0, 'params': 492902, 'time_iter': 0.0559, 'accuracy': 0.78545, 'f1': 0.78543, 'accuracy-SBM': 0.78544, 'auc': 0.96331}
2025-08-03 09:46:45,383 - INFO - > Epoch 90: took 80.6s (avg 89.4s) | Best so far: epoch 86	train_loss: 0.5466 train_accuracy-SBM: 0.8011	val_loss: 0.6084 val_accuracy-SBM: 0.7870	test_loss: 0.6094 test_accuracy-SBM: 0.7842
2025-08-03 09:47:59,704 - INFO - train: {'epoch': 91, 'time_epoch': 74.08933, 'eta': 648.68026, 'eta_hours': 0.18019, 'loss': 0.54317332, 'lr': 2.198e-05, 'params': 492902, 'time_iter': 0.11854, 'accuracy': 0.80246, 'f1': 0.80246, 'accuracy-SBM': 0.80246, 'auc': 0.97049}
2025-08-03 09:48:03,351 - INFO - val: {'epoch': 91, 'time_epoch': 3.60085, 'loss': 0.61220803, 'lr': 0, 'params': 492902, 'time_iter': 0.05716, 'accuracy': 0.78655, 'f1': 0.78645, 'accuracy-SBM': 0.78644, 'auc': 0.96292}
2025-08-03 09:48:06,941 - INFO - test: {'epoch': 91, 'time_epoch': 3.55893, 'loss': 0.60679487, 'lr': 0, 'params': 492902, 'time_iter': 0.05649, 'accuracy': 0.78556, 'f1': 0.78554, 'accuracy-SBM': 0.78555, 'auc': 0.96357}
2025-08-03 09:48:06,949 - INFO - > Epoch 91: took 81.6s (avg 89.3s) | Best so far: epoch 86	train_loss: 0.5466 train_accuracy-SBM: 0.8011	val_loss: 0.6084 val_accuracy-SBM: 0.7870	test_loss: 0.6094 test_accuracy-SBM: 0.7842
2025-08-03 09:49:20,645 - INFO - train: {'epoch': 92, 'time_epoch': 73.36703, 'eta': 567.0143, 'eta_hours': 0.1575, 'loss': 0.54249323, 'lr': 1.74e-05, 'params': 492902, 'time_iter': 0.11739, 'accuracy': 0.80254, 'f1': 0.80254, 'accuracy-SBM': 0.80254, 'auc': 0.97058}
2025-08-03 09:49:24,264 - INFO - val: {'epoch': 92, 'time_epoch': 3.57716, 'loss': 0.60971881, 'lr': 0, 'params': 492902, 'time_iter': 0.05678, 'accuracy': 0.78665, 'f1': 0.78654, 'accuracy-SBM': 0.78652, 'auc': 0.96317}
2025-08-03 09:49:27,829 - INFO - test: {'epoch': 92, 'time_epoch': 3.53232, 'loss': 0.60903182, 'lr': 0, 'params': 492902, 'time_iter': 0.05607, 'accuracy': 0.78488, 'f1': 0.78487, 'accuracy-SBM': 0.78488, 'auc': 0.96328}
2025-08-03 09:49:27,831 - INFO - > Epoch 92: took 80.9s (avg 89.2s) | Best so far: epoch 86	train_loss: 0.5466 train_accuracy-SBM: 0.8011	val_loss: 0.6084 val_accuracy-SBM: 0.7870	test_loss: 0.6094 test_accuracy-SBM: 0.7842
2025-08-03 09:50:40,411 - INFO - train: {'epoch': 93, 'time_epoch': 72.34861, 'eta': 485.45991, 'eta_hours': 0.13485, 'loss': 0.5416468, 'lr': 1.334e-05, 'params': 492902, 'time_iter': 0.11576, 'accuracy': 0.80301, 'f1': 0.80301, 'accuracy-SBM': 0.80301, 'auc': 0.97067}
2025-08-03 09:50:43,882 - INFO - val: {'epoch': 93, 'time_epoch': 3.4279, 'loss': 0.61079038, 'lr': 0, 'params': 492902, 'time_iter': 0.05441, 'accuracy': 0.78627, 'f1': 0.78616, 'accuracy-SBM': 0.78613, 'auc': 0.96299}
2025-08-03 09:50:47,643 - INFO - test: {'epoch': 93, 'time_epoch': 3.72814, 'loss': 0.60720103, 'lr': 0, 'params': 492902, 'time_iter': 0.05918, 'accuracy': 0.78521, 'f1': 0.7852, 'accuracy-SBM': 0.78522, 'auc': 0.96344}
2025-08-03 09:50:47,645 - INFO - > Epoch 93: took 79.8s (avg 89.1s) | Best so far: epoch 86	train_loss: 0.5466 train_accuracy-SBM: 0.8011	val_loss: 0.6084 val_accuracy-SBM: 0.7870	test_loss: 0.6094 test_accuracy-SBM: 0.7842
2025-08-03 09:51:59,815 - INFO - train: {'epoch': 94, 'time_epoch': 71.93756, 'eta': 404.07769, 'eta_hours': 0.11224, 'loss': 0.54167555, 'lr': 9.81e-06, 'params': 492902, 'time_iter': 0.1151, 'accuracy': 0.80259, 'f1': 0.80259, 'accuracy-SBM': 0.80259, 'auc': 0.97066}
2025-08-03 09:52:03,303 - INFO - val: {'epoch': 94, 'time_epoch': 3.44571, 'loss': 0.61305345, 'lr': 0, 'params': 492902, 'time_iter': 0.05469, 'accuracy': 0.78612, 'f1': 0.78601, 'accuracy-SBM': 0.78599, 'auc': 0.96288}
2025-08-03 09:52:06,717 - INFO - test: {'epoch': 94, 'time_epoch': 3.3822, 'loss': 0.60934808, 'lr': 0, 'params': 492902, 'time_iter': 0.05369, 'accuracy': 0.78518, 'f1': 0.78516, 'accuracy-SBM': 0.78517, 'auc': 0.96331}
2025-08-03 09:52:06,719 - INFO - > Epoch 94: took 79.1s (avg 89.0s) | Best so far: epoch 86	train_loss: 0.5466 train_accuracy-SBM: 0.8011	val_loss: 0.6084 val_accuracy-SBM: 0.7870	test_loss: 0.6094 test_accuracy-SBM: 0.7842
2025-08-03 09:53:18,163 - INFO - train: {'epoch': 95, 'time_epoch': 71.21765, 'eta': 322.86224, 'eta_hours': 0.08968, 'loss': 0.54018343, 'lr': 6.82e-06, 'params': 492902, 'time_iter': 0.11395, 'accuracy': 0.80351, 'f1': 0.80351, 'accuracy-SBM': 0.80351, 'auc': 0.97083}
2025-08-03 09:53:21,636 - INFO - val: {'epoch': 95, 'time_epoch': 3.43012, 'loss': 0.61185836, 'lr': 0, 'params': 492902, 'time_iter': 0.05445, 'accuracy': 0.78683, 'f1': 0.78671, 'accuracy-SBM': 0.78671, 'auc': 0.96302}
2025-08-03 09:53:25,061 - INFO - test: {'epoch': 95, 'time_epoch': 3.39496, 'loss': 0.6097081, 'lr': 0, 'params': 492902, 'time_iter': 0.05389, 'accuracy': 0.78542, 'f1': 0.78541, 'accuracy-SBM': 0.78542, 'auc': 0.96327}
2025-08-03 09:53:25,063 - INFO - > Epoch 95: took 78.3s (avg 88.9s) | Best so far: epoch 86	train_loss: 0.5466 train_accuracy-SBM: 0.8011	val_loss: 0.6084 val_accuracy-SBM: 0.7870	test_loss: 0.6094 test_accuracy-SBM: 0.7842
2025-08-03 09:54:36,497 - INFO - train: {'epoch': 96, 'time_epoch': 71.21119, 'eta': 241.85273, 'eta_hours': 0.06718, 'loss': 0.54028935, 'lr': 4.37e-06, 'params': 492902, 'time_iter': 0.11394, 'accuracy': 0.80365, 'f1': 0.80365, 'accuracy-SBM': 0.80365, 'auc': 0.97081}
2025-08-03 09:54:39,942 - INFO - val: {'epoch': 96, 'time_epoch': 3.40363, 'loss': 0.60925086, 'lr': 0, 'params': 492902, 'time_iter': 0.05403, 'accuracy': 0.7874, 'f1': 0.78727, 'accuracy-SBM': 0.78724, 'auc': 0.96326}
2025-08-03 09:54:43,368 - INFO - test: {'epoch': 96, 'time_epoch': 3.39435, 'loss': 0.60828459, 'lr': 0, 'params': 492902, 'time_iter': 0.05388, 'accuracy': 0.78521, 'f1': 0.78518, 'accuracy-SBM': 0.7852, 'auc': 0.96342}
2025-08-03 09:54:43,370 - INFO - > Epoch 96: took 78.3s (avg 88.8s) | Best so far: epoch 96	train_loss: 0.5403 train_accuracy-SBM: 0.8036	val_loss: 0.6093 val_accuracy-SBM: 0.7872	test_loss: 0.6083 test_accuracy-SBM: 0.7852
2025-08-03 09:55:55,378 - INFO - train: {'epoch': 97, 'time_epoch': 71.77372, 'eta': 161.05467, 'eta_hours': 0.04474, 'loss': 0.54137212, 'lr': 2.46e-06, 'params': 492902, 'time_iter': 0.11484, 'accuracy': 0.80282, 'f1': 0.80282, 'accuracy-SBM': 0.80282, 'auc': 0.9707}
2025-08-03 09:55:58,857 - INFO - val: {'epoch': 97, 'time_epoch': 3.43613, 'loss': 0.61143178, 'lr': 0, 'params': 492902, 'time_iter': 0.05454, 'accuracy': 0.78688, 'f1': 0.78676, 'accuracy-SBM': 0.78674, 'auc': 0.96303}
2025-08-03 09:56:02,342 - INFO - test: {'epoch': 97, 'time_epoch': 3.45378, 'loss': 0.6091936, 'lr': 0, 'params': 492902, 'time_iter': 0.05482, 'accuracy': 0.78501, 'f1': 0.78499, 'accuracy-SBM': 0.785, 'auc': 0.96333}
2025-08-03 09:56:02,344 - INFO - > Epoch 97: took 79.0s (avg 88.7s) | Best so far: epoch 96	train_loss: 0.5403 train_accuracy-SBM: 0.8036	val_loss: 0.6093 val_accuracy-SBM: 0.7872	test_loss: 0.6083 test_accuracy-SBM: 0.7852
2025-08-03 09:57:14,158 - INFO - train: {'epoch': 98, 'time_epoch': 71.58005, 'eta': 80.43696, 'eta_hours': 0.02234, 'loss': 0.53918347, 'lr': 1.09e-06, 'params': 492902, 'time_iter': 0.11453, 'accuracy': 0.80404, 'f1': 0.80405, 'accuracy-SBM': 0.80404, 'auc': 0.97094}
2025-08-03 09:57:17,624 - INFO - val: {'epoch': 98, 'time_epoch': 3.42433, 'loss': 0.61329074, 'lr': 0, 'params': 492902, 'time_iter': 0.05435, 'accuracy': 0.78683, 'f1': 0.78669, 'accuracy-SBM': 0.78666, 'auc': 0.96286}
2025-08-03 09:57:21,293 - INFO - test: {'epoch': 98, 'time_epoch': 3.4091, 'loss': 0.60800548, 'lr': 0, 'params': 492902, 'time_iter': 0.05411, 'accuracy': 0.78556, 'f1': 0.78554, 'accuracy-SBM': 0.78555, 'auc': 0.9635}
2025-08-03 09:57:21,296 - INFO - > Epoch 98: took 79.0s (avg 88.6s) | Best so far: epoch 96	train_loss: 0.5403 train_accuracy-SBM: 0.8036	val_loss: 0.6093 val_accuracy-SBM: 0.7872	test_loss: 0.6083 test_accuracy-SBM: 0.7852
2025-08-03 09:58:33,038 - INFO - train: {'epoch': 99, 'time_epoch': 71.51541, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.5405071, 'lr': 2.7e-07, 'params': 492902, 'time_iter': 0.11442, 'accuracy': 0.80356, 'f1': 0.80356, 'accuracy-SBM': 0.80356, 'auc': 0.97079}
2025-08-03 09:58:36,522 - INFO - val: {'epoch': 99, 'time_epoch': 3.44106, 'loss': 0.6110649, 'lr': 0, 'params': 492902, 'time_iter': 0.05462, 'accuracy': 0.78616, 'f1': 0.78604, 'accuracy-SBM': 0.78602, 'auc': 0.96301}
2025-08-03 09:58:39,989 - INFO - test: {'epoch': 99, 'time_epoch': 3.42938, 'loss': 0.6068323, 'lr': 0, 'params': 492902, 'time_iter': 0.05443, 'accuracy': 0.7857, 'f1': 0.78568, 'accuracy-SBM': 0.78569, 'auc': 0.9635}
2025-08-03 09:58:40,450 - INFO - > Epoch 99: took 78.7s (avg 88.5s) | Best so far: epoch 96	train_loss: 0.5403 train_accuracy-SBM: 0.8036	val_loss: 0.6093 val_accuracy-SBM: 0.7872	test_loss: 0.6083 test_accuracy-SBM: 0.7852
2025-08-03 09:58:40,451 - INFO - Avg time per epoch: 88.47s
2025-08-03 09:58:40,451 - INFO - Total train loop time: 2.46h
2025-08-03 09:58:42,660 - INFO - ============================================================
2025-08-03 09:58:42,661 - INFO - Starting PK-Explainer Analysis
2025-08-03 09:58:42,661 - INFO - ============================================================
2025-08-03 09:58:42,736 - INFO - Saved model state to results/Cluster/Cluster-GINE-45/model_for_ablation.pt
2025-08-03 09:58:42,736 - INFO - 
