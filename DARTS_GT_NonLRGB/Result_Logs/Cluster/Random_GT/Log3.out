Lmod has detected the following error: The following module(s) are unknown:
"cuda/12.0"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore-cache load "cuda/12.0"

Also make sure that all modulefiles written in TCL start with the string
#%Module



              total        used        free      shared  buff/cache   available
Mem:          251Gi       8.8Gi       187Gi       1.9Gi        55Gi       238Gi
Swap:         1.9Gi        21Mi       1.8Gi
Sun Aug  3 09:59:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA TITAN RTX               On  |   00000000:1B:00.0 Off |                  N/A |
| 41%   38C    P8             33W /  280W |       1MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Running experiment with seed: 47
Starting training for seed 47...
Changed working directory to: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/RAND_GT
Loading config from: /data/coml-deepcmb/lady6973/ABLATION_FINAL/RACE_TO_SUP/CLUSTER/FINAL_SINGLE/SPARSE_E/RAND_GT/confignas.yaml
Using device: cuda
2025-08-03 10:01:19,474 - INFO - GPU Mem: 25.2GB
2025-08-03 10:01:19,474 - INFO - Run directory: results/Cluster/Cluster-GINE-47
2025-08-03 10:01:19,475 - INFO - Seed: 47
2025-08-03 10:01:19,475 - INFO - === OPTIMIZED MOE CONFIGURATION ===
2025-08-03 10:01:19,475 - INFO - Routing mode: none
2025-08-03 10:01:19,475 - INFO - Expert types: ['GINE', 'CustomGatedGCN', 'GATV2']
2025-08-03 10:01:19,475 - INFO - Number of layers: 16
2025-08-03 10:01:19,475 - INFO - Uncertainty enabled: False
2025-08-03 10:01:19,475 - INFO - Training mode: custom
2025-08-03 10:01:19,475 - INFO - Optimization: Uncertainty only for last layer + test/val
2025-08-03 10:01:19,475 - INFO - Additional features: Router weights logging + JSON export
2025-08-03 10:01:34,333 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-08-03 10:01:34,358 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-08-03 10:01:34,449 - INFO -   undirected: True
2025-08-03 10:01:34,449 - INFO -   num graphs: 12000
/data/coml-deepcmb/lady6973/py310_new_venv/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
2025-08-03 10:01:34,450 - INFO -   avg num_nodes/graph: 117
2025-08-03 10:01:34,450 - INFO -   num node features: 7
2025-08-03 10:01:34,450 - INFO -   num edge features: 0
2025-08-03 10:01:34,451 - INFO -   num classes: 6
2025-08-03 10:01:34,451 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-08-03 10:01:34,451 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-08-03 10:01:34,459 - INFO -   ...estimated to be undirected: True

  0%|          | 0/12000 [00:00<?, ?it/s]
 16%|█▋        | 1975/12000 [00:10<00:50, 197.47it/s]
 33%|███▎      | 3933/12000 [00:20<00:41, 196.48it/s]
 49%|████▉     | 5935/12000 [00:30<00:30, 198.13it/s]
 66%|██████▌   | 7913/12000 [00:40<00:20, 198.00it/s]
 82%|████████▏ | 9858/12000 [00:50<00:10, 196.73it/s]
 99%|█████████▊| 11845/12000 [01:00<00:00, 197.38it/s]
100%|██████████| 12000/12000 [01:00<00:00, 197.42it/s]
2025-08-03 10:02:35,993 - INFO - Done! Took 00:01:01.54
2025-08-03 10:02:36,014 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GINE
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GATV2
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GATV2
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GATV2
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GATV2
RANDOMGTLayer: Randomly selected GNN type: GINE
RANDOMGTLayer: Randomly selected GNN type: CustomGatedGCN
RANDOMGTLayer: Randomly selected GNN type: GATV2
RANDOMGTLayer: Randomly selected GNN type: GINE
2025-08-03 10:02:36,647 - INFO - Created model type: <class 'torch_geometric.graphgym.model_builder.GraphGymModule'>
2025-08-03 10:02:36,647 - INFO - Inner model type: <class 'graphgps.network.RANDOM_GTModel_EDGE.RANDOM_GTModelEDGE'>
2025-08-03 10:02:36,647 - INFO - Inner model has get_darts_model: False
2025-08-03 10:02:36,652 - INFO - GraphGymModule(
  (model): RANDOM_GTModelEDGE(
    (encoder): FeatureEncoder(
      (node_encoder): LapPENodeEncoder(
        (linear_x): Linear(in_features=7, out_features=32, bias=True)
        (linear_A): Linear(in_features=2, out_features=32, bias=True)
        (pe_encoder): Sequential(
          (0): ReLU()
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): ReLU()
        )
      )
      (edge_encoder): DummyEdgeEncoder(
        (encoder): Embedding(1, 48)
      )
    )
    (layers): Sequential(
      (0): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (1): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (2): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (3): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (4): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (5): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (6): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (7): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (8): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (9): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (10): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (11): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (12): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (13): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GatedGCNLayer()
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (14): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GATv2Conv(48, 6, heads=8)
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (15): RANDOMGTLayer(
        summary: dim_h=48, global_model_type=SparseTransformer, heads=8
        (edge_processor): Sequential(
          (0): Linear(in_features=48, out_features=48, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=48, out_features=48, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
        (kv_model): GINEConv(nn=Sequential(
          (0): Linear(48, 48, bias=True)
          (1): ReLU()
          (2): Linear(48, 48, bias=True)
        ))
        (self_attn): SparseGraphAttention(
          (W_k): Linear(in_features=48, out_features=48, bias=True)
          (W_v): Linear(in_features=48, out_features=48, bias=True)
          (W_o): Linear(in_features=48, out_features=48, bias=True)
        )
        (norm1_local): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_kv): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_kv): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=48, out_features=96, bias=True)
        (ff_linear2): Linear(in_features=96, out_features=48, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (post_mp): GNNInductiveNodeHead(
      (layer_post_mp): MLP(
        (model): Sequential(
          (0): GeneralMultiLayer(
            (Layer_0): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
            (Layer_1): GeneralLayer(
              (layer): Linear(
                (model): Linear(48, 48, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
          )
          (1): Linear(
            (model): Linear(48, 6, bias=True)
          )
        )
      )
    )
  )
)
2025-08-03 10:02:36,657 - INFO - Number of parameters: 495,254
2025-08-03 10:02:36,657 - INFO - Starting optimized training: 2025-08-03 10:02:36.657489
2025-08-03 10:02:42,119 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset':
2025-08-03 10:02:42,119 - INFO -   Data(x=[1406436, 7], edge_index=[2, 51620680], y=[1406436])
2025-08-03 10:02:42,120 - INFO -   undirected: True
2025-08-03 10:02:42,120 - INFO -   num graphs: 12000
2025-08-03 10:02:42,120 - INFO -   avg num_nodes/graph: 117
2025-08-03 10:02:42,120 - INFO -   num node features: 7
2025-08-03 10:02:42,121 - INFO -   num edge features: 0
2025-08-03 10:02:42,122 - INFO -   num classes: 6
2025-08-03 10:02:42,122 - INFO - Parsed RWSE PE kernel times / steps: [1, 2, 4, 8, 16]
2025-08-03 10:02:42,122 - INFO - Precomputing Positional Encoding statistics: ['LapPE', 'RWSE'] for all graphs...
2025-08-03 10:02:42,130 - INFO -   ...estimated to be undirected: True

  0%|          | 0/12000 [00:00<?, ?it/s]
 17%|█▋        | 2002/12000 [00:10<00:49, 200.07it/s]
 33%|███▎      | 3970/12000 [00:20<00:40, 198.11it/s]
 50%|████▉     | 5974/12000 [00:30<00:30, 199.13it/s]
 66%|██████▋   | 7959/12000 [00:40<00:20, 198.86it/s]
 83%|████████▎ | 9916/12000 [00:50<00:10, 197.72it/s]
 99%|█████████▉| 11909/12000 [01:00<00:00, 198.22it/s]
100%|██████████| 12000/12000 [01:00<00:00, 198.46it/s]
2025-08-03 10:03:43,316 - INFO - Done! Took 00:01:01.19
2025-08-03 10:03:43,342 - INFO - [*] Loaded dataset 'CLUSTER' from 'PyG-GNNBenchmarkDataset'
2025-08-03 10:03:43,401 - INFO - === STARTING OPTIMIZED TRAINING ===
2025-08-03 10:03:43,401 - INFO - Start from epoch 0
2025-08-03 10:05:11,607 - INFO - train: {'epoch': 0, 'time_epoch': 87.0146, 'eta': 8614.44536, 'eta_hours': 2.3929, 'loss': 1.79954943, 'lr': 0.0, 'params': 495254, 'time_iter': 0.13922, 'accuracy': 0.1648, 'f1': 0.08753, 'accuracy-SBM': 0.16481, 'auc': 0.50187}
2025-08-03 10:05:11,614 - INFO - ...computing epoch stats took: 1.17s
2025-08-03 10:05:15,815 - INFO - val: {'epoch': 0, 'time_epoch': 4.15815, 'loss': 1.79949401, 'lr': 0, 'params': 495254, 'time_iter': 0.066, 'accuracy': 0.16244, 'f1': 0.08394, 'accuracy-SBM': 0.16534, 'auc': 0.50434}
2025-08-03 10:05:15,817 - INFO - ...computing epoch stats took: 0.04s
2025-08-03 10:05:20,008 - INFO - test: {'epoch': 0, 'time_epoch': 4.14205, 'loss': 1.80018082, 'lr': 0, 'params': 495254, 'time_iter': 0.06575, 'accuracy': 0.16622, 'f1': 0.08681, 'accuracy-SBM': 0.16677, 'auc': 0.49915}
2025-08-03 10:05:20,010 - INFO - ...computing epoch stats took: 0.05s
2025-08-03 10:05:20,011 - INFO - > Epoch 0: took 96.6s (avg 96.6s) | Best so far: epoch 0	train_loss: 1.7995 train_accuracy-SBM: 0.1648	val_loss: 1.7995 val_accuracy-SBM: 0.1653	test_loss: 1.8002 test_accuracy-SBM: 0.1668
2025-08-03 10:06:43,449 - INFO - train: {'epoch': 1, 'time_epoch': 83.13307, 'eta': 8337.23568, 'eta_hours': 2.3159, 'loss': 1.67706458, 'lr': 0.0002, 'params': 495254, 'time_iter': 0.13301, 'accuracy': 0.33621, 'f1': 0.32893, 'accuracy-SBM': 0.33618, 'auc': 0.68648}
2025-08-03 10:06:43,455 - INFO - ...computing epoch stats took: 0.29s
2025-08-03 10:06:47,534 - INFO - val: {'epoch': 1, 'time_epoch': 4.02543, 'loss': 1.6293193, 'lr': 0, 'params': 495254, 'time_iter': 0.0639, 'accuracy': 0.38635, 'f1': 0.33895, 'accuracy-SBM': 0.38382, 'auc': 0.73348}
2025-08-03 10:06:47,536 - INFO - ...computing epoch stats took: 0.05s
2025-08-03 10:06:51,589 - INFO - test: {'epoch': 1, 'time_epoch': 4.01658, 'loss': 1.6290371, 'lr': 0, 'params': 495254, 'time_iter': 0.06376, 'accuracy': 0.38492, 'f1': 0.33955, 'accuracy-SBM': 0.38368, 'auc': 0.73311}
2025-08-03 10:06:51,591 - INFO - ...computing epoch stats took: 0.04s
2025-08-03 10:06:51,591 - INFO - > Epoch 1: took 91.6s (avg 94.1s) | Best so far: epoch 1	train_loss: 1.6771 train_accuracy-SBM: 0.3362	val_loss: 1.6293 val_accuracy-SBM: 0.3838	test_loss: 1.6290 test_accuracy-SBM: 0.3837
2025-08-03 10:08:15,447 - INFO - train: {'epoch': 2, 'time_epoch': 83.50047, 'eta': 8201.2897, 'eta_hours': 2.27814, 'loss': 1.43158631, 'lr': 0.0004, 'params': 495254, 'time_iter': 0.1336, 'accuracy': 0.47128, 'f1': 0.4373, 'accuracy-SBM': 0.47126, 'auc': 0.79318}
2025-08-03 10:08:15,454 - INFO - ...computing epoch stats took: 0.34s
2025-08-03 10:08:19,606 - INFO - val: {'epoch': 2, 'time_epoch': 4.06931, 'loss': 1.65186604, 'lr': 0, 'params': 495254, 'time_iter': 0.06459, 'accuracy': 0.32116, 'f1': 0.28215, 'accuracy-SBM': 0.32077, 'auc': 0.77865}
2025-08-03 10:08:19,608 - INFO - ...computing epoch stats took: 0.08s
2025-08-03 10:08:23,717 - INFO - test: {'epoch': 2, 'time_epoch': 4.06174, 'loss': 1.63818798, 'lr': 0, 'params': 495254, 'time_iter': 0.06447, 'accuracy': 0.3268, 'f1': 0.28817, 'accuracy-SBM': 0.32657, 'auc': 0.77942}
2025-08-03 10:08:23,719 - INFO - ...computing epoch stats took: 0.05s
2025-08-03 10:08:23,719 - INFO - > Epoch 2: took 92.1s (avg 93.4s) | Best so far: epoch 1	train_loss: 1.6771 train_accuracy-SBM: 0.3362	val_loss: 1.6293 val_accuracy-SBM: 0.3838	test_loss: 1.6290 test_accuracy-SBM: 0.3837
2025-08-03 10:09:49,050 - INFO - train: {'epoch': 3, 'time_epoch': 84.9075, 'eta': 8125.3353, 'eta_hours': 2.25704, 'loss': 1.19309803, 'lr': 0.0006, 'params': 495254, 'time_iter': 0.13585, 'accuracy': 0.5421, 'f1': 0.51915, 'accuracy-SBM': 0.54208, 'auc': 0.85477}
2025-08-03 10:09:53,196 - INFO - val: {'epoch': 3, 'time_epoch': 4.08427, 'loss': 1.32263858, 'lr': 0, 'params': 495254, 'time_iter': 0.06483, 'accuracy': 0.4915, 'f1': 0.45981, 'accuracy-SBM': 0.48827, 'auc': 0.8404}
2025-08-03 10:09:57,298 - INFO - test: {'epoch': 3, 'time_epoch': 4.06922, 'loss': 1.3131878, 'lr': 0, 'params': 495254, 'time_iter': 0.06459, 'accuracy': 0.49232, 'f1': 0.46337, 'accuracy-SBM': 0.49209, 'auc': 0.8416}
2025-08-03 10:09:57,301 - INFO - > Epoch 3: took 93.6s (avg 93.5s) | Best so far: epoch 3	train_loss: 1.1931 train_accuracy-SBM: 0.5421	val_loss: 1.3226 val_accuracy-SBM: 0.4883	test_loss: 1.3132 test_accuracy-SBM: 0.4921
2025-08-03 10:11:22,609 - INFO - train: {'epoch': 4, 'time_epoch': 84.98284, 'eta': 8047.23105, 'eta_hours': 2.23534, 'loss': 1.07976265, 'lr': 0.0008, 'params': 495254, 'time_iter': 0.13597, 'accuracy': 0.56922, 'f1': 0.55554, 'accuracy-SBM': 0.56922, 'auc': 0.87758}
2025-08-03 10:11:26,743 - INFO - val: {'epoch': 4, 'time_epoch': 4.07944, 'loss': 1.11329876, 'lr': 0, 'params': 495254, 'time_iter': 0.06475, 'accuracy': 0.56062, 'f1': 0.52519, 'accuracy-SBM': 0.55816, 'auc': 0.87376}
2025-08-03 10:11:30,915 - INFO - test: {'epoch': 4, 'time_epoch': 4.13465, 'loss': 1.09310988, 'lr': 0, 'params': 495254, 'time_iter': 0.06563, 'accuracy': 0.56422, 'f1': 0.52898, 'accuracy-SBM': 0.56377, 'auc': 0.87875}
2025-08-03 10:11:30,917 - INFO - > Epoch 4: took 93.6s (avg 93.5s) | Best so far: epoch 4	train_loss: 1.0798 train_accuracy-SBM: 0.5692	val_loss: 1.1133 val_accuracy-SBM: 0.5582	test_loss: 1.0931 test_accuracy-SBM: 0.5638
2025-08-03 10:13:01,235 - INFO - train: {'epoch': 5, 'time_epoch': 90.02968, 'eta': 8045.90104, 'eta_hours': 2.23497, 'loss': 1.03546549, 'lr': 0.001, 'params': 495254, 'time_iter': 0.14405, 'accuracy': 0.58083, 'f1': 0.56691, 'accuracy-SBM': 0.58083, 'auc': 0.88536}
2025-08-03 10:13:05,629 - INFO - val: {'epoch': 5, 'time_epoch': 4.34071, 'loss': 1.05526419, 'lr': 0, 'params': 495254, 'time_iter': 0.0689, 'accuracy': 0.57675, 'f1': 0.55379, 'accuracy-SBM': 0.57455, 'auc': 0.88326}
2025-08-03 10:13:10,025 - INFO - test: {'epoch': 5, 'time_epoch': 4.35622, 'loss': 1.05365019, 'lr': 0, 'params': 495254, 'time_iter': 0.06915, 'accuracy': 0.57432, 'f1': 0.55207, 'accuracy-SBM': 0.5738, 'auc': 0.88369}
2025-08-03 10:13:10,029 - INFO - > Epoch 5: took 99.1s (avg 94.4s) | Best so far: epoch 5	train_loss: 1.0355 train_accuracy-SBM: 0.5808	val_loss: 1.0553 val_accuracy-SBM: 0.5746	test_loss: 1.0537 test_accuracy-SBM: 0.5738
2025-08-03 10:14:39,411 - INFO - train: {'epoch': 6, 'time_epoch': 89.11456, 'eta': 8007.07029, 'eta_hours': 2.22419, 'loss': 1.00265929, 'lr': 0.00099973, 'params': 495254, 'time_iter': 0.14258, 'accuracy': 0.59041, 'f1': 0.57773, 'accuracy-SBM': 0.59042, 'auc': 0.89144}
2025-08-03 10:14:43,813 - INFO - val: {'epoch': 6, 'time_epoch': 4.34071, 'loss': 0.976432, 'lr': 0, 'params': 495254, 'time_iter': 0.0689, 'accuracy': 0.60583, 'f1': 0.57761, 'accuracy-SBM': 0.60325, 'auc': 0.89783}
2025-08-03 10:14:48,203 - INFO - test: {'epoch': 6, 'time_epoch': 4.35344, 'loss': 0.98387401, 'lr': 0, 'params': 495254, 'time_iter': 0.0691, 'accuracy': 0.59875, 'f1': 0.57254, 'accuracy-SBM': 0.59785, 'auc': 0.89643}
2025-08-03 10:14:48,206 - INFO - > Epoch 6: took 98.2s (avg 95.0s) | Best so far: epoch 6	train_loss: 1.0027 train_accuracy-SBM: 0.5904	val_loss: 0.9764 val_accuracy-SBM: 0.6032	test_loss: 0.9839 test_accuracy-SBM: 0.5978
2025-08-03 10:16:18,515 - INFO - train: {'epoch': 7, 'time_epoch': 90.05691, 'eta': 7966.50562, 'eta_hours': 2.21292, 'loss': 0.98266754, 'lr': 0.00099891, 'params': 495254, 'time_iter': 0.14409, 'accuracy': 0.59672, 'f1': 0.58522, 'accuracy-SBM': 0.59673, 'auc': 0.89497}
2025-08-03 10:16:22,922 - INFO - val: {'epoch': 7, 'time_epoch': 4.35635, 'loss': 0.96289278, 'lr': 0, 'params': 495254, 'time_iter': 0.06915, 'accuracy': 0.6082, 'f1': 0.59296, 'accuracy-SBM': 0.6065, 'auc': 0.89984}
2025-08-03 10:16:27,297 - INFO - test: {'epoch': 7, 'time_epoch': 4.33897, 'loss': 0.96604666, 'lr': 0, 'params': 495254, 'time_iter': 0.06887, 'accuracy': 0.60404, 'f1': 0.58989, 'accuracy-SBM': 0.60315, 'auc': 0.89924}
2025-08-03 10:16:27,300 - INFO - > Epoch 7: took 99.1s (avg 95.5s) | Best so far: epoch 7	train_loss: 0.9827 train_accuracy-SBM: 0.5967	val_loss: 0.9629 val_accuracy-SBM: 0.6065	test_loss: 0.9660 test_accuracy-SBM: 0.6031
2025-08-03 10:17:56,658 - INFO - train: {'epoch': 8, 'time_epoch': 89.10544, 'eta': 7905.32224, 'eta_hours': 2.19592, 'loss': 0.97102155, 'lr': 0.00099754, 'params': 495254, 'time_iter': 0.14257, 'accuracy': 0.60041, 'f1': 0.58933, 'accuracy-SBM': 0.60044, 'auc': 0.89701}
2025-08-03 10:18:00,978 - INFO - val: {'epoch': 8, 'time_epoch': 4.26128, 'loss': 0.93507768, 'lr': 0, 'params': 495254, 'time_iter': 0.06764, 'accuracy': 0.6183, 'f1': 0.58452, 'accuracy-SBM': 0.61621, 'auc': 0.9046}
2025-08-03 10:18:05,289 - INFO - test: {'epoch': 8, 'time_epoch': 4.27108, 'loss': 0.93969186, 'lr': 0, 'params': 495254, 'time_iter': 0.06779, 'accuracy': 0.61194, 'f1': 0.57867, 'accuracy-SBM': 0.61099, 'auc': 0.90364}
2025-08-03 10:18:05,292 - INFO - > Epoch 8: took 98.0s (avg 95.8s) | Best so far: epoch 8	train_loss: 0.9710 train_accuracy-SBM: 0.6004	val_loss: 0.9351 val_accuracy-SBM: 0.6162	test_loss: 0.9397 test_accuracy-SBM: 0.6110
2025-08-03 10:19:35,427 - INFO - train: {'epoch': 9, 'time_epoch': 89.88406, 'eta': 7845.56208, 'eta_hours': 2.17932, 'loss': 0.96170464, 'lr': 0.00099563, 'params': 495254, 'time_iter': 0.14381, 'accuracy': 0.60258, 'f1': 0.59198, 'accuracy-SBM': 0.6026, 'auc': 0.89869}
2025-08-03 10:19:39,870 - INFO - val: {'epoch': 9, 'time_epoch': 4.38834, 'loss': 0.92462251, 'lr': 0, 'params': 495254, 'time_iter': 0.06966, 'accuracy': 0.62185, 'f1': 0.58078, 'accuracy-SBM': 0.62016, 'auc': 0.90615}
2025-08-03 10:19:44,279 - INFO - test: {'epoch': 9, 'time_epoch': 4.36913, 'loss': 0.93021801, 'lr': 0, 'params': 495254, 'time_iter': 0.06935, 'accuracy': 0.61553, 'f1': 0.57587, 'accuracy-SBM': 0.6146, 'auc': 0.90549}
2025-08-03 10:19:44,282 - INFO - > Epoch 9: took 99.0s (avg 96.1s) | Best so far: epoch 9	train_loss: 0.9617 train_accuracy-SBM: 0.6026	val_loss: 0.9246 val_accuracy-SBM: 0.6202	test_loss: 0.9302 test_accuracy-SBM: 0.6146
2025-08-03 10:21:13,904 - INFO - train: {'epoch': 10, 'time_epoch': 89.37588, 'eta': 7776.21317, 'eta_hours': 2.16006, 'loss': 0.95351539, 'lr': 0.00099318, 'params': 495254, 'time_iter': 0.143, 'accuracy': 0.60649, 'f1': 0.59542, 'accuracy-SBM': 0.60651, 'auc': 0.90013}
2025-08-03 10:21:18,236 - INFO - val: {'epoch': 10, 'time_epoch': 4.27237, 'loss': 0.94880106, 'lr': 0, 'params': 495254, 'time_iter': 0.06782, 'accuracy': 0.60865, 'f1': 0.59622, 'accuracy-SBM': 0.60612, 'auc': 0.90149}
2025-08-03 10:21:22,469 - INFO - test: {'epoch': 10, 'time_epoch': 4.19069, 'loss': 0.95480952, 'lr': 0, 'params': 495254, 'time_iter': 0.06652, 'accuracy': 0.60349, 'f1': 0.59215, 'accuracy-SBM': 0.60289, 'auc': 0.90058}
2025-08-03 10:21:22,471 - INFO - > Epoch 10: took 98.2s (avg 96.3s) | Best so far: epoch 9	train_loss: 0.9617 train_accuracy-SBM: 0.6026	val_loss: 0.9246 val_accuracy-SBM: 0.6202	test_loss: 0.9302 test_accuracy-SBM: 0.6146
2025-08-03 10:22:51,442 - INFO - train: {'epoch': 11, 'time_epoch': 88.72369, 'eta': 7698.7437, 'eta_hours': 2.13854, 'loss': 0.94710546, 'lr': 0.00099019, 'params': 495254, 'time_iter': 0.14196, 'accuracy': 0.60826, 'f1': 0.59704, 'accuracy-SBM': 0.60827, 'auc': 0.90125}
2025-08-03 10:22:55,820 - INFO - val: {'epoch': 11, 'time_epoch': 4.3269, 'loss': 0.92864887, 'lr': 0, 'params': 495254, 'time_iter': 0.06868, 'accuracy': 0.61968, 'f1': 0.59303, 'accuracy-SBM': 0.6174, 'auc': 0.90522}
2025-08-03 10:23:00,170 - INFO - test: {'epoch': 11, 'time_epoch': 4.31322, 'loss': 0.94131788, 'lr': 0, 'params': 495254, 'time_iter': 0.06846, 'accuracy': 0.61104, 'f1': 0.58517, 'accuracy-SBM': 0.60989, 'auc': 0.90321}
2025-08-03 10:23:00,172 - INFO - > Epoch 11: took 97.7s (avg 96.4s) | Best so far: epoch 9	train_loss: 0.9617 train_accuracy-SBM: 0.6026	val_loss: 0.9246 val_accuracy-SBM: 0.6202	test_loss: 0.9302 test_accuracy-SBM: 0.6146
2025-08-03 10:24:29,500 - INFO - train: {'epoch': 12, 'time_epoch': 88.97336, 'eta': 7621.21367, 'eta_hours': 2.117, 'loss': 0.9404846, 'lr': 0.00098666, 'params': 495254, 'time_iter': 0.14236, 'accuracy': 0.60953, 'f1': 0.59879, 'accuracy-SBM': 0.60955, 'auc': 0.9024}
2025-08-03 10:24:33,869 - INFO - val: {'epoch': 12, 'time_epoch': 4.31777, 'loss': 0.91429318, 'lr': 0, 'params': 495254, 'time_iter': 0.06854, 'accuracy': 0.62376, 'f1': 0.61137, 'accuracy-SBM': 0.62144, 'auc': 0.90724}
2025-08-03 10:24:38,170 - INFO - test: {'epoch': 12, 'time_epoch': 4.26774, 'loss': 0.92855353, 'lr': 0, 'params': 495254, 'time_iter': 0.06774, 'accuracy': 0.61695, 'f1': 0.60557, 'accuracy-SBM': 0.61647, 'auc': 0.90506}
2025-08-03 10:24:38,172 - INFO - > Epoch 12: took 98.0s (avg 96.5s) | Best so far: epoch 12	train_loss: 0.9405 train_accuracy-SBM: 0.6096	val_loss: 0.9143 val_accuracy-SBM: 0.6214	test_loss: 0.9286 test_accuracy-SBM: 0.6165
2025-08-03 10:26:08,208 - INFO - train: {'epoch': 13, 'time_epoch': 89.78515, 'eta': 7547.03562, 'eta_hours': 2.0964, 'loss': 0.93297566, 'lr': 0.0009826, 'params': 495254, 'time_iter': 0.14366, 'accuracy': 0.61322, 'f1': 0.6026, 'accuracy-SBM': 0.61324, 'auc': 0.90372}
2025-08-03 10:26:12,580 - INFO - val: {'epoch': 13, 'time_epoch': 4.31105, 'loss': 0.88326892, 'lr': 0, 'params': 495254, 'time_iter': 0.06843, 'accuracy': 0.63457, 'f1': 0.60167, 'accuracy-SBM': 0.6327, 'auc': 0.9125}
2025-08-03 10:26:16,928 - INFO - test: {'epoch': 13, 'time_epoch': 4.30839, 'loss': 0.89949243, 'lr': 0, 'params': 495254, 'time_iter': 0.06839, 'accuracy': 0.62433, 'f1': 0.59363, 'accuracy-SBM': 0.62352, 'auc': 0.90999}
2025-08-03 10:26:16,931 - INFO - > Epoch 13: took 98.8s (avg 96.7s) | Best so far: epoch 13	train_loss: 0.9330 train_accuracy-SBM: 0.6132	val_loss: 0.8833 val_accuracy-SBM: 0.6327	test_loss: 0.8995 test_accuracy-SBM: 0.6235
2025-08-03 10:27:45,597 - INFO - train: {'epoch': 14, 'time_epoch': 88.4218, 'eta': 7463.05098, 'eta_hours': 2.07307, 'loss': 0.92911558, 'lr': 0.00097802, 'params': 495254, 'time_iter': 0.14147, 'accuracy': 0.61441, 'f1': 0.60308, 'accuracy-SBM': 0.61444, 'auc': 0.90438}
2025-08-03 10:27:49,949 - INFO - val: {'epoch': 14, 'time_epoch': 4.30407, 'loss': 0.90153404, 'lr': 0, 'params': 495254, 'time_iter': 0.06832, 'accuracy': 0.6265, 'f1': 0.61373, 'accuracy-SBM': 0.62393, 'auc': 0.90941}
2025-08-03 10:27:54,280 - INFO - test: {'epoch': 14, 'time_epoch': 4.2965, 'loss': 0.90534818, 'lr': 0, 'params': 495254, 'time_iter': 0.0682, 'accuracy': 0.62215, 'f1': 0.61031, 'accuracy-SBM': 0.6214, 'auc': 0.90893}
2025-08-03 10:27:54,283 - INFO - > Epoch 14: took 97.4s (avg 96.7s) | Best so far: epoch 13	train_loss: 0.9330 train_accuracy-SBM: 0.6132	val_loss: 0.8833 val_accuracy-SBM: 0.6327	test_loss: 0.8995 test_accuracy-SBM: 0.6235
2025-08-03 10:29:22,750 - INFO - train: {'epoch': 15, 'time_epoch': 88.22536, 'eta': 7377.48036, 'eta_hours': 2.0493, 'loss': 0.92829742, 'lr': 0.00097291, 'params': 495254, 'time_iter': 0.14116, 'accuracy': 0.61328, 'f1': 0.60208, 'accuracy-SBM': 0.61329, 'auc': 0.9045}
2025-08-03 10:29:27,064 - INFO - val: {'epoch': 15, 'time_epoch': 4.26599, 'loss': 0.8872684, 'lr': 0, 'params': 495254, 'time_iter': 0.06771, 'accuracy': 0.62909, 'f1': 0.61412, 'accuracy-SBM': 0.62691, 'auc': 0.91209}
2025-08-03 10:29:31,352 - INFO - test: {'epoch': 15, 'time_epoch': 4.25342, 'loss': 0.90010063, 'lr': 0, 'params': 495254, 'time_iter': 0.06751, 'accuracy': 0.62249, 'f1': 0.60856, 'accuracy-SBM': 0.62157, 'auc': 0.90986}
2025-08-03 10:29:31,354 - INFO - > Epoch 15: took 97.1s (avg 96.7s) | Best so far: epoch 13	train_loss: 0.9330 train_accuracy-SBM: 0.6132	val_loss: 0.8833 val_accuracy-SBM: 0.6327	test_loss: 0.8995 test_accuracy-SBM: 0.6235
2025-08-03 10:31:00,290 - INFO - train: {'epoch': 16, 'time_epoch': 88.68595, 'eta': 7293.8462, 'eta_hours': 2.02607, 'loss': 0.92119683, 'lr': 0.00096728, 'params': 495254, 'time_iter': 0.1419, 'accuracy': 0.61645, 'f1': 0.60606, 'accuracy-SBM': 0.61647, 'auc': 0.90572}
2025-08-03 10:31:04,647 - INFO - val: {'epoch': 16, 'time_epoch': 4.30307, 'loss': 0.88573621, 'lr': 0, 'params': 495254, 'time_iter': 0.0683, 'accuracy': 0.63239, 'f1': 0.60551, 'accuracy-SBM': 0.63057, 'auc': 0.91199}
2025-08-03 10:31:08,952 - INFO - test: {'epoch': 16, 'time_epoch': 4.25802, 'loss': 0.89924377, 'lr': 0, 'params': 495254, 'time_iter': 0.06759, 'accuracy': 0.62332, 'f1': 0.59785, 'accuracy-SBM': 0.62251, 'auc': 0.90977}
2025-08-03 10:31:08,954 - INFO - > Epoch 16: took 97.6s (avg 96.8s) | Best so far: epoch 13	train_loss: 0.9330 train_accuracy-SBM: 0.6132	val_loss: 0.8833 val_accuracy-SBM: 0.6327	test_loss: 0.8995 test_accuracy-SBM: 0.6235
2025-08-03 10:32:37,354 - INFO - train: {'epoch': 17, 'time_epoch': 88.15361, 'eta': 7207.22561, 'eta_hours': 2.00201, 'loss': 0.91753496, 'lr': 0.00096114, 'params': 495254, 'time_iter': 0.14105, 'accuracy': 0.61682, 'f1': 0.60659, 'accuracy-SBM': 0.61684, 'auc': 0.90638}
2025-08-03 10:32:41,671 - INFO - val: {'epoch': 17, 'time_epoch': 4.25418, 'loss': 0.89315583, 'lr': 0, 'params': 495254, 'time_iter': 0.06753, 'accuracy': 0.62732, 'f1': 0.61422, 'accuracy-SBM': 0.62516, 'auc': 0.91086}
2025-08-03 10:32:45,965 - INFO - test: {'epoch': 17, 'time_epoch': 4.25898, 'loss': 0.90537617, 'lr': 0, 'params': 495254, 'time_iter': 0.0676, 'accuracy': 0.62216, 'f1': 0.61051, 'accuracy-SBM': 0.62153, 'auc': 0.909}
2025-08-03 10:32:45,967 - INFO - > Epoch 17: took 97.0s (avg 96.8s) | Best so far: epoch 13	train_loss: 0.9330 train_accuracy-SBM: 0.6132	val_loss: 0.8833 val_accuracy-SBM: 0.6327	test_loss: 0.8995 test_accuracy-SBM: 0.6235
2025-08-03 10:34:13,623 - INFO - train: {'epoch': 18, 'time_epoch': 87.30504, 'eta': 7116.82608, 'eta_hours': 1.9769, 'loss': 0.91450369, 'lr': 0.0009545, 'params': 495254, 'time_iter': 0.13969, 'accuracy': 0.61827, 'f1': 0.60778, 'accuracy-SBM': 0.61829, 'auc': 0.90688}
2025-08-03 10:34:17,908 - INFO - val: {'epoch': 18, 'time_epoch': 4.23408, 'loss': 0.86663563, 'lr': 0, 'params': 495254, 'time_iter': 0.06721, 'accuracy': 0.63629, 'f1': 0.60091, 'accuracy-SBM': 0.63332, 'auc': 0.91519}
2025-08-03 10:34:22,159 - INFO - test: {'epoch': 18, 'time_epoch': 4.20668, 'loss': 0.88288706, 'lr': 0, 'params': 495254, 'time_iter': 0.06677, 'accuracy': 0.62832, 'f1': 0.59313, 'accuracy-SBM': 0.62776, 'auc': 0.91255}
2025-08-03 10:34:22,161 - INFO - > Epoch 18: took 96.2s (avg 96.8s) | Best so far: epoch 18	train_loss: 0.9145 train_accuracy-SBM: 0.6183	val_loss: 0.8666 val_accuracy-SBM: 0.6333	test_loss: 0.8829 test_accuracy-SBM: 0.6278
2025-08-03 10:35:48,922 - INFO - train: {'epoch': 19, 'time_epoch': 86.51498, 'eta': 7023.57576, 'eta_hours': 1.95099, 'loss': 0.91069808, 'lr': 0.00094736, 'params': 495254, 'time_iter': 0.13842, 'accuracy': 0.61868, 'f1': 0.6084, 'accuracy-SBM': 0.61869, 'auc': 0.90759}
2025-08-03 10:35:53,176 - INFO - val: {'epoch': 19, 'time_epoch': 4.20772, 'loss': 0.87623852, 'lr': 0, 'params': 495254, 'time_iter': 0.06679, 'accuracy': 0.63373, 'f1': 0.61023, 'accuracy-SBM': 0.63105, 'auc': 0.91365}
2025-08-03 10:35:57,412 - INFO - test: {'epoch': 19, 'time_epoch': 4.19604, 'loss': 0.89175378, 'lr': 0, 'params': 495254, 'time_iter': 0.0666, 'accuracy': 0.62796, 'f1': 0.60478, 'accuracy-SBM': 0.62754, 'auc': 0.91136}
2025-08-03 10:35:57,414 - INFO - > Epoch 19: took 95.3s (avg 96.7s) | Best so far: epoch 18	train_loss: 0.9145 train_accuracy-SBM: 0.6183	val_loss: 0.8666 val_accuracy-SBM: 0.6333	test_loss: 0.8829 test_accuracy-SBM: 0.6278
2025-08-03 10:37:24,115 - INFO - train: {'epoch': 20, 'time_epoch': 86.44817, 'eta': 6930.71554, 'eta_hours': 1.9252, 'loss': 0.9078402, 'lr': 0.00093974, 'params': 495254, 'time_iter': 0.13832, 'accuracy': 0.61972, 'f1': 0.60901, 'accuracy-SBM': 0.61974, 'auc': 0.90808}
2025-08-03 10:37:28,384 - INFO - val: {'epoch': 20, 'time_epoch': 4.21257, 'loss': 0.88701443, 'lr': 0, 'params': 495254, 'time_iter': 0.06687, 'accuracy': 0.63324, 'f1': 0.62302, 'accuracy-SBM': 0.63121, 'auc': 0.91164}
2025-08-03 10:37:32,625 - INFO - test: {'epoch': 20, 'time_epoch': 4.20714, 'loss': 0.89249965, 'lr': 0, 'params': 495254, 'time_iter': 0.06678, 'accuracy': 0.62606, 'f1': 0.61664, 'accuracy-SBM': 0.62549, 'auc': 0.91111}
2025-08-03 10:37:32,628 - INFO - > Epoch 20: took 95.2s (avg 96.6s) | Best so far: epoch 18	train_loss: 0.9145 train_accuracy-SBM: 0.6183	val_loss: 0.8666 val_accuracy-SBM: 0.6333	test_loss: 0.8829 test_accuracy-SBM: 0.6278
2025-08-03 10:38:58,991 - INFO - train: {'epoch': 21, 'time_epoch': 86.12395, 'eta': 6837.28874, 'eta_hours': 1.89925, 'loss': 0.90244956, 'lr': 0.00093163, 'params': 495254, 'time_iter': 0.1378, 'accuracy': 0.62196, 'f1': 0.61164, 'accuracy-SBM': 0.62198, 'auc': 0.909}
2025-08-03 10:39:03,268 - INFO - val: {'epoch': 21, 'time_epoch': 4.23047, 'loss': 0.87123574, 'lr': 0, 'params': 495254, 'time_iter': 0.06715, 'accuracy': 0.63597, 'f1': 0.61244, 'accuracy-SBM': 0.63388, 'auc': 0.91393}
2025-08-03 10:39:07,519 - INFO - test: {'epoch': 21, 'time_epoch': 4.21616, 'loss': 0.88434884, 'lr': 0, 'params': 495254, 'time_iter': 0.06692, 'accuracy': 0.63048, 'f1': 0.60825, 'accuracy-SBM': 0.62953, 'auc': 0.91195}
2025-08-03 10:39:07,521 - INFO - > Epoch 21: took 94.9s (avg 96.6s) | Best so far: epoch 21	train_loss: 0.9024 train_accuracy-SBM: 0.6220	val_loss: 0.8712 val_accuracy-SBM: 0.6339	test_loss: 0.8843 test_accuracy-SBM: 0.6295
2025-08-03 10:40:35,401 - INFO - train: {'epoch': 22, 'time_epoch': 87.63631, 'eta': 6749.56008, 'eta_hours': 1.87488, 'loss': 0.89954955, 'lr': 0.00092305, 'params': 495254, 'time_iter': 0.14022, 'accuracy': 0.62353, 'f1': 0.61284, 'accuracy-SBM': 0.62356, 'auc': 0.90953}
2025-08-03 10:40:39,708 - INFO - val: {'epoch': 22, 'time_epoch': 4.2136, 'loss': 0.87246817, 'lr': 0, 'params': 495254, 'time_iter': 0.06688, 'accuracy': 0.63323, 'f1': 0.60825, 'accuracy-SBM': 0.63051, 'auc': 0.91468}
2025-08-03 10:40:43,970 - INFO - test: {'epoch': 22, 'time_epoch': 4.21685, 'loss': 0.87698456, 'lr': 0, 'params': 495254, 'time_iter': 0.06693, 'accuracy': 0.62853, 'f1': 0.60449, 'accuracy-SBM': 0.62782, 'auc': 0.91382}
2025-08-03 10:40:43,972 - INFO - > Epoch 22: took 96.5s (avg 96.5s) | Best so far: epoch 21	train_loss: 0.9024 train_accuracy-SBM: 0.6220	val_loss: 0.8712 val_accuracy-SBM: 0.6339	test_loss: 0.8843 test_accuracy-SBM: 0.6295
2025-08-03 10:42:11,203 - INFO - train: {'epoch': 23, 'time_epoch': 86.88917, 'eta': 6659.4732, 'eta_hours': 1.84985, 'loss': 0.89616839, 'lr': 0.000914, 'params': 495254, 'time_iter': 0.13902, 'accuracy': 0.62245, 'f1': 0.61225, 'accuracy-SBM': 0.62248, 'auc': 0.91001}
2025-08-03 10:42:15,478 - INFO - val: {'epoch': 23, 'time_epoch': 4.22729, 'loss': 0.8656951, 'lr': 0, 'params': 495254, 'time_iter': 0.0671, 'accuracy': 0.63786, 'f1': 0.62074, 'accuracy-SBM': 0.63521, 'auc': 0.91488}
2025-08-03 10:42:19,720 - INFO - test: {'epoch': 23, 'time_epoch': 4.2023, 'loss': 0.8808794, 'lr': 0, 'params': 495254, 'time_iter': 0.0667, 'accuracy': 0.63217, 'f1': 0.61586, 'accuracy-SBM': 0.63146, 'auc': 0.91253}
2025-08-03 10:42:19,722 - INFO - > Epoch 23: took 95.7s (avg 96.5s) | Best so far: epoch 23	train_loss: 0.8962 train_accuracy-SBM: 0.6225	val_loss: 0.8657 val_accuracy-SBM: 0.6352	test_loss: 0.8809 test_accuracy-SBM: 0.6315
2025-08-03 10:43:46,662 - INFO - train: {'epoch': 24, 'time_epoch': 86.6827, 'eta': 6569.02272, 'eta_hours': 1.82473, 'loss': 0.89484862, 'lr': 0.00090451, 'params': 495254, 'time_iter': 0.13869, 'accuracy': 0.62414, 'f1': 0.61388, 'accuracy-SBM': 0.62416, 'auc': 0.91029}
2025-08-03 10:43:50,932 - INFO - val: {'epoch': 24, 'time_epoch': 4.20895, 'loss': 0.8683476, 'lr': 0, 'params': 495254, 'time_iter': 0.06681, 'accuracy': 0.63654, 'f1': 0.59976, 'accuracy-SBM': 0.63454, 'auc': 0.91491}
2025-08-03 10:43:55,178 - INFO - test: {'epoch': 24, 'time_epoch': 4.20911, 'loss': 0.87577814, 'lr': 0, 'params': 495254, 'time_iter': 0.06681, 'accuracy': 0.63205, 'f1': 0.59728, 'accuracy-SBM': 0.63099, 'auc': 0.91376}
2025-08-03 10:43:55,181 - INFO - > Epoch 24: took 95.5s (avg 96.5s) | Best so far: epoch 23	train_loss: 0.8962 train_accuracy-SBM: 0.6225	val_loss: 0.8657 val_accuracy-SBM: 0.6352	test_loss: 0.8809 test_accuracy-SBM: 0.6315
2025-08-03 10:45:22,720 - INFO - train: {'epoch': 25, 'time_epoch': 87.29291, 'eta': 6480.5988, 'eta_hours': 1.80017, 'loss': 0.89000081, 'lr': 0.00089457, 'params': 495254, 'time_iter': 0.13967, 'accuracy': 0.62556, 'f1': 0.61571, 'accuracy-SBM': 0.62558, 'auc': 0.91118}
2025-08-03 10:45:27,029 - INFO - val: {'epoch': 25, 'time_epoch': 4.25831, 'loss': 0.86253507, 'lr': 0, 'params': 495254, 'time_iter': 0.06759, 'accuracy': 0.63904, 'f1': 0.60936, 'accuracy-SBM': 0.63702, 'auc': 0.91606}
2025-08-03 10:45:31,308 - INFO - test: {'epoch': 25, 'time_epoch': 4.2406, 'loss': 0.8685947, 'lr': 0, 'params': 495254, 'time_iter': 0.06731, 'accuracy': 0.63244, 'f1': 0.60375, 'accuracy-SBM': 0.63151, 'auc': 0.91517}
2025-08-03 10:45:31,310 - INFO - > Epoch 25: took 96.1s (avg 96.5s) | Best so far: epoch 25	train_loss: 0.8900 train_accuracy-SBM: 0.6256	val_loss: 0.8625 val_accuracy-SBM: 0.6370	test_loss: 0.8686 test_accuracy-SBM: 0.6315
2025-08-03 10:46:57,960 - INFO - train: {'epoch': 26, 'time_epoch': 86.40866, 'eta': 6389.86792, 'eta_hours': 1.77496, 'loss': 0.8886473, 'lr': 0.0008842, 'params': 495254, 'time_iter': 0.13825, 'accuracy': 0.62518, 'f1': 0.61448, 'accuracy-SBM': 0.62521, 'auc': 0.91136}
2025-08-03 10:47:02,234 - INFO - val: {'epoch': 26, 'time_epoch': 4.22599, 'loss': 0.84631597, 'lr': 0, 'params': 495254, 'time_iter': 0.06708, 'accuracy': 0.64383, 'f1': 0.62819, 'accuracy-SBM': 0.64151, 'auc': 0.91837}
2025-08-03 10:47:06,502 - INFO - test: {'epoch': 26, 'time_epoch': 4.21721, 'loss': 0.86313242, 'lr': 0, 'params': 495254, 'time_iter': 0.06694, 'accuracy': 0.63593, 'f1': 0.62206, 'accuracy-SBM': 0.63518, 'auc': 0.91593}
2025-08-03 10:47:06,504 - INFO - > Epoch 26: took 95.2s (avg 96.4s) | Best so far: epoch 26	train_loss: 0.8886 train_accuracy-SBM: 0.6252	val_loss: 0.8463 val_accuracy-SBM: 0.6415	test_loss: 0.8631 test_accuracy-SBM: 0.6352
2025-08-03 10:48:32,934 - INFO - train: {'epoch': 27, 'time_epoch': 86.18903, 'eta': 6298.881, 'eta_hours': 1.74969, 'loss': 0.88483409, 'lr': 0.00087341, 'params': 495254, 'time_iter': 0.1379, 'accuracy': 0.62757, 'f1': 0.61755, 'accuracy-SBM': 0.6276, 'auc': 0.91204}
2025-08-03 10:48:37,221 - INFO - val: {'epoch': 27, 'time_epoch': 4.22971, 'loss': 0.84540372, 'lr': 0, 'params': 495254, 'time_iter': 0.06714, 'accuracy': 0.64361, 'f1': 0.62689, 'accuracy-SBM': 0.64143, 'auc': 0.91873}
2025-08-03 10:48:41,475 - INFO - test: {'epoch': 27, 'time_epoch': 4.21859, 'loss': 0.8538524, 'lr': 0, 'params': 495254, 'time_iter': 0.06696, 'accuracy': 0.6368, 'f1': 0.62172, 'accuracy-SBM': 0.63592, 'auc': 0.91748}
2025-08-03 10:48:41,477 - INFO - > Epoch 27: took 95.0s (avg 96.4s) | Best so far: epoch 26	train_loss: 0.8886 train_accuracy-SBM: 0.6252	val_loss: 0.8463 val_accuracy-SBM: 0.6415	test_loss: 0.8631 test_accuracy-SBM: 0.6352
2025-08-03 10:50:08,722 - INFO - train: {'epoch': 28, 'time_epoch': 87.00248, 'eta': 6210.21652, 'eta_hours': 1.72506, 'loss': 0.88250387, 'lr': 0.00086221, 'params': 495254, 'time_iter': 0.1392, 'accuracy': 0.62798, 'f1': 0.61793, 'accuracy-SBM': 0.62801, 'auc': 0.91245}
2025-08-03 10:50:13,028 - INFO - val: {'epoch': 28, 'time_epoch': 4.25935, 'loss': 0.87389509, 'lr': 0, 'params': 495254, 'time_iter': 0.06761, 'accuracy': 0.6327, 'f1': 0.60935, 'accuracy-SBM': 0.62986, 'auc': 0.91382}
2025-08-03 10:50:17,288 - INFO - test: {'epoch': 28, 'time_epoch': 4.22042, 'loss': 0.8716585, 'lr': 0, 'params': 495254, 'time_iter': 0.06699, 'accuracy': 0.63253, 'f1': 0.61035, 'accuracy-SBM': 0.63188, 'auc': 0.91437}
2025-08-03 10:50:17,290 - INFO - > Epoch 28: took 95.8s (avg 96.3s) | Best so far: epoch 26	train_loss: 0.8886 train_accuracy-SBM: 0.6252	val_loss: 0.8463 val_accuracy-SBM: 0.6415	test_loss: 0.8631 test_accuracy-SBM: 0.6352
2025-08-03 10:51:44,840 - INFO - train: {'epoch': 29, 'time_epoch': 87.13786, 'eta': 6121.97874, 'eta_hours': 1.70055, 'loss': 0.87877817, 'lr': 0.00085062, 'params': 495254, 'time_iter': 0.13942, 'accuracy': 0.6281, 'f1': 0.61798, 'accuracy-SBM': 0.62813, 'auc': 0.91307}
2025-08-03 10:51:49,157 - INFO - val: {'epoch': 29, 'time_epoch': 4.26989, 'loss': 0.84742336, 'lr': 0, 'params': 495254, 'time_iter': 0.06778, 'accuracy': 0.64417, 'f1': 0.62771, 'accuracy-SBM': 0.64207, 'auc': 0.91827}
2025-08-03 10:51:53,448 - INFO - test: {'epoch': 29, 'time_epoch': 4.25066, 'loss': 0.85935736, 'lr': 0, 'params': 495254, 'time_iter': 0.06747, 'accuracy': 0.63835, 'f1': 0.6235, 'accuracy-SBM': 0.63751, 'auc': 0.91635}
2025-08-03 10:51:53,450 - INFO - > Epoch 29: took 96.2s (avg 96.3s) | Best so far: epoch 29	train_loss: 0.8788 train_accuracy-SBM: 0.6281	val_loss: 0.8474 val_accuracy-SBM: 0.6421	test_loss: 0.8594 test_accuracy-SBM: 0.6375
2025-08-03 10:53:21,144 - INFO - train: {'epoch': 30, 'time_epoch': 87.33149, 'eta': 6034.24289, 'eta_hours': 1.67618, 'loss': 0.87303471, 'lr': 0.00083864, 'params': 495254, 'time_iter': 0.13973, 'accuracy': 0.63007, 'f1': 0.62061, 'accuracy-SBM': 0.63009, 'auc': 0.91401}
2025-08-03 10:53:25,429 - INFO - val: {'epoch': 30, 'time_epoch': 4.23387, 'loss': 0.84651979, 'lr': 0, 'params': 495254, 'time_iter': 0.0672, 'accuracy': 0.64266, 'f1': 0.6089, 'accuracy-SBM': 0.64076, 'auc': 0.91845}
2025-08-03 10:53:29,709 - INFO - test: {'epoch': 30, 'time_epoch': 4.24004, 'loss': 0.85652917, 'lr': 0, 'params': 495254, 'time_iter': 0.0673, 'accuracy': 0.63648, 'f1': 0.60321, 'accuracy-SBM': 0.6355, 'auc': 0.91706}
2025-08-03 10:53:29,711 - INFO - > Epoch 30: took 96.3s (avg 96.3s) | Best so far: epoch 29	train_loss: 0.8788 train_accuracy-SBM: 0.6281	val_loss: 0.8474 val_accuracy-SBM: 0.6421	test_loss: 0.8594 test_accuracy-SBM: 0.6375
2025-08-03 10:54:57,184 - INFO - train: {'epoch': 31, 'time_epoch': 87.22967, 'eta': 5946.31594, 'eta_hours': 1.65175, 'loss': 0.8716079, 'lr': 0.00082629, 'params': 495254, 'time_iter': 0.13957, 'accuracy': 0.6315, 'f1': 0.62184, 'accuracy-SBM': 0.63152, 'auc': 0.91438}
2025-08-03 10:55:01,500 - INFO - val: {'epoch': 31, 'time_epoch': 4.26416, 'loss': 0.84395592, 'lr': 0, 'params': 495254, 'time_iter': 0.06769, 'accuracy': 0.64427, 'f1': 0.62574, 'accuracy-SBM': 0.64162, 'auc': 0.91882}
2025-08-03 10:55:05,800 - INFO - test: {'epoch': 31, 'time_epoch': 4.2643, 'loss': 0.85286075, 'lr': 0, 'params': 495254, 'time_iter': 0.06769, 'accuracy': 0.63832, 'f1': 0.6208, 'accuracy-SBM': 0.63774, 'auc': 0.91747}
2025-08-03 10:55:05,802 - INFO - > Epoch 31: took 96.1s (avg 96.3s) | Best so far: epoch 29	train_loss: 0.8788 train_accuracy-SBM: 0.6281	val_loss: 0.8474 val_accuracy-SBM: 0.6421	test_loss: 0.8594 test_accuracy-SBM: 0.6375
2025-08-03 10:56:32,655 - INFO - train: {'epoch': 32, 'time_epoch': 86.60592, 'eta': 5857.16485, 'eta_hours': 1.62699, 'loss': 0.8678344, 'lr': 0.00081359, 'params': 495254, 'time_iter': 0.13857, 'accuracy': 0.63211, 'f1': 0.62259, 'accuracy-SBM': 0.63214, 'auc': 0.91493}
2025-08-03 10:56:36,971 - INFO - val: {'epoch': 32, 'time_epoch': 4.26155, 'loss': 0.8465101, 'lr': 0, 'params': 495254, 'time_iter': 0.06764, 'accuracy': 0.64417, 'f1': 0.63115, 'accuracy-SBM': 0.64168, 'auc': 0.91841}
2025-08-03 10:56:41,246 - INFO - test: {'epoch': 32, 'time_epoch': 4.24174, 'loss': 0.85191194, 'lr': 0, 'params': 495254, 'time_iter': 0.06733, 'accuracy': 0.63974, 'f1': 0.62808, 'accuracy-SBM': 0.63909, 'auc': 0.91763}
2025-08-03 10:56:41,248 - INFO - > Epoch 32: took 95.4s (avg 96.3s) | Best so far: epoch 29	train_loss: 0.8788 train_accuracy-SBM: 0.6281	val_loss: 0.8474 val_accuracy-SBM: 0.6421	test_loss: 0.8594 test_accuracy-SBM: 0.6375
2025-08-03 10:58:08,558 - INFO - train: {'epoch': 33, 'time_epoch': 87.07013, 'eta': 5769.0646, 'eta_hours': 1.60252, 'loss': 0.86315052, 'lr': 0.00080054, 'params': 495254, 'time_iter': 0.13931, 'accuracy': 0.63383, 'f1': 0.62442, 'accuracy-SBM': 0.63384, 'auc': 0.91573}
2025-08-03 10:58:12,825 - INFO - val: {'epoch': 33, 'time_epoch': 4.21937, 'loss': 0.83386157, 'lr': 0, 'params': 495254, 'time_iter': 0.06697, 'accuracy': 0.64627, 'f1': 0.63172, 'accuracy-SBM': 0.64403, 'auc': 0.92037}
2025-08-03 10:58:17,060 - INFO - test: {'epoch': 33, 'time_epoch': 4.20126, 'loss': 0.84308463, 'lr': 0, 'params': 495254, 'time_iter': 0.06669, 'accuracy': 0.64217, 'f1': 0.62892, 'accuracy-SBM': 0.6413, 'auc': 0.91902}
2025-08-03 10:58:17,063 - INFO - > Epoch 33: took 95.8s (avg 96.3s) | Best so far: epoch 33	train_loss: 0.8632 train_accuracy-SBM: 0.6338	val_loss: 0.8339 val_accuracy-SBM: 0.6440	test_loss: 0.8431 test_accuracy-SBM: 0.6413
2025-08-03 10:59:43,809 - INFO - train: {'epoch': 34, 'time_epoch': 86.49918, 'eta': 5679.96289, 'eta_hours': 1.57777, 'loss': 0.8606052, 'lr': 0.00078716, 'params': 495254, 'time_iter': 0.1384, 'accuracy': 0.6343, 'f1': 0.625, 'accuracy-SBM': 0.63431, 'auc': 0.9161}
2025-08-03 10:59:48,059 - INFO - val: {'epoch': 34, 'time_epoch': 4.20365, 'loss': 0.84280767, 'lr': 0, 'params': 495254, 'time_iter': 0.06672, 'accuracy': 0.64479, 'f1': 0.6307, 'accuracy-SBM': 0.64222, 'auc': 0.91899}
2025-08-03 10:59:52,302 - INFO - test: {'epoch': 34, 'time_epoch': 4.20976, 'loss': 0.84867434, 'lr': 0, 'params': 495254, 'time_iter': 0.06682, 'accuracy': 0.64224, 'f1': 0.62922, 'accuracy-SBM': 0.64162, 'auc': 0.91828}
2025-08-03 10:59:52,305 - INFO - > Epoch 34: took 95.2s (avg 96.3s) | Best so far: epoch 33	train_loss: 0.8632 train_accuracy-SBM: 0.6338	val_loss: 0.8339 val_accuracy-SBM: 0.6440	test_loss: 0.8431 test_accuracy-SBM: 0.6413
2025-08-03 11:01:19,430 - INFO - train: {'epoch': 35, 'time_epoch': 86.88133, 'eta': 5591.68512, 'eta_hours': 1.55325, 'loss': 0.85748882, 'lr': 0.00077347, 'params': 495254, 'time_iter': 0.13901, 'accuracy': 0.63594, 'f1': 0.62666, 'accuracy-SBM': 0.63596, 'auc': 0.91666}
2025-08-03 11:01:23,738 - INFO - val: {'epoch': 35, 'time_epoch': 4.26128, 'loss': 0.83368491, 'lr': 0, 'params': 495254, 'time_iter': 0.06764, 'accuracy': 0.64541, 'f1': 0.63339, 'accuracy-SBM': 0.64281, 'auc': 0.92075}
2025-08-03 11:01:28,026 - INFO - test: {'epoch': 35, 'time_epoch': 4.25284, 'loss': 0.8432348, 'lr': 0, 'params': 495254, 'time_iter': 0.06751, 'accuracy': 0.642, 'f1': 0.631, 'accuracy-SBM': 0.64139, 'auc': 0.91931}
2025-08-03 11:01:28,028 - INFO - > Epoch 35: took 95.7s (avg 96.2s) | Best so far: epoch 33	train_loss: 0.8632 train_accuracy-SBM: 0.6338	val_loss: 0.8339 val_accuracy-SBM: 0.6440	test_loss: 0.8431 test_accuracy-SBM: 0.6413
2025-08-03 11:02:55,365 - INFO - train: {'epoch': 36, 'time_epoch': 86.98978, 'eta': 5503.66751, 'eta_hours': 1.5288, 'loss': 0.85567933, 'lr': 0.00075948, 'params': 495254, 'time_iter': 0.13918, 'accuracy': 0.63632, 'f1': 0.62678, 'accuracy-SBM': 0.63634, 'auc': 0.917}
2025-08-03 11:02:59,637 - INFO - val: {'epoch': 36, 'time_epoch': 4.22657, 'loss': 0.83248855, 'lr': 0, 'params': 495254, 'time_iter': 0.06709, 'accuracy': 0.64617, 'f1': 0.62097, 'accuracy-SBM': 0.64331, 'auc': 0.92079}
2025-08-03 11:03:03,891 - INFO - test: {'epoch': 36, 'time_epoch': 4.21949, 'loss': 0.85388634, 'lr': 0, 'params': 495254, 'time_iter': 0.06698, 'accuracy': 0.63796, 'f1': 0.61282, 'accuracy-SBM': 0.63737, 'auc': 0.91761}
2025-08-03 11:03:03,893 - INFO - > Epoch 36: took 95.9s (avg 96.2s) | Best so far: epoch 33	train_loss: 0.8632 train_accuracy-SBM: 0.6338	val_loss: 0.8339 val_accuracy-SBM: 0.6440	test_loss: 0.8431 test_accuracy-SBM: 0.6413
2025-08-03 11:04:30,673 - INFO - train: {'epoch': 37, 'time_epoch': 86.52171, 'eta': 5414.94029, 'eta_hours': 1.50415, 'loss': 0.85220953, 'lr': 0.00074521, 'params': 495254, 'time_iter': 0.13843, 'accuracy': 0.63718, 'f1': 0.62792, 'accuracy-SBM': 0.63719, 'auc': 0.91757}
2025-08-03 11:04:34,942 - INFO - val: {'epoch': 37, 'time_epoch': 4.21372, 'loss': 0.84021576, 'lr': 0, 'params': 495254, 'time_iter': 0.06688, 'accuracy': 0.64193, 'f1': 0.62944, 'accuracy-SBM': 0.63931, 'auc': 0.91969}
2025-08-03 11:04:39,185 - INFO - test: {'epoch': 37, 'time_epoch': 4.20034, 'loss': 0.85254447, 'lr': 0, 'params': 495254, 'time_iter': 0.06667, 'accuracy': 0.63635, 'f1': 0.62475, 'accuracy-SBM': 0.63571, 'auc': 0.91773}
2025-08-03 11:04:39,187 - INFO - > Epoch 37: took 95.3s (avg 96.2s) | Best so far: epoch 33	train_loss: 0.8632 train_accuracy-SBM: 0.6338	val_loss: 0.8339 val_accuracy-SBM: 0.6440	test_loss: 0.8431 test_accuracy-SBM: 0.6413
2025-08-03 11:06:05,683 - INFO - train: {'epoch': 38, 'time_epoch': 86.25526, 'eta': 5325.90942, 'eta_hours': 1.47942, 'loss': 0.85024924, 'lr': 0.00073067, 'params': 495254, 'time_iter': 0.13801, 'accuracy': 0.63818, 'f1': 0.62923, 'accuracy-SBM': 0.6382, 'auc': 0.91795}
2025-08-03 11:06:09,895 - INFO - val: {'epoch': 38, 'time_epoch': 4.16536, 'loss': 0.83540234, 'lr': 0, 'params': 495254, 'time_iter': 0.06612, 'accuracy': 0.64832, 'f1': 0.63588, 'accuracy-SBM': 0.6459, 'auc': 0.92036}
2025-08-03 11:06:14,066 - INFO - test: {'epoch': 38, 'time_epoch': 4.13707, 'loss': 0.8404216, 'lr': 0, 'params': 495254, 'time_iter': 0.06567, 'accuracy': 0.64164, 'f1': 0.63038, 'accuracy-SBM': 0.64081, 'auc': 0.91974}
2025-08-03 11:06:14,068 - INFO - > Epoch 38: took 94.9s (avg 96.2s) | Best so far: epoch 38	train_loss: 0.8502 train_accuracy-SBM: 0.6382	val_loss: 0.8354 val_accuracy-SBM: 0.6459	test_loss: 0.8404 test_accuracy-SBM: 0.6408
2025-08-03 11:07:40,418 - INFO - train: {'epoch': 39, 'time_epoch': 86.0123, 'eta': 5236.6529, 'eta_hours': 1.45463, 'loss': 0.84751395, 'lr': 0.00071588, 'params': 495254, 'time_iter': 0.13762, 'accuracy': 0.63945, 'f1': 0.63062, 'accuracy-SBM': 0.63947, 'auc': 0.91841}
2025-08-03 11:07:44,575 - INFO - val: {'epoch': 39, 'time_epoch': 4.10756, 'loss': 0.82460567, 'lr': 0, 'params': 495254, 'time_iter': 0.0652, 'accuracy': 0.65218, 'f1': 0.62067, 'accuracy-SBM': 0.64997, 'auc': 0.92232}
2025-08-03 11:07:48,647 - INFO - test: {'epoch': 39, 'time_epoch': 4.03923, 'loss': 0.84031587, 'lr': 0, 'params': 495254, 'time_iter': 0.06411, 'accuracy': 0.64249, 'f1': 0.61186, 'accuracy-SBM': 0.64159, 'auc': 0.9202}
2025-08-03 11:07:48,650 - INFO - > Epoch 39: took 94.6s (avg 96.1s) | Best so far: epoch 39	train_loss: 0.8475 train_accuracy-SBM: 0.6395	val_loss: 0.8246 val_accuracy-SBM: 0.6500	test_loss: 0.8403 test_accuracy-SBM: 0.6416
2025-08-03 11:09:14,272 - INFO - train: {'epoch': 40, 'time_epoch': 85.27519, 'eta': 5146.49391, 'eta_hours': 1.42958, 'loss': 0.84270835, 'lr': 0.00070085, 'params': 495254, 'time_iter': 0.13644, 'accuracy': 0.64122, 'f1': 0.6318, 'accuracy-SBM': 0.64123, 'auc': 0.91916}
2025-08-03 11:09:18,528 - INFO - val: {'epoch': 40, 'time_epoch': 4.21052, 'loss': 0.83352884, 'lr': 0, 'params': 495254, 'time_iter': 0.06683, 'accuracy': 0.6479, 'f1': 0.63035, 'accuracy-SBM': 0.64566, 'auc': 0.92069}
2025-08-03 11:09:22,768 - INFO - test: {'epoch': 40, 'time_epoch': 4.20571, 'loss': 0.83765108, 'lr': 0, 'params': 495254, 'time_iter': 0.06676, 'accuracy': 0.64368, 'f1': 0.62835, 'accuracy-SBM': 0.64282, 'auc': 0.92025}
2025-08-03 11:09:22,770 - INFO - > Epoch 40: took 94.1s (avg 96.1s) | Best so far: epoch 39	train_loss: 0.8475 train_accuracy-SBM: 0.6395	val_loss: 0.8246 val_accuracy-SBM: 0.6500	test_loss: 0.8403 test_accuracy-SBM: 0.6416
2025-08-03 11:10:50,458 - INFO - train: {'epoch': 41, 'time_epoch': 87.43653, 'eta': 5059.55219, 'eta_hours': 1.40543, 'loss': 0.84200802, 'lr': 0.0006856, 'params': 495254, 'time_iter': 0.1399, 'accuracy': 0.64123, 'f1': 0.63233, 'accuracy-SBM': 0.64125, 'auc': 0.91934}
2025-08-03 11:10:54,758 - INFO - val: {'epoch': 41, 'time_epoch': 4.25032, 'loss': 0.82750009, 'lr': 0, 'params': 495254, 'time_iter': 0.06747, 'accuracy': 0.64945, 'f1': 0.63584, 'accuracy-SBM': 0.64706, 'auc': 0.92166}
2025-08-03 11:10:59,037 - INFO - test: {'epoch': 41, 'time_epoch': 4.2435, 'loss': 0.84115443, 'lr': 0, 'params': 495254, 'time_iter': 0.06736, 'accuracy': 0.64045, 'f1': 0.62883, 'accuracy-SBM': 0.63976, 'auc': 0.91991}
2025-08-03 11:10:59,039 - INFO - > Epoch 41: took 96.3s (avg 96.1s) | Best so far: epoch 39	train_loss: 0.8475 train_accuracy-SBM: 0.6395	val_loss: 0.8246 val_accuracy-SBM: 0.6500	test_loss: 0.8403 test_accuracy-SBM: 0.6416
2025-08-03 11:12:25,754 - INFO - train: {'epoch': 42, 'time_epoch': 86.37382, 'eta': 4971.17875, 'eta_hours': 1.38088, 'loss': 0.83996446, 'lr': 0.00067015, 'params': 495254, 'time_iter': 0.1382, 'accuracy': 0.64172, 'f1': 0.63303, 'accuracy-SBM': 0.64174, 'auc': 0.91955}
2025-08-03 11:12:29,991 - INFO - val: {'epoch': 42, 'time_epoch': 4.19059, 'loss': 0.83022399, 'lr': 0, 'params': 495254, 'time_iter': 0.06652, 'accuracy': 0.64722, 'f1': 0.6321, 'accuracy-SBM': 0.64472, 'auc': 0.9217}
2025-08-03 11:12:34,180 - INFO - test: {'epoch': 42, 'time_epoch': 4.15482, 'loss': 0.83814287, 'lr': 0, 'params': 495254, 'time_iter': 0.06595, 'accuracy': 0.6437, 'f1': 0.62995, 'accuracy-SBM': 0.64314, 'auc': 0.92058}
2025-08-03 11:12:34,182 - INFO - > Epoch 42: took 95.1s (avg 96.1s) | Best so far: epoch 39	train_loss: 0.8475 train_accuracy-SBM: 0.6395	val_loss: 0.8246 val_accuracy-SBM: 0.6500	test_loss: 0.8403 test_accuracy-SBM: 0.6416
2025-08-03 11:14:01,162 - INFO - train: {'epoch': 43, 'time_epoch': 86.62643, 'eta': 4883.2177, 'eta_hours': 1.35645, 'loss': 0.83525835, 'lr': 0.00065451, 'params': 495254, 'time_iter': 0.1386, 'accuracy': 0.64426, 'f1': 0.63502, 'accuracy-SBM': 0.64427, 'auc': 0.9204}
2025-08-03 11:14:05,451 - INFO - val: {'epoch': 43, 'time_epoch': 4.22475, 'loss': 0.82175096, 'lr': 0, 'params': 495254, 'time_iter': 0.06706, 'accuracy': 0.65031, 'f1': 0.61695, 'accuracy-SBM': 0.64839, 'auc': 0.92251}
2025-08-03 11:14:09,709 - INFO - test: {'epoch': 43, 'time_epoch': 4.22033, 'loss': 0.83338883, 'lr': 0, 'params': 495254, 'time_iter': 0.06699, 'accuracy': 0.64473, 'f1': 0.6136, 'accuracy-SBM': 0.64379, 'auc': 0.92076}
2025-08-03 11:14:09,711 - INFO - > Epoch 43: took 95.5s (avg 96.1s) | Best so far: epoch 39	train_loss: 0.8475 train_accuracy-SBM: 0.6395	val_loss: 0.8246 val_accuracy-SBM: 0.6500	test_loss: 0.8403 test_accuracy-SBM: 0.6416
2025-08-03 11:15:37,104 - INFO - train: {'epoch': 44, 'time_epoch': 87.03127, 'eta': 4795.81076, 'eta_hours': 1.33217, 'loss': 0.83397839, 'lr': 0.0006387, 'params': 495254, 'time_iter': 0.13925, 'accuracy': 0.64429, 'f1': 0.63554, 'accuracy-SBM': 0.64431, 'auc': 0.92054}
2025-08-03 11:15:41,380 - INFO - val: {'epoch': 44, 'time_epoch': 4.22924, 'loss': 0.81990173, 'lr': 0, 'params': 495254, 'time_iter': 0.06713, 'accuracy': 0.64978, 'f1': 0.63856, 'accuracy-SBM': 0.64745, 'auc': 0.92261}
2025-08-03 11:15:45,634 - INFO - test: {'epoch': 44, 'time_epoch': 4.21919, 'loss': 0.83256522, 'lr': 0, 'params': 495254, 'time_iter': 0.06697, 'accuracy': 0.64361, 'f1': 0.63407, 'accuracy-SBM': 0.64289, 'auc': 0.92097}
2025-08-03 11:15:45,636 - INFO - > Epoch 44: took 95.9s (avg 96.0s) | Best so far: epoch 39	train_loss: 0.8475 train_accuracy-SBM: 0.6395	val_loss: 0.8246 val_accuracy-SBM: 0.6500	test_loss: 0.8403 test_accuracy-SBM: 0.6416
2025-08-03 11:17:12,667 - INFO - train: {'epoch': 45, 'time_epoch': 86.69043, 'eta': 4708.02005, 'eta_hours': 1.30778, 'loss': 0.83001029, 'lr': 0.00062274, 'params': 495254, 'time_iter': 0.1387, 'accuracy': 0.64474, 'f1': 0.63613, 'accuracy-SBM': 0.64476, 'auc': 0.92115}
2025-08-03 11:17:17,382 - INFO - val: {'epoch': 45, 'time_epoch': 4.24029, 'loss': 0.82396742, 'lr': 0, 'params': 495254, 'time_iter': 0.06731, 'accuracy': 0.65005, 'f1': 0.61172, 'accuracy-SBM': 0.64705, 'auc': 0.92227}
2025-08-03 11:17:21,651 - INFO - test: {'epoch': 45, 'time_epoch': 4.21536, 'loss': 0.83107056, 'lr': 0, 'params': 495254, 'time_iter': 0.06691, 'accuracy': 0.64555, 'f1': 0.60756, 'accuracy-SBM': 0.645, 'auc': 0.92117}
2025-08-03 11:17:21,653 - INFO - > Epoch 45: took 96.0s (avg 96.0s) | Best so far: epoch 39	train_loss: 0.8475 train_accuracy-SBM: 0.6395	val_loss: 0.8246 val_accuracy-SBM: 0.6500	test_loss: 0.8403 test_accuracy-SBM: 0.6416
2025-08-03 11:18:48,888 - INFO - train: {'epoch': 46, 'time_epoch': 86.98851, 'eta': 4620.61229, 'eta_hours': 1.2835, 'loss': 0.82896362, 'lr': 0.00060665, 'params': 495254, 'time_iter': 0.13918, 'accuracy': 0.64629, 'f1': 0.63737, 'accuracy-SBM': 0.6463, 'auc': 0.92141}
2025-08-03 11:18:53,472 - INFO - val: {'epoch': 46, 'time_epoch': 4.25307, 'loss': 0.82561804, 'lr': 0, 'params': 495254, 'time_iter': 0.06751, 'accuracy': 0.64891, 'f1': 0.62883, 'accuracy-SBM': 0.64703, 'auc': 0.92206}
2025-08-03 11:18:57,759 - INFO - test: {'epoch': 46, 'time_epoch': 4.24082, 'loss': 0.82925924, 'lr': 0, 'params': 495254, 'time_iter': 0.06731, 'accuracy': 0.64637, 'f1': 0.62704, 'accuracy-SBM': 0.64553, 'auc': 0.92144}
2025-08-03 11:18:57,761 - INFO - > Epoch 46: took 96.1s (avg 96.0s) | Best so far: epoch 39	train_loss: 0.8475 train_accuracy-SBM: 0.6395	val_loss: 0.8246 val_accuracy-SBM: 0.6500	test_loss: 0.8403 test_accuracy-SBM: 0.6416
2025-08-03 11:20:24,812 - INFO - train: {'epoch': 47, 'time_epoch': 86.68757, 'eta': 4532.89598, 'eta_hours': 1.25914, 'loss': 0.82516443, 'lr': 0.00059044, 'params': 495254, 'time_iter': 0.1387, 'accuracy': 0.64709, 'f1': 0.63841, 'accuracy-SBM': 0.64711, 'auc': 0.92196}
2025-08-03 11:20:29,068 - INFO - val: {'epoch': 47, 'time_epoch': 4.20519, 'loss': 0.81749301, 'lr': 0, 'params': 495254, 'time_iter': 0.06675, 'accuracy': 0.65188, 'f1': 0.6371, 'accuracy-SBM': 0.64948, 'auc': 0.92314}
2025-08-03 11:20:33,281 - INFO - test: {'epoch': 47, 'time_epoch': 4.16824, 'loss': 0.82563144, 'lr': 0, 'params': 495254, 'time_iter': 0.06616, 'accuracy': 0.64885, 'f1': 0.63591, 'accuracy-SBM': 0.64808, 'auc': 0.92204}
2025-08-03 11:20:33,283 - INFO - > Epoch 47: took 95.5s (avg 96.0s) | Best so far: epoch 39	train_loss: 0.8475 train_accuracy-SBM: 0.6395	val_loss: 0.8246 val_accuracy-SBM: 0.6500	test_loss: 0.8403 test_accuracy-SBM: 0.6416
2025-08-03 11:22:00,254 - INFO - train: {'epoch': 48, 'time_epoch': 86.72954, 'eta': 4445.26535, 'eta_hours': 1.2348, 'loss': 0.82146663, 'lr': 0.00057413, 'params': 495254, 'time_iter': 0.13877, 'accuracy': 0.64909, 'f1': 0.64076, 'accuracy-SBM': 0.6491, 'auc': 0.92264}
2025-08-03 11:22:04,516 - INFO - val: {'epoch': 48, 'time_epoch': 4.21475, 'loss': 0.81570906, 'lr': 0, 'params': 495254, 'time_iter': 0.0669, 'accuracy': 0.65154, 'f1': 0.64029, 'accuracy-SBM': 0.64918, 'auc': 0.92344}
2025-08-03 11:22:08,761 - INFO - test: {'epoch': 48, 'time_epoch': 4.21031, 'loss': 0.82896634, 'lr': 0, 'params': 495254, 'time_iter': 0.06683, 'accuracy': 0.64693, 'f1': 0.63678, 'accuracy-SBM': 0.64615, 'auc': 0.92145}
2025-08-03 11:22:08,762 - INFO - > Epoch 48: took 95.5s (avg 96.0s) | Best so far: epoch 39	train_loss: 0.8475 train_accuracy-SBM: 0.6395	val_loss: 0.8246 val_accuracy-SBM: 0.6500	test_loss: 0.8403 test_accuracy-SBM: 0.6416
2025-08-03 11:23:35,514 - INFO - train: {'epoch': 49, 'time_epoch': 86.4865, 'eta': 4357.42772, 'eta_hours': 1.2104, 'loss': 0.81829416, 'lr': 0.00055774, 'params': 495254, 'time_iter': 0.13838, 'accuracy': 0.65055, 'f1': 0.64207, 'accuracy-SBM': 0.65057, 'auc': 0.92317}
2025-08-03 11:23:39,762 - INFO - val: {'epoch': 49, 'time_epoch': 4.20111, 'loss': 0.82170817, 'lr': 0, 'params': 495254, 'time_iter': 0.06668, 'accuracy': 0.65696, 'f1': 0.6405, 'accuracy-SBM': 0.65442, 'auc': 0.92289}
2025-08-03 11:23:43,979 - INFO - test: {'epoch': 49, 'time_epoch': 4.18301, 'loss': 0.83283509, 'lr': 0, 'params': 495254, 'time_iter': 0.0664, 'accuracy': 0.64988, 'f1': 0.63407, 'accuracy-SBM': 0.6493, 'auc': 0.92147}
2025-08-03 11:23:43,981 - INFO - > Epoch 49: took 95.2s (avg 96.0s) | Best so far: epoch 49	train_loss: 0.8183 train_accuracy-SBM: 0.6506	val_loss: 0.8217 val_accuracy-SBM: 0.6544	test_loss: 0.8328 test_accuracy-SBM: 0.6493
2025-08-03 11:25:11,259 - INFO - train: {'epoch': 50, 'time_epoch': 87.03493, 'eta': 4270.17, 'eta_hours': 1.18616, 'loss': 0.81659382, 'lr': 0.00054129, 'params': 495254, 'time_iter': 0.13926, 'accuracy': 0.652, 'f1': 0.64398, 'accuracy-SBM': 0.65202, 'auc': 0.92339}
2025-08-03 11:25:15,575 - INFO - val: {'epoch': 50, 'time_epoch': 4.26872, 'loss': 0.82706751, 'lr': 0, 'params': 495254, 'time_iter': 0.06776, 'accuracy': 0.65303, 'f1': 0.64264, 'accuracy-SBM': 0.65071, 'auc': 0.92215}
2025-08-03 11:25:19,850 - INFO - test: {'epoch': 50, 'time_epoch': 4.24009, 'loss': 0.83190512, 'lr': 0, 'params': 495254, 'time_iter': 0.0673, 'accuracy': 0.64844, 'f1': 0.63945, 'accuracy-SBM': 0.6477, 'auc': 0.92153}
2025-08-03 11:25:19,852 - INFO - > Epoch 50: took 95.9s (avg 96.0s) | Best so far: epoch 49	train_loss: 0.8183 train_accuracy-SBM: 0.6506	val_loss: 0.8217 val_accuracy-SBM: 0.6544	test_loss: 0.8328 test_accuracy-SBM: 0.6493
2025-08-03 11:26:47,368 - INFO - train: {'epoch': 51, 'time_epoch': 87.27516, 'eta': 4183.1426, 'eta_hours': 1.16198, 'loss': 0.81453238, 'lr': 0.00052479, 'params': 495254, 'time_iter': 0.13964, 'accuracy': 0.65288, 'f1': 0.64487, 'accuracy-SBM': 0.6529, 'auc': 0.9237}
2025-08-03 11:26:51,657 - INFO - val: {'epoch': 51, 'time_epoch': 4.2355, 'loss': 0.81575129, 'lr': 0, 'params': 495254, 'time_iter': 0.06723, 'accuracy': 0.65806, 'f1': 0.64744, 'accuracy-SBM': 0.6557, 'auc': 0.92375}
2025-08-03 11:26:55,931 - INFO - test: {'epoch': 51, 'time_epoch': 4.23823, 'loss': 0.8181028, 'lr': 0, 'params': 495254, 'time_iter': 0.06727, 'accuracy': 0.6558, 'f1': 0.64568, 'accuracy-SBM': 0.65511, 'auc': 0.92333}
2025-08-03 11:26:55,933 - INFO - > Epoch 51: took 96.1s (avg 96.0s) | Best so far: epoch 51	train_loss: 0.8145 train_accuracy-SBM: 0.6529	val_loss: 0.8158 val_accuracy-SBM: 0.6557	test_loss: 0.8181 test_accuracy-SBM: 0.6551
2025-08-03 11:28:23,075 - INFO - train: {'epoch': 52, 'time_epoch': 86.89012, 'eta': 4095.7644, 'eta_hours': 1.13771, 'loss': 0.81039232, 'lr': 0.00050827, 'params': 495254, 'time_iter': 0.13902, 'accuracy': 0.65742, 'f1': 0.65, 'accuracy-SBM': 0.65743, 'auc': 0.92464}
2025-08-03 11:28:27,388 - INFO - val: {'epoch': 52, 'time_epoch': 4.26379, 'loss': 0.80356616, 'lr': 0, 'params': 495254, 'time_iter': 0.06768, 'accuracy': 0.66548, 'f1': 0.65539, 'accuracy-SBM': 0.66334, 'auc': 0.92592}
2025-08-03 11:28:31,667 - INFO - test: {'epoch': 52, 'time_epoch': 4.24426, 'loss': 0.81883431, 'lr': 0, 'params': 495254, 'time_iter': 0.06737, 'accuracy': 0.6574, 'f1': 0.64831, 'accuracy-SBM': 0.65681, 'auc': 0.92361}
2025-08-03 11:28:31,669 - INFO - > Epoch 52: took 95.7s (avg 96.0s) | Best so far: epoch 52	train_loss: 0.8104 train_accuracy-SBM: 0.6574	val_loss: 0.8036 val_accuracy-SBM: 0.6633	test_loss: 0.8188 test_accuracy-SBM: 0.6568
2025-08-03 11:29:59,035 - INFO - train: {'epoch': 53, 'time_epoch': 87.12083, 'eta': 4008.6008, 'eta_hours': 1.1135, 'loss': 0.80634319, 'lr': 0.00049173, 'params': 495254, 'time_iter': 0.13939, 'accuracy': 0.67161, 'f1': 0.66521, 'accuracy-SBM': 0.67162, 'auc': 0.92693}
2025-08-03 11:30:03,407 - INFO - val: {'epoch': 53, 'time_epoch': 4.29604, 'loss': 0.77282648, 'lr': 0, 'params': 495254, 'time_iter': 0.06819, 'accuracy': 0.73138, 'f1': 0.7281, 'accuracy-SBM': 0.73048, 'auc': 0.93684}
2025-08-03 11:30:07,742 - INFO - test: {'epoch': 53, 'time_epoch': 4.29946, 'loss': 0.79096509, 'lr': 0, 'params': 495254, 'time_iter': 0.06825, 'accuracy': 0.72627, 'f1': 0.72364, 'accuracy-SBM': 0.72592, 'auc': 0.93452}
2025-08-03 11:30:07,743 - INFO - > Epoch 53: took 96.1s (avg 96.0s) | Best so far: epoch 53	train_loss: 0.8063 train_accuracy-SBM: 0.6716	val_loss: 0.7728 val_accuracy-SBM: 0.7305	test_loss: 0.7910 test_accuracy-SBM: 0.7259
2025-08-03 11:31:34,575 - INFO - train: {'epoch': 54, 'time_epoch': 86.59109, 'eta': 3921.00534, 'eta_hours': 1.08917, 'loss': 0.71778862, 'lr': 0.00047521, 'params': 495254, 'time_iter': 0.13855, 'accuracy': 0.74664, 'f1': 0.74637, 'accuracy-SBM': 0.74665, 'auc': 0.94782}
2025-08-03 11:31:38,852 - INFO - val: {'epoch': 54, 'time_epoch': 4.2154, 'loss': 0.67244504, 'lr': 0, 'params': 495254, 'time_iter': 0.06691, 'accuracy': 0.76124, 'f1': 0.76101, 'accuracy-SBM': 0.76101, 'auc': 0.95491}
2025-08-03 11:31:43,090 - INFO - test: {'epoch': 54, 'time_epoch': 4.20354, 'loss': 0.68246989, 'lr': 0, 'params': 495254, 'time_iter': 0.06672, 'accuracy': 0.75904, 'f1': 0.75892, 'accuracy-SBM': 0.75904, 'auc': 0.95363}
2025-08-03 11:31:43,092 - INFO - > Epoch 54: took 95.3s (avg 96.0s) | Best so far: epoch 54	train_loss: 0.7178 train_accuracy-SBM: 0.7467	val_loss: 0.6724 val_accuracy-SBM: 0.7610	test_loss: 0.6825 test_accuracy-SBM: 0.7590
2025-08-03 11:33:09,946 - INFO - train: {'epoch': 55, 'time_epoch': 86.60214, 'eta': 3833.45442, 'eta_hours': 1.06485, 'loss': 0.67610621, 'lr': 0.00045871, 'params': 495254, 'time_iter': 0.13856, 'accuracy': 0.75676, 'f1': 0.75673, 'accuracy-SBM': 0.75676, 'auc': 0.95435}
2025-08-03 11:33:14,214 - INFO - val: {'epoch': 55, 'time_epoch': 4.22028, 'loss': 0.65743302, 'lr': 0, 'params': 495254, 'time_iter': 0.06699, 'accuracy': 0.76408, 'f1': 0.76386, 'accuracy-SBM': 0.76385, 'auc': 0.95674}
2025-08-03 11:33:18,449 - INFO - test: {'epoch': 55, 'time_epoch': 4.20166, 'loss': 0.66166165, 'lr': 0, 'params': 495254, 'time_iter': 0.06669, 'accuracy': 0.76078, 'f1': 0.76073, 'accuracy-SBM': 0.76079, 'auc': 0.9565}
2025-08-03 11:33:18,451 - INFO - > Epoch 55: took 95.4s (avg 96.0s) | Best so far: epoch 55	train_loss: 0.6761 train_accuracy-SBM: 0.7568	val_loss: 0.6574 val_accuracy-SBM: 0.7639	test_loss: 0.6617 test_accuracy-SBM: 0.7608
2025-08-03 11:34:45,347 - INFO - train: {'epoch': 56, 'time_epoch': 86.54497, 'eta': 3745.89367, 'eta_hours': 1.04053, 'loss': 0.66062501, 'lr': 0.00044226, 'params': 495254, 'time_iter': 0.13847, 'accuracy': 0.76153, 'f1': 0.76151, 'accuracy-SBM': 0.76153, 'auc': 0.95644}
2025-08-03 11:34:49,625 - INFO - val: {'epoch': 56, 'time_epoch': 4.23136, 'loss': 0.64856108, 'lr': 0, 'params': 495254, 'time_iter': 0.06716, 'accuracy': 0.76833, 'f1': 0.76819, 'accuracy-SBM': 0.76816, 'auc': 0.95794}
2025-08-03 11:34:53,872 - INFO - test: {'epoch': 56, 'time_epoch': 4.21158, 'loss': 0.65099997, 'lr': 0, 'params': 495254, 'time_iter': 0.06685, 'accuracy': 0.76523, 'f1': 0.76515, 'accuracy-SBM': 0.76515, 'auc': 0.95778}
2025-08-03 11:34:53,875 - INFO - > Epoch 56: took 95.4s (avg 96.0s) | Best so far: epoch 56	train_loss: 0.6606 train_accuracy-SBM: 0.7615	val_loss: 0.6486 val_accuracy-SBM: 0.7682	test_loss: 0.6510 test_accuracy-SBM: 0.7651
2025-08-03 11:36:20,561 - INFO - train: {'epoch': 57, 'time_epoch': 86.43851, 'eta': 3658.29086, 'eta_hours': 1.01619, 'loss': 0.65040691, 'lr': 0.00042587, 'params': 495254, 'time_iter': 0.1383, 'accuracy': 0.76508, 'f1': 0.76508, 'accuracy-SBM': 0.76508, 'auc': 0.95775}
2025-08-03 11:36:24,833 - INFO - val: {'epoch': 57, 'time_epoch': 4.22554, 'loss': 0.65040992, 'lr': 0, 'params': 495254, 'time_iter': 0.06707, 'accuracy': 0.76764, 'f1': 0.76721, 'accuracy-SBM': 0.7673, 'auc': 0.95765}
2025-08-03 11:36:29,072 - INFO - test: {'epoch': 57, 'time_epoch': 4.20491, 'loss': 0.65573316, 'lr': 0, 'params': 495254, 'time_iter': 0.06674, 'accuracy': 0.76542, 'f1': 0.76532, 'accuracy-SBM': 0.76531, 'auc': 0.95717}
2025-08-03 11:36:29,074 - INFO - > Epoch 57: took 95.2s (avg 96.0s) | Best so far: epoch 56	train_loss: 0.6606 train_accuracy-SBM: 0.7615	val_loss: 0.6486 val_accuracy-SBM: 0.7682	test_loss: 0.6510 test_accuracy-SBM: 0.7651
2025-08-03 11:37:55,641 - INFO - train: {'epoch': 58, 'time_epoch': 86.32061, 'eta': 3570.64558, 'eta_hours': 0.99185, 'loss': 0.64463837, 'lr': 0.00040956, 'params': 495254, 'time_iter': 0.13811, 'accuracy': 0.76715, 'f1': 0.76715, 'accuracy-SBM': 0.76715, 'auc': 0.95849}
2025-08-03 11:37:59,947 - INFO - val: {'epoch': 58, 'time_epoch': 4.25729, 'loss': 0.65133521, 'lr': 0, 'params': 495254, 'time_iter': 0.06758, 'accuracy': 0.7662, 'f1': 0.76604, 'accuracy-SBM': 0.76594, 'auc': 0.95765}
2025-08-03 11:38:04,229 - INFO - test: {'epoch': 58, 'time_epoch': 4.24835, 'loss': 0.64734134, 'lr': 0, 'params': 495254, 'time_iter': 0.06743, 'accuracy': 0.76683, 'f1': 0.76673, 'accuracy-SBM': 0.76672, 'auc': 0.95827}
2025-08-03 11:38:04,231 - INFO - > Epoch 58: took 95.2s (avg 95.9s) | Best so far: epoch 56	train_loss: 0.6606 train_accuracy-SBM: 0.7615	val_loss: 0.6486 val_accuracy-SBM: 0.7682	test_loss: 0.6510 test_accuracy-SBM: 0.7651
2025-08-03 11:39:31,675 - INFO - train: {'epoch': 59, 'time_epoch': 87.19068, 'eta': 3483.62451, 'eta_hours': 0.96767, 'loss': 0.63915452, 'lr': 0.00039335, 'params': 495254, 'time_iter': 0.13951, 'accuracy': 0.76801, 'f1': 0.768, 'accuracy-SBM': 0.76801, 'auc': 0.95921}
2025-08-03 11:39:36,020 - INFO - val: {'epoch': 59, 'time_epoch': 4.29603, 'loss': 0.63418651, 'lr': 0, 'params': 495254, 'time_iter': 0.06819, 'accuracy': 0.77216, 'f1': 0.77191, 'accuracy-SBM': 0.77194, 'auc': 0.95966}
2025-08-03 11:39:40,333 - INFO - test: {'epoch': 59, 'time_epoch': 4.27853, 'loss': 0.64400947, 'lr': 0, 'params': 495254, 'time_iter': 0.06791, 'accuracy': 0.76906, 'f1': 0.76901, 'accuracy-SBM': 0.76902, 'auc': 0.95859}
2025-08-03 11:39:40,335 - INFO - > Epoch 59: took 96.1s (avg 95.9s) | Best so far: epoch 59	train_loss: 0.6392 train_accuracy-SBM: 0.7680	val_loss: 0.6342 val_accuracy-SBM: 0.7719	test_loss: 0.6440 test_accuracy-SBM: 0.7690
2025-08-03 11:41:07,497 - INFO - train: {'epoch': 60, 'time_epoch': 86.92203, 'eta': 3396.42611, 'eta_hours': 0.94345, 'loss': 0.63369948, 'lr': 0.00037726, 'params': 495254, 'time_iter': 0.13908, 'accuracy': 0.77047, 'f1': 0.77047, 'accuracy-SBM': 0.77047, 'auc': 0.95985}
2025-08-03 11:41:11,764 - INFO - val: {'epoch': 60, 'time_epoch': 4.2164, 'loss': 0.64144995, 'lr': 0, 'params': 495254, 'time_iter': 0.06693, 'accuracy': 0.7707, 'f1': 0.77054, 'accuracy-SBM': 0.77048, 'auc': 0.95879}
2025-08-03 11:41:16,004 - INFO - test: {'epoch': 60, 'time_epoch': 4.20616, 'loss': 0.64052944, 'lr': 0, 'params': 495254, 'time_iter': 0.06676, 'accuracy': 0.76755, 'f1': 0.76754, 'accuracy-SBM': 0.76758, 'auc': 0.9592}
2025-08-03 11:41:16,007 - INFO - > Epoch 60: took 95.7s (avg 95.9s) | Best so far: epoch 59	train_loss: 0.6392 train_accuracy-SBM: 0.7680	val_loss: 0.6342 val_accuracy-SBM: 0.7719	test_loss: 0.6440 test_accuracy-SBM: 0.7690
2025-08-03 11:42:42,940 - INFO - train: {'epoch': 61, 'time_epoch': 86.57849, 'eta': 3309.02608, 'eta_hours': 0.91917, 'loss': 0.62959186, 'lr': 0.0003613, 'params': 495254, 'time_iter': 0.13853, 'accuracy': 0.77153, 'f1': 0.77153, 'accuracy-SBM': 0.77153, 'auc': 0.96038}
2025-08-03 11:42:47,223 - INFO - val: {'epoch': 61, 'time_epoch': 4.23466, 'loss': 0.64082303, 'lr': 0, 'params': 495254, 'time_iter': 0.06722, 'accuracy': 0.77155, 'f1': 0.77129, 'accuracy-SBM': 0.77127, 'auc': 0.9589}
2025-08-03 11:42:51,479 - INFO - test: {'epoch': 61, 'time_epoch': 4.22218, 'loss': 0.63934775, 'lr': 0, 'params': 495254, 'time_iter': 0.06702, 'accuracy': 0.76954, 'f1': 0.76945, 'accuracy-SBM': 0.76942, 'auc': 0.9592}
2025-08-03 11:42:51,481 - INFO - > Epoch 61: took 95.5s (avg 95.9s) | Best so far: epoch 59	train_loss: 0.6392 train_accuracy-SBM: 0.7680	val_loss: 0.6342 val_accuracy-SBM: 0.7719	test_loss: 0.6440 test_accuracy-SBM: 0.7690
2025-08-03 11:44:17,657 - INFO - train: {'epoch': 62, 'time_epoch': 85.93507, 'eta': 3221.27424, 'eta_hours': 0.8948, 'loss': 0.62526402, 'lr': 0.00034549, 'params': 495254, 'time_iter': 0.1375, 'accuracy': 0.77233, 'f1': 0.77233, 'accuracy-SBM': 0.77233, 'auc': 0.96093}
2025-08-03 11:44:21,855 - INFO - val: {'epoch': 62, 'time_epoch': 4.15258, 'loss': 0.6413386, 'lr': 0, 'params': 495254, 'time_iter': 0.06591, 'accuracy': 0.77169, 'f1': 0.77151, 'accuracy-SBM': 0.77153, 'auc': 0.95883}
2025-08-03 11:44:26,040 - INFO - test: {'epoch': 62, 'time_epoch': 4.15218, 'loss': 0.63899195, 'lr': 0, 'params': 495254, 'time_iter': 0.06591, 'accuracy': 0.77091, 'f1': 0.77089, 'accuracy-SBM': 0.77088, 'auc': 0.95928}
2025-08-03 11:44:26,043 - INFO - > Epoch 62: took 94.6s (avg 95.9s) | Best so far: epoch 59	train_loss: 0.6392 train_accuracy-SBM: 0.7680	val_loss: 0.6342 val_accuracy-SBM: 0.7719	test_loss: 0.6440 test_accuracy-SBM: 0.7690
2025-08-03 11:45:52,763 - INFO - train: {'epoch': 63, 'time_epoch': 86.47196, 'eta': 3133.88117, 'eta_hours': 0.87052, 'loss': 0.62331544, 'lr': 0.00032985, 'params': 495254, 'time_iter': 0.13836, 'accuracy': 0.77361, 'f1': 0.77361, 'accuracy-SBM': 0.77361, 'auc': 0.96117}
2025-08-03 11:45:57,043 - INFO - val: {'epoch': 63, 'time_epoch': 4.22469, 'loss': 0.63354618, 'lr': 0, 'params': 495254, 'time_iter': 0.06706, 'accuracy': 0.773, 'f1': 0.77287, 'accuracy-SBM': 0.77288, 'auc': 0.95979}
2025-08-03 11:46:01,300 - INFO - test: {'epoch': 63, 'time_epoch': 4.22169, 'loss': 0.63483373, 'lr': 0, 'params': 495254, 'time_iter': 0.06701, 'accuracy': 0.77182, 'f1': 0.77179, 'accuracy-SBM': 0.77179, 'auc': 0.95975}
2025-08-03 11:46:01,302 - INFO - > Epoch 63: took 95.3s (avg 95.9s) | Best so far: epoch 63	train_loss: 0.6233 train_accuracy-SBM: 0.7736	val_loss: 0.6335 val_accuracy-SBM: 0.7729	test_loss: 0.6348 test_accuracy-SBM: 0.7718
2025-08-03 11:47:27,883 - INFO - train: {'epoch': 64, 'time_epoch': 86.3397, 'eta': 3046.44524, 'eta_hours': 0.84623, 'loss': 0.61914208, 'lr': 0.0003144, 'params': 495254, 'time_iter': 0.13814, 'accuracy': 0.77531, 'f1': 0.77531, 'accuracy-SBM': 0.77531, 'auc': 0.96168}
2025-08-03 11:47:32,154 - INFO - val: {'epoch': 64, 'time_epoch': 4.20391, 'loss': 0.63165266, 'lr': 0, 'params': 495254, 'time_iter': 0.06673, 'accuracy': 0.77425, 'f1': 0.77416, 'accuracy-SBM': 0.7742, 'auc': 0.96015}
2025-08-03 11:47:36,385 - INFO - test: {'epoch': 64, 'time_epoch': 4.19666, 'loss': 0.63619437, 'lr': 0, 'params': 495254, 'time_iter': 0.06661, 'accuracy': 0.77159, 'f1': 0.7716, 'accuracy-SBM': 0.7716, 'auc': 0.95971}
2025-08-03 11:47:36,388 - INFO - > Epoch 64: took 95.1s (avg 95.9s) | Best so far: epoch 64	train_loss: 0.6191 train_accuracy-SBM: 0.7753	val_loss: 0.6317 val_accuracy-SBM: 0.7742	test_loss: 0.6362 test_accuracy-SBM: 0.7716
2025-08-03 11:49:02,926 - INFO - train: {'epoch': 65, 'time_epoch': 86.28961, 'eta': 2959.01671, 'eta_hours': 0.82195, 'loss': 0.61506122, 'lr': 0.00029915, 'params': 495254, 'time_iter': 0.13806, 'accuracy': 0.77677, 'f1': 0.77676, 'accuracy-SBM': 0.77677, 'auc': 0.96217}
2025-08-03 11:49:07,173 - INFO - val: {'epoch': 65, 'time_epoch': 4.19026, 'loss': 0.64585901, 'lr': 0, 'params': 495254, 'time_iter': 0.06651, 'accuracy': 0.77099, 'f1': 0.77082, 'accuracy-SBM': 0.77083, 'auc': 0.95832}
2025-08-03 11:49:11,416 - INFO - test: {'epoch': 65, 'time_epoch': 4.19952, 'loss': 0.63272392, 'lr': 0, 'params': 495254, 'time_iter': 0.06666, 'accuracy': 0.77209, 'f1': 0.77203, 'accuracy-SBM': 0.77196, 'auc': 0.96012}
2025-08-03 11:49:11,418 - INFO - > Epoch 65: took 95.0s (avg 95.9s) | Best so far: epoch 64	train_loss: 0.6191 train_accuracy-SBM: 0.7753	val_loss: 0.6317 val_accuracy-SBM: 0.7742	test_loss: 0.6362 test_accuracy-SBM: 0.7716
2025-08-03 11:50:37,543 - INFO - train: {'epoch': 66, 'time_epoch': 85.87602, 'eta': 2871.41848, 'eta_hours': 0.79762, 'loss': 0.61192135, 'lr': 0.00028412, 'params': 495254, 'time_iter': 0.1374, 'accuracy': 0.77747, 'f1': 0.77747, 'accuracy-SBM': 0.77747, 'auc': 0.96258}
2025-08-03 11:50:41,809 - INFO - val: {'epoch': 66, 'time_epoch': 4.21871, 'loss': 0.63738062, 'lr': 0, 'params': 495254, 'time_iter': 0.06696, 'accuracy': 0.77298, 'f1': 0.77287, 'accuracy-SBM': 0.77286, 'auc': 0.9594}
2025-08-03 11:50:46,058 - INFO - test: {'epoch': 66, 'time_epoch': 4.20508, 'loss': 0.63169645, 'lr': 0, 'params': 495254, 'time_iter': 0.06675, 'accuracy': 0.77335, 'f1': 0.77329, 'accuracy-SBM': 0.77326, 'auc': 0.96019}
2025-08-03 11:50:46,060 - INFO - > Epoch 66: took 94.6s (avg 95.9s) | Best so far: epoch 64	train_loss: 0.6191 train_accuracy-SBM: 0.7753	val_loss: 0.6317 val_accuracy-SBM: 0.7742	test_loss: 0.6362 test_accuracy-SBM: 0.7716
2025-08-03 11:52:12,393 - INFO - train: {'epoch': 67, 'time_epoch': 86.0871, 'eta': 2783.97023, 'eta_hours': 0.77333, 'loss': 0.60940707, 'lr': 0.00026933, 'params': 495254, 'time_iter': 0.13774, 'accuracy': 0.77899, 'f1': 0.77899, 'accuracy-SBM': 0.77899, 'auc': 0.96285}
2025-08-03 11:52:16,626 - INFO - val: {'epoch': 67, 'time_epoch': 4.18593, 'loss': 0.63526414, 'lr': 0, 'params': 495254, 'time_iter': 0.06644, 'accuracy': 0.77382, 'f1': 0.77358, 'accuracy-SBM': 0.77351, 'auc': 0.95968}
2025-08-03 11:52:20,840 - INFO - test: {'epoch': 67, 'time_epoch': 4.16347, 'loss': 0.63304583, 'lr': 0, 'params': 495254, 'time_iter': 0.06609, 'accuracy': 0.77378, 'f1': 0.7737, 'accuracy-SBM': 0.77371, 'auc': 0.96015}
2025-08-03 11:52:20,851 - INFO - > Epoch 67: took 94.8s (avg 95.8s) | Best so far: epoch 64	train_loss: 0.6191 train_accuracy-SBM: 0.7753	val_loss: 0.6317 val_accuracy-SBM: 0.7742	test_loss: 0.6362 test_accuracy-SBM: 0.7716
2025-08-03 11:53:47,200 - INFO - train: {'epoch': 68, 'time_epoch': 86.10064, 'eta': 2696.56751, 'eta_hours': 0.74905, 'loss': 0.60502241, 'lr': 0.00025479, 'params': 495254, 'time_iter': 0.13776, 'accuracy': 0.78054, 'f1': 0.78054, 'accuracy-SBM': 0.78054, 'auc': 0.9634}
2025-08-03 11:53:51,439 - INFO - val: {'epoch': 68, 'time_epoch': 4.1929, 'loss': 0.62808963, 'lr': 0, 'params': 495254, 'time_iter': 0.06655, 'accuracy': 0.77608, 'f1': 0.776, 'accuracy-SBM': 0.77596, 'auc': 0.96056}
2025-08-03 11:53:55,667 - INFO - test: {'epoch': 68, 'time_epoch': 4.19408, 'loss': 0.6291739, 'lr': 0, 'params': 495254, 'time_iter': 0.06657, 'accuracy': 0.77626, 'f1': 0.77624, 'accuracy-SBM': 0.77623, 'auc': 0.96051}
2025-08-03 11:53:55,668 - INFO - > Epoch 68: took 94.8s (avg 95.8s) | Best so far: epoch 68	train_loss: 0.6050 train_accuracy-SBM: 0.7805	val_loss: 0.6281 val_accuracy-SBM: 0.7760	test_loss: 0.6292 test_accuracy-SBM: 0.7762
2025-08-03 11:55:22,884 - INFO - train: {'epoch': 69, 'time_epoch': 86.87447, 'eta': 2609.53365, 'eta_hours': 0.72487, 'loss': 0.60409754, 'lr': 0.00024052, 'params': 495254, 'time_iter': 0.139, 'accuracy': 0.78059, 'f1': 0.78059, 'accuracy-SBM': 0.78059, 'auc': 0.96351}
2025-08-03 11:55:27,105 - INFO - val: {'epoch': 69, 'time_epoch': 4.17385, 'loss': 0.63030688, 'lr': 0, 'params': 495254, 'time_iter': 0.06625, 'accuracy': 0.77595, 'f1': 0.77584, 'accuracy-SBM': 0.77584, 'auc': 0.9604}
2025-08-03 11:55:31,307 - INFO - test: {'epoch': 69, 'time_epoch': 4.16961, 'loss': 0.63374065, 'lr': 0, 'params': 495254, 'time_iter': 0.06618, 'accuracy': 0.77576, 'f1': 0.77572, 'accuracy-SBM': 0.77574, 'auc': 0.95998}
2025-08-03 11:55:31,310 - INFO - > Epoch 69: took 95.6s (avg 95.8s) | Best so far: epoch 68	train_loss: 0.6050 train_accuracy-SBM: 0.7805	val_loss: 0.6281 val_accuracy-SBM: 0.7760	test_loss: 0.6292 test_accuracy-SBM: 0.7762
2025-08-03 11:56:57,433 - INFO - train: {'epoch': 70, 'time_epoch': 85.88203, 'eta': 2522.0989, 'eta_hours': 0.70058, 'loss': 0.60014475, 'lr': 0.00022653, 'params': 495254, 'time_iter': 0.13741, 'accuracy': 0.78222, 'f1': 0.78221, 'accuracy-SBM': 0.78222, 'auc': 0.96399}
2025-08-03 11:57:01,651 - INFO - val: {'epoch': 70, 'time_epoch': 4.17132, 'loss': 0.63236409, 'lr': 0, 'params': 495254, 'time_iter': 0.06621, 'accuracy': 0.77507, 'f1': 0.7749, 'accuracy-SBM': 0.77489, 'auc': 0.95991}
2025-08-03 11:57:05,857 - INFO - test: {'epoch': 70, 'time_epoch': 4.17126, 'loss': 0.62676668, 'lr': 0, 'params': 495254, 'time_iter': 0.06621, 'accuracy': 0.77624, 'f1': 0.7762, 'accuracy-SBM': 0.7762, 'auc': 0.96076}
2025-08-03 11:57:05,859 - INFO - > Epoch 70: took 94.5s (avg 95.8s) | Best so far: epoch 68	train_loss: 0.6050 train_accuracy-SBM: 0.7805	val_loss: 0.6281 val_accuracy-SBM: 0.7760	test_loss: 0.6292 test_accuracy-SBM: 0.7762
2025-08-03 11:58:32,288 - INFO - train: {'epoch': 71, 'time_epoch': 86.07838, 'eta': 2434.78365, 'eta_hours': 0.67633, 'loss': 0.59691779, 'lr': 0.00021284, 'params': 495254, 'time_iter': 0.13773, 'accuracy': 0.78327, 'f1': 0.78326, 'accuracy-SBM': 0.78327, 'auc': 0.96437}
2025-08-03 11:58:36,522 - INFO - val: {'epoch': 71, 'time_epoch': 4.18665, 'loss': 0.62814054, 'lr': 0, 'params': 495254, 'time_iter': 0.06645, 'accuracy': 0.77679, 'f1': 0.77668, 'accuracy-SBM': 0.77663, 'auc': 0.96047}
2025-08-03 11:58:40,704 - INFO - test: {'epoch': 71, 'time_epoch': 4.1476, 'loss': 0.62584877, 'lr': 0, 'params': 495254, 'time_iter': 0.06583, 'accuracy': 0.77665, 'f1': 0.77661, 'accuracy-SBM': 0.77662, 'auc': 0.96086}
2025-08-03 11:58:40,707 - INFO - > Epoch 71: took 94.8s (avg 95.8s) | Best so far: epoch 71	train_loss: 0.5969 train_accuracy-SBM: 0.7833	val_loss: 0.6281 val_accuracy-SBM: 0.7766	test_loss: 0.6258 test_accuracy-SBM: 0.7766
2025-08-03 12:00:07,284 - INFO - train: {'epoch': 72, 'time_epoch': 86.32447, 'eta': 2347.59331, 'eta_hours': 0.65211, 'loss': 0.59531731, 'lr': 0.00019946, 'params': 495254, 'time_iter': 0.13812, 'accuracy': 0.78349, 'f1': 0.78349, 'accuracy-SBM': 0.78349, 'auc': 0.96457}
2025-08-03 12:00:11,661 - INFO - val: {'epoch': 72, 'time_epoch': 4.33075, 'loss': 0.62457216, 'lr': 0, 'params': 495254, 'time_iter': 0.06874, 'accuracy': 0.77915, 'f1': 0.77903, 'accuracy-SBM': 0.77903, 'auc': 0.96092}
2025-08-03 12:00:15,916 - INFO - test: {'epoch': 72, 'time_epoch': 4.2213, 'loss': 0.62320101, 'lr': 0, 'params': 495254, 'time_iter': 0.067, 'accuracy': 0.77718, 'f1': 0.77717, 'accuracy-SBM': 0.77717, 'auc': 0.96124}
2025-08-03 12:00:15,919 - INFO - > Epoch 72: took 95.2s (avg 95.8s) | Best so far: epoch 72	train_loss: 0.5953 train_accuracy-SBM: 0.7835	val_loss: 0.6246 val_accuracy-SBM: 0.7790	test_loss: 0.6232 test_accuracy-SBM: 0.7772
2025-08-03 12:01:42,063 - INFO - train: {'epoch': 73, 'time_epoch': 85.80296, 'eta': 2260.24313, 'eta_hours': 0.62785, 'loss': 0.5940047, 'lr': 0.00018641, 'params': 495254, 'time_iter': 0.13728, 'accuracy': 0.78373, 'f1': 0.78373, 'accuracy-SBM': 0.78373, 'auc': 0.96473}
2025-08-03 12:01:46,351 - INFO - val: {'epoch': 73, 'time_epoch': 4.10685, 'loss': 0.62797036, 'lr': 0, 'params': 495254, 'time_iter': 0.06519, 'accuracy': 0.7762, 'f1': 0.7761, 'accuracy-SBM': 0.77607, 'auc': 0.96067}
2025-08-03 12:01:50,628 - INFO - test: {'epoch': 73, 'time_epoch': 4.13126, 'loss': 0.62702175, 'lr': 0, 'params': 495254, 'time_iter': 0.06558, 'accuracy': 0.7757, 'f1': 0.77569, 'accuracy-SBM': 0.7757, 'auc': 0.9609}
2025-08-03 12:01:50,640 - INFO - > Epoch 73: took 94.7s (avg 95.8s) | Best so far: epoch 72	train_loss: 0.5953 train_accuracy-SBM: 0.7835	val_loss: 0.6246 val_accuracy-SBM: 0.7790	test_loss: 0.6232 test_accuracy-SBM: 0.7772
2025-08-03 12:03:16,317 - INFO - train: {'epoch': 74, 'time_epoch': 85.43756, 'eta': 2172.81241, 'eta_hours': 0.60356, 'loss': 0.59182636, 'lr': 0.00017371, 'params': 495254, 'time_iter': 0.1367, 'accuracy': 0.78477, 'f1': 0.78477, 'accuracy-SBM': 0.78477, 'auc': 0.96498}
2025-08-03 12:03:20,485 - INFO - val: {'epoch': 74, 'time_epoch': 4.12241, 'loss': 0.62901668, 'lr': 0, 'params': 495254, 'time_iter': 0.06544, 'accuracy': 0.77795, 'f1': 0.7778, 'accuracy-SBM': 0.77784, 'auc': 0.96046}
2025-08-03 12:03:24,622 - INFO - test: {'epoch': 74, 'time_epoch': 4.09601, 'loss': 0.62575584, 'lr': 0, 'params': 495254, 'time_iter': 0.06502, 'accuracy': 0.77637, 'f1': 0.77635, 'accuracy-SBM': 0.77634, 'auc': 0.96102}
2025-08-03 12:03:24,625 - INFO - > Epoch 74: took 94.0s (avg 95.7s) | Best so far: epoch 72	train_loss: 0.5953 train_accuracy-SBM: 0.7835	val_loss: 0.6246 val_accuracy-SBM: 0.7790	test_loss: 0.6232 test_accuracy-SBM: 0.7772
2025-08-03 12:04:50,101 - INFO - train: {'epoch': 75, 'time_epoch': 85.13615, 'eta': 2085.33897, 'eta_hours': 0.57926, 'loss': 0.58902875, 'lr': 0.00016136, 'params': 495254, 'time_iter': 0.13622, 'accuracy': 0.78586, 'f1': 0.78586, 'accuracy-SBM': 0.78586, 'auc': 0.96532}
2025-08-03 12:04:54,227 - INFO - val: {'epoch': 75, 'time_epoch': 4.08045, 'loss': 0.63516338, 'lr': 0, 'params': 495254, 'time_iter': 0.06477, 'accuracy': 0.77758, 'f1': 0.77749, 'accuracy-SBM': 0.77747, 'auc': 0.95966}
2025-08-03 12:04:58,330 - INFO - test: {'epoch': 75, 'time_epoch': 4.06932, 'loss': 0.62506721, 'lr': 0, 'params': 495254, 'time_iter': 0.06459, 'accuracy': 0.77636, 'f1': 0.77634, 'accuracy-SBM': 0.77639, 'auc': 0.96104}
2025-08-03 12:04:58,331 - INFO - > Epoch 75: took 93.7s (avg 95.7s) | Best so far: epoch 72	train_loss: 0.5953 train_accuracy-SBM: 0.7835	val_loss: 0.6246 val_accuracy-SBM: 0.7790	test_loss: 0.6232 test_accuracy-SBM: 0.7772
2025-08-03 12:06:23,864 - INFO - train: {'epoch': 76, 'time_epoch': 85.18882, 'eta': 1997.94196, 'eta_hours': 0.55498, 'loss': 0.58697885, 'lr': 0.00014938, 'params': 495254, 'time_iter': 0.1363, 'accuracy': 0.78617, 'f1': 0.78616, 'accuracy-SBM': 0.78617, 'auc': 0.96556}
2025-08-03 12:06:28,027 - INFO - val: {'epoch': 76, 'time_epoch': 4.11053, 'loss': 0.62535658, 'lr': 0, 'params': 495254, 'time_iter': 0.06525, 'accuracy': 0.77767, 'f1': 0.7776, 'accuracy-SBM': 0.77763, 'auc': 0.96082}
2025-08-03 12:06:32,185 - INFO - test: {'epoch': 76, 'time_epoch': 4.11627, 'loss': 0.62568675, 'lr': 0, 'params': 495254, 'time_iter': 0.06534, 'accuracy': 0.7768, 'f1': 0.77677, 'accuracy-SBM': 0.77678, 'auc': 0.96085}
2025-08-03 12:06:32,187 - INFO - > Epoch 76: took 93.9s (avg 95.7s) | Best so far: epoch 72	train_loss: 0.5953 train_accuracy-SBM: 0.7835	val_loss: 0.6246 val_accuracy-SBM: 0.7790	test_loss: 0.6232 test_accuracy-SBM: 0.7772
2025-08-03 12:07:56,749 - INFO - train: {'epoch': 77, 'time_epoch': 84.3165, 'eta': 1910.35554, 'eta_hours': 0.53065, 'loss': 0.58464794, 'lr': 0.00013779, 'params': 495254, 'time_iter': 0.13491, 'accuracy': 0.78778, 'f1': 0.78778, 'accuracy-SBM': 0.78778, 'auc': 0.96583}
2025-08-03 12:08:00,822 - INFO - val: {'epoch': 77, 'time_epoch': 4.02571, 'loss': 0.63080608, 'lr': 0, 'params': 495254, 'time_iter': 0.0639, 'accuracy': 0.77733, 'f1': 0.77719, 'accuracy-SBM': 0.77719, 'auc': 0.96023}
2025-08-03 12:08:04,878 - INFO - test: {'epoch': 77, 'time_epoch': 4.02381, 'loss': 0.62681033, 'lr': 0, 'params': 495254, 'time_iter': 0.06387, 'accuracy': 0.77648, 'f1': 0.77645, 'accuracy-SBM': 0.77645, 'auc': 0.9608}
2025-08-03 12:08:04,880 - INFO - > Epoch 77: took 92.7s (avg 95.7s) | Best so far: epoch 72	train_loss: 0.5953 train_accuracy-SBM: 0.7835	val_loss: 0.6246 val_accuracy-SBM: 0.7790	test_loss: 0.6232 test_accuracy-SBM: 0.7772
2025-08-03 12:09:29,793 - INFO - train: {'epoch': 78, 'time_epoch': 84.46468, 'eta': 1822.89128, 'eta_hours': 0.50636, 'loss': 0.5838269, 'lr': 0.00012659, 'params': 495254, 'time_iter': 0.13514, 'accuracy': 0.78753, 'f1': 0.78753, 'accuracy-SBM': 0.78754, 'auc': 0.96592}
2025-08-03 12:09:34,039 - INFO - val: {'epoch': 78, 'time_epoch': 4.19912, 'loss': 0.6287972, 'lr': 0, 'params': 495254, 'time_iter': 0.06665, 'accuracy': 0.77801, 'f1': 0.77788, 'accuracy-SBM': 0.77787, 'auc': 0.96042}
2025-08-03 12:09:38,319 - INFO - test: {'epoch': 78, 'time_epoch': 4.24467, 'loss': 0.62331547, 'lr': 0, 'params': 495254, 'time_iter': 0.06738, 'accuracy': 0.7771, 'f1': 0.77704, 'accuracy-SBM': 0.77703, 'auc': 0.96118}
2025-08-03 12:09:38,321 - INFO - > Epoch 78: took 93.4s (avg 95.6s) | Best so far: epoch 72	train_loss: 0.5953 train_accuracy-SBM: 0.7835	val_loss: 0.6246 val_accuracy-SBM: 0.7790	test_loss: 0.6232 test_accuracy-SBM: 0.7772
2025-08-03 12:11:04,508 - INFO - train: {'epoch': 79, 'time_epoch': 85.84509, 'eta': 1735.84712, 'eta_hours': 0.48218, 'loss': 0.58037513, 'lr': 0.0001158, 'params': 495254, 'time_iter': 0.13735, 'accuracy': 0.78918, 'f1': 0.78917, 'accuracy-SBM': 0.78918, 'auc': 0.96632}
2025-08-03 12:11:08,648 - INFO - val: {'epoch': 79, 'time_epoch': 4.07745, 'loss': 0.63027977, 'lr': 0, 'params': 495254, 'time_iter': 0.06472, 'accuracy': 0.77786, 'f1': 0.77771, 'accuracy-SBM': 0.77769, 'auc': 0.96041}
2025-08-03 12:11:12,748 - INFO - test: {'epoch': 79, 'time_epoch': 4.06648, 'loss': 0.62625712, 'lr': 0, 'params': 495254, 'time_iter': 0.06455, 'accuracy': 0.77784, 'f1': 0.77776, 'accuracy-SBM': 0.77775, 'auc': 0.96096}
2025-08-03 12:11:12,751 - INFO - > Epoch 79: took 94.4s (avg 95.6s) | Best so far: epoch 72	train_loss: 0.5953 train_accuracy-SBM: 0.7835	val_loss: 0.6246 val_accuracy-SBM: 0.7790	test_loss: 0.6232 test_accuracy-SBM: 0.7772
2025-08-03 12:12:37,572 - INFO - train: {'epoch': 80, 'time_epoch': 84.58042, 'eta': 1648.53592, 'eta_hours': 0.45793, 'loss': 0.58093995, 'lr': 0.00010543, 'params': 495254, 'time_iter': 0.13533, 'accuracy': 0.78812, 'f1': 0.78812, 'accuracy-SBM': 0.78812, 'auc': 0.96627}
2025-08-03 12:12:41,779 - INFO - val: {'epoch': 80, 'time_epoch': 4.11872, 'loss': 0.62449208, 'lr': 0, 'params': 495254, 'time_iter': 0.06538, 'accuracy': 0.77982, 'f1': 0.77971, 'accuracy-SBM': 0.77972, 'auc': 0.96104}
2025-08-03 12:12:45,931 - INFO - test: {'epoch': 80, 'time_epoch': 4.11719, 'loss': 0.62827549, 'lr': 0, 'params': 495254, 'time_iter': 0.06535, 'accuracy': 0.77666, 'f1': 0.77665, 'accuracy-SBM': 0.77665, 'auc': 0.96069}
2025-08-03 12:12:45,934 - INFO - > Epoch 80: took 93.2s (avg 95.6s) | Best so far: epoch 80	train_loss: 0.5809 train_accuracy-SBM: 0.7881	val_loss: 0.6245 val_accuracy-SBM: 0.7797	test_loss: 0.6283 test_accuracy-SBM: 0.7766
2025-08-03 12:14:10,344 - INFO - train: {'epoch': 81, 'time_epoch': 84.17204, 'eta': 1561.20167, 'eta_hours': 0.43367, 'loss': 0.57640348, 'lr': 9.549e-05, 'params': 495254, 'time_iter': 0.13468, 'accuracy': 0.79015, 'f1': 0.79015, 'accuracy-SBM': 0.79016, 'auc': 0.96679}
2025-08-03 12:14:14,411 - INFO - val: {'epoch': 81, 'time_epoch': 4.0188, 'loss': 0.62735555, 'lr': 0, 'params': 495254, 'time_iter': 0.06379, 'accuracy': 0.78038, 'f1': 0.78026, 'accuracy-SBM': 0.78025, 'auc': 0.96076}
2025-08-03 12:14:18,395 - INFO - test: {'epoch': 81, 'time_epoch': 3.95294, 'loss': 0.6307473, 'lr': 0, 'params': 495254, 'time_iter': 0.06275, 'accuracy': 0.77679, 'f1': 0.77678, 'accuracy-SBM': 0.77677, 'auc': 0.96051}
2025-08-03 12:14:18,398 - INFO - > Epoch 81: took 92.5s (avg 95.5s) | Best so far: epoch 81	train_loss: 0.5764 train_accuracy-SBM: 0.7902	val_loss: 0.6274 val_accuracy-SBM: 0.7802	test_loss: 0.6307 test_accuracy-SBM: 0.7768
2025-08-03 12:15:44,147 - INFO - train: {'epoch': 82, 'time_epoch': 85.506, 'eta': 1474.21684, 'eta_hours': 0.4095, 'loss': 0.57590067, 'lr': 8.6e-05, 'params': 495254, 'time_iter': 0.13681, 'accuracy': 0.79084, 'f1': 0.79084, 'accuracy-SBM': 0.79084, 'auc': 0.96683}
2025-08-03 12:15:48,370 - INFO - val: {'epoch': 82, 'time_epoch': 4.17698, 'loss': 0.62867073, 'lr': 0, 'params': 495254, 'time_iter': 0.0663, 'accuracy': 0.77864, 'f1': 0.77852, 'accuracy-SBM': 0.77851, 'auc': 0.96061}
2025-08-03 12:15:52,563 - INFO - test: {'epoch': 82, 'time_epoch': 4.15935, 'loss': 0.62929183, 'lr': 0, 'params': 495254, 'time_iter': 0.06602, 'accuracy': 0.77663, 'f1': 0.77662, 'accuracy-SBM': 0.77658, 'auc': 0.96065}
2025-08-03 12:15:52,566 - INFO - > Epoch 82: took 94.2s (avg 95.5s) | Best so far: epoch 81	train_loss: 0.5764 train_accuracy-SBM: 0.7902	val_loss: 0.6274 val_accuracy-SBM: 0.7802	test_loss: 0.6307 test_accuracy-SBM: 0.7768
2025-08-03 12:17:18,729 - INFO - train: {'epoch': 83, 'time_epoch': 85.92186, 'eta': 1387.34644, 'eta_hours': 0.38537, 'loss': 0.57462256, 'lr': 7.695e-05, 'params': 495254, 'time_iter': 0.13747, 'accuracy': 0.79122, 'f1': 0.79122, 'accuracy-SBM': 0.79122, 'auc': 0.96699}
2025-08-03 12:17:22,968 - INFO - val: {'epoch': 83, 'time_epoch': 4.18401, 'loss': 0.62669459, 'lr': 0, 'params': 495254, 'time_iter': 0.06641, 'accuracy': 0.77926, 'f1': 0.77912, 'accuracy-SBM': 0.77912, 'auc': 0.9607}
2025-08-03 12:17:27,144 - INFO - test: {'epoch': 83, 'time_epoch': 4.14248, 'loss': 0.62610558, 'lr': 0, 'params': 495254, 'time_iter': 0.06575, 'accuracy': 0.77802, 'f1': 0.77799, 'accuracy-SBM': 0.77799, 'auc': 0.96092}
2025-08-03 12:17:27,146 - INFO - > Epoch 83: took 94.6s (avg 95.5s) | Best so far: epoch 81	train_loss: 0.5764 train_accuracy-SBM: 0.7902	val_loss: 0.6274 val_accuracy-SBM: 0.7802	test_loss: 0.6307 test_accuracy-SBM: 0.7768
2025-08-03 12:18:53,191 - INFO - train: {'epoch': 84, 'time_epoch': 85.59778, 'eta': 1300.44116, 'eta_hours': 0.36123, 'loss': 0.57468505, 'lr': 6.837e-05, 'params': 495254, 'time_iter': 0.13696, 'accuracy': 0.79072, 'f1': 0.79072, 'accuracy-SBM': 0.79072, 'auc': 0.967}
2025-08-03 12:18:57,386 - INFO - val: {'epoch': 84, 'time_epoch': 4.1483, 'loss': 0.62500726, 'lr': 0, 'params': 495254, 'time_iter': 0.06585, 'accuracy': 0.77959, 'f1': 0.77947, 'accuracy-SBM': 0.77947, 'auc': 0.96097}
2025-08-03 12:19:01,574 - INFO - test: {'epoch': 84, 'time_epoch': 4.15324, 'loss': 0.62754203, 'lr': 0, 'params': 495254, 'time_iter': 0.06592, 'accuracy': 0.77767, 'f1': 0.77764, 'accuracy-SBM': 0.77763, 'auc': 0.96077}
2025-08-03 12:19:01,576 - INFO - > Epoch 84: took 94.4s (avg 95.5s) | Best so far: epoch 81	train_loss: 0.5764 train_accuracy-SBM: 0.7902	val_loss: 0.6274 val_accuracy-SBM: 0.7802	test_loss: 0.6307 test_accuracy-SBM: 0.7768
2025-08-03 12:20:27,928 - INFO - train: {'epoch': 85, 'time_epoch': 86.10256, 'eta': 1213.64847, 'eta_hours': 0.33712, 'loss': 0.57323145, 'lr': 6.026e-05, 'params': 495254, 'time_iter': 0.13776, 'accuracy': 0.79167, 'f1': 0.79167, 'accuracy-SBM': 0.79167, 'auc': 0.96715}
2025-08-03 12:20:32,177 - INFO - val: {'epoch': 85, 'time_epoch': 4.20226, 'loss': 0.62477864, 'lr': 0, 'params': 495254, 'time_iter': 0.0667, 'accuracy': 0.77957, 'f1': 0.77946, 'accuracy-SBM': 0.77945, 'auc': 0.96094}
2025-08-03 12:20:36,394 - INFO - test: {'epoch': 85, 'time_epoch': 4.18173, 'loss': 0.62782025, 'lr': 0, 'params': 495254, 'time_iter': 0.06638, 'accuracy': 0.77788, 'f1': 0.77786, 'accuracy-SBM': 0.77785, 'auc': 0.96074}
2025-08-03 12:20:36,397 - INFO - > Epoch 85: took 94.8s (avg 95.5s) | Best so far: epoch 81	train_loss: 0.5764 train_accuracy-SBM: 0.7902	val_loss: 0.6274 val_accuracy-SBM: 0.7802	test_loss: 0.6307 test_accuracy-SBM: 0.7768
2025-08-03 12:22:02,804 - INFO - train: {'epoch': 86, 'time_epoch': 86.1506, 'eta': 1126.87881, 'eta_hours': 0.31302, 'loss': 0.57119833, 'lr': 5.264e-05, 'params': 495254, 'time_iter': 0.13784, 'accuracy': 0.7927, 'f1': 0.7927, 'accuracy-SBM': 0.7927, 'auc': 0.96739}
2025-08-03 12:22:07,005 - INFO - val: {'epoch': 86, 'time_epoch': 4.14012, 'loss': 0.62544562, 'lr': 0, 'params': 495254, 'time_iter': 0.06572, 'accuracy': 0.78046, 'f1': 0.78031, 'accuracy-SBM': 0.78031, 'auc': 0.96093}
2025-08-03 12:22:11,188 - INFO - test: {'epoch': 86, 'time_epoch': 4.14934, 'loss': 0.627693, 'lr': 0, 'params': 495254, 'time_iter': 0.06586, 'accuracy': 0.77797, 'f1': 0.77794, 'accuracy-SBM': 0.77793, 'auc': 0.96082}
2025-08-03 12:22:11,190 - INFO - > Epoch 86: took 94.8s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:23:37,808 - INFO - train: {'epoch': 87, 'time_epoch': 86.37403, 'eta': 1040.1537, 'eta_hours': 0.28893, 'loss': 0.57053559, 'lr': 4.55e-05, 'params': 495254, 'time_iter': 0.1382, 'accuracy': 0.79244, 'f1': 0.79244, 'accuracy-SBM': 0.79244, 'auc': 0.96747}
2025-08-03 12:23:42,073 - INFO - val: {'epoch': 87, 'time_epoch': 4.21829, 'loss': 0.62440501, 'lr': 0, 'params': 495254, 'time_iter': 0.06696, 'accuracy': 0.78023, 'f1': 0.78008, 'accuracy-SBM': 0.78007, 'auc': 0.96109}
2025-08-03 12:23:46,235 - INFO - test: {'epoch': 87, 'time_epoch': 4.12731, 'loss': 0.6278515, 'lr': 0, 'params': 495254, 'time_iter': 0.06551, 'accuracy': 0.77776, 'f1': 0.77773, 'accuracy-SBM': 0.77771, 'auc': 0.9608}
2025-08-03 12:23:46,237 - INFO - > Epoch 87: took 95.0s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:25:13,471 - INFO - train: {'epoch': 88, 'time_epoch': 86.99022, 'eta': 953.51263, 'eta_hours': 0.26486, 'loss': 0.56842515, 'lr': 3.886e-05, 'params': 495254, 'time_iter': 0.13918, 'accuracy': 0.79337, 'f1': 0.79337, 'accuracy-SBM': 0.79337, 'auc': 0.9677}
2025-08-03 12:25:17,767 - INFO - val: {'epoch': 88, 'time_epoch': 4.23323, 'loss': 0.62948297, 'lr': 0, 'params': 495254, 'time_iter': 0.06719, 'accuracy': 0.77954, 'f1': 0.77938, 'accuracy-SBM': 0.77937, 'auc': 0.96056}
2025-08-03 12:25:22,021 - INFO - test: {'epoch': 88, 'time_epoch': 4.21038, 'loss': 0.63016504, 'lr': 0, 'params': 495254, 'time_iter': 0.06683, 'accuracy': 0.77678, 'f1': 0.77675, 'accuracy-SBM': 0.77675, 'auc': 0.96063}
2025-08-03 12:25:22,023 - INFO - > Epoch 88: took 95.8s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:26:48,869 - INFO - train: {'epoch': 89, 'time_epoch': 86.5987, 'eta': 866.8203, 'eta_hours': 0.24078, 'loss': 0.57067701, 'lr': 3.272e-05, 'params': 495254, 'time_iter': 0.13856, 'accuracy': 0.79263, 'f1': 0.79263, 'accuracy-SBM': 0.79263, 'auc': 0.96744}
2025-08-03 12:26:53,148 - INFO - val: {'epoch': 89, 'time_epoch': 4.23112, 'loss': 0.62644234, 'lr': 0, 'params': 495254, 'time_iter': 0.06716, 'accuracy': 0.77941, 'f1': 0.77927, 'accuracy-SBM': 0.77925, 'auc': 0.96085}
2025-08-03 12:26:57,408 - INFO - test: {'epoch': 89, 'time_epoch': 4.21376, 'loss': 0.62748115, 'lr': 0, 'params': 495254, 'time_iter': 0.06689, 'accuracy': 0.7784, 'f1': 0.77838, 'accuracy-SBM': 0.77836, 'auc': 0.96084}
2025-08-03 12:26:57,441 - INFO - > Epoch 89: took 95.4s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:28:24,587 - INFO - train: {'epoch': 90, 'time_epoch': 86.89106, 'eta': 780.15894, 'eta_hours': 0.21671, 'loss': 0.56987609, 'lr': 2.709e-05, 'params': 495254, 'time_iter': 0.13903, 'accuracy': 0.7924, 'f1': 0.79239, 'accuracy-SBM': 0.7924, 'auc': 0.96754}
2025-08-03 12:28:28,883 - INFO - val: {'epoch': 90, 'time_epoch': 4.24857, 'loss': 0.6272446, 'lr': 0, 'params': 495254, 'time_iter': 0.06744, 'accuracy': 0.78009, 'f1': 0.77998, 'accuracy-SBM': 0.77997, 'auc': 0.96084}
2025-08-03 12:28:33,171 - INFO - test: {'epoch': 90, 'time_epoch': 4.25381, 'loss': 0.62649342, 'lr': 0, 'params': 495254, 'time_iter': 0.06752, 'accuracy': 0.77845, 'f1': 0.77844, 'accuracy-SBM': 0.77844, 'auc': 0.96103}
2025-08-03 12:28:33,173 - INFO - > Epoch 90: took 95.7s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:30:00,541 - INFO - train: {'epoch': 91, 'time_epoch': 87.12037, 'eta': 693.51253, 'eta_hours': 0.19264, 'loss': 0.56733342, 'lr': 2.198e-05, 'params': 495254, 'time_iter': 0.13939, 'accuracy': 0.79376, 'f1': 0.79376, 'accuracy-SBM': 0.79376, 'auc': 0.96783}
2025-08-03 12:30:04,910 - INFO - val: {'epoch': 91, 'time_epoch': 4.30698, 'loss': 0.62918154, 'lr': 0, 'params': 495254, 'time_iter': 0.06836, 'accuracy': 0.7795, 'f1': 0.77938, 'accuracy-SBM': 0.77937, 'auc': 0.96058}
2025-08-03 12:30:09,252 - INFO - test: {'epoch': 91, 'time_epoch': 4.3061, 'loss': 0.6290345, 'lr': 0, 'params': 495254, 'time_iter': 0.06835, 'accuracy': 0.77797, 'f1': 0.77796, 'accuracy-SBM': 0.77797, 'auc': 0.96073}
2025-08-03 12:30:09,254 - INFO - > Epoch 91: took 96.1s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:31:36,315 - INFO - train: {'epoch': 92, 'time_epoch': 86.8161, 'eta': 606.83303, 'eta_hours': 0.16856, 'loss': 0.56743645, 'lr': 1.74e-05, 'params': 495254, 'time_iter': 0.13891, 'accuracy': 0.79359, 'f1': 0.79359, 'accuracy-SBM': 0.79359, 'auc': 0.9678}
2025-08-03 12:31:40,569 - INFO - val: {'epoch': 92, 'time_epoch': 4.20901, 'loss': 0.62793071, 'lr': 0, 'params': 495254, 'time_iter': 0.06681, 'accuracy': 0.78012, 'f1': 0.78, 'accuracy-SBM': 0.78, 'auc': 0.96067}
2025-08-03 12:31:44,790 - INFO - test: {'epoch': 92, 'time_epoch': 4.18725, 'loss': 0.6271011, 'lr': 0, 'params': 495254, 'time_iter': 0.06646, 'accuracy': 0.77822, 'f1': 0.7782, 'accuracy-SBM': 0.77819, 'auc': 0.96088}
2025-08-03 12:31:44,792 - INFO - > Epoch 92: took 95.5s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:33:11,871 - INFO - train: {'epoch': 93, 'time_epoch': 86.83172, 'eta': 520.15161, 'eta_hours': 0.14449, 'loss': 0.56676621, 'lr': 1.334e-05, 'params': 495254, 'time_iter': 0.13893, 'accuracy': 0.79402, 'f1': 0.79402, 'accuracy-SBM': 0.79402, 'auc': 0.96789}
2025-08-03 12:33:16,167 - INFO - val: {'epoch': 93, 'time_epoch': 4.24095, 'loss': 0.62851394, 'lr': 0, 'params': 495254, 'time_iter': 0.06732, 'accuracy': 0.77984, 'f1': 0.77972, 'accuracy-SBM': 0.77972, 'auc': 0.96071}
2025-08-03 12:33:20,414 - INFO - test: {'epoch': 93, 'time_epoch': 4.213, 'loss': 0.62850414, 'lr': 0, 'params': 495254, 'time_iter': 0.06687, 'accuracy': 0.77794, 'f1': 0.77792, 'accuracy-SBM': 0.77791, 'auc': 0.96082}
2025-08-03 12:33:20,416 - INFO - > Epoch 93: took 95.6s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:34:47,546 - INFO - train: {'epoch': 94, 'time_epoch': 86.88065, 'eta': 433.46961, 'eta_hours': 0.12041, 'loss': 0.56645632, 'lr': 9.81e-06, 'params': 495254, 'time_iter': 0.13901, 'accuracy': 0.79388, 'f1': 0.79388, 'accuracy-SBM': 0.79388, 'auc': 0.96792}
2025-08-03 12:34:51,829 - INFO - val: {'epoch': 94, 'time_epoch': 4.23633, 'loss': 0.63099127, 'lr': 0, 'params': 495254, 'time_iter': 0.06724, 'accuracy': 0.77874, 'f1': 0.77862, 'accuracy-SBM': 0.77862, 'auc': 0.96041}
2025-08-03 12:34:56,097 - INFO - test: {'epoch': 94, 'time_epoch': 4.23348, 'loss': 0.6286188, 'lr': 0, 'params': 495254, 'time_iter': 0.0672, 'accuracy': 0.77774, 'f1': 0.77772, 'accuracy-SBM': 0.77771, 'auc': 0.96083}
2025-08-03 12:34:56,099 - INFO - > Epoch 94: took 95.7s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:36:23,363 - INFO - train: {'epoch': 95, 'time_epoch': 87.00354, 'eta': 346.78859, 'eta_hours': 0.09633, 'loss': 0.56655632, 'lr': 6.82e-06, 'params': 495254, 'time_iter': 0.13921, 'accuracy': 0.79358, 'f1': 0.79358, 'accuracy-SBM': 0.79358, 'auc': 0.96792}
2025-08-03 12:36:27,671 - INFO - val: {'epoch': 95, 'time_epoch': 4.26102, 'loss': 0.62942478, 'lr': 0, 'params': 495254, 'time_iter': 0.06764, 'accuracy': 0.78001, 'f1': 0.77986, 'accuracy-SBM': 0.77985, 'auc': 0.96055}
2025-08-03 12:36:31,962 - INFO - test: {'epoch': 95, 'time_epoch': 4.25727, 'loss': 0.62731334, 'lr': 0, 'params': 495254, 'time_iter': 0.06758, 'accuracy': 0.77808, 'f1': 0.77805, 'accuracy-SBM': 0.77804, 'auc': 0.96092}
2025-08-03 12:36:31,965 - INFO - > Epoch 95: took 95.9s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:37:59,118 - INFO - train: {'epoch': 96, 'time_epoch': 86.90924, 'eta': 260.098, 'eta_hours': 0.07225, 'loss': 0.56676875, 'lr': 4.37e-06, 'params': 495254, 'time_iter': 0.13905, 'accuracy': 0.79371, 'f1': 0.79371, 'accuracy-SBM': 0.79371, 'auc': 0.9679}
2025-08-03 12:38:03,426 - INFO - val: {'epoch': 96, 'time_epoch': 4.25314, 'loss': 0.62832139, 'lr': 0, 'params': 495254, 'time_iter': 0.06751, 'accuracy': 0.77992, 'f1': 0.77981, 'accuracy-SBM': 0.77981, 'auc': 0.96063}
2025-08-03 12:38:07,715 - INFO - test: {'epoch': 96, 'time_epoch': 4.25474, 'loss': 0.6249873, 'lr': 0, 'params': 495254, 'time_iter': 0.06754, 'accuracy': 0.77908, 'f1': 0.77906, 'accuracy-SBM': 0.77905, 'auc': 0.96114}
2025-08-03 12:38:07,717 - INFO - > Epoch 96: took 95.8s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:39:35,164 - INFO - train: {'epoch': 97, 'time_epoch': 87.20629, 'eta': 173.40901, 'eta_hours': 0.04817, 'loss': 0.56581504, 'lr': 2.46e-06, 'params': 495254, 'time_iter': 0.13953, 'accuracy': 0.79424, 'f1': 0.79424, 'accuracy-SBM': 0.79424, 'auc': 0.968}
2025-08-03 12:39:39,391 - INFO - val: {'epoch': 97, 'time_epoch': 4.16773, 'loss': 0.62818977, 'lr': 0, 'params': 495254, 'time_iter': 0.06615, 'accuracy': 0.77974, 'f1': 0.77964, 'accuracy-SBM': 0.77962, 'auc': 0.96069}
2025-08-03 12:39:43,572 - INFO - test: {'epoch': 97, 'time_epoch': 4.14728, 'loss': 0.62564189, 'lr': 0, 'params': 495254, 'time_iter': 0.06583, 'accuracy': 0.77883, 'f1': 0.77881, 'accuracy-SBM': 0.77881, 'auc': 0.96111}
2025-08-03 12:39:43,574 - INFO - > Epoch 97: took 95.9s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:41:10,482 - INFO - train: {'epoch': 98, 'time_epoch': 86.66125, 'eta': 86.70407, 'eta_hours': 0.02408, 'loss': 0.56437419, 'lr': 1.09e-06, 'params': 495254, 'time_iter': 0.13866, 'accuracy': 0.79474, 'f1': 0.79474, 'accuracy-SBM': 0.79474, 'auc': 0.96816}
2025-08-03 12:41:14,746 - INFO - val: {'epoch': 98, 'time_epoch': 4.21494, 'loss': 0.62902521, 'lr': 0, 'params': 495254, 'time_iter': 0.0669, 'accuracy': 0.77985, 'f1': 0.77974, 'accuracy-SBM': 0.77973, 'auc': 0.96055}
2025-08-03 12:41:18,975 - INFO - test: {'epoch': 98, 'time_epoch': 4.1952, 'loss': 0.62794341, 'lr': 0, 'params': 495254, 'time_iter': 0.06659, 'accuracy': 0.77796, 'f1': 0.77793, 'accuracy-SBM': 0.77793, 'auc': 0.96082}
2025-08-03 12:41:18,978 - INFO - > Epoch 98: took 95.4s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:42:45,843 - INFO - train: {'epoch': 99, 'time_epoch': 86.62102, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.56657395, 'lr': 2.7e-07, 'params': 495254, 'time_iter': 0.13859, 'accuracy': 0.79407, 'f1': 0.79407, 'accuracy-SBM': 0.79407, 'auc': 0.96791}
2025-08-03 12:42:50,126 - INFO - val: {'epoch': 99, 'time_epoch': 4.2342, 'loss': 0.63194459, 'lr': 0, 'params': 495254, 'time_iter': 0.06721, 'accuracy': 0.77883, 'f1': 0.7787, 'accuracy-SBM': 0.7787, 'auc': 0.96031}
2025-08-03 12:42:54,373 - INFO - test: {'epoch': 99, 'time_epoch': 4.21278, 'loss': 0.62908185, 'lr': 0, 'params': 495254, 'time_iter': 0.06687, 'accuracy': 0.77789, 'f1': 0.77786, 'accuracy-SBM': 0.77785, 'auc': 0.96079}
2025-08-03 12:42:54,569 - INFO - > Epoch 99: took 95.4s (avg 95.5s) | Best so far: epoch 86	train_loss: 0.5712 train_accuracy-SBM: 0.7927	val_loss: 0.6254 val_accuracy-SBM: 0.7803	test_loss: 0.6277 test_accuracy-SBM: 0.7779
2025-08-03 12:42:54,570 - INFO - Avg time per epoch: 95.51s
2025-08-03 12:42:54,570 - INFO - Total train loop time: 2.65h
2025-08-03 12:42:55,548 - INFO - ============================================================
2025-08-03 12:42:55,548 - INFO - Starting PK-Explainer Analysis
2025-08-03 12:42:55,548 - INFO - ============================================================
2025-08-03 12:42:55,623 - INFO - Saved model state to results/Cluster/Cluster-GINE-47/model_for_ablation.pt
2025-08-03 12:42:55,623 - INFO - 
All experiments completed!
